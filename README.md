# Eval_Summarization

[A pyton package] (https://github.com/Yale-LILY/SummEval)

1. Kryściński, Wojciech, et al. “[Neural text summarization: A critical evaluation.](https://aclanthology.org/D19-1051.pdf)” .
2. Zhang, Tianyi, et al. “[Benchmarking large language models for news summarization.](https://arxiv.org/pdf/2301.13848.pdf)”. [code](https://github.com/Tiiiger/benchmark_llm_summarization)
3. Kryściński, Wojciech, et al. “[Evaluating the factual consistency of abstractive text summarization.](https://aclanthology.org/2020.emnlp-main.750.pdf)”  [code](https://github.com/salesforce/factCC)
4. Pagnoni, Artidoro, Vidhisha Balachandran, and Yulia Tsvetkov. “[Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.](https://aclanthology.org/2021.naacl-main.383.pdf)”. [code](https://github.com/artidoro/frank)
5. Maynez, Joshua, et al. “[On faithfulness and factuality in abstractive summarization.](https://aclanthology.org/2020.acl-main.173.pdf)” . 
6. Lin, Chin-Yew. “[Rouge: A package for automatic evaluation of summaries.](https://aclanthology.org/W04-1013.pdf)”.
7. Banerjee, Satanjeev, and Alon Lavie. “[METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.](https://aclanthology.org/W05-0909.pdf)” 
8. Zhang, Tianyi, et al. “[Bertscore: Evaluating text generation with bert.](https://arxiv.org/abs/1904.09675)” [code](https://github.com/Tiiiger/bert_score)
9. Zhao, Wei, et al. “[MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.](https://aclanthology.org/D19-1053/)”  [code](https://github.com/AIPHES/emnlp19-moverscore)
10. Fabbri, Alexander R., et al. “[Summeval: Re-evaluating summarization evaluation.](https://arxiv.org/abs/2007.12626)” 
11. He, Tingting, et al. “[ROUGE-C: A fully automated evaluation method for multi-document summarization.](https://ieeexplore.ieee.org/document/4664680)” 
12. Liu, Yang, et al. “[Gpteval: NLG evaluation using gpt-4 with better human alignment.](https://arxiv.org/abs/2303.16634)” 
13. Laban, Philippe, et al. “[SummaC: Re-visiting NLI-based models for inconsistency detection in summarization.](https://arxiv.org/abs/2111.09525)” 
14. Gekhman, Zorik, et al. “[Trueteacher: Learning factual consistency evaluation with large language models.](https://arxiv.org/abs/2305.11171)” 
15. Scialom, Thomas, et al. “[Answers unite! unsupervised metrics for reinforced summarization models.](https://arxiv.org/abs/1909.01610)” 
16. Durmus, Esin, He He, and Mona Diab. “[FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization.](https://aclanthology.org/2020.acl-main.454/)” 
17. Scialom, Thomas, et al. “[Questeval: Summarization asks for fact-based evaluation.](https://aclanthology.org/2021.emnlp-main.529/)” 
18. Fabbri, Alexander R., et al. “[QAFactEval: Improved QA-based factual consistency evaluation for summarization.](https://aclanthology.org/2022.naacl-main.187/)” 
19. Böhm, Florian, et al. “[Better rewards yield better summaries: Learning to summarise without references.](https://arxiv.org/abs/1909.01214)” 
20. Stiennon, Nisan, et al. “[Learning to summarize with human feedback.](https://arxiv.org/abs/2009.01325)” 
21. Wu, Jeff, et al. “[Recursively summarizing books with human feedback.](https://arxiv.org/abs/2109.10862)” 
22. Manakul, Potsawee, Adian Liusie, and Mark JF Gales. “[Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.](https://arxiv.org/abs/2303.08896)” 
