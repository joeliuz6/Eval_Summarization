# Eval_Summarization

1. Kryściński, Wojciech, et al. “Neural text summarization: A critical evaluation.” arXiv preprint arXiv:1908.08960 (2019).
2. Zhang, Tianyi, et al. “Benchmarking large language models for news summarization.” arXiv preprint arXiv:2301.13848 (2023).
3. Kryściński, Wojciech, et al. “Evaluating the factual consistency of abstractive text summarization.” arXiv preprint arXiv:1910.12840 (2019).
4. Pagnoni, Artidoro, Vidhisha Balachandran, and Yulia Tsvetkov. “Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.” arXiv preprint arXiv:2104.13346 (2021).
5. Maynez, Joshua, et al. “On faithfulness and factuality in abstractive summarization.” arXiv preprint arXiv:2005.00661 (2020).
6. Lin, Chin-Yew. “Rouge: A package for automatic evaluation of summaries.” Text summarization branches out. 2004.
7. Banerjee, Satanjeev, and Alon Lavie. “METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.” Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 2005.
8. Zhang, Tianyi, et al. “Bertscore: Evaluating text generation with bert.” arXiv preprint arXiv:1904.09675 (2019).
9. Zhao, Wei, et al. “MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.” arXiv preprint arXiv:1909.02622 (2019).
10. Fabbri, Alexander R., et al. “Summeval: Re-evaluating summarization evaluation.” Transactions of the Association for Computational Linguistics 9 (2021): 391-409.
11. He, Tingting, et al. “ROUGE-C: A fully automated evaluation method for multi-document summarization.” 2008 IEEE International Conference on Granular Computing. IEEE, 2008.
12. Liu, Yang, et al. “Gpteval: NLG evaluation using gpt-4 with better human alignment.” arXiv preprint arXiv:2303.16634 (2023).
13. Laban, Philippe, et al. “SummaC: Re-visiting NLI-based models for inconsistency detection in summarization.” Transactions of the Association for Computational Linguistics 10 (2022): 163-177.
14. Gekhman, Zorik, et al. “Trueteacher: Learning factual consistency evaluation with large language models.” arXiv preprint arXiv:2305.11171 (2023).
15. Scialom, Thomas, et al. “Answers unite! unsupervised metrics for reinforced summarization models.” arXiv preprint arXiv:1909.01610 (2019).
16. Durmus, Esin, He He, and Mona Diab. “FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization.” arXiv preprint arXiv:2005.03754 (2020).
17. Scialom, Thomas, et al. “Questeval: Summarization asks for fact-based evaluation.” arXiv preprint arXiv:2103.12693 (2021).
18. Fabbri, Alexander R., et al. “QAFactEval: Improved QA-based factual consistency evaluation for summarization.” arXiv preprint arXiv:2112.08542 (2021).
19. Böhm, Florian, et al. “Better rewards yield better summaries: Learning to summarise without references.” arXiv preprint arXiv:1909.01214 (2019).
20. Stiennon, Nisan, et al. “Learning to summarize with human feedback.” Advances in Neural Information Processing Systems 33 (2020): 3008-3021.
21. Wu, Jeff, et al. “Recursively summarizing books with human feedback.” arXiv preprint arXiv:2109.10862 (2021).
22. Manakul, Potsawee, Adian Liusie, and Mark JF Gales. “Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.” arXiv preprint arXiv:2303.08896 (2023).
