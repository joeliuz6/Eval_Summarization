[
    {
        "Summary": "\n\nAbstract: This paper introduces transfer learning as a method to address the challenges of training CNNs from random initializations to achieve high task accuracy with limited data samples. It discusses a common paradigm for transfer learning, which involves training a model on a large source dataset and then fine-tuning the pre-trained weights with regularization methods on the target dataset. The paper also explains the responsibility of transfer learning, which is to preserve the source knowledge acquired by important neurons, allowing them to extract features from the source domain and contribute to the network's performance on the target dataset.",
        "Abstract": "Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation reduction at the same accuracy budget, outperforming all existing transfer learning methods. With a 10x MACs reduction budget, a ResNet-101 equipped with AFDS transfer learned from ImageNet to Stanford Dogs 120, can achieve an accuracy 11.07% higher than its best competitor.",
        "Introduction": "  Introduction Despite recent successes of CNNs achieving state-of-the-art performance in vision applica- tions ( Tan & Le, 2019 ;  Cai & Vasconcelos, 2018 ;  Zhao et al., 2018 ;  Ren et al., 2015 ), there are two major shortcomings limiting their deployments in real life. First, training CNNs from random initializations to achieve high task accuracy generally requires a large amount of data that is expensive to collect. Second, CNNs are typically compute-intensive and memory-demanding, hindering their adoption to power-limited scenarios. To address the former challenge, transfer learning ( Pan & Yang, 2009 ) is thus designed to transfer knowledge learned from the source task to a target dataset that has limited data samples. In practice, we often choose a source dataset such that the input domain of the source comprises the domain of the target. A common paradigm for transfer learning is to train a model on a large source dataset, and then fine-tune the pre-trained weights with regularization methods on the target dataset ( Zagoruyko & Komodakis, 2017 ;  Yim et al., 2017 ;  Li et al., 2018 ;  Li & Hoiem, 2018 ;  Li et al., 2019 ). For example, one regularization method, L 2 -SP ( Li et al., 2018 ), penalizes the L 2 -distances of pretrained weights on the source dataset and the weights being trained on the target dataset. The pretrained source weights serves as a starting point when training on the target data. During fine-tuning on the target dataset, the regularization constrains the search space around this starting point, which in turn prevents overfitting the target dataset. Intuitively, the responsibility of transfer learning is to preserve the source knowledge acquired by important neurons. The neurons thereby retain their abilities to extract features from the source domain, and contribute to the network's performance on the target dataset.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes the Neural Symbolic Reader (NeRd) for reading comprehension, which consists of a reader that encodes passages and questions into vector representations and a programmer that generates programs, which are executed to produce answers. NeRd outperforms the previous state-of-the-art on DROP by 1.37%/1.18% on EM/F1, and the baselines on MathQA by a large margin of 25.5% on accuracy. It is more scalable (domain-agnostic and compositional) and provides better interpretability.",
        "Abstract": "Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning. In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer. Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the predefined operators, which become executable and interpretable representations for more complex reasoning. Furthermore, to overcome the challenge of training NeRd with weak supervision, we apply data augmentation techniques and hard Expectation-Maximization (EM) with thresholding. On DROP, a challenging reading comprehension dataset that requires discrete reasoning, NeRd achieves 1.37%/1.18% absolute improvement over the state-of-the-art on EM/F1 metrics. With the same architecture, NeRd significantly outperforms the baselines on MathQA, a math problem benchmark that requires multiple steps of reasoning, by 25.5% absolute increment on accuracy when trained on all the annotated programs. More importantly, NeRd still beats the baselines even when only 20% of the program annotations are given.",
        "Introduction": "  INTRODUCTION Deep neural networks have achieved remarkable successes in natural language processing recently. In particular, pretrained language models, e.g., BERT ( Devlin et al., 2019 ), have significantly ad- vanced the state-of-the-art in reading comprehension. While neural models have demonstrated per- formance superior to humans on some benchmarks, e.g., SQuAD ( Rajpurkar et al., 2016 ), so far such progress is mostly limited to extractive question answering, in which the answer is a single span from the text. In other words, this type of benchmarks usually test the capability of text pattern matching, but not of reasoning. Some recent datasets, e.g., DROP ( Dua et al., 2019 ) and MathQA ( Amini et al., 2019 ), are collected to examine the capability of both language understanding and discrete reason- ing, where the direct application of the state-of-the-art pre-trained language models, such as BERT or QANet ( Yu et al., 2018 ), achieves very low accuracy. This is especially challenging for pure neu- ral network approaches, because discrete operators learned by neural networks, such as addition and sorting, can hardly generalize to inputs of arbitrary size without specialized design ( Reed & de Fre- itas, 2016 ;  Cai et al., 2017 ;  Kaiser & Sutskever, 2015 ). Therefore, integrating neural networks with symbolic reasoning is crucial for solving those new tasks. The recent progress on neural semantic parsing ( Jia & Liang, 2016 ;  Liang et al., 2017 ) is sparked to address this problem. However, such success is mainly restricted to question answering with structured data sources, e.g., knowledge graphs ( Berant et al., 2013 ) or tabular databases (Pasupat Reader Reader Neural Semantic Parser Structured Parser: SRL... Programmer Answer Type Span Add/Sub Count Negation NeRd Specialized Modules Neural Semantic Parser Answer Answer Answer Passage Question Passage Question Passage Question Compositional Programs Compositional Programs Structured table Execution Execution &  Liang, 2015 ). Extending it to reading comprehension by parsing the text into structured repre- sentations suffers severely from the cascade errors, i.e., the issues of the structured parsing for data preprocessing account for the poor performance of the learned neural model ( Dua et al., 2019 ). A recent line of work ( Dua et al., 2019 ;  Hu et al., 2019 ;  Andor et al., 2019 ) extends BERT/QANet to perform reasoning on the DROP dataset. However, they cannot easily scale to multiple domains or multi-step complex reasoning because: (1) they usually rely on handcrafted and specialized modules for each type of questions; (2) they don't support compositional applications of the operators, so it is hard to perform reasoning of more than one step. In this work, we propose the Neural Symbolic Reader (NeRd) for reading comprehension, which consists of (1) a reader that encodes passages and questions into vector representations; and (2) a programmer that generates programs, which are executed to produce answers. The key insights behind NeRd are as follows: (1) by introducing a set of span selection operators, the compositional programs, usually executed against structured data such as databases in semantic parsing, can now be executed over text; (2) the same architecture can be applied to different domains by simply extending the set of symbolic operators. A main challenge of training NeRd is that it is often expensive to collect program annotations, so the model needs to learn from weak supervision, i.e., with access only to the final answers. This raises two problems for learning: (1) cold start problem. There are no programs available at the beginning of training, so the training cannot proceed. We address this problem through data augmentation that generates noisy training data to bootstrap the training; (2) spurious program problem, where some programs produce the right answer for wrong rationales. We propose an iterative process using hard EM with thresholding, which filters out the spurious programs during training. In our evaluation, NeRd demonstrates three major advantages over previous methods: (1) better ac- curacy. It outperforms the previous state-of-the-art on DROP by 1.37%/1.18% on EM/F1, and the baselines on MathQA by a large margin of 25.5% on accuracy if trained with all annotated programs. Notably, it still outperforms the MathQA baselines using only 20% of the program annotations; (2) more scalable (domain-agnostic and compositional). Unlike previous approaches, which rely on specialized modules that do not support compositional application of the operators, NeRd can be applied to tasks of different domains, e.g., DROP and MathQA, without changing the architecture, and more complex programs can be simply generated by extending the set of operators and compo- sitionally applying them; (3) better interpretability. It is easier to interpret and verify an answer by inspecting the program that produces it, especially for the questions involving complex reasoning such as counting and sorting.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel neural inspector model for machine reading comprehension (MRC) that forms a list of conditions from a given question and leverages a condition-checking module to determine whether the candidate answer generated by the reader satisfies all the conditions. Additionally, a novel regularization method is proposed to properly train the condition-checking model. The model is evaluated on SQuAD 2.0, NewsQA, and MS MARCO datasets, with experimental results showing consistent improvements across a wide range of MRC models. The explainability of the model is also demonstrated.",
        "Abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.",
        "Introduction": "  INTRODUCTION Machine reading comprehension (MRC), where a machine understands a given document and an- swers a question, is a challenging task, but it has a significant impact in real-world applications such as dialog systems. In practice, given a user-initiated question, potentially relevant paragraphs (often called contexts) are first retrieved from a search engine, which may or may not contain an actual answer. In this case, it is important for an MRC model (or in short, a reader) to be able to determine whether the retrieved context contains the answer before actually predicting the answer. In most previous MRC tasks and datasets, such an answerability issue was out of scope as the pro- vided context was guaranteed to contain an answer for a given question. Recently, a new dataset called SQuAD 2.0 (Rajpurkar et al., 2018) was released, containing instances with unanswerable questions for a given context, so that models can be properly trained to classify this case. Addition- ally, this dataset also contains information about plausible answers in the context when the question is unanswerable, which can be used to prevent our model from wrongly predicting it as an answer. Previously, Liu et al. (2018) addressed the problem of classifying unanswerable cases by adding an auxiliary no-answer classifier to the last layer of the MRC model. Clark & Gardner (2018) tackled answerability classification through a joint softmax layer of the answerability score as well as the scores of all possible answer spans. Hu et al. (2019) attempted to verify the question against the sentence(s) containing the candidate answer. The answerability score from the verifier and the score from the reader were combined to generate the final score of having no answer. However, these existing approaches do not pinpoint where the mismatch occurs between the question and the candidate answer in the unanswerable case, thus being prone to choosing a plausible but wrong answer. This task often becomes tricky when particular conditions from the question are not met. For example, in a question \"What was the projection of sea level increases in the fourth Published as a conference paper at ICLR 2020 assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). Motivated by this, we propose a novel neural inspector model that forms a list of conditions from the question, each of which should be satisfied by the candidate answer generated by the reader. To check whether each condition is met, we leverage and extend the idea proposed by Kiddon et al. (2016), which introduced a recurrent unit that records the used ingredients of cooking recipes by accumulating an attention mechanism during the generation of the recipe in a natural language text. They encourage the model to use all the ingredients by the end of the recipe text generation. Extending this idea, we present a novel condition-checking module that determines whether the candidate answer satisfies all the conditions from the question. Furthermore, we propose a novel regularization method that can properly train our condition-checking model, leading to a correct candidate answer. Finally, we evaluate our proposed model on SQuAD 2.0, NewsQA (Trischler et al., 2017) and MS MARCO (Bajaj et al., 2016) datasets. Our experimental results show consistent improvements across a wide range of MRC models and also demonstrate the explainability of our model regarding which conditions of a given question are not met, or a reason why our model classified the question as unanswerable in a given context.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a method to robustly train deep neural networks (DNNs) under label noise. The method, termed Learning with Ensemble Consensus (LEC), is based on the hypothesis that out of small-loss examples, training losses of noisy examples would increase by injecting certain perturbation to network parameters, while those of clean examples would not. Three LECs with different perturbations are presented and evaluated on three benchmark datasets with random label noise, open-set noise, and semantic noise. Results show that LEC outperforms existing robust training methods by efficiently removing noisy examples from training batches.",
        "Abstract": "Since deep neural networks are over-parameterized, they can memorize noisy examples. We address such a memorization issue in the presence of label noise. From the fact that deep neural networks cannot generalize to neighborhoods of memorized features, we hypothesize that noisy examples do not consistently incur small losses on the network under a certain perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) that prevents overfitting to noisy examples by removing them based on the consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and CIFAR-100 in an efficient manner.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have shown excellent performance ( Krizhevsky et al., 2012 ;  He et al., 2016 ) on visual recognition datasets ( Deng et al., 2009 ). However, it is difficult to obtain high- quality labeled datasets in practice ( Wang et al., 2018a ). Even worse, DNNs might not learn patterns from the training data in the presence of noisy examples ( Zhang et al., 2016 ). Therefore, there is an increasing demand for robust training methods. In general, DNNs optimized with SGD first learn patterns relevant to clean examples under label noise ( Arpit et al., 2017 ). Based on this, recent studies regard examples that incur small losses on the network that does not overfit noisy examples as clean ( Han et al., 2018 ;  Shen & Sanghavi, 2019 ). However, such small-loss examples could be noisy, especially under a high level of noise. Therefore, sampling trainable examples from a noisy dataset by relying on small-loss criteria might be impractical. To address this, we find the method to identify noisy examples among small-loss ones based on well- known observations: (i) noisy examples are learned via memorization rather than via pattern learning and (ii) under a certain perturbation, network predictions for memorized features easily fluctuate, while those for generalized features do not. Based on these two observations, we hypothesize that out of small-loss examples, training losses of noisy examples would increase by injecting certain perturbation to network parameters, while those of clean examples would not. This suggests that examples that consistently incur small losses under multiple perturbations can be regarded as clean. This idea comes from an artifact of SGD optimization, thereby being applicable to any architecture optimized with SGD. In this work, we introduce a method to perturb parameters to distinguish noisy examples from small- loss examples. We then propose a method to robustly train neural networks under label noise, which is termed learning with ensemble consensus (LEC). In LEC, the network is initially trained on the entire training set for a while and then trained on the intersection of small-loss examples of the ensemble of perturbed networks. We present three LECs with different perturbations and evaluate their effectiveness on three benchmark datasets with random label noise ( Goldberger & Ben-Reuven, 2016 ;  Ma et al., 2018 ), open-set noise ( Wang et al., 2018b ), and semantic noise. Our proposed LEC outperforms existing robust training methods by efficiently removing noisy examples from training batches.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a modular and hierarchical approach for autonomous agents to efficiently learn complicated tasks with coordination of different skills from multiple end-effectors. The proposed method consists of two parts: (1) acquiring primitive skills with diverse behaviors by mutual information maximization, and (2) learning a meta policy that selects a skill for each end-effector and coordinates the chosen skills by controlling the behavior of each skill. The proposed method is evaluated on challenging collaborative control tasks such as picking up a long bar, placing a block inside the container on the right side, and pushing a box with two ant agents. Results indicate that the proposed method is able to efficiently learn primitive skills with diverse behaviors and coordinate these skills to solve the tasks.",
        "Abstract": "When mastering a complex manipulation task, humans often decompose the task into sub-skills of their body parts, practice the sub-skills independently, and then execute the sub-skills together. Similarly, a robot with multiple end-effectors can perform complex tasks by coordinating sub-skills of each end-effector. To realize temporal and behavioral coordination of skills, we propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification, and then learns to coordinate end-effectors using diverse behaviors of the skills. We demonstrate that our proposed framework is able to efficiently coordinate skills to solve challenging collaborative control tasks such as picking up a long bar, placing a block inside a container while pushing the container with two robot arms, and pushing a box with two ant agents. Videos and code are available at https://clvrai.com/coordination",
        "Introduction": "  INTRODUCTION Imagine you wish to play Chopin's Fantaisie Impromptu on the piano. With little prior knowledge about the piece, you would first practice playing the piece with each hand separately. After inde- pendently mastering the left and right hand parts, you would move on to practicing with both hands simultaneously. To find the synchronized and non-interfering movements of two hands, you would try variable ways of playing the same melody with each hand, and eventually create a complete piece of music. Through the decomposition of skills into sub-skills of two hands and learning variations of sub-skills, humans make the learning process of manipulation skills much faster than learning everything at once. Can autonomous agents efficiently learn complicated tasks with coordination of different skills from multiple end-effectors like humans? Learning to perform collaborative and composite tasks from scratch requires a huge amount of environment interaction and extensive reward engineering, which often results in undesired behaviors ( Riedmiller et al., 2018 ). Hence, instead of learning a task at once, modular approaches ( Andreas et al., 2017 ;  Oh et al., 2017 ;  Frans et al., 2018 ;  Lee et al., 2019 ;  Peng et al., 2019 ;  Goyal et al., 2020 ) suggest to learn reusable primitive skills and solve more complex tasks by recombining the skills. However, all these approaches either focus on working with single end-effector manipulation or single agent locomotion, and these do not scale to multi-agent problems. To this end, we propose a modular framework that learns to coordinate multiple end-effectors with their primitive skills for various robotics tasks, such as bimanual manipulation. The main challenge is that naive simultaneous execution of primitive skills from multiple end-effectors can often cause unintended behaviors (e.g. collisions between end-effectors). Thus, as illustrated in  Figure 1 , our model needs to learn to appropriately coordinate end-effectors; and hence needs a way to obtain, represent, and control detailed behaviors of each primitive skill. Inspired by these intuitions, our method consists of two parts: (1) acquiring primitive skills with diverse behaviors by mutual information maximization, and (2) learning a meta policy that selects a skill for each end-effector and coordinates the chosen skills by controlling the behavior of each skill. The main contribution of this paper is a modular and hierarchical approach that tackles cooperative manipulation tasks with multiple end-effectors by (1) learning primitive skills of each end-effector independently with skill behavior diversification and (2) coordinating end-effectors using diverse Pick ( ) Push ( ) Place ( ) Push ( ) behaviors of the skills. Our empirical results indicate that our proposed method is able to efficiently learn primitive skills with diverse behaviors and coordinate these skills to solve challenging collabo- rative control tasks such as picking up a long bar, placing a block inside the container on the right side, and pushing a box with two ant agents. We provide additional qualitative results and code at https://clvrai.com/coordination.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel approach to imitation learning under a relaxed assumption, where the expert and the imitator share the same state space but their dynamics may be different. The approach consists of a local state alignment method based on β-VAE and a global state alignment method based on Wasserstein distance, which are combined into a reinforcement learning framework by a regularized policy update objective. Experiments in a number of control tasks and a cross-agent imitation task demonstrate the effectiveness of the proposed approach.",
        "Abstract": "Consider an imitation learning problem that the imitator and the expert have different dynamics models. Most of existing imitation learning methods fail because they focus on the imitation of actions. We propose a novel state alignment-based imitation learning method to train the imitator by following the state sequences in the expert demonstrations as much as possible. The alignment of states comes from both local and global perspectives. We combine them into a reinforcement learning framework by a regularized policy update objective. We show the superiority of our method on standard imitation learning settings as well as the challenging settings in which the expert and the imitator have different dynamics models.",
        "Introduction": "  INTRODUCTION Learning from demonstrations (imitation learning, abbr. as IL) is a basic strategy to train agents for solving complicated tasks. Imitation learning methods can be generally divided into two categories: behavior cloning (BC) and inverse reinforcement learning (IRL). Behavior cloning ( Ross et al., 2011b ) formulates a supervised learning problem to learn a policy that maps states to actions using demonstration trajectories. Inverse reinforcement learning (Russell, 1998;  Ng et al., 2000 ) tries to find a proper reward function that can induce the given demonstration trajectories. GAIL ( Ho & Ermon, 2016 ) and its variants ( Fu et al.; Qureshi et al., 2018 ;  Xiao et al., 2019 ) are the recently proposed IRL-based methods, which uses a GAN-based reward to align the distribution of state- action pairs between the expert and the imitator. Although state-of-the-art BC and IRL methods have demonstrated compelling performance in stan- dard imitation learning settings, e.g. control tasks ( Ho & Ermon, 2016 ;  Fu et al.; Qureshi et al., 2018 ;  Xiao et al., 2019 ) and video games ( Aytar et al., 2018b ), these approaches are developed based on a strong assumption: the expert and the imitator share the same dynamics model; specifically, they have the same action space, and any feasible state-action pair leads to the same next state in proba- bility for both agents. The assumption brings severe limitation in practical scenarios: Imagine that a robot with a low speed limit navigates through a maze by imitating another robot which moves fast, then, it is impossible for the slow robot to execute the exact actions as the fast robot. However, the demonstration from the fast robot should still be useful because it shows the path to go through the maze. We are interested in the imitation learning problem under a relaxed assumption: Given an imita- tor that shares the same state space with the expert but their dynamics may be different, we train the imitator to follow the state sequence in expert demonstrations as much as possible. This is a more general formulation since it poses fewer requirements on the experts and makes demonstra- tion collection easier. Due to the dynamics mismatch, the imitator becomes more likely to deviate from the demonstrations compared with the traditional imitation learning setting. Therefore, it is very important that the imitator should be able to resume to the demonstration trajectory by itself. Note that neither BC-based methods nor GAIL-based IRL methods have learned to handle dynamics misalignment and deviation correction. To address the issues, we propose a novel approach with four main features: 1) State-based. Com- pared to the majority of literature in imitation learning, our approach is state-based rather than action-based. Not like BC and IRL that essentially match state-action pairs between the expert and the imitator, we only match states. An inverse model of the imitator dynamics is learned to recover the action; 2) Deviation Correction. A state-based β-VAE ( Higgins et al., 2017 ) is learned as the prior for the next state to visit. Compared with ordinary behavior cloning, this VAE-based next state predictor can advise the imitator to return to the demonstration trajectory when it deviates. The Published as a conference paper at ICLR 2020 robustness benefits from VAE's latent stochastic sampling; 3) Global State Alignment. While the VAE can help the agent to correct its trajectory to some extent, the agent may still occasionally enter states that are far away from demonstrations, where the VAE has no clue how to correct it. So we have to add a global constraint to align the states in demonstration and imitation. Inspired by GAIL that uses reward to align the distribution of state-action pairs, we also formulate an IRL problem whose maximal cumulative reward is the Wasserstein Distance between states of demonstration and imitation. Note that we choose not to involve state-action pairs as in GAIL( Ho & Ermon, 2016 ), or state-state pairs as in an observation-based GAIL ( Torabi et al., 2018a ), because our state-only formulation imposes weaker constraints than the two above options, thus providing more flexibility to handle different agent dynamics; 4) Regularized Policy Update. We combine the prior for next state learned from VAE and the Wasserstein distance-based global constraint from IRL in a unified framework, by imposing a Kullback-Leibler divergence based regularizer to the policy update in the Proximal Policy Optimization algorithm. To empirically justify our ideas, we conduct experiments in two different settings. We first show that our approach can achieve similar or better results on the standard imitation learning setting, which assumes the same dynamics between the expert and the imitator. We then evaluate our approach in the more challenging setting that the dynamics of the expert and the imitator are different. In a number of control tasks, we either change the physics properties of the imitators or cripple them by changing their geometries. Existing approaches either fail or can only achieve very low rewards, but our approach can still exhibit decent performance. Finally, we show that even for imitation across agents of completely different actuators, it is still possible for the state-alignment based method to work. Surprisingly, a point mass and an ant in MuJoCo ( Todorov et al., 2012 ) can imitate each other to navigate in a maze environment. Our contributions can be summarized as follows: • Propose to use a state alignment based method in the imitation learning problems where the expert's and the imitator's dynamics are different. • Propose a local state alignment method based on β-VAE and a global state alignment method based on Wasserstein distance. • Combine the local alignment and global alignment components into a reinforcement learn- ing framework by a regularized policy update objective.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes DeepHoyer, a Hoyer-inspired regularizer for DNN sparsification. DeepHoyer consists of two regularizers, Hoyer-Square (HS) and Group-HS, which are both almost everywhere differentiable and scale invariant. Experiments show that DeepHoyer outperforms state-of-the-art methods in both element-wise and structural weight pruning of modern DNNs.",
        "Abstract": "In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning.",
        "Introduction": "  INTRODUCTION The use of deep neural network (DNN) models has been expanded from handwritten digit recogni- tion ( LeCun et al., 1998 ) to real-world applications, such as large-scale image classification ( Simonyan & Zisserman, 2014 ), self driving ( Makantasis et al., 2015 ) and complex control problems ( Mnih et al., 2013 ). However, a modern DNN model like AlexNet ( Krizhevsky et al., 2012 ) or ResNet ( He et al., 2016 ) often introduces a large number of parameters and computation load, which makes the deployment and real-time processing on embedded and edge devices extremely difficult (Han et al., 2015b; a ;  Wen et al., 2016 ). Thus, model compression techniques, especially pruning methods that increase the sparsity of weight matrices, have been extensively studied to reduce the memory consumption and computation cost of DNNs (Han et al., 2015b; a ;  Wen et al., 2016 ;  Guo et al., 2016 ;  Louizos et al., 2017b ;  Luo et al., 2017 ;  Zhang et al., 2018 ;  Liu et al., 2015 ). Most of the previous works utilize some form of sparsity-inducing regularizer in searching for sparse neural networks. The 1 regularizer, originally proposed by  Tibshirani (1996) , can be easily optimized through gradient descent for its convex and almost everywhere differentiable property. Therefore it is widely used in DNN pruning:  Liu et al. (2015)  directly apply 1 regularization to all the weights of a DNN to achieve element-wise sparsity;  Wen et al. (2016 ; 2017) present structural sparsity via group lasso, which applies an 1 regularization over the 2 norms of different groups of parameters. However, it has been noted that the value of the 1 regularizer is proportional to the scaling of parameters (i.e. ||αW || 1 = |α|·||W || 1 ), so it \"scales down\" all the elements in the weight matrices with the same speed. This is not efficient in finding sparsity and may sacrifice the flexibility of the trained model. On the other hand, the 0 regularizer directly reflects the real sparsity of weights and is scale invariant (i.e. ||αW || 0 = ||W || 0 , ∀α = 0), yet the 0 norm cannot provide useful gradients. Han et al. (2015b) enforce an element-wise 0 constraint by iterative pruning a fixed percentage of smallest weight elements, which is a heuristic method and therefore can hardly achieve optimal compression rate. Some recent works mitigate the lack of gradient information by integrating 0 regularization with stochastic approximation ( Louizos et al., 2017b ) or more complex optimization methods (e.g. Published as a conference paper at ICLR 2020 ADMM) ( Zhang et al., 2018 ). These additional measures brought overheads to the optimization process, making the use of these methods on larger networks difficult. To achieve even sparser neural networks, we argue to move beyond 0 and 1 regularizers and seek for a sparsity-inducing regularizer that is both almost everywhere differentiable (like 1 ) and scale-invariant (like 0 ). Beyond the 1 regularizer, plenty of non-convex sparsity measurements have been used in the field of feature selection and compressed sensing ( Hurley & Rickard, 2009 ;  Wen et al., 2018 ). Some popular regularizers like SCAD ( Fan & Li, 2001 ), MDP ( Zhang et al., 2010 ) and Trimmed 1 ( Yun et al., 2019 ) use a piece-wise formulation to mitigate the proportional scaling problem of 1 . The piece-wise formulation protects larger elements by having zero penalty to elements greater than a predefined threshold. However, it is extremely costly to manually seek for the optimal trimming threshold, so it is hard to obtain optimal result in DNN pruning by using these regularizers. The transformed 1 regularizer formulated as N i=1 (a+1)|wi| a+|wi| manages to smoothly interpolate between 1 and 0 by tuning the hyperparameter a ( Ma et al., 2019 ). However, such an approximation is close to 0 only when a approaches infinity, so the practical formulation of the transformed 1 (i.e. a = 1) is still not scale-invariant. Particularly, we are interested in the Hoyer regularizer ( Hoyer, 2004 ), which estimates the sparsity of a vector with the ratio between its 1 and 2 norms. Comparing to other sparsity-inducing regularizers, Hoyer regularizer achieves superior performance in the fields of non-negative matrix factorization ( Hoyer, 2004 ), sparse reconstruction ( Esser et al., 2013 ;  Tran et al., 2018 ) and blend deconvolution ( Krishnan et al., 2011 ;  Repetti et al., 2015 ). We note that Hoyer regularizer is both almost everywhere differentiable and scale invariant, satisfying the desired property of a sparsity- inducing regularizer. We therefore propose DeepHoyer, which is the first Hoyer-inspired regularizers for DNN sparsification. Specifically, the contributions of this work include: • Hoyer-Square (HS) regularizer for element-wise sparsity: We enhance the original Hoyer regularizer to the HS regularizer and achieve element-wise sparsity by applying it in the training of DNNs. The HS regularizer is both almost everywhere differentiable and scale invariant. It has the same range and minima structure as the 0 norm. Thus, the HS regularizer presents the ability of turning small weights to zero while protecting and maintaining those weights that are larger than an induced, gradually adaptive threshold; • Group-HS regularizer for structural sparsity, which is extended from the HS regularizer; • Generating sparser DNN models: Our experiments show that the proposed regularizers beat state-of-the-arts in both element-wise and structural weight pruning of modern DNNs.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel pruning strategy, Lookahead Pruning (LAP), which is a generalization of the \"magnitude-equals-saliency\" approach for eliminating unnecessary weights from over-parametrized neural networks. LAP is a score-based approach agnostic to model and data, which can be implemented by computationally light elementary tensor operations. It has no hyper-parameter to tune, in contrast to other sophisticated training-based and optimization-based schemes. Empirical results demonstrate that LAP consistently outperforms its magnitude-based pruning counterpart under various setups, including linear networks, fully-connected networks, and deep convolutional and residual networks. In particular, LAP consistently enables more than ×2 gain in the compression rate of the considered models, with increasing benefits under the high-sparsity regime.",
        "Abstract": "Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants demonstrated remarkable performances for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See https://github.com/alinlab/lookahead_pruning for codes.",
        "Introduction": "  INTRODUCTION The \"magnitude-equals-saliency\" approach has been long underlooked as an overly simplistic base- line among all imaginable techniques to eliminate unnecessary weights from over-parametrized neural networks. Since the early works of  LeCun et al. (1989) ;  Hassibi & Stork (1993)  which provided more theoretically grounded alternatives of magnitude-based pruning (MP) based on sec- ond derivatives of the loss function, a wide range of methods including Bayesian / information- theoretic approaches ( Neal, 1996 ;  Louizos et al., 2017 ; Molchanov et al., 2017;  Dai et al., 2018 ), p -regularization ( Wen et al., 2016 ;  Liu et al., 2017 ;  Louizos et al., 2018 ), sharing redundant chan- nels ( Zhang et al., 2018 ;  Ding et al., 2019 ), and reinforcement learning approaches ( Lin et al., 2017 ;  Bellec et al., 2018 ;  He et al., 2018 ) have been proposed as more sophisticated alternatives. On the other hand, the capabilities of MP heuristics are gaining attention once more. Combined with minimalistic techniques including iterative pruning ( Han et al., 2015 ) and dynamic reestablishment of connections ( Zhu & Gupta, 2017 ), a recent large-scale study by  Gale et al. (2019)  claims that MP can achieve a state-of-the-art trade-off between sparsity and accuracy on ResNet-50. The unreason- able effectiveness of magnitude scores often extends beyond the strict domain of network pruning; a recent experiment by  Frankle & Carbin (2019)  suggests the existence of an automatic subnetwork discovery mechanism underlying the standard gradient-based optimization procedures of deep, over- parametrized neural networks by showing that the MP algorithm finds an efficient trainable subnet- work. These observations constitute a call to revisit the \"magnitude-equals-saliency\" approach for a better understanding of the deep neural network itself. As an attempt to better understand the nature of MP methods, we study a generalization of magnitude scores under a functional approximation framework; by viewing MP as a relaxed minimization of distortion in layerwise operators introduced by zeroing out parameters, we consider a multi-layer extension of the distortion minimization problem. Minimization of the newly suggested distortion measure, which 'looks ahead' the impact of pruning on neighboring layers, gives birth to a novel pruning strategy, coined lookahead pruning (LAP). In this paper, we focus on the comparison of the proposed LAP scheme to its MP counterpart. We empirically demonstrate that LAP consistently outperforms MP under various setups, including lin- ear networks, fully-connected networks, and deep convolutional and residual networks. In particular, LAP consistently enables more than ×2 gain in the compression rate of the considered models, with increasing benefits under the high-sparsity regime. Apart from its performance, lookahead pruning enjoys additional attractive properties: • Easy-to-use: Like magnitude-based pruning, the proposed LAP is a simple score-based approach agnostic to model and data, which can be implemented by computationally light elementary tensor operations. Unlike most Hessian-based methods, LAP does not rely on the availability of training data except for the retraining phase. It also has no hyper-parameter to tune, in contrast to other sophisticated training-based and optimization-based schemes. • Versatility: As our method simply replaces the \"magnitude-as-saliency\" criterion with a looka- head alternative, it can be deployed jointly with algorithmic tweaks developed for magnitude- based pruning, such as iterative pruning and retraining ( Han et al., 2015 ) or joint pruning and training with dynamic reconnections ( Zhu & Gupta, 2017 ;  Gale et al., 2019 ). The remainder of this manuscript is structured as follows: In Section 2, we introduce a functional approximation perspective toward MP and motivate LAP and its variants as a generalization of MP for multiple layer setups; in Section 3 we explore the capabilities of LAP and its variants with simple models, then move on to apply LAP to larger-scale models.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a regularization method, Projected Error Function (PER), which regularizes activations in the Wasserstein probability distribution space by pushing the distribution of activations to be close to the standard normal distribution. PER considers different statistics simultaneously, such as all orders of moments and correlation between hidden units, and is shown to be effective on multiple challenging tasks.",
        "Abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n",
        "Introduction": "  INTRODUCTION Training of deep neural networks is very challenging due to the vanishing and exploding gradient problem (Hochreiter, 1998;  Glorot & Bengio, 2010 ), the presence of many flat regions and saddle points ( Shalev-Shwartz et al., 2017 ), and the shattered gradient problem ( Balduzzi et al., 2017 ). To remedy these issues, various methods for controlling hidden activations have been proposed such as normalization ( Ioffe & Szegedy, 2015 ;  Huang et al., 2018 ), regularization ( Littwin & Wolf, 2018 ), initialization ( Mishkin & Matas, 2016 ;  Zhang et al., 2019 ), and architecture design ( He et al., 2016 ). Among various techniques of controlling activations, one well-known and successful path is con- trolling their first and second moments. Back in the 1990s, it has been known that the neural net- work training can be benefited from normalizing input statistics so that samples have zero mean and identity covariance matrix ( LeCun et al., 1998 ;  Schraudolph, 1998 ). This idea motivated batch normalization (BN) that considers hidden activations as the input to the next layer and normalizes scale and shift of the activations ( Ioffe & Szegedy, 2015 ). Recent works show the effectiveness of different sample statistics of activations for normalization and regularization.  Deecke et al. (2019)  and  Kalayeh & Shah (2019)  normalize activations to several modes with different scales and translations. Variance constancy loss (VCL) implicitly normalizes the fourth moment by minimizing the variance of sample variances, which enables adaptive mode separation or collapse based on their prior probabilities ( Littwin & Wolf, 2018 ). BN is also extended to whiten activations ( Huang et al., 2018 ; 2019), and to normalize general order of central moment in the sense of L p norm including L 0 and L ∞ ( Liao et al., 2016 ;  Hoffer et al., 2018 ). In this paper, we propose a projected error function regularization (PER) that regularizes activations in the Wasserstein probability distribution space. Specifically, PER pushes the distribution of acti- vations to be close to the standard normal distribution. PER shares a similar strategy with previous approaches that dictates the ideal distribution of activations. Previous approaches, however, deal with single or few sample statistics of activations. On the contrary, PER regularizes the activations Published as a conference paper at ICLR 2020 (a) (b) (c) (d) by matching the probability distributions, which considers different statistics simultaneously, e.g., all orders of moments and correlation between hidden units. The extensive experiments on multiple challenging tasks show the effectiveness of PER.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a general framework called difference-seeking generative adversarial network (DSGAN) to generate a variety of unseen data. DSGAN is a generative approach based on a game-theoretic framework, consisting of two functions: generator and discriminator. The discriminator network is trained to determine whether the inputs belong to the real dataset or fake dataset created by the generator, while the generator learns to map a sample from a latent space to some distribution to increase the classification errors of the discriminator.",
        "Abstract": "\nUnseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\\em e.g.,} novelty detection, semi-supervised learning, and adversarial training).  In this paper, we introduce a general framework called  \\textbf{d}ifference-\\textbf{s}eeking \\textbf{g}enerative \\textbf{a}dversarial \\textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect.\n\nThe DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)  from only the samples from the two distributions, $p_{d}$ and $p_{\\bar{d}}$. In our scenario, $p_d$ is the distribution of the seen data, and $p_{\\bar{d}}$ can be obtained from $p_{d}$ via simple operations, so that  we only need the samples of $p_{d}$ during the training. \nTwo key applications, semi-supervised learning and novelty detection, are taken as case studies to illustrate that the DSGAN enables the production of various unseen data. We also provide theoretical analyses about the convergence of the DSGAN.\n\n",
        "Introduction": "  INTRODUCTION Unseen data 1 are not samples from the distribution of the training data and are difficult to collect. It has been demonstrated that unseen samples can be applied to several applications.  Dai et al. (2017)  proposed how to create complement data, and theoretically showed that complement data, considered as unseen data, could improve semi-supervised learning. In novelty detection,  Yu et al. (2017)  proposed a method to generate unseen data and used them to train an anomaly detector. Another related area is adversarial training  Goodfellow et al. (2015) , where classifiers are trained to resist adversarial examples, which are unseen during the training phase. However, the aforementioned methods only focus on producing specific types of unseen data, instead of enabling the generation of general types of unseen data. In this paper, we propose a general framework called difference-seeking generative adversarial network (DSGAN), to generate a variety of unseen data. The DSGAN is a generative approach. Traditionally, generative approaches, which are usually conducted in an unsupervised learning manner, are developed for learning the data distribution from its samples, from which subsequently, they produce novel and high-dimensional samples, such as the synthesized image  Saito et al. (2018) . A state-of-the-art approach is the so-called generative adversarial network (GAN)  Goodfellow et al. (2014) . GAN produces sharp images based on a game-theoretic framework, but it can be difficult and unstable to train owing to multiple interaction losses. Specifically, GAN consists of two functions: generator and discriminator. Both functions are represented as parameterized neural networks. The discriminator network is trained to determine whether the inputs belong to the real dataset or fake dataset created by the generator. The generator learns to map a sample from a latent space to some distribution to increase the classification errors of the discriminator.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper studies the unsupervised learning of latent representations of high-dimensional data, specifically overcomplete dictionary learning (ODL) and convolutional dictionary learning (CDL). It examines the connections between these problems and convolutional neural networks, and reviews prior arts on dictionary learning. It also introduces a new discovery that reduces the problem of learning complete dictionaries to finding the sparsest vector in a subspace, and discusses the challenges of generalizing this approach to the overcomplete setting.",
        "Abstract": "Learning overcomplete representations finds many applications in machine learning and data analytics. In the past decade, despite the empirical success of heuristic methods, theoretical understandings and explanations of these algorithms are still far from satisfactory. In this work, we provide new theoretical insights for several important representation learning problems: learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. We formulate these problems as $\\ell^4$-norm optimization problems over the sphere and study the geometric properties of their nonconvex optimization landscapes. For both problems, we show the nonconvex objective has benign (global) geometric structures, which enable the development of efficient optimization methods finding the target solutions. Finally, our theoretical results are justified by numerical simulations.\n",
        "Introduction": "  INTRODUCTION High dimensional data often has low-complexity structures (e.g., sparsity or low rankness). The performance of modern machine learning and data analytical methods heavily depends on appropriate low-complexity data representations (or features) which capture hidden information underlying the data. While we used to manually craft representations in the past, it has been demonstrated that learned representations from the data show much superior performance (Elad, 2010). Therefore, (unsupervised) learning of latent representations of high-dimensional data becomes a fundamental problem in signal processing, machine learning, theoretical neuroscience and many other fields (Bengio et al., 2013). Moreover, overcomplete representations for which the number of latent features exceeds the data dimensionality, have shown better representation of the data in various applications compared to complete representations (Lewicki & Sejnowski, 2000; Chen et al., 2001; Rubinstein et al., 2010). In this paper, we study the following overcomplete representation learning problems. • Overcomplete dictionary learning (ODL). One of the most important unsupervised representa- tion learning problems is learning sparsely used dictionaries (Olshausen & Field, 1997), which finds many applications in image processing and computer vision (Wright et al., 2010; Mairal et al., 2014). The task is given data Y lo omo on data \" A lo omo on dictionary¨X lo omo on sparse code , (1.1) we want to learn the compact representation (or dictionary) A P R nˆm along with the sparse code X P R mˆp . For better representation of the data, it is often more desired that the dictionary A is overcomplete m ą n, where it provides greater flexibility in matching structures in the data. • Convolutional dictionary learning (CDL). Inspired by deconvolutional networks (Zeiler et al., 2010), the convolutional form of sparse representations (Bristow et al., 2013; Garcia-Cardona & The full version of this work can be found at https://arxiv.org/abs/1912.02427. Published as a conference paper at ICLR 2020 Wohlberg, 2018) replaces the unstructured dictionary A with a set of convolution filters ta 0k u K k\"1 . Namely, the problem is that given multiple circulant convolutional measurements y i \" K ÿ k\"1 a 0k lo omo on filter f x ik lo omo on sparse code , 1 ď i ď p, (1.2) one wants to learn the filters ta 0k u K k\"1 along with the sparse codes. The problem resembles a lot similarities to classical ODL. Indeed, one can show that Equation (1.2) reduces to Equation (1.1) in overcomplete settings by reformulation (Huang & Anandkumar, 2015). The interest of studying CDL was spurred by its better modeling ability of human visual and cognitive systems and the development of more efficient computational methods (Bristow et al., 2013), and has led to a number of applications in which the convolutional form provides state-of-art performance (Gu et al., 2015; Papyan et al., 2017b; Lau et al., 2019). Recently, the connections between CDL and convolutional neural network have also been extensively studied (Papyan et al., 2017a; 2018). In addition, variants of finding overcomplete representations appear in many other problems beyond the dictionary learning problems we introduced here, such as overcomplete tensor decomposition (Anandkumar et al., 2017; Ge & Ma, 2017), overcomplete ICA (Lewicki & Sejnowski, 1998; Le et al., 2011), and short-and-sparse blind deconvolution (Zhang et al., 2017; 2018; Kuo et al., 2019). Prior arts on dictionary learning (DL). In the past decades, numerous heuristic methods have been developed for solving DL (Lee et al., 2007; Aharon et al., 2006; Mairal et al., 2010). Despite their empirical success (Wright et al., 2010; Mairal et al., 2014), theoretical understandings of when and why these methods work are still limited. When the dictionary A is complete (Spielman et al., 2012) (i.e., square and invertible, m \" n), by the fact that the row space of Y equals to that of X (i.e., rowpY q \" rowpXq), Sun et al. (2016a) reduced the problem to finding the sparsest vector in a subspace (Demanet & Hand, 2014; Qu et al., 2016). By considering a (smooth) variant of the following 1 -minimization problem over the sphere, min q 1 p › › q J Y › › 1 , s.t. q P S n´1 , (1.3) Sun et al. (2016a) showed that the nonconvex problem has no spurious local minima when the sparsity level 1 θ P Op1q, and every local minimizer q ‹ is a global minimizer with q J ‹ Y corresponding to one row of X. The new discovery has led to efficient, guaranteed optimization methods for complete DL from random initializations (Sun et al., 2016b; Bai et al., 2018; Gilboa et al., 2019). However, all these methods critically rely on the fact that rowpY q \" rowpXq for complete A, there is no obvious way to generalize the approach to the overcomplete setting m ą n. On the other hand, for learning incoherent overcomplete dictionaries, with sparsity θ P Op1{ ? nq and stringent assumptions on X, most of the current theoretical analysis results are local (Geng et al., 2011; Arora et al., 2015; Agarwal et al., 2016; Chatterji & Bartlett, 2017), in the sense that they require complicated initializations that could be difficult to implement in practice. Therefore, the legitimate question remains: why do heuristic methods solve ODL with simple initializations?",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes AutoQ, a hierarchical-DRL-based agent for kernel-wise network quantization, to automatically and rapidly search a QBN for each weight kernel and choose a QBN for each activation layer of a CNN for accurate kernel-wise network quantization. AutoQ comprises a high-level controller (HLC) and a low-level controller (LLC) and is evaluated on three popular CNNs, ResNet-18, ResNet-50 and MobileNetV2. The experimental results show that AutoQ achieves higher accuracy than the state-of-the-art kernel-wise network quantization techniques with the same inference latency and hardware cost.",
        "Abstract": "Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) DDPG-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06%, and decrease the inference energy consumption by 50.69%, while achieving the same inference accuracy.",
        "Introduction": "  INTRODUCTION Although convolutional neural networks (CNNs) have been the dominant approach ( Sandler et al., 2018 ) to solving a wide variety of problems such as computer vision and recommendation sys- tems, it is challenging to deploy CNNs to mobile devices having only limited hardware resources and tight power budgets, due to their huge essential computing overhead, e.g., an inference of Mo- bileNetV2 ( Sandler et al., 2018 ) involves 6.9M weights and 585M floating point operations. Several approaches such as pruning ( He et al., 2018 ) and low-rank approximation ( Denton et al., 2014 ) are proposed to reduce the inference computing overhead of CNNs. Network quantiza- tion ( Wang et al., 2019 ;  Lin et al., 2017 ) becomes one of the most hardware friendly CNN ac- celeration techniques by approximating real-valued weights and activations to QBN -bit fixed-point representations, and performing inferences using cheaper fixed-point multiple-accumulation (MAC) operations, where QBN is the quantization bit number. Instead of using one QBN for the whole CNN, the layer-wise network quantization ( Wang et al., 2019 ;  Elthakeb et al., 2018 ) assigns a QBN to the weights of each convolutional layer, and searches another QBN for the activations of the same layer to decrease the inference computing overhead. But the inference cost of the layer-wise quantized CNNs is still prohibitive for low-power mobile de- vices powered by batteries. Recent works ( Zeng et al., 2019 ;  Choukroun et al., 2019b ;  Zhang et al., 2018 ;  Li et al., 2019 ;  Krishnamoorthi, 2018 ;  Sasaki et al., 2019 ) find that various weight kernels of a convolutional layer (ResNet-18) exhibit different variances shown in  Figure 1  and hence have differ- ent amounts of redundancy. Therefore, they quantize each weight kernel independently for higher accuracy by calculating a QBN -element scaling factor vector for each kernel, rather than globally quantize all the kernels of a layer as a whole. To reduce different amounts of redundancy among different weight kernels, these kernel-wise network quantization techniques should have searched a QBN for each kernel of each layer in a CNN. However, the search space of choosing a QBN for each weight kernel is too large, so prior kernel-wise network quantization ( Zeng et al., 2019 ;  Choukroun et al., 2019b ;  Zhang et al., 2018 ;  Li et al., 2019 ;  Krishnamoorthi, 2018 ;  Sasaki et al., 2019 ) still uses the same QBN for the entire CNN. As  Figure 2  shows, compared to the layer-wise quantized model, on the same FPGA accelerator ( Umuroglu et al., 2019a ), the kernel-wise quantized model (assigning a QBN to each weight kernel and choosing a QBN for each activation layer) improves the inference accuracy by ∼ 2% (ImageNet) with the same computing overhead (inference latency). How to decide a QBN for each weight kernel is the most important task of the kernel-wise network quantization, since the QBNs have a large impact on the inference accuracy, latency and hardware overhead. Determining a QBN for each weight kernel via hand-crafted heuristics is so sophisticated that even machine learning experts can obtain only sub-optimal results. Recent works ( Wang et al., 2019 ;  Elthakeb et al., 2018 ) automatically select a QBN for each layer of a CNN through a deep reinforcement learning (DRL) agent without human intervention. However, it is still difficult for low-power mobile devices such as drones and smart glasses to adopt the layer-wise quantized CNN models. These mobile devices are very sensitive to the bit-width of fixed-point MAC operations and memory access during inferences due to their limited battery lifetime and hardware resources. Kernel-wise network quantization assigning a QBN to each weight kernel and searching a QBN for each activation layer of a CNN becomes a must to enable the efficient deployment of deep CNNs on mobile devices by reducing the inference computing overhead. Although it is straightforward to perform kernel-wise quantization via DRL, it takes ultra-long time for a DRL agent to find a proper QBN for each weight kernel of a CNN. As CNN architectures are becoming deeper, it is infeasible to employ rule-based domain expertise or conventional DRL-based techniques to explore the exponentially enlarging search space of kernel-wise network quantization. In this paper, we propose a hierarchical-DRL-based agent, AutoQ, to automatically and rapidly search a QBN for each weight kernel and choose a QBN for each activation layer of a CNN for accurate kernel-wise network quantization. AutoQ comprises a high-level controller (HLC) and a low-level controller (LLC). The HLC chooses a QBN for each activation layer and generates a goal, the average QBN for all weight kernels of a convolutional layer, for each layer. Based on the goal, the LLC produces an action, QBN, to quantize each weight kernel of the layer. The HLC and LLC simultaneously learn by trials and errors, i.e., penalizing inference accuracy loss while rewarding a smaller QBN. We also build a state space, a goal and an action space, an intrinsic reward and an extrinsic reward for AutoQ. Instead of proxy signals including FLOPs, number of memory access and model sizes, we design the extrinsic reward to take the inference latency, energy consumption and hardware cost into consideration.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel network architecture, Action Semantics Network (ASN), to explicitly consider action semantics and facilitate learning in multiagent systems. ASN can be combined with existing deep reinforcement learning algorithms to boost its learning performance. Experimental results on StarCraft II micromanagement and Neural MMO show that ASN leads to better performance compared to state-of-the-art approaches in terms of both convergence speed and final performance.",
        "Abstract": "In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influences on other agents. In this paper, we propose a novel network architecture, named Action Semantics Network (ASN), that explicitly represents such action semantics between agents. ASN characterizes different actions' influence on other agents using neural networks based on the action semantics between them. ASN can be easily combined with existing deep reinforcement learning (DRL) algorithms to boost their performance. Experimental results on StarCraft II micromanagement and Neural MMO show ASN significantly improves the performance of state-of-the-art DRL approaches compared with several network architectures.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (DRL) ( Sutton & Barto, 2018 ) has achieved a lot of success at finding optimal policies to address single-agent complex tasks ( Mnih et al., 2015 ;  Lillicrap et al., 2016 ;  Silver et al., 2017 ). However, there also exist a lot of challenges in multiagent systems (MASs) since agents' behaviors are influenced by each other and the environment exhibits more stochasticity and uncertainties ( Claus & Boutilier, 1998 ;  Hu & Wellman, 1998 ;  Bu et al., 2008 ;  Hauwere et al., 2016 ). Recently, a number of deep multiagent reinforcement learning (MARL) approaches have been pro- posed to address complex multiagent problems, e.g., coordination of robot swarm systems ( Sosic et al., 2017 ) and autonomous cars ( Oh et al., 2015 ). One major class of works incorporates various multiagent coordination mechanisms into deep multiagent learning architecture ( Lowe et al., 2017 ;  Foerster et al., 2018 ;  Yang et al., 2018 ;  Palmer et al., 2018 ).  Lowe et al. (2017)  proposed a cen- tralized actor-critic architecture to address the partial observability in MASs. They also incorporate the idea of joint action learner (JAL) ( Littman, 1994 ) to facilitate multiagent coordination. Later,  Foerster et al. (2018)  proposed Counterfactual Multi-Agent Policy Gradients (COMA) motivated from the difference reward mechanism ( Wolpert & Tumer, 2001 ) to address the challenges of multi- agent credit assignment. Recently,  Yang et al. (2018)  proposed applying mean-field theory ( Stanley, 1971 ) to solve large-scale multiagent learning problems. More recently,  Palmer et al. (2018)  extend- ed the idea of leniency ( Potter & Jong, 1994 ;  Panait et al., 2008 ) to deep MARL and proposed the retroactive temperature decay schedule to address stochastic rewards problems. However, all these Published as a conference paper at ICLR 2020 works ignore the natural property of the action influence between agents, which we aim to exploit to facilitate multiagent coordination. Another class of works focus on specific network structure design to address multiagent learning problems ( Sunehag et al., 2018 ;  Rashid et al., 2018 ;  Sukhbaatar et al., 2016 ;  Singh et al., 2019 ).  Sunehag et al. (2018)  designed a value-decomposition network (VDN) to learn an optimal linear value decomposition from the team reward signal based on the assumption that the joint action- value function for the system can be additively decomposed into value functions across agents. Later,  Rashid et al. (2018)  relaxed the linear assumption in VDN by assuming that the Q-values of individual agents and the global one are also monotonic, and proposed QMIX employing a network that estimates joint action-values as a complex non-linear combination of per-agent values. Recently,  Zambaldi et al. (2019)  proposed the relational deep RL to learn environmental entities relations. However, they considered the entity relations on the pixel-level of raw visual data, which ignores the natural property of the influence of actions between agents.  Tacchetti et al. (2019)  proposed a novel network architecture called Relational Forward Model (RFM) for predictive modeling in multiagent learning. RFM takes a semantic description of the state of an environment as input, and outputs either an action prediction for each agent or a prediction of the cumulative reward of an episode. However, RFM does not consider from the perspective of the influence of each action on other agents. OpenAI designed network structures to address multiagent learning problems in a famous Multiplayer Online Battle Arena (MOBA), Dota2. They used a scaled-up version of PPO ( Schulman et al. (2017) ), adopted the attention mechanism to compute the weight of choosing the target unit, with some of information selected from all information as input. However, this selection is not considered from the influence of each action on other agents. There are also a number of works designing network structures for multiagent communication ( Sukhbaatar et al., 2016 ;  Singh et al., 2019 ). However, none of the above works explicitly leverage the fact that an agent's different actions may have different impacts on other agents, which is a natural property in MASs and should be considered in the decision-making process. In multiagent settings, each agent's action set can be naturally divided into two types: one type containing actions that affect environmental information or its private properties and the other type containing actions that directly influence other agents (i.e., their private properties). Intuitively, the estimation of performing actions with different types should be evaluated separately by explicitly considering different information. We refer to the property that different actions may have different impacts on other agents as action semantics. We can leverage the action semantics information to improve an agent's policy/Q network design toward more efficient multiagent learning. To this end, we propose a novel network architecture, named Action Semantics Network (ASN) to characterize such action semantics for more efficient multiagent coordination. The main contribu- tions of this paper can be summarized as follows: 1) to the best of our knowledge, we are the first to explicitly consider action semantics and design a novel network to extract it to facilitate learn- ing in MASs; 2) ASN can be easily combined with existing DRL algorithms to boost its learning performance; 3) experimental results * on StarCraft II micromanagement ( Samvelyan et al., 2019 ) and Neural MMO ( Suarez et al., 2019 ) show our ASN leads to better performance compared with state-of-the-art approaches in terms of both convergence speed and final performance.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a novel method for representation learning of heterogeneous hypergraphs with variable-sized hyperedges. The proposed method is able to capture higher-order interactions in graphs and predict variable-sized hyperedges. It is applicable to real-world applications such as co-authorship involving more than two authors or relationships among multiple heterogeneous objects.",
        "Abstract": "Graph representation learning for hypergraphs can be utilized to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. We believe that Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications. ",
        "Introduction": "  INTRODUCTION Graph structure is a widely used representation for data with complex interactions. Learning on graphs has also been an active research area in machine learning on how to predict or dis- cover patterns based on the graph structure ( Hamilton et al., 2017b ). Although existing meth- ods can achieve strong performance in tasks such as link prediction and node classification, they are mostly designed for analyzing pair-wise interactions and thus are unable to effectively capture higher-order interactions in graphs. In many real-world applications, however, rela- tionships among multiple instances are key to capturing critical properties, e.g., co-authorship involving more than two authors or relationships among multiple heterogeneous objects such as \"(human, location, activity)\". Hypergraphs can be used to represent higher-order interac- tions ( Zhou et al., 2007 ). To analyze higher-order interaction data, it is straightforward to ex- pand each hyperedge into pair-wise edges with the assumption that the hyperedge is decomposable. Several previous methods were developed based on this no- tion ( Sun et al., 2008 ;  Feng et al., 2018 ). However, earlier work DHNE (Deep Hyper-Network Embedding) ( Tu et al., 2018 ) suggested the existence of heterogeneous indecom- posable hyperedges where relationships within an incom- plete subset of a hyperedge do not exist. Although DHNE provides a potential solution by modeling the hyperedge di- rectly without decomposing it, due to the neural network structure used in DHNE, the method is limited to the fixed type and fixed-size heterogeneous hyperedges and is unable to consider relationships among multiple types of instances with variable size. For example,  Fig. 1  shows a heteroge- neous co-authorship hypergraph with two types of nodes (corresponding author and coauthor). Due to the variable number of both authors and corresponding authors in a publication, the hyperedges (co-authorship) have different sizes or types. Unfortunately, methods for representation learning of heterogeneous hypergraph with variable-sized hyperedges, especially those that can predict variable-sized hyperedges, have not been developed.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel deep reinforcement learning approach to solve the large-scale sequential expensive coordination problem. The proposed approach models the leader's decision-making process as a semi-Markov Decision Process and uses an event-based policy gradient to learn the leader's policy. A leader-follower consistency scheme is proposed to predict the followers' behaviors precisely, and an action abstraction-based policy gradient algorithm is proposed to accelerate the training process of followers. Experiments in resource collections, navigation and predator-prey show that the proposed method outperforms the state-of-the-art methods.",
        "Abstract": "Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader's long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers' behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers' decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically.",
        "Introduction": "  INTRODUCTION Deep Multi-Agent Reinforcement Learning (MARL) has been widely used in coordinating cooper- ative agents to jointly complete certain tasks where the agent is assumed to be selfless (fully coop- erative), i.e., the agent is willing to sacrifice itself to maximize the team reward. However, in many cases of the real world, the agents are self-interested, such as taxi drivers in a taxi company (fleets) and clubs in a league. For instance, in the example of taxi fleets (Miao et al., 2016), drivers may pre- fer to stay in the area with high customer demand to gain more reward. It is unfair and not efficient to compel the taxi driver to selflessly contribute to the company, e.g., to stay in the low customer demand area. Forcing the drivers to selflessly contribute may increase the income for the company in a short-term but it will finally causes the low efficient and unsustainable of that company in the Published as a conference paper at ICLR 2020 long run because the unsatisfied drivers may be demotivated and even leave the company. Another important example is that the government wants some companies to invest on the poverty area to achieve the fairness of the society, which may inevitably reduce the profits of companies. Similar to previous example, the companies may leave when the government forces them to invest. A better way to achieve coordination among followers and achieve the leader's goals is that the manager of the company or the government needs to provide bonuses to followers, like the taxi company pays extra bonuses for serving the customers in rural areas and the government provides subsidies for investing in the poverty areas, which we term as expensive coordination. In this paper, we solve the large-scale sequential expensive coordination problem with a novel RL training scheme. There are several lines of works related to the expensive coordination problem, including mecha- nism design (Nisan & Ronen, 2001) and the principal-agent model (Laffont & Martimort, 2009). However, these works focus more on static decisions (each agent only makes a single decision). To consider sequential decisions, the leader-follower MDP game (Sabbadin & Viet, 2013; 2016) and the RL-based mechanism design (Tang, 2017; Shen et al., 2017) are introduced but most of their works only focus on matrix games or small-scale Markov games, which cannot be applied to the case with the large-scale action or state space. The most related work is M 3 RL (Shu & Tian, 2019) where the leader assigns goals and bonuses by using a simple attention mechanism (summing/averaging the features together) and mind (behaviors) tracking to predict the followers' behaviors and makes response to the followers' behaviors. But they only consider the rule-based followers, i.e., followers with fixed preference, and ignore the followers' behaviors responding to the leader's policy, which significantly simplifies the problem and leads the unreasonability of the model. In the expensive coordination problem, there are two critical issues which should be considered: 1) the leader's long-term decision process where the leader has to consider both the long-term effect of itself and long-term behaviors of the followers when determining his action to incentivise the coordination among followers, which is not considered in (Sabbadin & Viet, 2013; Mguni et al., 2019); and 2) the complex interactions between the leader and followers where the followers will adapt their policies to maximize their own utility given the leader's policy, which makes the training process unstable and hard, if not unable, to converge in large-scale environment, especially when the leader changes his actions frequently, which is ignored by (Tharakunnel & Bhattacharyya, 2007; Shu & Tian, 2019). In this work, we address these two issues in the expensive coordination problem through an abstraction-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi- Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to learn the leader's policy considering the long-term effect (leader takes actions at important points rather than at each step to avoid myopic decisions.) (Section 4.1). (2) A well-performing leader's policy is also highly dependent on how well the leader knows the followers. To predict the followers' behaviors precisely, we show the leader-follower consistency scheme. Based on the scheme, the follower-aware module, the follower-specific attention module, and the sequential decision module are proposed to capture these followers' behaviors and make accurate response to their behaviors (Section 4.2). (3) To accelerate the training process, we propose an action abstraction-based policy gradient algorithm for the followers. This approach is able to reduce followers' decision space and thus simplifies the interaction between the leader and followers as well as accelerates the training process of followers (Section 4.3). Experiments in resource collections, navigation and predator- prey show that our method outperforms the state-of-the-art methods dramatically.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Online Convex Optimization (OCO) is a well-established learning framework which seeks to minimize regret, defined as the difference between the cumulative loss of the learner and that of the best decision in hindsight. The most classic algorithm for OCO is Online Gradient Descent (OGD), which attains an O( √ T ) regret. To address the limitation of OGD, various of adaptive gradient methods have been proposed, such as Adagrad, RMSprop, Adadelta and Adam. Despite the outstanding performance, Adam suffers the non-convergence issue, and two modified versions, AMSgrad and AdamNC, have been developed with data-dependent regret bounds.",
        "Abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.",
        "Introduction": "  INTRODUCTION Online Convex Optimization (OCO) is a well-established learning framework which has both theoret- ical and practical appeals ( Shalev-Shwartz et al., 2012 ). It is performed in a sequence of consecutive rounds: In each round t, firstly a learner chooses a decision x t from a convex set D ⊆ R d , at the same time, an adversary reveals a loss function f t (·) : D → R, and consequently the learner suffers a loss f t (x t ). The goal is to minimize regret, defined as the difference between the cumulative loss of the learner and that of the best decision in hindsight (Hazan et al., 2016): The most classic algorithm for OCO is Online Gradient Descent (OGD) ( Zinkevich, 2003 ), which attains an O( √ T ) regret. OGD iteratively performs descent step towards gradient direction with a predetermined step size, which is oblivious to the characteristics of the data being observed. As a result, its regret bound is data-independent, and can not benefit from the structure of data. To address this limitation, various of adaptive gradient methods, such as Adagrad ( Duchi et al., 2011 ), RMSprop ( Tieleman & Hinton, 2012 ) and Adadelta ( Zeiler, 2012 ) have been proposed to exploit the geometry of historical data. Among them, Adam ( Kingma & Ba, 2015 ), which dynamically adjusts the step size and the update direction by exponential average of the past gradients, has been extensively popular and successfully applied to many applications ( Xu et al., 2015 ;  Gregor et al., 2015 ;  Kiros et al., 2015 ;  Denkowski & Neubig, 2017 ;  Bahar et al., 2017 ). Despite the outstanding performance,  Reddi et al. (2018)  pointed out that Adam suffers the non-convergence issue, and developed two modified versions, namely AMSgrad and AdamNC. These variants are equipped with data-dependent regret bounds, which are O( √ T ) in the worst case and become tighter when gradients are sparse 1 .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents Neural Stored-program Memory (NSM), a Memory Augmented Neural Network (MANN) that couples a MANN with an external program memory. This program memory stores the weights of the MANN's controller network, which are retrieved quickly via a key-value attention mechanism across timesteps yet updated slowly via back-propagation. The NSM is tested on a variety of synthetic tasks including algorithmic tasks, composition of algorithmic tasks, continual procedure learning, few-shot learning, and linguistic problems. Results demonstrate clear improvements of NSM over existing MANNs, and competitive performances against state-of-the-art models. This study advances neural network simulation of Turing Machines to neural architecture for Universal Turing Machines, and develops a new class of MANNs that can store and query both the weights and data of their own controllers, thereby following the stored-program principle.",
        "Abstract": "Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks. ",
        "Introduction": "  Introduction Recurrent Neural Networks (RNNs) are Turing-complete (Siegelmann & Sontag, 1995). However, in practice RNNs struggle to learn simple procedures as they lack explicit memory (Graves et al., 2014; Mozer & Das, 1993). These findings have sparked a new research direc- tion called Memory Augmented Neural Networks (MANNs) that emulate modern computer behavior by detaching memorization from computation via memory and controller network, respectively. MANNs have demonstrated significant improvements over memory-less RNNs in various sequential learning tasks (Graves et al., 2016; Le et al., 2018a; Sukhbaatar et al., 2015). Nonetheless, MANNs have barely simulated general-purpose computers. Current MANNs miss a key concept in computer design: stored-program memory. The concept has emerged from the idea of Universal Turing Machine (UTM) (Turing, 1936) and further developed in Harvard Architecture (Broesch, 2009), Von Neumann Architecture (von Neumann, 1993). In UTM, both data and programs that manipulate the data are stored in memory. A control unit then reads the programs from the memory and executes them with the data. This mechanism allows flexibility to perform universal computations. Unfortunately, current MANNs such as Neural Turing Machine (NTM) (Graves et al., 2014), Differentiable Neural Computer (DNC) (Graves et al., 2016) and Least Recently Used Access (LRUA) (Santoro et al., 2016) only support memory for data and embed a single program into the controller network, which goes against the stored-program memory principle. Our goal is to advance a step further towards UTM by coupling a MANN with an external program memory. The program memory co-exists with the data memory in the MANN, providing more flexibility, reuseability and modularity in learning complicated tasks. The program memory stores the weights of the MANN's controller network, which are retrieved quickly via a key-value attention mechanism across timesteps yet updated slowly via back- propagation. By introducing a meta network to moderate the operations of the program memory, our model, henceforth referred to as Neural Stored-program Memory (NSM), can learn to switch the programs/weights in the controller network appropriately, adapting to different functionalities aligning with different parts of a sequential task, or different tasks in continual and few-shot learning. To validate our proposal, the NTM armed with NSM, namely Neural Universal Turing Machine (NUTM), is tested on a variety of synthetic tasks including algorithmic tasks from Published as a conference paper at ICLR 2020 Graves et al. (2014), composition of algorithmic tasks and continual procedure learning. For these algorithmic problems, we demonstrate clear improvements of NUTM over NTM. Further, we investigate NUTM in few-shot learning by using LRUA as the MANN and achieve notably better results. Finally, we expand NUTM application to linguistic problems by equipping NUTM with DNC core and achieve competitive performances against state- of-the-arts in the bAbI task (Weston et al., 2015). Taken together, our study advances neural network simulation of Turing Machines to neural architecture for Universal Turing Machines. This develops a new class of MANNs that can store and query both the weights and data of their own controllers, thereby following the stored-program principle. A set of five diverse experiments demonstrate the computational universality of the approach.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel meta-learning framework, Automated Relational Meta-Learning (ARML), to address the challenge of task heterogeneity in meta-learning. ARML automatically builds a meta-knowledge graph from meta-training tasks to memorize and organize learned knowledge from historical tasks. When a new task arrives, it queries the meta-knowledge graph and quickly attends to the most relevant entities (vertices), and then takes advantage of the relational knowledge structures between them to boost the learning effectiveness with the limited training data. The proposed ARML is empirically shown to outperform the state-of-the-art meta-learning algorithms, and the meta-knowledge graph well captures the relationship among tasks and improves the interpretability of meta-learning algorithms.",
        "Abstract": "In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines.",
        "Introduction": "  INTRODUCTION Learning quickly is the key characteristic of human intelligence, which remains a daunting problem in machine intelligence. The mechanism of meta-learning is widely used to generalize and transfer prior knowledge learned from previous tasks to improve the effectiveness of learning on new tasks, which has benefited various applications, such as computer vision (Kang et al., 2019; Liu et al., 2019), natural language processing (Gu et al., 2018; Lin et al., 2019) and social good (Zhang et al., 2019; Yao et al., 2019a). Most of existing meta-learning algorithms learn a globally shared meta-learner (e.g., parameter initialization (Finn et al., 2017; 2018), meta-optimizer (Ravi & Larochelle, 2016), metric space (Snell et al., 2017; Garcia & Bruna, 2017; Oreshkin et al., 2018)). However, globally shared meta-learners fail to handle tasks lying in different distributions, which is known as task heterogeneity (Vuorio et al., 2018; Yao et al., 2019b). Task heterogeneity has been regarded as one of the most challenging issues in meta-learning, and thus it is desirable to design meta-learning models that effectively optimize each of the heterogeneous tasks. The key challenge to deal with task heterogeneity is how to customize globally shared meta-learner by using task-specific information? Recently, a handful of works try to solve the problem by learning a task-specific representation for tailoring the transferred knowledge to each task (Oreshkin et al., 2018; Vuorio et al., 2018; Lee & Choi, 2018). However, the expressiveness of these methods is limited due to the impaired knowledge generalization between highly related tasks. Recently, learning the underlying structure across tasks provides a more effective way for balancing the customization and generalization. Representatively, Yao et al. propose a hierarchically structured meta-learning method to customize the globally shared knowledge to each cluster (Yao et al., 2019b). Nonetheless, the hierarchical clustering structure completely relies on the handcrafted design which needs to be tuned carefully and may lack the capability to capture complex relationships. Hence, we are motivated to propose a framework to automatically extract underlying relational structures from historical tasks and leverage those relational structures to facilitate knowledge Published as a conference paper at ICLR 2020 customization on a new task. This inspiration comes from the way of structuring knowledge in knowledge bases (i.e., knowledge graphs). In knowledge bases, the underlying relational structures across text entities are automatically constructed and applied to a new query to improve the searching efficiency. In the meta-learning problem, similarly, we aim at automatically establishing the meta- knowledge graph between prior knowledge learned from previous tasks. When a new task arrives, it queries the meta-knowledge graph and quickly attends to the most relevant entities (vertices), and then takes advantage of the relational knowledge structures between them to boost the learning effectiveness with the limited training data. The proposed meta-learning framework is named as Automated Relational Meta-Learning (ARML). Specifically, the ARML automatically builds the meta-knowledge graph from meta-training tasks to memorize and organize learned knowledge from historical tasks, where each vertex represents one type of meta-knowledge (e.g., the common contour between birds and aircrafts). To learn the meta-knowledge graph at meta-training time, for each task, we construct a prototype-based relational graph for each class, where each vertex represents one prototype. The prototype-based relational graph not only captures the underlying relationship behind samples, but alleviates the potential effects of abnormal samples. The meta-knowledge graph is then learned by summarizing the information from the corresponding prototype-based relational graphs of meta-training tasks. After constructing the meta-knowledge graph, when a new task comes in, the prototype-based relational graph of the new task taps into the meta-knowledge graph for acquiring the most relevant knowledge, which further enhances the task representation and facilitates its training process. Our major contributions of the proposed ARML are three-fold: (1) it automatically constructs the meta-knowledge graph to facilitate learning a new task; (2) it empirically outperforms the state-of- the-art meta-learning algorithms; (3) the meta-knowledge graph well captures the relationship among tasks and improves the interpretability of meta-learning algorithms.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a Fast Neural Network Adaptation (FNA) method based on a novel parameter remapping paradigm to adapt both the architecture and parameters of one network to a new task with negligible cost. The adaptation is performed on both the architecture- and parameter-level, using neural architecture search (NAS) methods. Experiments on both segmentation and detection tasks demonstrate FNA's effectiveness and efficiency, surpassing both manually designed and NAS searched networks in terms of both performance and model MAdds. Compared to NAS methods, FNA costs significantly less.",
        "Abstract": "Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art~(SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737$\\times$ less than DPC, 6.8$\\times$ less than Auto-DeepLab and 7.4$\\times$ less than DetNAS. The code is available at https://github.com/JaminFong/FNA .",
        "Introduction": "  INTRODUCTION Deep convolutional neural networks have achieved great successes in computer vision tasks such as image classification ( Krizhevsky et al., 2012 ;  He et al., 2016 ;  Howard et al., 2017 ), semantic segmentation (Long et al., 2015; Ronneberger et al., 2015;  Chen et al., 2017b ) and object detec- tion ( Ren et al., 2015 ;  Liu et al., 2016 ;  Lin et al., 2017 ) etc. Image classification has always served as a fundamental task for neural architecture design. It is common to use networks designed and pre-trained on the classification task as the backbone and fine-tune them for segmentation or detec- tion tasks. However, the backbone plays an important role in the performance on these tasks and the difference between these tasks calls for different design principles of the backbones. For example, segmentation tasks require high-resolution features and object detection tasks need to make both localization and classification predictions from each convolutional feature. Such distinctions make neural architectures designed for classification tasks fall short. Some attempts ( Li et al., 2018 ;  Wang et al., 2019 ) have been made to tackle this problem. Handcrafted neural architecture design is inefficient, requires a lot of human expertise, and may not find the best-performing networks. Recently, neural architecture search (NAS) methods ( Zoph et al., 2017 ;  Pham et al., 2018 ;  Liu et al., 2018 ) see a rise in popularity. Some works ( Liu et al., Published as a conference paper at ICLR 2020  As ImageNet ( Deng et al., 2009 ) pre-training has been a standard practice for many computer vi- sion tasks, there are lots of models trained on ImageNet available in the community. To take full advantages of these models, we propose a Fast Neural Network Adaptation (FNA) method based on a novel parameter remapping paradigm. Our method can adapt both the architecture and parameters of one network to a new task with negligible cost.  Fig. 1  shows the whole framework. The adapta- tion is performed on both the architecture- and parameter-level. We adopt the NAS methods ( Zoph et al., 2017 ;  Real et al., 2018 ;  Liu et al., 2019b ) to implement the architecture-level adaptation. We select a manually designed network (MobileNetV2 ( Sandler et al., 2018 ) in our experiments) as the seed network, which is pre-trained on ImageNet. Then, we expand the seed network to a super network which is the representation of the search space in FNA. New parameters in the super net- work are initialized by mapping those from the seed network using parameter remapping. Thanks to that, the neural architecture search can be performed efficiently on the detection and segmentation tasks. With FNA we obtain a new optimal target architecture for the new task. Similarly, we remap the parameters of the seed network to the target architecture for initialization and fine-tune it on the target task with no need of pre-training on a large-scale dataset. We demonstrate FNA's effectiveness and efficiency via experiments on both segmentation and de- tection tasks. We adapt the manually designed network MobileNetV2 ( Sandler et al., 2018 ) to seg- mentation framework DeepLabv3 ( Chen et al., 2017b ), detection framework RetinaNet ( Lin et al., 2017 ) and SSDLite ( Liu et al., 2016 ;  Sandler et al., 2018 ). Networks adapted by FNA surpass both manually designed and NAS searched networks in terms of both performance and model MAdds. Compared to NAS methods, FNA costs 1737× less than DPC ( Chen et al., 2018a ), 6.8× less than Auto-DeepLab ( Liu et al., 2019a ) and 7.4× less than DetNAS ( Chen et al., 2019b ).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the influence of misclassified and correctly classified examples on the final robustness of deep neural networks (DNNs) when trained using adversarial training. We find that the manipulation of misclassified examples has more impact on the final robustness, and the minimization techniques are more crucial than maximization ones under the min-max optimization framework. We propose a regularized adversarial risk which incorporates an explicit differentiation of misclassified examples as a regularizer, and a new defense algorithm, called Misclassification Aware adveRsarial Training (MART). Experimentally, we show that adversarial robustness can be significantly improved over the state-of-the-art, by a specific focus on misclassified examples.",
        "Abstract": "Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, we investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. Specifically, we find that misclassified examples indeed have a significant impact on the final robustness. More surprisingly, we find that different maximization techniques on misclassified examples may have a negligible influence on the final robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called {\\em Misclassification Aware adveRsarial Training} (MART), which explicitly differentiates the misclassified and correctly classified examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could significantly improve the state-of-the-art adversarial robustness.",
        "Introduction": "  INTRODUCTION Despite their great success in applications such as computer vision (He et al., 2016), speech recogni- tion (Wang et al., 2017) and natural language processing (Devlin et al., 2018; Zeng et al., 2019), deep neural networks (DNNs) are extremely vulnerable to adversarial examples crafted by adding small adversarial perturbations to natural examples (Szegedy et al., 2013; Goodfellow et al., 2015; Wu et al., 2020). Given a DNN classifier h θ with parameter θ and a correctly classified natural example x with class label y (h θ (x) = y), an adversarial example x can be generated by perturbing x such that h θ (x ) = y, i.e., the natural example is correctly classified before perturbation but misclassified after perturbation. The perturbation required for misclassification is often small and bounded by an L p -norm x − x p ≤ , which keeps x within the -ball centered at x, so that it is visually the \"same\" for human observers. This vulnerability of DNNs raises serious security concerns about their practicability in security critical applications (Chen et al., 2015adversarial training has been demonstrated to be the most effective (Athalye et al., 2018). Adversarial training can be regarded as a data augmentation technique that trains DNNs on adversarial examples, and can be viewed as solving the following min-max optimization problem (Madry et al., 2018): min θ 1 n n i=1 max x i −xi p ≤ (h θ (x i ), y i ), (1) where n is the number of training examples and (·) is the classification loss, such as the commonly used cross-entropy (CE) loss. The inner maximization generates adversarial examples that can be used by the outer minimization to train robust DNNs. Recently, adversarial training with adversarial examples generated by Projected Gradient Descent (PGD) (Madry et al., 2018) has been demonstrated to be the only method that can train moderately robust DNNs without being fully attacked (Athalye et al., 2018). However, there is still a significant gap between adversarial robustness (test accuracy on adversarial examples) and natural accuracy (test accuracy on natural examples), even for simple image datasets like CIFAR-10 (Krizhevsky & Hinton, 2009). Compared with natural training (on natural examples), training adversarially robust DNNs is partic- ularly difficult (Madry et al., 2018). Nakkiran (2019) showed that a model requires more capacity to be robust (i.e., simple models can have high natural accuracy but are less likely to be robust). In addition, the sample complexity of adversarial training can be significantly higher than that of natural training, that is, training robust DNNs tends to require more data either labeled (Schmidt et al., 2018) or unlabeled ones (Uesato et al., 2019; Carmon et al., 2019; Najafi et al., 2019; Zhai et al., 2019). Moreover, Tsipras et al. (2019); Zhang et al. (2019) demonstrated that adversarial robustness may be inherently at odds with natural accuracy. Parallel to these studies, in this paper, we provide some new insights on the adversarial examples used for adversarial training. Recall that the formal definition of an adversarial example is conditioned on it being correctly classi- fied * (Carlini et al., 2019). From this perspective, adversarial examples generated from misclassified examples are \"undefined\". Most adversarial training variants neglect this distinction, where all training examples are treated equally in both the maximization and the minimization processes, regardless of whether or not they are correctly classified. The only exception we are aware of is Ding et al. (2018), which proposes to use maximal margin optimization for correctly classified examples. Yet they did not pay sufficient attention to misclassified examples. A deeper understanding about the influence of misclassified and correctly classified examples on the robustness is still missing in the literature. Therefore, we raise the following questions: Are the adversarial examples generated from i) misclassified and ii) correctly classified examples, equally important for adversarial robustness? If not, how can one make better use of the difference to improve robustness? In this paper, we investigate this intriguing, yet thus far overlooked aspect of adversarial training, and find that misclassified and correctly classified examples exhibit a distinctive influence on the final robustness. To illustrate this phenomenon, we conduct a proof-of-concept experiment on CIFAR- 10 in a white-box setting with L ∞ maximum perturbation = 8/255. We first train an 8-layer Published as a conference paper at ICLR 2020 Convolutional Neural Network (CNN) using standard adversarial training with 10-step PGD (PGD 10 ) and step size /4, then use this network (87% training accuracy) to select two subsets of natural training examples to investigate: 1) a subset of misclassified examples S − (13% of training data), and 2) a subset of correctly classified examples S + (also 13% of training data, |S + | = |S − |). Using these two subsets, we explore different ways to re-train the same network, and evaluate its robustness against white-box PGD 20 (step size /10) attacks on the test dataset. In Figure 1(a), we find that misclassified examples have a significant impact on the final robustness. Compared with standard adversarial training (dashed blue line), the final robustness drops drastically, if examples in subset S − are not perturbed (solid green line) during adversarial training (other examples are still perturbed by PGD 10 ). In contrast, the same operation on subset S + only slightly affects the final robustness (solid orange line). Previous work has found that removing a small proportion of training examples does not reduce the robustness (Ding et al., 2019), which seems to be true for correctly classified examples, but is apparently not true for misclassified examples. To further understand the distinctive influence of misclassified and correctly classified examples, we test different techniques on them within either the maximization or the minimization process of adversarial training. Firstly, we apply different maximization techniques while keeping the minimization loss CE unchanged. As shown in Figure 1(b), the final robustness is barely affected when we use a weak attack (e.g., Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015)) to perturb misclassified examples S − (all other training examples are still perturbed by PGD 10 ). This suggests that different maximization techniques on misclassified examples S − may have a negligible influence on the final robustness, provided that the inner maximization problem is solved to a moderate precision (Wang et al., 2019). However, for subset S + , a weak attack for the maximization tends to degenerate the robustness. Secondly, we test different minimization techniques with the inner maximization still solved by PGD 10 . Interestingly, we find that different minimization techniques on misclassified examples make a significant difference to the final robustness. As shown in Figure 1(c), compared with standard adversarial training (dashed blue line) with the CE loss, the final robustness is significantly improved when the outer minimization on misclassified examples is \"regularized\" (solid green line) by an additional term (a KL-divergence term that was used previously in Zheng et al. (2016); Zhang et al. (2019)). The same regularization applied to correctly classified examples also helps the final robustness (solid orange line), though not as significantly as for misclassified examples. Motivated by the above observations, we reformulate the adversarial risk to incorporate an explicit differentiation of misclassified examples in a form of regularization. We then propose a new defense algorithm to achieve this in a dynamic way during adversarial training. Our main contributions are: • We investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. We find that the manipulation on misclassified examples has more impact on the final robustness, and the minimization techniques are more crucial than maximization ones under the min-max optimization framework. • We propose a regularized adversarial risk which incorporates an explicit differentiation of misclas- sified examples as a regularizer. Based on that, we further propose a new defense algorithm, called Misclassification Aware adveRsarial Training (MART). • Experimentally, we show that adversarial robustness can be significantly improved over the state- of-the-art, by a specific focus on misclassified examples. It also helps improve recently proposed adversarial training with unlabeled data.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a generic framework that leverages matrix estimation to exploit the low-rank structures of Markov Decision Processes (MDPs) in both classical planning and modern deep reinforcement learning (RL). We demonstrate the effectiveness of our approach on classical stochastic control tasks, where the low-rank structure allows for efficient planning with less computation. We extend our scheme to deep RL, which is applicable for value-based techniques such as DQN, double DQN, and dueling DQN. Experimental results on all Atari games show that our approach, Structured Value-based Deep RL (SV-RL), can consistently improve the performance of value-based methods, achieving higher scores for tasks when low-rank structures are confirmed to exist.",
        "Abstract": "Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on \"low-rank\" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach.",
        "Introduction": "  INTRODUCTION Value-based methods are widely used in control, planning, and reinforcement learning ( Gorodetsky et al., 2018 ;  Alora et al., 2016 ;  Mnih et al., 2015 ). To solve a Markov Decision Process (MDP), one common method is value iteration, which finds the optimal value function. This process can be done by iteratively computing and updating the state-action value function, represented by Q(s, a) (i.e., the Q-value function). In simple cases with small state and action spaces, value iteration can be ideal for efficient and accurate planning. However, for modern MDPs, the data that encodes the value function usually lies in thousands or millions of dimensions ( Gorodetsky et al., 2018 ;  2019 ), including images in deep reinforcement learning ( Mnih et al., 2015 ;  Tassa et al., 2018 ). These practical constraints significantly hamper the efficiency and applicability of the vanilla value iteration. Yet, the Q-value function is intrinsically induced by the underlying system dynamics. These dynamics are likely to possess some structured forms in various settings, such as being governed by partial differential equations. In addition, states and actions may also contain latent features (e.g., similar states could have similar optimal actions). Thus, it is reasonable to expect the structured dynamic to impose a structure on the Q-value. Since the Q function can be treated as a giant matrix, with rows as states and columns as actions, a structured Q function naturally translates to a structured Q matrix. In this work, we explore the low-rank structures. To check whether low-rank Q matrices are common, we examine the benchmark Atari games, as well as 4 classical stochastic control tasks. As we demonstrate in Sections 3 and 4, more than 40 out of 57 Atari games and all 4 control tasks exhibit low-rank Q matrices. This leads us to a natural question: How do we leverage the low-rank structure in Q matrices to allow value-based techniques to achieve better performance on \"low-rank\" tasks? We propose a generic framework that allows for exploiting the low-rank structure in both classical planning and modern deep RL. Our scheme leverages Matrix Estimation (ME), a theoretically guar- anteed framework for recovering low-rank matrices from noisy or incomplete measurements (Chen & Chi, 2018). In particular, for classical control tasks, we propose Structured Value-based Planning (SVP). For the Q matrix of dimension |S| × |A|, at each value iteration, SVP randomly updates a small portion of the Q(s, a) and employs ME to reconstruct the remaining elements. We show that planning problems can greatly benefit from such a scheme, where fewer samples (only sample around 20% of (s, a) pairs at each iteration) can achieve almost the same performance as the optimal policy. For more advanced deep RL tasks, we extend our intuition and propose Structured Value-based Deep RL (SV-RL), applicable for deep Q-value based methods such as DQN ( Mnih et al., 2015 ). Here, instead of the full Q matrix, SV-RL naturally focuses on the \"sub-matrix\", corresponding to the sampled batch of states at the current iteration. For each sampled Q matrix, we again apply ME to represent the deep Q learning target in a structured way, which poses a low rank regularization on this \"sub-matrix\" throughout the training process, and hence eventually the Q-network's predictions. Intuitively, as learning a deep RL policy is often noisy with high variance, if the task possesses a low-rank property, this scheme will give a clear guidance on the learning space during training, after which a better policy can be anticipated. We confirm that SV-RL indeed can improve the performance of various value-based methods on \"low-rank\" Atari games: SV-RL consistently achieves higher scores on those games. Interestingly, for complex, \"high-rank\" games, SV-RL performs comparably. ME naturally seeks solutions that balance low rank and a small reconstruction error (cf. Section 3.1). Such a balance on reconstruction error helps to maintain or only slightly degrade the performance for \"high-rank\" situation. We summarize our contributions as follows: • We are the first to propose a framework that leverages matrix estimation as a general scheme to exploit the low-rank structures, from planning to deep reinforcement learning. • We demonstrate the effectiveness of our approach on classical stochastic control tasks, where the low-rank structure allows for efficient planning with less computation. • We extend our scheme to deep RL, which is naturally applicable for value-based techniques. Across a variety of methods, such as DQN, double DQN, and dueling DQN, experimental results on all Atari games show that SV-RL can consistently improve the performance of value-based methods, achieving higher scores for tasks when low-rank structures are confirmed to exist.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a learning-based agglomerative clustering framework for zero-shot part discovery on 3D shapes. The framework learns to group sub-parts and gradually increase recognition context, while restricting the features to convey information within the local context of a part. Experiments on the PartNet dataset demonstrate the state-of-the-art results for part discovery in unseen categories.",
        "Abstract": "We address the problem of learning to discover 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to novel categories. On a recently proposed large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen testing categories without seeing any annotated samples. Quantitative comparisons against four strong shape segmentation baselines show that we achieve the state-of-the-art performance.",
        "Introduction": "  INTRODUCTION Perceptual grouping has been a long-standing problem in the study of vision systems (Hoffman & Richards, 1984). The process of perceptual grouping determines which regions of the visual input belong together as parts of higher-order per- ceptual units. Back to the 1930s, Wertheimer (1938) listed several vital factors, such as similarity, proximity, and good continuation, which lead to visual grouping. To this era of deep learning, grouping cues can be learned from massive annotated datasets. However, compared with human visual system, these learning-based segmentation algorithms are far inferior for objects from unknown categories. We are interested in attacking a specific problem of this kind - zero-shot part discovery for 3D shapes. We choose to study the zero-shot learning problem on 3D shape data instead of 2D image data, because part-level similarity across object cate- gories in 3D is more salient and less affected by various dis- tortions introduced in the imaging process. To motive our approach, we first review the key idea and limitation of existing 3D part segmenta- tion methods. With the power of big data, deep neural networks that learn data-driven features to segment shape parts, such as (Kalogerakis et al., 2010; Graham et al., 2018; Mo et al., 2019b), have demonstrated the state-of-the-art performance on many shape segmentation benchmarks (Yi et al., 2016; Mo et al., 2019b). These networks usually have large receptive fields that cover the whole input shape, so that global context can be leveraged to improve the recognition of part semantics and shape structures. While learning such features leads to superior performance on the training Published as a conference paper at ICLR 2020 categories, they often fail miserably on unseen categories ( Figure 1 ) due to the difference of global shapes. On the contrary, classical shape segmentation methods, such as (Kaick et al., 2014) that use manually designed features with relatively local context, can often perform much better on unseen object categories, although they tend to give inferior segmentation results on training categories ( Table 1 ). In fact, many globally different shapes share similar part-level structures. For example, airplanes, cars, and swivel chairs all have wheels, even though their global geometries are totally different. Having learned the geometry of wheels from airplanes should help recognize wheels for cars and swivel chairs. In this paper, we aim to invent a learning-based framework that will by design avoid using exces- sive context information that hurts cross-category generalization. We start from proposing a pool of superpixel-like sub-parts for each shape. Then, we learn a grouping policy that seeks to pro- gressively group sub-parts and gradually increase recognition context. What lies in the heart of our algorithm is to learn a function to assess whether two parts should be grouped. Different from prior deep segmentation work that learns point features for segmentation mask prediction, our formula- tion essentially learns part-level features. Borrowing ideas from Reinforcement Learning (RL), we formalize the process as a contextual bandit problem and train a local grouping policy to iteratively pick a pair of most promising sub-parts for grouping. In this way, we restrict that our features only convey information within the local context of a part. Our learning-based agglomerative clustering framework deviates drastically from the prevailing deep segmentation pipelines and makes one step towards generalizable part discovery in unseen object categories. To summarize, we make the following contributions: • We formulate the task of zero-shot part discovery on the large-scale fine-grained 3D part dataset PartNet (Mo et al., 2019b); • We propose a learning-based agglomerative clustering framework that learns to group for proposing parts from training categories and generalizes to unseen categories; • We quantitatively compare our approach to four baseline methods and demonstrate the state-of-the-art results for part discovery in unseen categories.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel distributed backdoor attack (DBA) strategy against federated learning (FL). DBA decomposes a global trigger pattern into local patterns and embeds them to different adversarial parties respectively. Through extensive experiments on financial and image datasets, we show that DBA is more persistent and effective than centralized backdoor attack. We also evaluate the robustness of two recent robust FL methods against DBA and find that DBA is more effective and stealthy. We provide in-depth explanations for the effectiveness of DBA from different perspectives and perform comprehensive analysis and ablation studies on several trigger factors. To the best of our knowledge, this paper is the first work studying distributed backdoor attacks.",
        "Abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking.\nTo further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.",
        "Introduction": "  INTRODUCTION they embed the same global trigger pattern to all adversarial parties. We call such attacking scheme centralized backdoor attack. Leveraging the power of FL in aggregating dispersed information from local parties to train a shared model, in this paper we propose distributed backdoor attack (DBA) against FL. Given the same global trigger pattern as the centralized attack, DBA decomposes it into local patterns and embed them to different adversarial parties respectively. A schematic comparison between the centralized and distributed backdoor attacks is illustrated in Fig.1. Through extensive experiments on several financial and image datasets and in-depth analysis, we summarize our main contributions and findings as follows. • We propose a novel distributed backdoor attack strategy DBA on FL and show that DBA is more persistent and effective than centralized backdoor attack. Based on extensive experiments, we report a prominent phenomenon that although each adversarial party is only implanted with a local trigger pattern via DBA, their assembled pattern (i.e., global trigger) attains significantly better attack performance on the global model compared with the centralized attack. The results are consistent across datasets and under different attacking scenarios such as one-time (single-shot) and continuous (multiple-shot) poisoning settings. To the best of our knowledge, this paper is the first work studying distributed backdoor attacks. • When evaluating the robustness of two recent robust FL methods against centralized backdoor attack (Fung et al., 2018; Pillutla et al., 2019), we find that DBA is more effective and stealthy, as its local trigger pattern is more insidious and hence easier to bypass the robust aggregation rules. • We provide in-depth explanations for the effectiveness of DBA from different perspectives, including feature visual interpretation and feature importance ranking. • We perform comprehensive analysis and ablation studies on several trigger factors in DBA, including the size, gap, and location of local triggers, scaling effect in FL, poisoning interval, data poisoning ratio, and data distribution.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new generalization bound for Recurrent Neural Networks (RNNs) based on Fisher-Rao norm and matrix 1-norm. The bound has no explicit dependence on the size of networks and applies to general types of noises. The paper also proposes a new technique to decompose RNNs with ReLU activation into a sum of linear network and difference terms, which can potentially be applied to other neural networks architectures. The results of this paper provide insight into the generalization performance of RNNs and can potentially explain the effect of noise training on generalization of recurrent neural networks.",
        "Abstract": "In this work, we develop the theory for analyzing the generalization performance of recurrent neural networks. We first present a new generalization bound for recurrent neural networks based on matrix 1-norm and Fisher-Rao norm. The definition of Fisher-Rao norm relies on a structural lemma about the gradient of RNNs. This new generalization bound assumes that the covariance matrix of the input data is positive definite, which might limit its use in practice. To address this issue, we propose to add random noise to the input data and prove a generalization bound for training with random noise, which is an extension of the former one. Compared with existing results, our generalization bounds have no explicit dependency on the size of networks. We also discover that Fisher-Rao norm for RNNs can be interpreted as a measure of gradient, and incorporating this gradient measure not only can tighten the bound, but allows us to build a relationship between generalization and trainability. Based on the bound, we theoretically analyze the effect of covariance of features on generalization of RNNs and discuss how weight decay and gradient clipping in the training can help improve generalization. ",
        "Introduction": "  INTRODUCTION The Recurrent Neural network (RNN) is a neural sequence model that has achieved the state-of- the-art performance on numerous tasks, including natural language processing ( Yang et al., 2018 ;  Mikolov & Zweig, 2012 ), speech recognition ( Chiu et al., 2018 ;  Graves, 2013 ) and machine transla- tion ( Wu et al., 2016 ;  Kalchbrenner & Blunsom, 2013 ). Unlike feedforward neural networks, RNNs allow connections among hidden units associated with a time delay. Through these connections, RNNs can maintain a \"memory\" that summarizes the past sequence of inputs, enabling it to capture correlations between temporally distant events in the data. RNNs are very powerful, and empirical studies have shown that they have a very good generaliza- tion property. For example, Graves (2013) showed that deep LSTM RNNs achieved a test error of 17.7% on TIMT phoneme recognition benchmark after training with only 462 speech samples. Despite of the popularity of RNNs in practice, their theory is still not well understood. A number of recent works have sought to shed light on the effective representational properties of recurrent networks trained in practice. For example, Oymak (2018) studied the state equation of recurrent neural networks and showed that SGD can efficiently learn the unknown dynamics from few obser- vations under proper assumptions.  Miller & Hardt (2019)  tried to explain why feed-forward neural networks are competitive with recurrent networks in practice. They identified stability as a necessary condition and proved that stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Despite of the impres- sive progress in understanding the training behavior of RNNs, there is no generalization guarantee in these works. Understanding the generalization performance in machine learning has been a central problem for many years and revived in recent years with the advent of deep learning. One classical approach to proving generalization bound is via notions of complexity. For deep neural networks, numer- ous complexity measures have been proposed to capture the generalization behavior such as VC Published as a conference paper at ICLR 2020 dimension ( Harvey et al., 2017 ) and norm-based capacity including spectral norm ( Bartlett et al., 2017 ;  Neyshabur et al., 2019 ), Frobenius norm ( Neyshabur et al., 2015b ;a;  2018 ) and l p -path norm ( Neyshabur et al., 2015b ;  Bartlett & Mendelson, 2002 ;  Golowich et al., 2018 ). These existing norm- based complexity measures depend on the number of hidden units of the network explicitly and thus can not explain why neural networks generalize so well in practice, despite that they operate in an overparametrized setting ( Zhang et al., 2017 ).  Neyshabur et al. (2019)  proved generalization bounds for two layer ReLU feedforward networks, which decreased with the increasing number of hidden unit in the network. However their results only applied to two layer ReLU networks and some specific experiments. More recently, a new generalization bound based on Fisher-Rao norm was proposed ( Liang et al., 2017 ). This notion of Fisher-Rao norm is motivated by information geometry and has good invariance properties. But they proved the bound only for linear deep neural networks. There are also some works about the generalization of recurrent neural networks ( Zhang et al., 2018 ;  Chen et al., 2019 ;  Allen-Zhu & Li, 2019 ). However these bounds also depend on the size of networks, which makes them vacuous for very large neural networks. Our main contributions are summarized as follows. • We define the Fisher-Rao norm for RNNs based on its gradient structure and derive new Rademacher complexity bound and generalization bound for recurrent neural networks based on Fisher-Rao norm and matrix 1-norm. In contrast to existing results such as spec- tral norm-based bounds, our bound has no explicit dependence on the size of networks. • We prove a generalization bound for RNNs when training with random noises. Our bound applies to general types of noises and can potentially explain the effect of noise training on generalization of recurrent neural networks as demonstrated by our empirical results. • We propose a new technique to decompose RNNs with ReLU activation into a sum of linear network and difference terms. As a result, each term in the decomposition can be treated independently and easily when estimating the Rademacher complexity. This decomposi- tion technique can potentially be applied to other neural networks architectures such as convolutional neural networks, which might be of independent interest. The remainder of this paper is structured as follows. We define the problem and notations in Sec- tion 2. The notion of Fisher-Rao norm for RNNs is introduced in Section 3.1. We prove the general- ization bound for RNNs in Section 3.2, and the generalization bound for training with random noise is derived in Section 3.3. Section 3.4 gives a detailed analysis of our generalization bound. Finally we conclude and discuss future directions.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a partially policy-agnostic method, EMP, for infinite-horizon off-policy policy evaluation with multiple known or unknown behavior policies. EMP includes a pre-estimation step using a parametric model to learn a \"virtual\" policy (the mixture policy) and obtains OPPE via learning the state stationary distribution correction. We provide a theoretic guarantee that EMP yields smaller mean square error (MSE) than the policy-aware methods in stationary distribution corrections learning, even in the single-behavior policy setting. EMP is compared with both policy-aware and policy-agnostic methods in a set of continuous and discrete control tasks and shows significant improvement.",
        "Abstract": "We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods.",
        "Introduction": "  INTRODUCTION In many real-world decision-making scenarios, evaluating a novel policy by directly executing it in the environment is generally costly and can even be downright risky. Examples include evaluating a recommendation policy ( Swaminathan et al., 2017 ;  Zheng et al., 2018 ), a treatment policy ( Hirano et al., 2003 ;  Murphy et al., 2001 ), and a traffic light control policy ( Van der Pol & Oliehoek, 2016 ). Off-policy policy evaluation methods (OPPE) utilize a set of previously-collected trajectories (for example, website interaction logs, patient trajectories, or robot trajectories) to estimate the value of a novel decision-making policy without interacting with the environment ( Precup et al., 2001 ;  Dudík et al., 2011 ). For many reinforcement learning applications, the value of the decision is defined in a long- or infinite-horizon, which makes OPPE more challenging. The state-of-the-art methods for infinite-horizon off-policy policy evaluation rely on learning (dis- counted) state stationary distribution corrections or ratios. In particular, for each state in the environments, these methods estimate the likelihood ratio of the long-term probability measure for the state to be visited in a trajectory generated by the target policy, normalized by the probability measure generated by the behavior policy. This approach can effectively avoid the exponentially high variance compared to the more classic importance sampling (IS) estimation methods (pre;  Dudík et al., 2011 ;  Hirano et al., 2003 ;  Wang et al., 2017 ;  Murphy et al., 2001 ), especially for infinite-horizon policy evaluation ( Liu et al., 2018 ;  Nachum et al., 2019 ;  Hallak & Mannor, 2017 ). However, learning state stationary distribution requires detailed information on distributions of the behavior policy, and we call them policy-aware methods. As a consequence, policy-aware methods are difficult to apply Published as a conference paper at ICLR 2020 when off-policy data are pre-generated by multiple behavior policies or when the behavior policy's form is unknown. To address this issue,  Nachum et al. (2019)  proposes a policy-agnostic method, DualDice, which learns the joint state-action stationary distribution correction that is much higher dimension, and therefore needs more model parameters than the state stationary distribution. Besides, there is no theoretic comparison between policy-aware and policy-agnostic methods. In this paper, we propose a OPPE method with behavior policy learning, EMP (estimated mixture policy) for infinite-horizon off-policy policy evaluation with multiple known or unknown behavior policies. We call EMP a partially policy-agnostic method in the sense that, EMP does not require any information on each\"physical\" behavior policy, instead, it utilizes some aggregated information of the behavior policies learned from data. In detail, EMP includes a pre-estimation step using certain parametric model to learn a \"virtual\" policy (we call it the mixture policy and formally define it in Section 4). Hence, its performance depends on the accuracy of mixture policy estimation. Like the method in  Liu et al. (2018) , EMP obtain OPPE also via learning the state stationary distribution correction, so it remains computationally cheap and is scalable in terms of the number of behavior policies. Besides, inspired by  Hanna et al. (2019) , we provide a theoretic guarantee that EMP yields smaller mean square error (MSE) than the policy-aware methods in stationary distribution corrections learning, even in the single-behavior policy setting. On the other hand, compared to DualDice, EMP learns the state stationary distribution correction of smaller dimension, more importantly the estimation of the mixture policy can be considered as an inductive bias as far as the stationary distribution correction is concerned, and hence could achieve better performance when the pre-estimation is not expensive. In addition, we propose an ad-hoc improvement of EMP, whose theoretical analysis is left for future studies. EMP is compared with both policy-aware and policy-agnostic methods in a set of continuous and discrete control tasks and shows significant improvement.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new approach to model reasoning in graph-structured contexts, such as knowledge base completion tasks. The proposed approach learns a dynamically induced subgraph which starts with a head node and ends with a predicted tail node. This approach is compared to existing embedding-based and path-based models, and is shown to provide more interpretable explanations of reasoning in graph contexts.",
        "Abstract": "We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, we learn an input-dependent subgraph to explicitly model a sequential reasoning process. Each subgraph is dynamically constructed, expanding itself selectively under a flow-style attention mechanism. In this way, we can not only construct graphical explanations to interpret prediction, but also prune message passing in Graph Neural Networks (GNNs) to scale with the size of graphs. We take the inspiration from the consciousness prior proposed by Bengio to design a two-GNN framework to encode global input-invariant graph-structured representation and learn local input-dependent one coordinated by an attention module. Experiments show the reasoning capability in our model that is providing a clear graphical explanation as well as predicting results accurately, outperforming most state-of-the-art methods in knowledge base completion tasks.",
        "Introduction": "  INTRODUCTION Modern deep learning systems should bring in explicit reasoning modeling to complement their black-box models, where reasoning takes a step-by-step form about organizing facts to yield new knowledge and finally draw a conclusion. Particularly, we rely on graph-structured representation to model reasoning by manipulating nodes and edges where semantic entities or relations can be explicitly represented (Battaglia et al., 2018). Here, we choose knowledge graph scenarios to study reasoning where semantics have been defined on nodes and edges. For example, in knowledge base completion tasks, each edge is represented by a triple head, rel, tail that contains two entities and their relation. The goal is to predict which entity might be a tail given query head, rel, ? . Existing models can be categorized into embedding-based and path-based model families. The embedding-based (Bordes et al., 2013; Sun et al., 2018; Lacroix et al., 2018) often achieves a high score by fitting data using various neural network techniques but lacks interpretability. The path- based (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Wang, 2018) attempts to construct an explanatory path to model an iterative decision-making process using reinforcement learning and recurrent networks. A question is: can we construct structured explanations other than a path to better explain reasoning in graph context. To this end, we propose to learn a dynamically induced subgraph which starts with a head node and ends with a predicted tail node as shown in  Figure 1 .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a general framework, SEED (Sampling, Encoding, and Embedding Distributions), for inductive and unsupervised representation learning on graph structured objects. SEED takes arbitrary graphs as input, where nodes and edges could have rich features, or have no features at all. SEED decomposes the reconstruction problem into two sub-problems: (1) How to efficiently autoencode and compare structural data in an unsupervised fashion? (2) How to measure the difference of two graphs in a tractable way? SEED focuses on a class of subgraphs whose encoding, decoding, and reconstruction errors can be evaluated in polynomial time, and utilizes deep architectures to efficiently autoencode these subgraphs. By embedding distribution of subgraph representations, SEED outputs a vector representation for an input graph, where distance between two graphs' vector representations reflects the distance between their subgraph distributions. The effectiveness of the SEED framework is evaluated via classification and clustering tasks on public benchmark datasets, and results show that graph representations generated by SEED are able to effectively capture structural information, and maintain stable performance even when the node attributes are not available.",
        "Abstract": "Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Instead of directly dealing with the computational challenges raised by graph similarity evaluation, given an input graph, the SEED framework samples a number of subgraphs whose reconstruction errors could be efficiently evaluated, encodes the subgraph samples into a collection of subgraph vectors, and employs the embedding of the subgraph vector distribution as the output vector representation for the input graph. By theoretical analysis, we demonstrate the close connection between SEED and graph isomorphism. Using public benchmark datasets, our empirical study suggests the proposed SEED framework is able to achieve up to 10% improvement, compared with competitive baseline methods.",
        "Introduction": "  INTRODUCTION Representation learning has been the core problem of machine learning tasks on graphs. Given a graph structured object, the goal is to represent the input graph as a dense low-dimensional vec- tor so that we are able to feed this vector into off-the-shelf machine learning or data manage- ment techniques for a wide spectrum of downstream tasks, such as classification (Niepert et al., 2016), anomaly detection (Akoglu et al., 2015), information retrieval (Li et al., 2019), and many others (Santoro et al., 2017b; Nickel et al., 2015). In this paper, our work focuses on learning graph representations in an inductive and unsupervised manner. As inductive methods provide high efficiency and generalization for making inference over unseen data, they are desired in critical applications. For example, we could train a model that encodes graphs generated from computer program execution traces into vectors so that we can perform malware detection in a vector space. During real-time inference, efficient encoding and the capability of processing unseen programs are expected for practical usage. Meanwhile, for real-life applications where labels are expensive or difficult to obtain, such as anomaly detection (Zong et al., 2018) and information retrieval (Yan et al., 2005), unsupervised methods could provide effective feature representations shared among different tasks. Inductive and unsupervised graph learning is challenging, even compared with its transductive or supervised counterparts. First, when inductive capability is required, it is inevitable to deal with the problem of node alignment such that we can discover common patterns across graphs. Second, in the case of unsupervised learning, we have limited options to design objectives that guide learning processes. To evaluate the quality of the learned latent representations, reconstruction errors are Published as a conference paper at ICLR 2020 commonly adopted. When node alignment meets reconstruction error, we have to answer a basic question: Given two graphs G 1 and G 2 , are they identical or isomorphic (Chartrand, 1977)? To this end, it could be computationally intractable to compute reconstruction errors (e.g., using graph edit distance (Zeng et al., 2009) as the metric) in order to capture detailed structural information. Previous deep graph learning techniques mainly focus on transductive (Perozzi et al., 2014) or su- pervised settings (Li et al., 2019). A few recent studies focus on autoencoding specific structures, such as directed acyclic graphs (Zhang et al., 2019), trees or graphs that can be decomposed into trees (Jin et al., 2018), and so on. From the perspective of graph generation, You et al. (2018) pro- pose to generate graphs of similar graph statistics (e.g., degree distribution), and Bojchevski et al. (2018) provide a GAN based method to generate graphs of similar random walks. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Dis- tributions) for inductive and unsupervised representation learning on graph structured objects. As shown in  Figure 1 , SEED consists of three major components: subgraph sampling, subgraph encod- ing, and embedding subgraph distributions. SEED takes arbitrary graphs as input, where nodes and edges could have rich features, or have no features at all. By sequentially going through the three components, SEED outputs a vector representation for an input graph. One can further feed such vector representations to off-the-shelf machine learning or data management tools for downstream learning or retrieval tasks. Instead of directly addressing the computational challenge raised by evaluation of graph reconstruc- tion errors, SEED decomposes the reconstruction problem into the following two sub-problems. Q1: How to efficiently autoencode and compare structural data in an unsupervised fashion? SEED focuses on a class of subgraphs whose encoding, decoding, and reconstruction errors can be eval- uated in polynomial time. In particular, we propose random walks with earliest visiting time (WEAVE) serving as the subgraph class, and utilize deep architectures to efficiently autoencode WEAVEs. Note that reconstruction errors with respect to WEAVEs are evaluated in linear time. Q2: How to measure the difference of two graphs in a tractable way? As one subgraph only covers partial information of an input graph, SEED samples a number of subgraphs to enhance information coverage. With each subgraph encoded as a vector, an input graph is represented by a collection of vectors. If two graphs are similar, their subgraph distribution will also be similar. Based on this intuition, we evaluate graph similarity by computing distribution distance between two collections of vectors. By embedding distribution of subgraph representations, SEED outputs a vector repre- sentation for an input graph, where distance between two graphs' vector representations reflects the distance between their subgraph distributions. Unlike existing message-passing based graph learning techniques whose expressive power is upper bounded by Weisfeiler-Lehman graph kernels (Xu et al., 2019; Shervashidze et al., 2011), we show the direct relationship between SEED and graph isomorphism in Section 3.5. We empirically evaluate the effectiveness of the SEED framework via classification and clustering tasks on public benchmark datasets. We observe that graph representations generated by SEED are able to effectively capture structural information, and maintain stable performance even when the node attributes are not available. Compared with competitive baseline methods, the proposed SEED framework could achieve up to 10% improvement in prediction accuracy. In addition, SEED Published as a conference paper at ICLR 2020 achieves high-quality representations when a reasonable number of small subgraph are sampled. By adjusting sample size, we are able to make trade-off between effectiveness and efficiency.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel Bayesian meta-learning framework, Bayesian Task-Adaptive Meta-Learning (Bayesian TAML), to optimally leverage meta-learning under various imbalances. This framework learns variables to adaptively balance the effect of meta- and task-specific learning, and is validated on realistic imbalanced few-shot classification tasks with a varying number of shots per task and class. Results show that Bayesian TAML significantly outperforms existing meta-learning models.",
        "Abstract": "While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution by relying on the meta-knowledge or task-specific learning. We formulate this objective into a Bayesian inference framework and tackle it using variational inference. We validate our Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) on two realistic task- and class-imbalanced datasets, on which it significantly outperforms existing meta-learning approaches. Further ablation study confirms the effectiveness of each balancing component and the Bayesian learning framework. ",
        "Introduction": "  INTRODUCTION Despite the success of deep learning in many real-world tasks such as visual recognition and machine translation, such good performances are achievable at the availability of large training data, and many fail to generalize well in small data regimes. To overcome this limitation of conventional deep learning, recently, researchers have explored meta-learning ( Schmidhuber, 1987 ;  Thrun & Pratt, 1998 ) approaches, whose goal is to learn a model that generalizes well over distribution of tasks, rather than instances from a single task, in order to utilize the obtained meta-knowledge across tasks to compensate for the lack of training data for each task. However, so far, most existing meta-learning approaches ( Santoro et al., 2016 ;  Vinyals et al., 2016 ;  Snell et al., 2017 ;  Ravi & Larochelle, 2017 ;  Finn et al., 2017 ;  Li et al., 2017 ) have only targeted an artificial scenario where all tasks participating in the multi-class classification problem have equal number of training instances per class. Yet, this is a highly restrictive setting, as in real-world scenarios, tasks that arrive at the model may have different training instances (task imbalance), and within each task, the number of training instances per class may largely vary (class imbalance). Moreover, the new task may come from a distribution that is different from the task distribution the model has been trained on (out-of-distribution task) (See (a) of  Figure 1 ). Under such a realistic setting, the meta-knowledge may have a varying degree of utility to each task. Tasks with small number of training data, or close to the tasks trained in meta-training step may want to rely mostly on meta-knowledge obtained over other tasks, whereas tasks that are out-of-distribution or come with more number of training data may obtain better solutions when trained in a task-specific manner. Furthermore, for multi-class classification, we may want to treat the learning for each class differently to handle class imbalance. Thus, to optimally leverage meta-learning under various imbalances, it would be beneficial for the model to task- and class-adaptively decide how much to use from the meta-learner, and how much to learn specifically for each task and class. To this end, we propose a novel Bayesian meta-learning framework, which we refer to as Bayesian Task-Adaptive Meta-Learning (Bayesian TAML), that learns variables to adaptively balance the effect of meta- and task-specific learning. Specifically, we first obtain set-representations for each task, which are learned to convey useful statistics about the task or class distribution, such as mean, variance, and cardinality (the number of elements in the set), and then learn the distribution of three balancing variables a function of the set: 1) task-dependent learning rate multiplier, which decides how far away to deviate from the meta-knowledge, when performing task-specific learning. Tasks with higher shots could benefit from taking gradient steps afar, while tasks with few shots may need to stay close to the initial parameter. 2) class-dependent learning rate, which decides how much information to use from each class, to automatically handle class imbalance where the number of instances per class can largely vary. 3) task-dependent modulator for initial model parameter, which modifies the shared initialization for each task, such that each task can decide how much and what to use from the shared initial model parameter and what to ignore based on its set representation. This is especially useful when handling out-of-distribution task, which may need to ignore some of the meta-knowledge. We validate our model on CIFAR-FS and miniImageNet dataset, as well as a new dataset that consists of heterogeneous datasets, under a scenario where every class in each episode can have any number of shots, that leads to task and class imbalance, and where the dataset at meta-test time is different from that of meta-training time. The experimental results show that our Bayesian TAML significantly improves the performance over the existing approaches under these realistic scenarios. Further analysis of each component reveals that the improvement is due to the effectiveness of the balancing terms for handling task and class imbalance, and out-of-distribution tasks. To summarize, our contribution in this work is threefold: • We consider a novel problem of meta-learning under a realistic task distribution, where the number of instances across classes and tasks could largely vary, or the unseen task at the meta-test time is largely different from the seen tasks. • For effective meta-learning with such imbalances, we propose a Bayesian task-adaptive meta-learning (Bayesian TAML) framework that can adaptively adjust the effect of the meta-learner and the task-specific learner, differently for each task and class. • We validate our model on realistic imbalanced few-shot classification tasks with a varying number of shots per task and class and show that it significantly outperforms existing meta-learning models.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a new task of fact verification with semi-structured Wikipedia tables as evidence. To this end, a large-scale dataset called TABFACT is introduced, which consists of 118K manually annotated statements with regard to 16K Wikipedia tables. Two approaches are proposed to address the mixed-reasoning challenge posed by the task: Table-BERT, which views the task as an NLI problem and excels at linguistic reasoning, and Latent Program Algorithm, which applies lexical matching and a discriminator to select the most \"consistent\" latent programs and excels at symbolic reasoning. Extensive experiments are performed to investigate the performance of the two models, with the best-achieved accuracy being reasonable but far below human performance. All data, code and intermediate results are released to facilitate future research.",
        "Abstract": "The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into LISP-like programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities.",
        "Introduction": "  INTRODUCTION Verifying whether a textual hypothesis is entailed or refuted by the given evidence is a fundamental problem in natural language understanding ( Katz & Fodor, 1963 ;  Van Benthem et al., 2008 ). It can benefit many downstream applications like misinformation detection, fake news detection, etc. Recently, the first-ever end-to-end fact-checking system has been designed and proposed in  Hassan et al. (2017) . The verification problem has been extensively studied under different natural language tasks such as recognizing textual entailment (RTE) ( Dagan et al., 2005 ), natural language inference (NLI) ( Bowman et al., 2015 ), claim verification ( Popat et al., 2017 ;  Hanselowski et al., 2018 ;  Thorne et al., 2018 ) and multimodal language reasoning (NLVR/NLVR2) ( Suhr et al., 2017 ; 2019). RTE and NLI view a premise sentence as the evidence, claim verification views passage collection like Wikipedia 1 as the evidence, NLVR/NLVR2 views images as the evidence. These problems have been previously addressed using a variety of techniques including logic rules, knowledge bases, and neural networks. Recently large-scale pre-trained language models ( Devlin et al., 2019 ;  Peters et al., 2018 ;  Yang et al., 2019 ;  Liu et al., 2019 ) have surged to dominate the other algorithms to approach human performance on several textual entailment tasks ( Wang et al., 2018 ; 2019). However, existing studies are restricted to dealing with unstructured text as the evidence, which would not generalize to the cases where the evidence has a highly structured format. Since such structured evidence (graphs, tables, or databases) are also ubiquitous in real-world applications like Published as a conference paper at ICLR 2020 District United States House of Representatives Elections, 1972 Entailed Statement Refuted Statement database systems, dialog systems, commercial management systems, social networks, etc, we argue that the fact verification under structured evidence forms is an equivalently important yet under- explored problem. Therefore, in this paper, we are specifically interested in studying fact verification with semi-structured Wikipedia tables ( Bhagavatula et al., 2013 ) 2 as evidence owing to its structured and ubiquitous nature ( Jauhar et al., 2016 ;  Zhong et al., 2017 ;  Pasupat & Liang, 2015 ). To this end, we introduce a large-scale dataset called TABFACT, which consists of 118K manually annotated statements with regard to 16K Wikipedia tables, their relations are classified as ENTAILED and REFUTED 3 . The entailed and refuted statements are both annotated by human workers. With some examples in  Figure 1 , we can clearly observe that unlike the previous verification related problems, TABFACT combines two different forms of reasoning in the statements, (i) Linguistic Reasoning: the verification requires semantic-level understanding. For example, \"John J. Mcfall failed to be re-elected though being unopposed.\" requires understanding over the phrase \"lost renomination ...\" in the table to correctly classify the entailment relation. Unlike the existing QA datasets ( Zhong et al., 2017 ;  Pasupat & Liang, 2015 ), where the linguistic reasoning is dominated by paraphrasing, TABFACT requires more linguistic inference or common sense. (ii) Symbolic Reasoning: the verifi- cation requires symbolic execution on the table structure. For example, the phrase \"There are three Democrats incumbents\" requires both condition operation (where condition) and arithmetic oper- ation (count). Unlike question answering, a statement could contain compound facts, all of these facts need to be verified to predict the verdict. For example, the \"There are ...\" in  Figure 1  requires verifying three QA pairs (total count=5, democratic count=2, republic count=3). The two forms of reasoning are interleaved across the statements making it challenging for existing models. In this paper, we particularly propose two approaches to deal with such mixed-reasoning challenge: (i) Table-BERT, this model views the verification task completely as an NLI problem by linearizing a table as a premise sentence p, and applies state-of-the-art language understanding pre-trained model to encode both the table and statements h into distributed representation for classification. This model excels at linguistic reasoning like paraphrasing and inference but lacks symbolic reasoning skills. (ii) Latent Program Algorithm, this model applies lexical matching to find linked entities and triggers to filter pre-defined APIs (e.g. argmax, argmin, count, etc). We adopt bread-first-search with memorization to construct the potential program candidates, a discriminator is further utilized to select the most \"consistent\" latent programs. This model excels at the symbolic reasoning aspects by executing database queries, which also provides better interpretability by laying out the decision rationale. We perform extensive experiments to investigate their performances: the best-achieved accuracy of both models are reasonable, but far below human performance. Thus, we believe that the proposed table-based fact verification task can serve as an important new benchmark towards the goal of building powerful AI that can reason over both soft linguistic form and hard symbolic forms. To facilitate future research, we released all the data, code with the intermediate results.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Determinantal point processes (DPP) have been utilized in a large body of diversity-oriented tasks due to its ability to incorporate only one single metric and deliver genuine diversity on any bounded space. This paper focuses on learning DPPs, which can be done by either learning the kernel function κ directly or learning the L-ensemble L directly. This paper provides a comprehensive introduction to the mathematical fundamentals of DPP for sampling from a discrete space and discusses two lines of strategies to learn DPPs.",
        "Abstract": "Determinantal point processes (DPPs) is an effective tool to deliver diversity on multiple machine learning and computer vision tasks. Under deep learning framework, DPP is typically optimized via approximation, which is not straightforward and has some conflict with diversity requirement. We note, however, there has been no deep learning paradigms to optimize DPP directly since it involves matrix inversion which may result in highly computational instability. This fact greatly hinders the wide use of DPP on some specific objectives where DPP serves as a term to measure the feature diversity. In this paper, we devise a simple but effective algorithm to address this issue to optimize DPP term directly expressed with L-ensemble in spectral domain over gram matrix, which is more flexible than learning on parametric kernels. By further taking into account some geometric constraints, our algorithm seeks to generate valid sub-gradients of DPP term in case when the DPP gram matrix is not invertible (no gradients exist in this case). In this sense, our algorithm can be easily incorporated with multiple deep learning tasks. Experiments show the effectiveness of our algorithm, indicating promising performance for practical learning problems. ",
        "Introduction": "  INTRODUCTION Diversity is desired in multiple machine learning and computer vision tasks (e.g., image hashing ( Chen et al., 2017 ;  Carreira-Perpinán & Raziperchikolaei, 2016 ), descriptor learning ( Zhang et al., 2017 ), metric learning ( Mishchuk et al., 2017 ) and video summarization ( Sharghi et al., 2018 ;  Liu et al., 2017 )), in which sub-sampled points or learned features need to spread out through a specific bounded space. Originated from quantum physics, determinantal point processes (DPP) have shown its power in delivering such properties ( Kulesza et al., 2012 ;  Kulesza & Taskar, 2011b ). Compared with other diversity-oriented techniques (e.g., entropy ( Zadeh et al., 2017 ) and orthogonality ( Zhang et al., 2017 )), DPP shows its superiority as it incorporates only one single metric and delivers genuine diversity on any bounded space ( Kulesza et al., 2012 ;  Affandi et al., 2013 ;  Gillenwater et al., 2012 ). Therefore, DPP has been utilized in a large body of diversity-oriented tasks. In general, sample points from a DPP tend to distribute diversely within a bounded space A ( Kulesza et al., 2012 ). Given a positive semi-definite kernel function κ : A × A → R, the probability of a discrete point set X ⊂ A under a DPP with kernel function κ can be characterized as: P κ (X ) ∝ det(L X ) (1) where L is a |X | × |X | matrix with entry L ij = κ(x i , x j ) and x i , x j ∈ X . L is called L-ensemble. Note that A is a continuous space, whereas X is finite. In the Hilbert space associated with κ, larger determinant implies larger spanned volume, thus the mapped points tend not to be similar or linearly dependent. DPP can be viewed from two perspectives: sampling and learning. A comprehensive introduction to mathematical fundamentals of DPP for sampling from a discrete space can be found in  Kulesza et al. (2012) . Based on this, a line of works has been proposed ( Kulesza & Taskar, 2011a ;  Kang, 2013 ;  Hennig & Garnett, 2016 ). In this paper, we concentrate on learning DPPs. In learning of DPP, the term det(L) is typically treated as a singleton diversity measurement and is extended to learning paradigms on continuous space ( Chao et al., 2015 ;  Kulesza & Taskar, 2010 ;  Affandi et al., 2014 ). There are generally two lines of strategies to learn DPPs:",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents two graph smoothness metrics to measure the quantity and quality of neighborhood information of nodes, and a new GNN model, CS-GNN, which uses the smoothness metrics to selectively aggregate neighborhood information to improve performance on tasks such as node classification. Experiments validate the effectiveness of the proposed metrics and model, demonstrating significant improvements over existing methods.",
        "Abstract": "Graph neural networks (GNNs) have been widely used for representation learning on graph data. However, there is limited understanding on how much performance GNNs actually gain from graph data. This paper introduces a context-surrounding GNN framework and proposes two smoothness metrics to measure the quantity and quality of information obtained from graph data. A new, improved GNN model, called CS-GNN, is then devised to improve the use of graph information based on the smoothness values of a graph. CS-GNN is shown to achieve better performance than existing methods in different types of real graphs. ",
        "Introduction": "  INTRODUCTION Graphs are powerful data structures that allow us to easily express various relationships (i.e., edges) between objects (i.e., nodes). In recent years, extensive studies have been conducted on GNNs for tasks such as node classification and link predication. GNNs utilize the relationship information in graph data and significant improvements over traditional methods have been achieved on benchmark datasets ( Kipf & Welling, 2017 ;  Hamilton et al., 2017 ;  Velickovic et al., 2018 ;  Xu et al., 2019 ;  Hou et al., 2019 ). Such breakthrough results have led to the exploration of using GNNs and their variants in different areas such as computer vision ( Satorras & Estrach, 2018 ;  Marino et al., 2017 ), natural language processing ( Peng et al., 2018 ;  Yao et al., 2019 ), chemistry ( Duvenaud et al., 2015 ), biology ( Fout et al., 2017 ), and social networks ( Wang et al., 2018 ). Thus, understanding why GNNs can outperform traditional methods that are designed for Euclidean data is important. Such understanding can help us analyze the performance of existing GNN models and develop new GNN models for different types of graphs. In this paper, we make two main contributions: (1) two graph smoothness metrics to help understand the use of graph information in GNNs, and (2) a new GNN model that improves the use of graph information using the smoothness values. We elaborate the two contributions as follows. One main reason why GNNs outperform existing Euclidean-based methods is because rich informa- tion from the neighborhood of an object can be captured. GNNs collect neighborhood information with aggregators ( Zhou et al., 2018 ), such as the mean aggregator that takes the mean value of neigh- bors' feature vectors ( Hamilton et al., 2017 ), the sum aggregator that applies summation ( Duvenaud et al., 2015 ), and the attention aggregator that takes the weighted sum value ( Velickovic et al., 2018 ). Then, the aggregated vector and a node's own feature vector are combined into a new feature vector. After some rounds, the feature vectors of nodes can be used for tasks such as node classification. Thus, the performance improvement brought by graph data is highly related to the quantity and quality of the neighborhood information. To this end, we propose two smoothness metrics on node features and labels to measure the quantity and quality of neighborhood information of nodes. The metrics are used to analyze the performance of existing GNNs on different types of graphs. In practice, not all neighbors of a node contain relevant information w.r.t. a specific task. Thus, neighborhood provides both positive information and negative disturbance for a given task. Simply aggregating the feature vectors of neighbors with manually-picked aggregators (i.e., users choose Published as a conference paper at ICLR 2020 a type of aggregator for different graphs and tasks by trial or by experience) often cannot achieve optimal performance. To address this problem, we propose a new model, CS-GNN, which uses the smoothness metrics to selectively aggregate neighborhood information to amplify useful information and reduce negative disturbance. Our experiments validate the effectiveness of our two smoothness metrics and the performance improvements obtained by CS-GNN over existing methods.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an algorithm for reinforcement learning from a small number of demonstrations that addresses the covariate shift problem. The algorithm learns a conservatively-extrapolated value function, which is guaranteed to induce a policy that stays close to the demonstration states. The algorithm is designed using a negative sampling technique inspired by work on learning embeddings and a dynamical model is learned by standard supervised learning. The algorithm is empirically shown to help correct errors of the behavioral cloning policy without using any additional environment interactions.",
        "Abstract": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results\nin cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform prior works in sample efficiency.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) algorithms, especially with sparse rewards, often require a large amount of trial-and-errors. Imitation learning from a small number of demonstrations followed by RL fine- tuning is a promising paradigm to improve the sample efficiency ( Rajeswaran et al., 2017 ;  Večerík et al., 2017 ;  Hester et al., 2018 ;  Nair et al., 2018 ;  Gao et al., 2018 ). The key technical challenge of learning from demonstrations is the covariate shift: the distribution of the states visited by the demonstrations often has a low-dimensional support; however, knowledge learned from this distribution may not necessarily transfer to other distributions of interests. This phenomenon applies to both learning the policy and the value function. The policy learned from behavioral cloning has compounding errors after we execute the policy for multiple steps and reach unseen states (Bagnell, 2015;  Ross & Bagnell, 2010 ). The value function learned from the demonstrations can also extrapolate falsely to unseen states. See Figure 1a for an illustration of the false extrapolation in a toy environment. We develop an algorithm that learns a value function that extrapolates to unseen states more con- servatively, as an approach to attack the optimistic extrapolation problem ( Fujimoto et al., 2018a ). Consider a state s in the demonstration and its nearby states that is not in the demonstration. The key intuition is thats should have a lower value than s, because otherwises likely should have been visited by the demonstrations in the first place. If a value function has this property for most of the pair (s,s) of this type, the corresponding policy will tend to correct its errors by driving back to the demonstration states because the demonstration states have locally higher values. We formalize the intuition in Section 4 by defining the so-called conservatively-extrapolated value function, which is guaranteed to induce a policy that stays close to the demonstrations states (Theorem 4.4). In Section 5, we design a practical algorithm for learning the conservatively-extrapolated value function by a negative sampling technique inspired by work on learning embeddings  Mikolov et al. (2013) ;  Gutmann & Hyvärinen (2012) . We also learn a dynamical model by standard supervised learning so that we compute actions by maximizing the values of the predicted next states. This algorithm does not use any additional environment interactions, and we show that it empirically helps correct errors of the behavioral cloning policy.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes SNOW, an efficient transfer and lifelong learning strategy, which consists of an inference-only source model, multiple task-specific delta models, and channel pooling in-between. SNOW is fundamentally different from prior arts in transfer and lifelong learning in that it does not require persistent training data or episodic memories, and it can produce target-specific features with a much smaller model size than the source model. Experimental results are presented to demonstrate the effectiveness of SNOW.",
        "Abstract": "SNOW is an efficient learning method to improve training/serving throughput as well as accuracy for transfer and lifelong learning of convolutional neural networks based on knowledge subscription. SNOW selects the top-K useful intermediate\nfeature maps for a target task from a pre-trained and frozen source model through a novel channel pooling scheme, and utilizes them in the task-specific delta model. The source model is responsible for generating a large number of generic feature maps. Meanwhile, the delta model selectively subscribes to those feature maps and fuses them with its local ones to deliver high accuracy for the target task. Since a source model takes part in both training and serving of all target tasks\nin an inference-only mode, one source model can serve multiple delta models, enabling significant computation sharing. The sizes of such delta models are fractional of the source model, thus SNOW also provides model-size efficiency.\nOur experimental results show that SNOW offers a superior balance between accuracy and training/inference speed for various image classification tasks to the existing transfer and lifelong learning practices.",
        "Introduction": "  INTRODUCTION Learning new tasks from old tasks over time as natural intelligence does is a key challenge in artificial intelligence, and transfer and lifelong learning are two popular strategies in this direction. Transfer learning delivers a neural network with good predictive power by duplicating and tuning the parameters for a pre-trained source model against a dataset for a new task ( Dauphin et al., 2012 ;  Donahue et al., 2014 ). However, transfer learning from a source task to many target tasks incurs overall significant training and space overhead due to multiple large models to customize and store ( Mudrakarta et al., 2019 ). On the other hand, lifelong learning can enable substantial parameter sharing and deliver multiple target tasks with less training time and smaller model size, but may suffer from catastrophic forgetting or lower accuracy ( McCloskey & Cohen, 1989 ). Comprehensive efforts to tackle such challenges have been proposed but at significant computational overhead in general ( Rusu et al., 2016 ;  Guo et al., 2019 ;  Mudrakarta et al., 2019 ;  Li & Hoiem, 2018 ). In this work, we propose SNOW for efficient transfer and lifelong learning, which consists of an inference-only source model, multiple task-specific delta models, and channel pooling in-between. Unlike transfer learning, we let multiple delta models subscribe to a source model through a channel pooling layer. SNOW is fundamentally different from the prior arts in transfer and lifelong learning in the following aspects: a) the source model is frozen during both training and inference, b) the top-K intermediate feature maps of the source model are learned for each delta model using channel pooling powered by Gaussian reparametrization technique ( Kingma & Welling, 2014 ). And, SNOW does require neither persistent training data nor episodic memories ( Sprechmann et al., 2018 ;  Li & Hoiem, 2018 ) to overcome catastrophic forgetting.  Table 1  compares SNOW with prior arts in transfer/lifelong learning, and  Fig. 1  visualizes the key structural differences between SNOW and transfer/lifelong learning where the source model for task0 is leveraged to build the target models for task1-3 without persistent datasets as follows: large enough to produce target-specific features, but can be much smaller than the source model. Fusing those features with the K features from the source model leads to significant parameter count saving compared with existing transfer learning practices. The rest of the paper is organized as follows. Section 2 details SNOW, and experimental results are in Section 3. Review on prior arts is in Section 4, followed by the conclusion in Section 5.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on solving two-player zero-sum extensive games with imperfect information (TEGI) using counterfactual regret minimization (CFR). CFR bounds the original regret with a summation of many immediate counterfactual regrets on each information set (infoset). Existing works on avoiding traversing the whole game tree can be mainly divided into two categories: Pruning-based CFR and Monte-Carlo CFR (MC-CFR). These algorithms can significantly speed up the vanilla CFR in practice, however, pruning-based algorithms may degenerate in the worst case and the performance of MC-CFR depends on the structure of the game and the chosen online learning algorithm.",
        "Abstract": "Counterfactual regret minimization (CFR) methods are effective for solving two-player zero-sum extensive games with imperfect information with  state-of-the-art results.  However,  the vanilla CFR has to traverse the whole game tree in each round, which is time-consuming in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round.  We prove that the regret of Lazy-CFR is almost the same to the regret of the vanilla CFR and only needs to visit a small portion of the game tree.  Thus, Lazy-CFR is provably faster than CFR. Empirical results consistently show that Lazy-CFR is significantly faster than the vanilla CFR.",
        "Introduction": "  INTRODUCTION Extensive games provide a mathematical framework for modeling the sequential decision-making problems with imperfect information, which is common in economic decisions, negotiations and security. We focus on solving two-player zero-sum extensive games with imperfect information (TEGI). In a TEGI, there is an environment with uncertainty and two players on opposite sides ( Koller & Megiddo, 1992 ). Counterfactual regret minimization (CFR) ( Zinkevich et al., 2008 ) provides a state-of-the-art approach for solving TEGIs with much progress in practice ( Brown & Sandholm, 2017b ;  Moravčík et al., 2017 ;  Brown & Sandholm, 2019a ). Regret minimization techniques are first introduced to solve TEGIs based on the observation that minimizing the regrets of both players makes the time-averaged strategy converge to the Nash Equilibrium (NE) ( Nisan et al., 2007 ). CFR ( Zinkevich et al., 2008 ) further bounds the original regret with a summation of many immediate counterfactual regrets on each information set (infoset). These immediate counterfactual regrets are defined by the counterfactual rewards and can be iteratively minimized by existing online learning algorithms, e.g., regret matching (RM) ( Blackwell et al., 1956 ). A limitation of CFR is that it requires traversing the whole game tree in each round, which is time- consuming in large-scale games due to the fact that we have to apply RM to every immediate regret in each round. Existing works on avoiding traversing the whole game tree can be mainly divided into two categories: Pruning-based CFR ( Brown & Sandholm, 2015 ;  2017a ) and Monte-Carlo CFR (MC-CFR) ( Lanctot et al., 2009 ). These algorithms can significantly speed up the vanilla CFR in practice. However, pruning-based algorithms may degenerate in the worst case. And the performance of MC-CFR depends on the structure of the game and the chosen online learning algorithm.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes an attack-free and scalable method to train robust deep neural networks, called MAximizing the CErtified Radius (MACER). MACER avoids time-consuming attack iterations and can be applied to architectures of any size, making it more practical in real scenarios. Extensive experiments on Cifar-10, ImageNet, MNIST, and SVHN show that MACER achieves better performance than state-of-the-art algorithms and is exceptionally fast.",
        "Abstract": "Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius.",
        "Introduction": "  INTRODUCTION Modern neural network classifiers are able to achieve very high accuracy on image classification tasks but are sensitive to small, adversarially chosen perturbations to the inputs (Szegedy et al., 2013; Biggio et al., 2013). Given an image x that is correctly classified by a neural network, a malicious attacker may find a small adversarial perturbation δ such that the perturbed image x + δ, though visually indistinguishable from the original image, is assigned to a wrong class with high confidence by the network. Such vulnerability creates security concerns in many real-world applications. Researchers have proposed a variety of defense methods to improve the robustness of neural net- works. Most of the existing defenses are based on adversarial training (Szegedy et al., 2013; Madry et al., 2017; Goodfellow et al., 2015; Huang et al., 2015; Athalye et al., 2018; Ding et al., 2020). During training, these methods first learn on-the-fly adversarial examples of the inputs with multiple attack iterations and then update model parameters using these perturbed samples together with the original labels. However, such approaches depend on a particular (class of) attack method. It cannot be formally guaranteed whether the resulting model is also robust against other attacks. Moreover, attack iterations are usually quite expensive. As a result, adversarial training runs very slowly. Another line of algorithms trains robust models by maximizing the certified radius provided by ro- bust certification methods (Weng et al., 2018; Wong & Kolter, 2018; Zhang et al., 2018; Mirman et al., 2018; Wang et al., 2018; Gowal et al., 2018; Zhang et al., 2019c). Using linear or convex relaxations of fully connected ReLU networks, a robust certification method computes a \"safe ra- dius\" r for a classifier at a given input such that at any point within the neighboring radius-r ball of the input, the classifier is guaranteed to have unchanged predictions. However, the certification methods are usually computationally expensive and can only handle shallow neural networks with ReLU activations, so these training algorithms have troubles in scaling to modern networks. In this work, we propose an attack-free and scalable method to train robust deep neural networks. We mainly leverage the recent randomized smoothing technique (Cohen et al., 2019). A randomized smoothed classifier g for an arbitrary classifier f is defined as g(x) = E η f (x + η), in which η ∼ N (0, σ 2 I). While Cohen et al. (2019) derived how to analytically compute the certified radius of the randomly smoothed classifier g, they did not show how to maximize that radius to make the classifier Published as a conference paper at ICLR 2020 g robust. Salman et al. (2019) proposed SmoothAdv to improve the robustness of g, but it still relies on the expensive attack iterations. Instead of adversarial training, we propose to learn robust models by directly taking the certified radius into the objective. We outline a few challenging desiderata any practical instantiation of this idea would however have to satisfy, and provide approaches to address each of these in turn. A discussion of these desiderata, as well as a detailed implementation of our approach is provided in Section 4. And as we show both theoretically and empirically, our method is numerically stable and accounts for both classification accuracy and robustness. Our contributions are summarized as follows: • We propose an attack-free and scalable robust training algorithm by MAximizing the CErti- fied Radius (MACER). MACER has the following advantages compared to previous works: - Different from adversarial training, we train robust models by directly maximizing the certified radius without specifying any attack strategies, and the learned model can achieve provable robustness against any possible attack in the certified region. Additionally, by avoiding time-consuming attack iterations, our proposed algorithm runs much faster than adversarial training. - Different from other methods (Wong & Kolter, 2018) that maximize the certified ra- dius but are not scalable to deep neural networks, our method can be applied to archi- tectures of any size. This makes our algorithm more practical in real scenarios. • We empirically evaluate our proposed method through extensive experiments on Cifar-10, ImageNet, MNIST, and SVHN. On all tasks, MACER achieves better performance than state-of-the-art algorithms. MACER is also exceptionally fast. For example, on ImageNet, MACER uses 39% less training time than adversarial training but still performs better.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes an unsupervised Mutual Mean-Teaching (MMT) framework to address the problem of noisy pseudo labels in clustering-based Unsupervised Domain Adaptation (UDA) methods for person re-identification (re-ID). The MMT framework provides robust soft pseudo labels in an on-line peer-teaching manner, which is inspired by the teacher-student approaches. To enable using the triplet loss with soft pseudo labels in the MMT framework, a novel soft softmax-triplet loss is proposed. The MMT framework shows strong performances on all UDA tasks of person re-ID, leading to significant improvements of 14.4%, 18.2%, 13.4%, 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT, Duke-to-MSMT re-ID tasks.",
        "Abstract": "Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner.  In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",
        "Introduction": "  INTRODUCTION Person re-identification (re-ID) aims at retrieving the same persons' images from images captured by different cameras. In recent years, person re-ID datasets with increasing numbers of images were proposed to facilitate the research along this direction. All the datasets require time-consuming an- notations and are keys for re-ID performance improvements. However, even with such large-scale datasets, for person images from a new camera system, the person re-ID models trained on exist- ing datasets generally show evident performance drops because of the domain gaps. Unsupervised Domain Adaptation (UDA) is therefore proposed to adapt the model trained on the source image do- main (dataset) with identity labels to the target image domain (dataset) with no identity annotations. State-of-the-art UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) for person re-ID group unannotated images with clustering algorithms and train the network with clustering- generated pseudo labels. Although the pseudo label generation and feature learning with pseudo labels are conducted alternatively to refine the pseudo labels to some extent, the training of the neural network is still substantially hindered by the inevitable label noise. The noise derives from the limited transferability of source-domain features, the unknown number of target-domain identities, and the imperfect results of the clustering algorithm. The refinery of noisy pseudo labels has crucial influences to the final performance, but is mostly ignored by the clustering-based UDA methods. To effectively address the problem of noisy pseudo labels in clustering-based UDA methods (Song et al., 2018; Zhang et al., 2019b; Yang et al., 2019) ( Figure 1 ), we propose an unsupervised Mutual Mean-Teaching (MMT) framework to effectively perform pseudo label refinery by optimizing the neural networks under the joint supervisions of off-line refined hard pseudo labels and on-line refined soft pseudo labels. Specifically, our proposed MMT framework provides robust soft pseudo labels in an on-line peer-teaching manner, which is inspired by the teacher-student approaches (Tarvainen & Valpola, 2017; Zhang et al., 2018b) to simultaneously train two same networks. The networks gradually capture target-domain data distributions and thus refine pseudo labels for better feature learning. To avoid training error amplification, the temporally average model of each network is proposed to produce reliable soft labels for supervising the other network in a collaborative training strategy. By training peer-networks with such on-line soft pseudo labels on the target domain, the learned feature representations can be iteratively improved to provide more accurate soft pseudo labels, which, in turn, further improves the discriminativeness of learned feature representations. The classification and triplet losses are commonly adopted together to achieve state-of-the-art per- formances in both fully-supervised (Luo et al., 2019) and unsupervised (Zhang et al., 2019b; Yang et al., 2019) person re-ID models. However, the conventional triplet loss (Hermans et al., 2017) can- not work with such refined soft labels. To enable using the triplet loss with soft pseudo labels in our MMT framework, we propose a novel soft softmax-triplet loss so that the network can benefit from softly refined triplet labels. The introduction of such soft softmax-triplet loss is also the key to the superior performance of our proposed framework. Note that the collaborative training strategy on the two networks is only adopted in the training process. Only one network is kept in the inference stage without requiring any additional computational or memory cost. The contributions of this paper could be summarized as three-fold. (1) We propose to tackle the label noise problem in state-of-the-art clustering-based UDA methods for person re-ID, which is mostly ignored by existing methods but is shown to be crucial for achieving superior final per- formance. The proposed Mutual Mean-Teaching (MMT) framework is designed to provide more reliable soft labels. (2) Conventional triplet loss can only work with hard labels. To enable train- ing with soft triplet labels for mitigating the pseudo label noise, we propose the soft softmax-triplet loss to learn more discriminative person features. (3) The MMT framework shows exceptionally strong performances on all UDA tasks of person re-ID. Compared with state-of-the-art methods, it leads to significant improvements of 14.4%, 18.2%, 13.4%, 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT, Duke-to-MSMT re-ID tasks.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a Ranking Policy Gradient (RPG) method to address the challenge of sample-efficient reinforcement learning. RPG directly optimizes relative action values to maximize the return and is a policy gradient method. The paper also proposes a general off-policy learning framework that equips the generalized policy iteration with an external step of supervised learning. This framework not only preserves optimality but also reduces the variance of policy gradient. The paper demonstrates that the proposed approach significantly outperforms the state-of-the-art.",
        "Abstract": "Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art.",
        "Introduction": "  INTRODUCTION One of the major challenges in reinforcement learning (RL) is the high sample complexity (Kakade et al., 2003), which is the number of samples must be collected to conduct successful learning. There are different reasons leading to poor sample efficiency of RL ( Yu, 2018 ). Because policy gradient algorithms directly optimizing return estimated from rollouts (e.g., REINFORCE (Williams, 1992)) could suffer from high variance ( Sutton & Barto, 2018 ), value function baselines were introduced by actor-critic methods to reduce the variance and improve the sample-efficiency. However, since a value function is associated with a certain policy, the samples collected by former policies cannot be readily used without complicated manipulations (Degris et al., 2012) and extensive parameter tuning (Nachum et al., 2017). Such an on-policy requirement increases the difficulty of sample-efficient learning. On the other hand, off-policy methods, such as one-step Q-learning (Watkins & Dayan, 1992) and variants of deep Q networks (DQN) (Mnih et al., 2015; Hessel et al., 2017;  Dabney et al., 2018 ; Van Hasselt et al., 2016; Schaul et al., 2015), enjoys the advantage of learning from any trajectory sampled from the same environment (i.e., off-policy learning), are currently among the most sample- efficient algorithms. These algorithms, however, often require extensive searching ( Bertsekas & Tsitsiklis, 1996 , Chap. 5) over the large state-action space to estimate the optimal action value function. Another deficiency is that, the combination of off-policy learning, bootstrapping, and function approximation, making up what  Sutton & Barto (2018)  called the \"deadly triad\", can easily lead to unstable or even divergent learning ( Sutton & Barto, 2018 , Chap. 11). These inherent issues limit their sample-efficiency. Towards addressing the aforementioned challenge, we approach the sample-efficient reinforcement learning from a ranking perspective. Instead of estimating optimal action value function, we concen- trate on learning optimal rank of actions. The rank of actions depends on the relative action values. As long as the relative action values preserve the same rank of actions as the optimal action values (Q-values), we choose the same optimal action. To learn optimal relative action values, we propose the ranking policy gradient (RPG) that optimizes the actions' rank with respect to the long-term reward by learning the pairwise relationship among actions. Ranking Policy Gradient (RPG) that directly optimizes relative action values to maximize the return is a policy gradient method. The track of off-policy actor-critic methods (Degris et al., 2012; Gu et al., 2016; Wang et al., 2016) have made substantial progress on improving the sample-efficiency Published as a conference paper at ICLR 2020 of policy gradient. However, the fundamental difficulty of learning stability associated with the bias-variance trade-off remains (Nachum et al., 2017). In this work, we first exploit the equivalence between RL optimizing the lower bound of return and supervised learning that imitates a specific optimal policy. Build upon this theoretical foundation, we propose a general off-policy learning framework that equips the generalized policy iteration ( Sutton & Barto, 2018 , Chap. 4) with an external step of supervised learning. The proposed off-policy learning not only enjoys the property of optimality preserving (unbiasedness), but also largely reduces the variance of policy gradient because of its independence of the horizon and reward scale. Besides, we empirically show that there is a trade-off between optimality and sample-efficiency. Last but not least, we demonstrate that the proposed approach, consolidating the RPG with off-policy learning, significantly outperforms the state-of-the-art (Hessel et al., 2017;  Bellemare et al., 2017 ;  Dabney et al., 2018 ; Mnih et al., 2015).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel Neural Execution Tree (NExT) framework for deep neural networks to learn from natural language (NL) explanations. NExT is able to model the compositionality of NL explanations and improve the generalization ability of NL explanations so that neural models can leverage unlabeled data for augmenting model training. Experiments on two representative tasks (relation extraction and sentiment analysis) demonstrate the superiority of NExT over various baselines. Additionally, NExT is adapted for multi-hop question answering task, in which it achieves performance improvement with only 21 explanations and 5 rules.",
        "Abstract": "While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.",
        "Introduction": "  INTRODUCTION Deep neural networks have achieved state-of-the-art performance on a wide range of natural lan- guage processing tasks. However, they usually require massive labeled data, which restricts their applications in scenarios where data annotation is expensive. The traditional way of providing su- pervision is human-generated labels. See  Figure 1  as an example. The sentiment polarity of the sentence \"Quality ingredients preparation all around, and a very fair price for NYC\" can be la- beled as \"Positive\". However, the label itself does not provide information about how the decision is made. A more informative method is to allow annotators to explain their decisions in natural language so that the annotation can be generalized to other examples. Such an explanation can be \"Positive, because the word price is directly preceded by fair\", which can be generalized to other instances like \"It has delicious food with a fair price\". Natural language (NL) explanations have shown effectiveness in providing additional supervision, especially in low-resource settings ( Srivas- tava et al., 2017 ;  Hancock et al., 2018 ). Also, they can be easily collected from human annotators without significantly increasing annotation efforts. However, exploiting NL explanations as supervision is challenging due to the complex nature of human languages. First of all, textual data is not well-structured, and thus we have to parse explana- tions into logical forms for machine to better utilize them. Also, linguistic variants are ubiquitous, which makes it difficult to generalize an NL explanation for matching sentences that are semantically * Equal contribution. The order is decided by a coin toss. The work was done when visiting USC. Attempts have been made to train classifiers with NL explanations.  Srivastava et al. (2017)  use NL explanations as additional features of data. They map explanations to logical forms with a semantic parser and use them to generate binary features for all instances.  Hancock et al. (2018)  employ a rule- based semantic parser to get logical forms (i.e. \"la- beling function\") from NL explanations that gen- erate noisy labeled datasets used for training mod- els. While both methods claim huge performance improvements, they neglect the importance of lin- guistic variants, thus resulting in a very low recall. Also, their methods of evaluating explanations on new instances are oversimplified (e.g. compari- son/logic operators), making their methods overly confident. In the above example, sentence \"Decent sushi at a fair enough price\" will be rejected because of the \"directly preceded\" requirement. To address these issues, we propose Neural Execution Tree (NExT) framework for deep neural networks to learn from NL explanations, as illustrated in  Figure 2 . Given a raw corpus and a set of NL explanations, we first parse the NL explanations into machine-actionable logical forms by a combinatory categorial grammar (CCG) based semantic parser. Different from previous work, we \"soften\" the annotation process by generalizing the predicates using neural module networks and changing the labeling process from exact matching to fuzzy matching. We introduce four types of matching modules in total, namely String Matching Module, Soft Counting Module, Logical Calculation Module, and Deterministic Function Module. We calculate the matching scores and find for each instance the most similar logical form. Thus, all instances in the raw corpus can be assigned a label and used to train neural models. The major contributions of our work are summarized as follows: (1) We propose a novel NExT framework to utilize NL explanations. NExT is able to model the compositionality of NL explana- tions and improve the generalization ability of NL explanations so that neural models can leverage unlabeled data for augmenting model training. (2) We conduct extensive experiments on two rep- resentative tasks (relation extraction and sentiment analysis). Experimental results demonstrate the superiority of NExT over various baselines. Also, we adapted NExT for multi-hop question answer- ing task, in which it achieves performance improvement with only 21 explanations and 5 rules.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel attack technique, called tracker hijacking, which can effectively fool the Multiple Object Tracking (MOT) process in autonomous driving using Adversarial Examples (AEs) on object detection. We demonstrate that our attack can succeed with successful AEs on as few as one frame, and 2 to 3 consecutive frames on average, while attacks that blindly target object detection only have up to 25% success rate when attacking 3 consecutive frames. We evaluate our attack on 20 randomly sampled video clips from the Berkeley Deep Drive dataset and compare with previous attacks.",
        "Abstract": "Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target objection detection: we find that a success rate of over 98% is needed for them to actually affect the tracking results, a requirement that no existing attack technique can satisfy. In this paper, we are the first to study adversarial machine learning attacks against the complete visual perception pipeline in autonomous driving, and discover a novel attack technique, tracker hijacking, that can effectively fool MOT using AEs on object detection. Using our technique, successful AEs on as few as one single frame can move an existing object in to or out of the headway of an autonomous vehicle to cause potential safety hazards. We perform evaluation using the Berkeley Deep Drive dataset and find that on average when 3 frames are attacked, our attack can have a nearly 100% success rate while attacks that blindly target object detection only have up to 25%.",
        "Introduction": "  INTRODUCTION Since the first Adversarial Example (AE) against traffic sign image classification discovered by Eykholt et al. ( Eykholt et al., 2018 ), several research work in adversarial machine learning ( Eykholt et al., 2017 ;  Xie et al., 2017 ;  Lu et al., 2017a ;b;  Zhao et al., 2018b ;  Chen et al., 2018 ;  Cao et al., 2019 ) started to focus on the context of visual perception in autonomous driving, and studied AEs on object detection models. For example, Eykholt et al. ( Eykholt et al., 2017 ) and Zhong et al. (Zhong et al., 2018) studied AEs in the form of adversarial stickers on stop signs or the back of front cars against YOLO object detectors ( Redmon & Farhadi, 2017 ), and performed indoor experiments to demonstrate the attack feasibility in the real world. Building upon these work, most recently Zhao et al. ( Zhao et al., 2018b ) leveraged image transformation techniques to improve the robustness of such adversarial sticker attacks in outdoor settings, and were able to achieve a 72% attack success rate with a car running at a constant speed of 30 km/h on real roads. While these results from prior work are alarming, object detection is in fact only the first half of the visual perception pipeline in autonomous driving, or in robotic systems in general - in the second half, the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories, called trackers, of surrounding obstacles. This is required for the subsequent driving decision making process, which needs the built trajectories to predict future moving trajectories for these obstacles and then plan a driving path accordingly to avoid collisions with them. To ensure high tracking accuracy and robustness against errors in object detection, in MOT only the detection results with sufficient consistency and stability across multiple frames can be included in the tracking results and actually influence the driving decisions. Thus, MOT in the visual perception of autonomous driving poses a general challenge to existing attack techniques that blindly target objection detection. For example, as shown by our analysis later in §4, an attack on objection detection needs to succeed consecutively for at least 60 frames to fool a representative MOT process, which requires an at least 98% attack success rate (§4). To the best of our knowledge, no existing attacks on objection detection can achieve such a high success rate ( Eykholt et al., 2017 ;  Xie et al., 2017 ;  Lu et al., 2017a ;b;  Zhao et al., 2018b ;  Chen et al., 2018 ). In this paper, we are the first to study adversarial machine learning attacks considering the complete visual perception pipeline in autonomous driving, i.e., both object detection and object tracking, and discover a novel attack technique, called tracker hijacking, that can effectively fool the MOT process using AEs on object detection. Our key insight is that although it is highly difficult to directly create a tracker for fake objects or delete a tracker for existing objects, we can carefully design AEs to attack the tracking error reduction process in MOT to deviate the tracking results of existing objects towards an attacker-desired moving direction. Such process is designed for increasing the robustness and accuracy of the tracking results, but ironically, we find that it can be exploited by attackers to substantially alter the tracking results. Leveraging such attack technique, successful AEs on as few as one single frame is enough to move an existing object in to or out of the headway of an autonomous vehicle and thus may cause potential safety hazards. We select 20 out of 100 randomly sampled video clips from the Berkeley Deep Drive dataset for evaluation. Under recommended MOT configurations in practice (Zhu et al., 2018) and normal measurement noise levels, we find that our attack can succeed with successful AEs on as few as one frame, and 2 to 3 consecutive frames on average. We reproduce and compare with previous attacks that blindly target object detection, and find that when attacking 3 consecutive frames, our attack has a nearly 100% success rate while attacks that blindly target object detection only have up to 25%.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the trade-off between accuracy and robustness in deep networks, and proposes a novel input-adaptive dynamic inference scheme to achieve a \"triple-win\" of accuracy, robustness, and efficiency. The proposed Robust Dynamic Inference Networks (RDI-Nets) are augmented with multiple early-branch output layers and adaptively choose which output layer to take for each input's prediction. Experiments demonstrate that the input-adaptive inference and multi-loss flexibility can be used to achieve better accuracy and robustness, while reducing inference computation by over 30%.",
        "Abstract": "Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies multi-exit networks associated with input-adaptive efficient inference, showing their strong promise in achieving a “sweet point\" in co-optimizing model accuracy, robustness, and efficiency. Our proposed solution, dubbed Robust Dynamic Inference Networks (RDI-Nets), allows for each input (either clean or adversarial) to adaptively choose one of the multiple output layers (early branches or the final one) to output its prediction. That multi-loss adaptivity adds new variations and flexibility to adversarial attacks and defenses, on which we present a systematical investigation. We show experimentally that by equipping existing backbones with such robust adaptive inference, the resulting RDI-Nets can achieve better accuracy and robustness, yet with over 30% computational savings, compared to the defended original models.\n",
        "Introduction": "  INTRODUCTION Deep networks, despite their high predictive accuracy, are notoriously vulnerable to adversarial attacks ( Goodfellow et al., 2015 ;  Biggio et al., 2013 ;  Szegedy et al., 2014 ;  Papernot et al., 2016 ). While many defense methods have been proposed to increase a model's robustness to adversarial examples, they were typically observed to hamper its accuracy on original clean images.  Tsipras et al. (2019)  first pointed out the inherent tension between the goals of adversarial robustness and standard accuracy in deep networks, whose provable existence was shown in a simplified setting.  Zhang et al. (2019)  theoretically quantified the accuracy-robustness trade-off, in terms of the gap between the risk for adversarial examples versus the risk for non-adversarial examples. It is intriguing to consider whether and why the model accuracy and robustness have to be at odds.  Schmidt et al. (2018)  demonstrated that the number of samples needed to achieve adversarially robust generalization is polynomially larger than that needed for standard generalization, under the adversarial training setting. A similar conclusion was concurred by  Sun et al. (2019)  in the standard training setting.  Tsipras et al. (2019)  considered the accuracy-robustness trade-off as an inherent trait of the data distribution itself, indicating that this phenomenon persists even in the limit of infinite data.  Nakkiran (2019)  argued from a different perspective, that the complexity (e.g. capacity) of a robust classifier must be higher than that of a standard classifier. Therefore, replacing a larger- capacity classifier might effectively alleviate the trade-off. Overall, those existing works appear to suggest that, while accuracy and robustness are likely to trade off for a fixed classification model Published as a conference paper at ICLR 2020 and on a given dataset, such trade-off might be effectively alleviated (\"win-win\"), if supplying more training data and/or replacing a larger-capacity classifier. On a separate note, deep networks also face the pressing challenge to be deployed on resource- constrained platforms due to the prosperity of smart Internet-of-Things (IoT) devices. Many IoT applications naturally demand security and trustworthiness, e.g., , biometrics and identity verifica- tion, but can only afford limited latency, memory and energy budget. Hereby we extend the question: can we achieve a triple-win, i.e., , an accurate and robust classfier while keeping it efficient? This paper makes an attempt in providing a positive answer to the above question. Rather than proposing a specific design of robust light-weight models, we reduce the average computation loads by input-adaptive routing to achieve triple-win. To this end, we introduce the input-adaptive dynamic inference (Teerapittayanon et al., 2017;  Wang et al., 2018a ), an emerging efficient inference scheme in contrast to the (non-adaptive) model compression, to the adversarial defense field for the first time. Given any deep network backbone (e.g., , ResNet, MobileNet), we first follow (Teerapittayanon et al., 2017) to augment it with multiple early-branch output layers in addition to the original final output. Each input, regardless of clean or adversarial samples, adaptively chooses which output layer to take for its own prediction. Therefore, a large portion of input inferences can be terminated early when the samples can already be inferred with high confidence. Up to our best knowledge, no existing work studied adversarial attacks and defenses for an adap- tive multi-output model, as the multiple sources of losses provide much larger flexibility to com- pose attacks (and therefore defenses), compared to the typical single-loss backbone. We present a systematical exploration on how to (white-box) attack and defense our proposed multi-output net- work with adaptive inference, demonstrating that the composition of multiple-loss information is critical in making the attack/defense strong.  Fig. 1  illustrates our proposed Robust Dynamic Infer- ence Networks (RDI-Nets). We show experimentally that the input-adaptive inference and multi- loss flexibility can be our friend in achieving the desired \"triple wins\". With our best defended RDI-Nets, we achieve better accuracy and robustness, yet with over 30% inference computational savings, compared to the defended original models as well as existing solutions co-designing ro- bustness and efficiency ( Gui et al., 2019 ;  Guo et al., 2018 ). The codes can be referenced from https://github.com/TAMU-VITA/triple-wins.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Path planning is a fundamental problem with many real-world applications, such as robot manipulation and autonomous driving. Sampling-based planning algorithms are typically employed to solve high-dimensional planning problems, but they may require a large number of samples to obtain a feasible solution. This paper investigates online adaptation in path planning to improve sample efficiency in current planning problems. It proposes a shared latent representation to learn common characteristics across problems, which can then be transferred to new problems with improved sample efficiency.",
        "Abstract": "We propose a meta path planning algorithm named \\emph{Neural Exploration-Exploitation Trees~(NEXT)} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in  high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between \\emph{exploration} and \\emph{exploitation} when solving a new problem. We conduct thorough experiments to show that NEXT accomplishes new planning problems with more compact search trees and significantly outperforms state-of-the-art methods on several benchmarks.",
        "Introduction": "  INTRODUCTION Path planning is a fundamental problem with many real-world applications, such as robot manipulation and autonomous driving. A simple planning problem within low-dimensional state space can be solved by first discretizing the continuous state space into a grid, and then searching for a path on top of it using graph search algorithms such as A * (Hart et al., 1968). However, due to the curse of dimensionality, these approaches do not scale well with the number of dimensions of the state space. For high-dimensional planning problems, people often resort to sampling-based approaches to avoid explicit discretization. Sampling-based planning algorithms, such as probabilistic roadmaps (PRM) (Kavraki et al., 1996), rapidly-exploring random trees (RRT) (LaValle, 1998), and their variants (Karaman & Frazzoli, 2011) incrementally build an implicit representation of the state space using probing samples. These generic algorithms typically employ a uniform sampler which does not make use of the structures of the problem. Therefore they may require lots of samples to obtain a feasible solution for complicated problems. To improve the sample efficiency, heuristic biased samplers, such as Gaussian sampler (Boor et al., 1999), bridge test (Hsu et al., 2003) and reachability-guided sampler (Shkolnik et al., 2009) have been proposed. All these sampling heuristics are designed manually to address specific structural properties, which may or may not be valid for a new problem, and may lead to even worse performance compared to the uniform proposal. Online adaptation in path planning has also been investigated for improving sample efficiency in current planning problem. Specifically, Hsu et al. (2005) exploits online algorithms to dynamically adapts the mixture weights of several manually designed biased samplers. Burns & Brock (2005a;b) fit a model for the planning environment incrementally and use the model for planning. Yee et al. (2016) mimics the Monte-Carlos tree search (MCTS) for problems with continuous state and action spaces. These algorithms treat each planning problem independently, and the collected data from previous experiences and built model will be simply discarded when solving a new problem. However, in practice, similar planning problems may be solved again and again, where the problems are different but sharing common structures. For instance, grabbing a coffee cup on a table at different time are different problems, since the layout of paper and pens, the position and orientation of coffee cups may be different every time; however, all these problems show common structures of handling similar objects which are placed in similar fashions. Intuitively, if the common characteristics across problems can be learned via some shared latent representation, a planner based on such representation can then be transferred to new problems with improved sample efficiency.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a new approach for edge embedding via channel-wise operation, namely channel-independent embedding (CIE), and a new mechanism to adjust the loss function based on the Hungarian method, termed Hungarian attention. The proposed techniques are evaluated on three public benchmarks and show that they are orthogonal and beneficial to existing techniques, effectively boosting the accuracy by exploring edge attributes and introducing higher smoothness against traditional loss functions.",
        "Abstract": "Graph matching aims to establishing node-wise correspondence between two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the information in each channel to be merged independently. In contrast, only node embedding is accounted in previous works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of objective learning for graph matching. Using Hungarian algorithm, it dynamically constructs a structured and sparsely connected layer, taking into account the most contributing matching pairs as hard attention. Our approach performs competitively, and can also improve state-of-the-art methods as plugin, regarding with matching accuracy on three public benchmarks.",
        "Introduction": "  INTRODUCTION Without loss of generality, we consider the bijection problem for graph matching: given graph G 1 and G 2 of equal size n, graph matching seeks to find the one-vs-one node correspondence 1 : max x x Kx s.t. Px = 1 (1) where x = vec(X) ∈ {0, 1} n 2 which is the column-wise vectorized form of the permutation ma- trix X that encodes the node-to-node correspondence between two graphs, and K ∈ R n 2 ×n 2 + is the so-called affinity matrix 2 , respectively. Note P is a selection matrix encoding the one-to-one correspondence constraint. This problem is called Lawler's QAP (Lawler, 1963) and has attracted enormous attention for its generally NP-complete (Hartmanis, 1982) challenge, as well as a wide spectrum of applications in computer vision, graphics, machine learning and operational research etc. In particular, Koopmans-Beckmann's QAP (Loiola et al., 2007) with objective tr(X F 1 XF 2 ) is a special case of Eq. (1), which can be converted to Lawler's QAP by K = F 2 ⊗ F 1 and F i refers to the weighted adjacency matrix. A series of solvers haven been developed to solve graph match- ing problem (Leordeanu & Hebert, 2005; Cho et al., 2010; Bernard et al., 2018; Yan et al., 2015; Yu et al., 2018). All these methods are based on deterministic optimization, which are conditioned with pre-defined affinity matrix and no learning paradigm is involved. This fact greatly limits the performance and broad application w.r.t. different problem settings considering its NP-hard nature. Recently, the seminal work namely deep graph matching (DGM) (Zanfir & Sminchisescu, 2018) is proposed to exploit the high capacity of deep networks for graph matching, which achieves state- of-the-art performance. This is in contrast to some early works which incorporate learning strategy Published as a conference paper at ICLR 2020 separately in local stages (Caetano et al., 2009; Cho et al., 2013). On the other hand, Graph Convolu- tional Networks (GCN) (Kipf & Welling, 2017) brings about new capability on tasks over graph-like data, as it naturally integrates the intrinsic graph structure in a general updating rule: H (l+1) = σ Â H (l) W (l) (2) whereÂ is the normalized connectivity matrix. H (l) and W (l) are the features and weights at layer l, respectively. Node embedding is updated by aggregation from 1-neighboring nodes, which is akin to the convolution operator in CNN. By taking advantages of both DGM and GCN, Wang et al. (2019) and Zhang & Lee (2019) incorporate permutation loss instead of displacement loss in (Zanfir & Sminchisescu, 2018), with notable improvement across both synthetic and real data. Note that Eq. (1) involves both node and edge information, which exactly correspond to the diag- onal and off-diagonal elements in K, respectively. Edges can carry informative multi-dimensional attributes (namely weights) which are fundamental to graph matching. However existing embed- ding based graph matching methods (Wang et al., 2019; Xu et al., 2019) are focused on the explicit modeling of node level features, whereby the edges are only used as topological node connection for message passing in GCN. Besides, edge attributes are neither well modeled in the embedding-free model (Zanfir & Sminchisescu, 2018) since the edge information is derived from the concatena- tion of node features. To our best knowledge, there is no deep graph matching method explicitly incorporating edge attributes. In contrast, edge attributes e.g. length and orientation are widely used in traditional graph matching models (Cho et al., 2010; Yan et al., 2015; Yu et al., 2018) for constructing the affinity matrix K. Such a gap shall be filled in the deep graph matching pipeline. Another important consideration refers to the design of loss function. There are mainly two forms in existing deep graph matching works: i) displacement loss (Zanfir & Sminchisescu, 2018) similar to the use in optical flow estimation (Ren et al., 2017); ii) the so-called permutation loss (Wang et al., 2019) involving iterative Sinkhorn procedure followed by a cross-entropy loss. Results in (Wang et al., 2019) show the latter is an effective improvement against the former regression based loss. However, we argue that the continuous Sinkhorn procedure (in training stage) is yet an unnatural approximation to Hungarian sampling (in testing stage) for discretization. If the network is equipped with a continuous loss function (e.g. cross-entropy), we argue that the training process will make a great \"meaningless effort\" to enforce some network output digits of the final matching matrix into binary and neglect the resting digits which might have notable impact on accuracy. This paper strikes an endeavor on the above two gaps and makes the following main contributions: i) We propose a new approach for edge embedding via channel-wise operation, namely channel- independent embedding (CIE). The hope is to effectively explore the edge attribute and simulate the multi-head strategy in attention models (Veličković et al., 2018) by decoupling the calculations parallel and orthogonal to channel direction. In fact, edge attribute information has not been consid- ered in existing embedding based graph matching methods (Wang et al., 2019; Xu et al., 2019). ii) We devise a new mechanism to adjust the loss function based on the Hungarian method which is widely used for linear assignment problem, as termed by Hungarian attention. It resorts to dynami- cally generating sparse matching mask according to Hungarian sampling during training, rather than approximating Hungarian sampling with a differentiable function. As such, the Hungarian attention introduces higher smoothness against traditional loss functions to ease the training. iii) The empirical results on three public benchmarks shows that the two proposed techniques are orthogonal and beneficial to existing techniques. Specifically, on the one hand, our CIE module can effectively boost the accuracy by exploring the edge attributes which otherwise are not consid- ered in state-of-the-art deep graph matching methods; on the other hand, our Hungarian attention mechanism also shows generality and it is complementary to existing graph matching loss.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents ExpressGNN, a simple variant of graph neural networks that combines hard logic rules and probabilistic graphical models to improve the quality of knowledge graphs. ExpressGNN is trained in the variational EM framework for Markov Logic Networks, and is capable of efficient inference and learning, leveraging prior knowledge encoded in logic rules and supervision from graph structured data. It is a compact and expressive model that can also handle zero-shot learning problems.",
        "Abstract": "Markov Logic Networks (MLNs), which elegantly combine logic rules and probabilistic graphical models, can be used to address many knowledge graph problems. However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, we explore the combination of MLNs and GNNs, and use graph neural networks for variational inference in MLN. We propose a GNN variant, named ExpressGNN, which strikes a nice balance between the representation power and the simplicity of the model. Our extensive experiments on several benchmark datasets demonstrate that ExpressGNN leads to effective and efficient probabilistic logic reasoning.",
        "Introduction": "  INTRODUCTION Knowledge graphs collect and organize relations and attributes about entities, which are playing an increasingly important role in many applications, including question answering and information retrieval. Since knowledge graphs may contain incorrect, incomplete or duplicated records, additional processing such as link prediction, attribute classification, and record de-duplication is typically needed to improve the quality of knowledge graphs and derive new facts. Markov Logic Networks (MLNs) were proposed to combine hard logic rules and probabilistic graphical models, which can be applied to various tasks on knowledge graphs (Richardson & Domingos, 2006). The logic rules incorporate prior knowledge and allow MLNs to generalize in tasks with small amount of labeled data, while the graphical model formalism provides a principled framework for dealing with uncertainty in data. However, inference in MLN is computationally intensive, typically exponential in the number of entities, limiting the real-world application of MLN. Also, logic rules can only cover a small part of the possible combinations of knowledge graph relations, hence limiting the application of models that are purely based on logic rules. Graph neural networks (GNNs) have recently gained increasing popularity for addressing many graph related problems effectively (Dai et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Schlichtkrull et al., 2018). GNN-based methods typically require sufficient labeled instances on specific end tasks to achieve good performance, however, knowledge graphs have the long-tail nature (Xiong et al., 2018), i.e., a large portion the relations in only are a few triples. Such data scarcity problem among long-tail relations poses tough challenge for purely data-driven methods. In this paper, we explore the combination of the best of both worlds, aiming for a method which is data-driven yet can still exploit the prior knowledge encoded in logic rules. To this end, we design a simple variant of graph neural networks, named ExpressGNN, which can be efficiently trained in the variational EM framework for MLN. An overview of our method is illustrated in  Fig. 1 . ExpressGNN and the corresponding reasoning framework lead to the following desiderata: • Efficient inference and learning: ExpressGNN can be viewed as the inference network for MLN, which scales up MLN inference to much larger knowledge graph problems. • Combining logic rules and data supervision: ExpressGNN can leverage the prior knowledge encoded in logic rules, as well as the supervision from graph structured data. • Compact and expressive model: ExpressGNN may have small number of parameters, yet it is sufficient to represent mean-field distributions in MLN. • Capability of zero-shot learning: ExpressGNN can deal with the zero-shot learning problem where the target predicate has few or zero labeled instances.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel methodology, Maximum Discrepancy (MAD) competition, to evaluate and compare the performance of image classifiers on a large-scale image collection. MAD starts with a large-scale unlabeled image set, and attempts to falsify a classifier by finding a set of images, whose predictions are in strong disagreement with the rest competing classifiers. A weighted distance over WordNet hierarchy is proposed to quantify the discrepancy between two classifiers on one image. Subjective experiments on the MAD test set reveal the relative strengths and weaknesses among the classifiers, and identify the training techniques and architecture choices that improve the generalizability to natural image manifold. MAD is applied to compare eleven ImageNet classifiers, and verifies the relative improvements achieved by recent DNN-based methods, with a minimal subjective testing budget.",
        "Abstract": "The learning of hierarchical representations for image classification has experienced an impressive series of successes due in part to the availability of large-scale labeled data for training. On the other hand, the trained classifiers have traditionally been evaluated on small and fixed sets of test images, which are deemed to be extremely sparsely distributed in the space of all natural images. It is thus questionable whether recent performance improvements on the excessively re-used test sets generalize to real-world natural images with much richer content variations. Inspired by efficient stimulus selection for testing perceptual models in psychophysical and physiological studies, we present an alternative framework for comparing image classifiers, which we name the MAximum Discrepancy (MAD) competition. Rather than comparing image classifiers using fixed test images, we adaptively sample a small test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy. Human labeling on the resulting model-dependent image sets reveals the relative performance of the competing classifiers, and provides useful insights on potential ways to improve them. We report the MAD competition results of eleven ImageNet classifiers while noting that the framework is readily extensible and cost-effective to add future classifiers into the competition. Codes can be found at https://github.com/TAMU-VITA/MAD.",
        "Introduction": "  INTRODUCTION Large-scale human-labeled image datasets such as ImageNet ( Deng et al., 2009 ) have greatly con- tributed to the rapid progress of research in image classification. In recent years, considerable effort has been put into designing novel network architectures ( He et al., 2016 ;  Hu et al., 2018 ) and ad- vanced optimization algorithms ( Kingma & Ba, 2015 ) to improve the training of image classifiers based on deep neural networks (DNNs), while little attention has been paid to comprehensive and fair evaluation/comparison of their model performance. Conventional model evaluation methodol- ogy for image classification generally follows a three-step approach ( Burnham & Anderson, 2003 ). First, pre-select a number of images from the space of all possible natural images (i.e., natural im- age manifold) to form the test set. Second, collect the human label for each image in the test set to identify its ground-truth category. Third, rank the competing classifiers according to their goodness of fit (e.g., accuracy) on the test set; the one with the best result is declared the winner. A significant problem with this methodology is the apparent contradiction between the enormous size and high dimensionality of natural image manifold and the limited scale of affordable testing (i.e., human labeling, or verifying predicted labels, which is expensive and time consuming). As a result, a typical \"large-scale\" test set for image classification allows for tens of thousands of natural images to be examined, which are deemed to be extremely sparsely distributed in natural image manifold. Model comparison based on a limited number of samples assumes that they are sufficiently representative of the whole population, an assumption that has been proven to be doubtful in image classification. Specifically,  Recht et al. (2019)  found that a minute natural distribution shift leads to a large drop in accuracy for a broad range of image classifiers on both CIFAR-10 ( Krizhevsky, 2009 ) and ImageNet ( Deng et al., 2009 ), suggesting that the current test sets may be far from sufficient to represent hard natural images encountered in the real world. Another problem with the conventional model comparison methodology is that the test sets are pre-selected and therefore fixed. This leaves the door open for adapting classifiers to the test images, deliberately or unintentionally, via extensive hyperparameter tuning, raising the risk of overfitting. As a result, it is never guaranteed that image classifiers with highly competitive performance on such small and fixed test sets can generalize to real-world natural images with much richer content variations. In order to reliably measure the progress in image classification and to fairly test the generalizability of existing classifiers in a natural setting, we believe that it is necessary to compare the classifiers on a much larger image collection in the order of millions or even billions. Apparently, the main challenge here is how to exploit such a large-scale test set under the constraint of very limited budgets for human labeling, knowing that collecting ground-truth labels for all images is extremely difficult, if not impossible. In this work, we propose an efficient and practical methodology, namely the MAximum Discrepancy (MAD) competition, to meet this challenge. Inspired by  Wang & Simoncelli (2008)  and  Ma et al. (2019) , instead of trying to prove an image classifier to be correct using a small and fixed test set, MAD starts with a large-scale unlabeled image set, and attempts to falsify a classifier by finding a set of images, whose predictions are in strong disagreement with the rest competing classifiers (see  Figure 1 ). A classifier that is harder to be falsified in MAD is considered better. The initial image set for MAD to explore can be made arbitrarily large provided that the cost of computational prediction for all competing classifiers is cheap. To quantify the discrepancy between two classifiers on one image, we propose a weighted distance over WordNet hierarchy ( Miller, 1998 ), which is more semantically aligned with human cognition compared with traditional binary judgment (agree Published as a conference paper at ICLR 2020 vs. disagree). The set of model-dependent images selected by MAD are the most informative in discriminating the competing classifiers. Subjective experiments on the MAD test set reveal the relative strengths and weaknesses among the classifiers, and identify the training techniques and architecture choices that improve the generalizability to natural image manifold. This suggests potential ways to improve a classifier or to combine aspects of multiple classifiers. We apply the MAD competition to compare eleven ImageNet classifiers, and find that MAD verifies the relative improvements achieved by recent DNN-based methods, with a minimal subjective test- ing budget. MAD is readily extensible, allowing future classifiers to be added into the competition with little additional cost.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces open domain dialogue systems, which are used in applications such as social chatbots and virtual assistants. It discusses the challenges of grounding open domain dialogue generation with unstructured documents, and presents recent work that has resorted to crowd-sourcing and built benchmarks with the source of Wikipedia. It also argues that existing models still have a long way to go for application in real scenarios, due to overfitting to small training data and the difficulty of collecting enough training data for a new domain or language.",
        "Abstract": "Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining small parameters can be well fitted using the limited training examples. Evaluation results on two benchmarks indicate that with only $1/8$ training data, our model can achieve the state-of-the-art performance and generalize well on out-of-domain knowledge. ",
        "Introduction": "  INTRODUCTION Open domain dialogue systems, due to the applications on social chatbots such as Microsoft XiaoIce ( Shum et al., 2018 ) and virtual assistants such as Amazon Alexa ( Ram et al., 2018 ), have drawn increasing attention from the research community of natural language processing and artificial intelligence. Thanks to the advances in neural sequence modeling ( Vaswani et al., 2017 ;  Sutskever et al., 2014 ) and machine learning techniques ( Li et al., 2017 ; 2016), such systems now are able to reply with plausible responses regarding to conversation history, and thus allow an agent to have a natural conversation with humans. On the other hand, when people attempt to dive into a specific topic, they may clearly realize the gap between the conversation with a state-of-the-art system and the conversation with humans, as the system is only able to awkwardly catch up with the conversation, owing to the lack of knowledge of the subject. We consider grounding open domain dialogue generation with knowledge which is assumed to be unstructured documents. While documents are abundant on the Web, it is difficult to obtain large scale dialogues that are naturally grounded on the documents for learning of a neural generation model. To overcome the challenge, some recent work ( Zhou et al., 2018b ;  Dinan et al., 2019 ) resorts to crowd-sourcing and builds benchmarks with the source of Wikipedia. On the one hand, the datasets pave the way to the recent research on knowledge-grounded response generation/selection ( Zhao et al., 2019 ;  Lian et al., 2019 ;  Li et al., 2019 ); on the other hand, we argue that there still a long way to go for application of the existing models in real scenarios, since (1) the models, especially those achieve state-of-the-art performance via sophisticated neural architectures, just overfit to the small training data (e.g., ∼ 18k dialogues). An evidence is that when they are applied to documents out of the domain of the training data, their performance drops dramatically, as will be seen in our experiments; and (2) it is difficult to collect enough training data for a new domain or a new language, as human effort is expensive.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel end-to-end neural network called the Sequential Variational Soft Q-Learning Network (SVQN) for solving Partially Observable Markov Decision Processes (POMDPs). SVQN integrates the learning of hidden states and the optimization of the planning within the same framework, using a deep recurrent neural network to reduce the computational complexity. Experiments show that SVQN can utilize past information to help in decision making for efficient inference, and outperforms other baselines on several challenging tasks. Ablation studies demonstrate the generalization ability of SVQN over time and its robustness to the disturbance of the observation.",
        "Abstract": "Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.",
        "Introduction": "  INTRODUCTION In recent years, substantial progress has been made in deep reinforcement learning for solving var- ious challenging tasks, including the computer Go game ( Silver et al., 2016 ), Atari games ( Mnih et al., 2015 ), StarCraft ( Zambaldi et al., 2018 ;  Pang et al., 2018 ) and the first-person shooting (FPS) games ( Lample & Chaplot, 2017 ;  Wu & Tian, 2016 ;  Huang et al., 2019 ). However, in many real-world applications, decision-making problems are partially observable ( Aström, 1965 ), preventing such problems from being solved by standard reinforcement learning algorithms. For- mally, these kinds of problems are often defined as Partially Observable Markov Decision Processes (POMDPs) ( Kaelbling et al., 1998 ), which demand information from past observations to help in the decision-making process ( McCallum, 1993 ). Although numerous efforts ( Hausknecht & Stone, 2015 ;  Foerster et al., 2016 ;  Igl et al., 2018 ;  Zhu et al., 2018 ) have been paid to tackle this problem, there still exist various challenges. For example,  Egorov (2015)  tries to solve POMDPs by using the belief of the agent as the input of DQN ( Mnih et al., 2015 ), but this algorithm needs access to the environment model. However, in many rein- forcement learning tasks, it is not possible for the agent to acquire the underlying transition function, making such algorithms inapplicable. Some recent work ( Karkus et al., 2017 ;  McAllester & Singh, 2013 ;  Babayan et al., 2018 ) tries to solve POMDPs under the model-free setting, i.e., the agent does not need to know and learn the transition function of the environment. For instance,  Karkus et al. (2017)  trained an agent to navigate in a partially observable grid world under the model-free setting, i.e., the agent can only observe a part of the grid world and does not learn the transition function. The agent uses its local observations to update its beliefs ( McAllester & Singh, 2013 ;  Babayan et al., 2018 ). In their experiments, the ground truth of the state is the full map plus the location of the agent, which means the representation of the state is explicit. However, in some complex tasks, it is impossible to acquire or design the state or beliefs. To solve the unknown representation problem of the state,  Hausknecht & Stone (2015)  and  Zhu et al. (2018)  try to represent the state as latent variables of neural networks. However, they only use Published as a conference paper at ICLR 2020 a deep recurrent neural network to capture the historical information and fail to utilize the Markov property of the state in POMDPs.  Igl et al. (2018)  apply sequential Monte Carlo (SMC) ( Le et al., 2017 ) to introduce inductive bias to the neural network, which can embody the Markov properties of the state. They can infer the state from the past observations online. However, they separate the planning algorithm from the inference of the state. To infer the hidden states and optimize the planning module jointly, we represent POMDPs as a unified probabilistic graphical model (PGM) and derive a single evidence lower bound (ELBO). We apply structured variational inference to optimize the ELBO. In our implementation, we design generative models to infer the hidden variables, however, the distribution of the latent variables is conditioned on previous hidden states. This is different from standard VAEs ( Kingma & Welling, 2013 ), whose prior of the latent variables can be a standard Gaussian distribution. So, we ap- ply an additional approximate function to tackle the conditional prior problem. The planning can also be solved under the PGM framework. Fortunately, maximum entropy reinforcement learning (MERL) ( Levine, 2018 ) provide a tool to formalize the planning as a probabilistic inference task. In this paper, we propose a novel end-to-end neural network called the sequential variational soft Q-learning network (SVQN), which integrates the learning of hidden states and the optimization of the planning within the same framework. A deep recurrent neural network (RNN) ( Cho et al., 2014 ;  Hochreiter & Schmidhuber, 1997 ) in SVQNs is used to reduce the computational complexity, because the feature extraction can share the same weights in the RNN. Experimental results show that the SVQN can utilize past information to help in decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new training scheme, BinaryDuo, for binary activation networks to reduce the accuracy loss caused by gradient mismatch. BinaryDuo is based on the observation that ternary (or higher bitwidth) activation suffers much less from gradient mismatch problem than the binary activation. Experiments on various benchmarks (VGG-7, AlexNet and ResNet) show that BinaryDuo can achieve state-of-the-art performance. The paper also proposes a new measure for better estimation of gradient mismatch.",
        "Abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have been achieving remarkable performance improvement in vari- ous cognitive tasks. However, the performance improvement generally comes from the increase in the size of the DNN model, which also greatly increases the burden on memory usage and com- puting cost. While server-level environment with large number of GPUs can easily handle such a large model, deploying a large-size DNN model on resource-constrained platforms such as mobile application is still a challenge. To address the challenge, several network compression techniques such as quantization ( Wu et al., 2018b ;  Zhou et al., 2016 ;  Lin et al., 2016 ), pruning ( Han et al., 2015 ;  Hu et al., 2016 ;  Wen et al., 2016 ), and efficient architecture design ( Howard et al., 2017 ;  Wu et al., 2018a ;  Tan & Le, 2019 ) have been introduced. For quantization, previous works tried to reduce the precision of the weights and/or activations of a DNN model to even 1-bit ( Courbariaux et al., 2016 ;  Rastegari et al., 2016 ). In Binary Neural Networks (BNNs), in which both weights and activations have 1-bit precision, high-precision multiplications and accumulations can be replaced by XNOR and pop-count logics which are much cheaper in terms of hardware cost. In addition, the model parameter size of a BNN is 32x less than its full-precision counterpart. Although BNNs can exploit various hardware-friendly features, their main drawback is the accuracy loss. An interesting observation is that binarization of activation causes much larger accuracy drop than the binarization of weights ( Cai et al., 2017 ;  Mishra et al., 2018 ). In particular, it can be ob- served that the performance degradation is much larger when the bit precision is reduced from 2-bit to 1-bit than other multi-bit cases (>2-bit) as shown in  Table 1 . To explain the poor performance of binary activation networks, the gradient mismatch concept has been introduced ( Lin & Talathi, 2016 ;  Cai et al., 2017 ) and several works proposed to modify the backward pass of activation function to reduce the gradient mismatch problem ( Cai et al., 2017 ;  Darabi et al., 2018 ;  Liu et al., 2018 ). In this work, we first show that previous approaches to minimize the gradient mismatch have not been very successful in reducing accuracy loss because the target measure which the approaches tried to minimize is not effective. To back up the claim, we propose a new measure for better estimation of gradient mismatch. Then, we propose a new training scheme for binary activation net- work called BinaryDuo based on the observation that ternary (or higher bitwidth) activation suffers much less from gradient mismatch problem than the binary activation. We show that BinaryDuo can achieve state-of-the-art performance on various benchmarks (VGG-7, AlexNet and ResNet). We also provide our reference implementation code online 1 .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to contextual categorical sequence generation, which combines the augment-REINFORCE-swap-merge (ARSM) estimator for low-variance gradient estimation and a binary-tree construction for low-cost generation. The ARSM estimator provides unbiased, low-variance gradients for categorical variables, using token-level rewards from correlated MC rollouts that naturally serve as the baselines for each other. The binary-tree construction reduces the cost of generating a categorical token from O(V) to O(log2(V)), where V is the vocabulary size. The proposed approach is evaluated on two representative contextual categorical sequence generation tasks, with the number of actions ranging from 53 (neural program synthesis) to 9978 (image captioning).",
        "Abstract": "Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. However, high gradient variance hinders the practical use of this method. To stabilize this method, we adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. We also demonstrate the use of correlated MC rollouts for binary-tree softmax models, which reduce the high generation cost in large vocabulary scenarios by decomposing each categorical action into a sequence of binary actions. We evaluate our methods on both neural program synthesis and image captioning. The proposed methods yield lower gradient variance and consistent improvement over related baselines. ",
        "Introduction": "  INTRODUCTION Contextual categorical sequence generation is a core modeling component in a wide variety of machine learning tasks, such as neural program synthesis ( Bunel et al., 2018 ;  Devlin et al., 2017b ;  Si et al., 2018 ;  Chen et al., 2019 ) and image captioning ( Vinyals et al., 2015 ;  Xu et al., 2015 ). Typically, an encoder-decoder framework is applied. The encoder maps a contextual input to a latent representation, conditioning on which and previously generated tokens the decoder generates categorical tokens in a consecutive manner ( Bahdanau et al., 2014 ; Sutskever et al., 2014;  Cho et al., 2014 ;  Rush et al., 2015 ;  Chopra et al., 2016 ). It is common to train contextual sequence generation models using maximum likelihood estimation (MLE), which attempts to maximize the likelihood of each token in a target sequence given its preceding tokens. Learning with MLE is often sub-optimal as it does not directly optimize the evaluation metric of the end task. It generally suffers from the exposure bias ( Bengio et al., 2015 ; Ranzato et al., 2016) , which refers to the discrepancy between training and generation using the Teacher Forcing ( Williams & Zipser, 1989 ) strategy, i.e., during training ground truth tokens are used as inputs, while during generation, only generated tokens are available. Thus giving higher likelihoods to target sequences does not guarantee the model to generate sequences close to the target or good sequences. Moreover, MLE requires target sequences for training, while for many scenarios in task-oriented dialogue ( Williams & Young, 2007 ) and program synthesis ( Zhong et al., 2017 ), only the final rewards to the generated sequences are available. To overcome the aforementioned issues of MLE, it is common to refine a contextual sequence generation model pre-trained with MLE under the reinforcement learning (RL) framework ( Zaremba & Sutskever, 2015 ; Ranzato et al., 2016;  Bahdanau et al., 2016 ;  Wu et al., 2018 ;  Paulus et al., 2017 ). The objective becomes maximizing the expected rewards of model generated sequences. During training, only the model generated tokens are fed into the model so that the exposure bias is avoided. The reward to guide RL can be: 1) a task-dependent user-defined metric, such as CIDEr for image captioning (Vedantam et al., 2015) and Generalization for neural program synthesis ( Bunel et al., 2018 ); and 2) automatically learned reward using a discriminator or language model ( Yang et al., 2018 ;  Yu et al., 2017 ; Lamb et al., 2016;  Caccia et al., 2018 ;  d'Autume et al., 2019 ). The RL training Published as a conference paper at ICLR 2020 enables direct improvement of the user-defined or learned reward. Moreover, in cases where only weak-supervision is available, e.g., in neural program synthesis, RL may considerably improve the model performance ( Bunel et al., 2018 ;  Zhong et al., 2017 ). However, the gradients of the expected reward in RL often suffer from high Monte Carlo (MC) estimation variance, due to noisy and/or sparse rewards and the large action space that grows exponentially with the sequence length. There has been significant recent interest in variance reduction methods for MC gradient estimation ( Mohamed et al., 2019 ). A highly effective solution is the reparameterization trick ( Kingma & Welling, 2013 ; Rezende et al., 2014), which, however, is applicable to neither discrete variables nor non-differentiable reward functions. For variance reduction involving discrete variables, one potential solution is to combine the Gumbel-softmax trick, which relaxes the discrete variables to continuous ones, with reparameterization to produce low-variance but biased gradients ( Jang et al., 2017 ;  Maddison et al., 2017 ). Another common way for variance reduction is adding appropriate baselines ( Owen, 2013 ;  Williams, 1992 ; Paisley et al., 2012; Ranganath et al., 2014;  Mnih & Gregor, 2014 ), and there exist several such methods customized for discrete variables ( Tucker et al., 2017 ;  Grathwohl et al., 2018 ). However, due to either the inherent biases or difficulty to learn the parameters of the baselines, it is unclear how effective these newly proposed estimators are in backpropagating the gradients through a sequence of discrete variables (Yin et al., 2019). This is exacerbated in contextual categorical sequence generation problems, where it is common for a sequence to contain quite a few tokens/actions, each of which is selected from a set of thousands of candidates. Another practical issue is that generating a token from a large vocabulary via the softmax output layer is often computationally heavy. This prevents a categorical sequence generation model from being deployed to low-power devices. Despite significant recent efforts in addressing the computation bottleneck due to a wide softmax layer ( Shim et al., 2017 ;  Zhang et al., 2018 ;  Chen et al., 2018 ), for categorical sequence generation, it is so far unclear how to address the softmax computational bottleneck while at the same time providing low-variance gradient of its RL objective. This paper makes two primary contributions: 1) To address the high gradient variance issue, we adapt to contextual categorical sequence generation tasks the augment-REINFORCE-swap-merge (ARSM) estimator (Yin et al., 2019), which provides unbiased, low-variance gradients for categorical variables, using token-level rewards from correlated MC rollouts that naturally serve as the baselines for each other. We show that the number of rollouts is adapted to the model uncertainty on the latest generated token, and can also be manually controlled to balance variance reduction and computation. 2) To address the high generation cost issue, we replace the generation of each categorical variable in the sequence, via a categorical softmax output layer, with the generation of a sequence of binary decisions on a binary tree from its root node to a leaf node, which is occupied by a unique term of the vocabulary. For training, we adapt the augment-REINFORCE-merge (ARM) estimator ( Yin & Zhou, 2019 ), which provides unbiased, low-variance gradients for binary variables, to backpropagate the gradients through the sequence of binary sequences. Under this binary-tree construction, the cost of generating a categorical token reduces from O(V ) to O(log 2 (V )), where V is the vocabulary size. We demonstrate our methods on two representative contextual categorical sequence generation tasks, with the number of actions ranging from 53 (neural program synthesis) to 9978 (image captioning).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an abstract summarizing the introduction of a paper on model-free deep reinforcement learning (RL) algorithms for solving difficult control and decision-making tasks in partially observable (PO) environments. It discusses the challenges of solving PO tasks, such as those encountered in video games and robotic control, and the various methods proposed for tackling them. It also highlights the drawbacks of the simple solution of including a history of raw observations in the current observation.",
        "Abstract": "In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations to solve the task, and how to improve the policy. In this study, we propose an RL algorithm for solving PO tasks. Our method comprises two parts: a variational recurrent model (VRM) for modeling the environment, and an RL controller that has access to both the environment and the VRM. The proposed algorithm was tested in two types of PO robotic control tasks, those in which either coordinates or velocities were not observable and those that require long-term memorization. Our experiments show that the proposed algorithm achieved better data efficiency and/or learned more optimal policy than other alternative approaches in tasks in which unobserved states cannot be inferred from raw observations in a simple manner.",
        "Introduction": "  INTRODUCTION Model-free deep reinforcement learning (RL) algorithms have been developed to solve difficult control and decision-making tasks by self-exploration ( Sutton & Barto, 1998 ;  Mnih et al., 2015 ;  Silver et al., 2016 ). While various kinds of fully observable environments have been well investigated, recently, partially observable (PO) environments ( Hafner et al., 2018 ;  Igl et al., 2018 ;  Lee et al., 2019 ;  Jaderberg et al., 2019 ) have commanded greater attention, since real-world applications often need to tackle incomplete information and a non-trivial solution is highly desirable. There are many types of PO tasks; however, those that can be solved by taking the history of observations into account are more common. These tasks are often encountered in real life, such as videos games that require memorization of previous events ( Kapturowski et al., 2018 ;  Jaderberg et al., 2019 ) and robotic control using real-time images as input ( Hafner et al., 2018 ;  Lee et al., 2019 ). While humans are good at solving these tasks by extracting crucial information from the past observations, deep RL agents often have difficulty acquiring satisfactory policy and achieving good data efficiency, compared to those in fully observable tasks ( Hafner et al., 2018 ;  Lee et al., 2019 ). For solving such PO tasks, several categories of methods have been proposed. One simple, straight- forward solution is to include a history of raw observations in the current \"observation\" ( McCallum, 1993 ;  Lee et al., 2019 ). Unfortunately, this method can be impractical when decision-making requires a long-term memory because dimension of observation become unacceptably large if a long history is included.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes GraphZoom, a multi-level spectral approach to enhancing both the quality and scalability of unsupervised graph embedding methods. GraphZoom consists of four major kernels: graph fusion, spectral graph coarsening, graph embedding, and embedding refinement. Experiments show that GraphZoom can improve the classification accuracy over all baseline embedding methods for both transductive and inductive tasks, and can accelerate the entire embedding process by up to 40.8x while producing a similar or better accuracy than state-of-the-art techniques.",
        "Abstract": "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. GraphZoom allows any existing embedding methods to be applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that GraphZoom can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to $40.8 \\times$, when compared to the  state-of-the-art unsupervised embedding methods. ",
        "Introduction": "  INTRODUCTION Recent years have seen a surge of interest in graph embedding, which aims to encode nodes, edges, or (sub)graphs into low dimensional vectors that maximally preserve graph structural information. Graph embedding techniques have shown promising results for various applications such as ver- tex classification, link prediction, and community detection (Zhou et al., 2018); (Cai et al., 2018); (Goyal & Ferrara, 2018). However, current graph embedding methods have several drawbacks in terms of either accuracy or scalability. On the one hand, random-walk-based embedding algorithms, such as DeepWalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016), attempt to embed a graph based on its topology without incorporating node attribute information, which limits their embedding power. Later, graph convolutional networks (GCN) are developed with the basic notion that node embeddings should be smoothed over the entire graph (Kipf & Welling, 2016). While GCN leverages both topology and node attribute information for simplified graph convolution in each layer, it may suffer from high-frequency noise in the initial node features, which compromises Published as a conference paper at ICLR 2020 the embedding quality (Maehara, 2019). On the other hand, few embedding algorithms can scale well to large graphs with millions of nodes due to their high computation and storage cost (Zhang et al., 2018a). For example, graph neural networks (GNNs) such as GraphSAGE (Hamilton et al., 2017) collectively aggregate feature information from the neighborhood. When stacking multiple GNN layers, the final embedding vector of a node involves the computation of a large number of in- termediate embeddings from its neighbors. This will not only cause drastic increase in the amount of computation among nodes, but also lead to high memory usage for storing the intermediate results. In literature, increasing the accuracy and improving the scalability of graph embedding methods are largely viewed as two orthogonal problems. Hence most research efforts are devoted to addressing only one of the problems. For instance, Chen et al. (2018) and Fu et al. (2019) proposed multi- level methods to obtain high-quality embeddings by training unsupervised models at every level; but their techniques do not improve scalability due to the additional training overhead. Liang et al. (2018) developed a heuristic algorithm to coarsen the graph by merging nodes with similar local structures. They use GCN to refine the embedding results on the coarsened graphs, which is not only time consuming to train but also potentially degrading accuracy when the number of GCN layers increases. More recently, Akbas & Aktas (2019) proposed a similar strategy to coarsen the graph, where certain useful structural properties of the graph are preserved (e.g., local neighborhood proximity). However, this work lacks proper refinement methods to improve the embedding quality. In this paper we propose GraphZoom, a multi-level spectral approach to enhancing both the quality and scalability of unsupervised graph embedding methods. More concretely, GraphZoom consists of four major kernels: (1) graph fusion, (2) spectral graph coarsening, (3) graph embedding, and (4) embedding refinement. The graph fusion kernel first converts the node feature matrix into a feature graph and then fuses it with the original topology graph. The fused graph provides richer information to the ensuing graph embedding step to achieve a higher accuracy. Spectral graph coarsening produces a series of successively coarsened graphs by merging nodes based on their spectral similarities. We show that our coarsening algorithm can effectively and efficiently retain the first few eigenvectors of the graph Laplacian matrix, which is critical for preserving the key graph structures. During the graph embedding step, any of the existing unsupervised graph embedding techniques can be applied to obtain node embeddings for the graph at the coarsest level. 2 Embedding refinement is then employed to refine the embeddings back to the original graph by applying a proper graph filter to ensure embeddings are smoothed over the graph. We evaluate the proposed GraphZoom framework on three transductive benchmarks: Cora, Citeseer and Pubmed citation networks as well as two inductive dataset: PPI and Reddit for vertex classifi- cation task. We further test the scalability of our approach on friendster dataset, which contains 8 million nodes and 400 million edges. Our experiments show that GraphZoom can improve the clas- sification accuracy over all baseline embedding methods for both transductive and inductive tasks. Our main technical contributions are summarized as follows: • GraphZoom generates high-quality embeddings. We propose novel algorithms to encode graph structures and node attribute information in a fused graph and exploit graph filtering during re- finement to remove high-frequency noise. This results in a relative increase of the embedding accuracy over the prior arts by up to 19.4% while reducing the execution time by at least 2×. • GraphZoom improves scalability. Our approach can significantly reduce the embedding run time by effectively coarsening the graph without losing the key spectral properties. Experiments show that GraphZoom can accelerate the entire embedding process by up to 40.8× while produc- ing a similar or better accuracy than state-of-the-art techniques. • GraphZoom is highly composable. Our framework is agnostic to underlying graph embedding techniques. Any of the existing unsupervised embedding methods, either transductive or inductive, can be incorporated by GraphZoom in a plug-and-play manner.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the effectiveness of manifold-based defenses against adversarial examples, and suggests that these defenses need to be aware of the topology of the underlying data manifold in order to be effective. We demonstrate that if the generative model does not capture the topology of the underlying manifold, it can adversely affect these defenses. This paper opens a rich avenue for future work on using topology-aware generative models for defense to adversarial examples.",
        "Abstract": "ML algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, recently researchers have demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause mis-classification). Existence of adversarial examples has hindered deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifold-based defenses, where a sample is \"pulled back\" into the data manifold before classifying. These defenses rely on the manifold assumption (data lie in a manifold of lower dimension than the input space). These defenses use a generative model to approximate the input distribution. This paper asks the following question: do the generative models used in manifold-based defenses need to be topology-aware? Our paper suggests the answer is yes. We provide theoretical and empirical evidence to support our claim.",
        "Introduction": "  INTRODUCTION Machine-learning (ML) algorithms, especially deep-neural networks (DNNs), have had resounding success in several domains. However, adversarial examples have hindered their deployment in safety-critical domains, such as autonomous driving and malware detection. Adversarial examples are constructed by an adversary adding a small perturbation to a data-point so that it is misclassified. Several algorithms for constructing adversarial examples exist in the literature (Biggio et al., 2013; Szegedy et al., 2013; Goodfellow et al., 2014b; Kurakin et al., 2016a; Carlini & Wagner, 2017; Madry et al., 2017; Papernot et al., 2017). Numerous defenses for adversarial examples also have been explored (Kurakin et al., 2016b; Guo et al., 2017; Sinha et al., 2017; Song et al., 2017; Tramèr et al., 2017; Xie et al., 2017; Dhillon et al., 2018; Raghunathan et al., 2018; Cohen et al., 2019; Dubey et al., 2019). In this paper, we focus on \"manifold-based\" defenses (Ilyas et al., 2017; Samangouei et al., 2018). The general idea in these defenses is to \"pull back\" the data point into the data manifold before classifica- tion. These defenses leverage the fact that, in several domains, natural data lies in a low-dimensional manifold (henceforth referred to as the manifold assumptions) (Zhu & Goldberg, 2009). The data distribution and hence actual manifold that the natural data lies in is usually unknown, so these defenses use a generative model to \"approximate\" the data distribution. Generative models attempt to learn to generate data according to the underlying data distribution. (The input to a generative model is usually random noise from a known distribution, such as Gaussian or uniform.) There are Published as a conference paper at ICLR 2020 various types of generative models in the literature, such as variational autoencoder (VAE) (Kingma & Welling, 2013), generative adversarial network (GAN) (Goodfellow et al., 2014a) and reversible generative models, e.g., real-valued non-volume preserving transform (Real NVP) (Dinh et al., 2016). This paper addresses the following question: Do manifold-based defenses need to be aware of the topology of the underlying data manifold? In this paper, we suggest the answer to this question is yes. We demonstrate that if the generative model does not capture the topology of the underlying manifold, it can adversely affect these defenses. In these cases, the underlying generative model is being used as an approximation of the underlying manifold. We believe this opens a rich avenue for future work on using topology-aware generative models for defense to adversarial examples.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a physics-aware difference graph network (PA-DGN) to model real world phenomena, such as climate observations, traffic flow, physics and chemistry simulation. PA-DGN leverages differences of sparsely available data from the physical systems to learn latent representations from data-rich applications. The proposed architecture consists of spatial difference layers and recurrent graph networks to efficiently learn local representations and predict temporal differences, respectively. Two representative tasks are provided to demonstrate the applicability of PA-DGN; the approximation of directional derivatives and the prediction of graph signals.",
        "Abstract": "Sparsely available data points cause numerical error on finite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn finite differences inspired by physics equations. PA-DGN leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. We demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.",
        "Introduction": "  INTRODUCTION Modeling real world phenomena, such as climate observations, traffic flow, physics and chemistry simulation ( Li et al., 2018 ;  Geng et al., 2019 ;  Long et al., 2018 ;  de Bezenac et al., 2018 ;  Sanchez- Gonzalez et al., 2018 ;  Gilmer et al., 2017 ), is important but extremely challenging. While deep learning has achieved remarkable successes in prediction tasks by learning latent representations from data-rich applications such as image recognition ( Krizhevsky et al., 2012 ), text understanding ( Wu et al., 2016 ), and speech recognition ( Hinton et al., 2012 ), we confront many challenging scenarios in modeling natural phenomena with deep neural networks when only a limited number of observations are available. Particularly, the sparsely available data points cause substantial numerical error when we utilize existing finite difference operators and the limitation requires a more principled way to redesign deep learning models. While many methods have been proposed to model physics-simulated observations using deep learning, many of them are designed under the assumption that input is on a continuous domain. For example,  Raissi et al. (2017a ; b ) proposed physics-informed neural networks (PINNs) to learn nonlinear relations between input (spatial- and temporal-coordinates (x, t)) and output simulated with a given partial differential equation (PDE). Since  Raissi et al. (2017a ; b ) use the coordinates as input and compute derivatives based on the coordinates to represent the equation, the setting is only valid when the data are densely observed over spatial and temporal space. Prior knowledge related to physics equations has been combined with data-driven models for var- ious purposes.  Chen et al. (2015)  proposed a nonlinear diffusion process for image restoration and de  Bezenac et al. (2018)  incorporated the transport physics (advection-diffusion equation) with deep neural networks for forecasting sea surface temperature by extracting the motion field.  Lutter et al. (2019)  introduced deep Lagrangian networks specialized to learn Lagrangian mechanics with learnable parameters.  Seo & Liu (2019)  proposed a physics-informed regularizer to impose data- specific physics equations. In common, the methods in  Chen et al. (2015) ;  de Bezenac et al. (2018) ;  Lutter et al. (2019)  are not efficiently applicable to sparsely discretized input as only a small number of data points are available and continuous properties on given space are not easily recovered. It is Published as a conference paper at ICLR 2020 unsuitable to directly use continuous differential operators to provide local behaviors because it is hard to approximate the continuous derivatives precisely with the sparse points ( Shewchuk, 2002 ;  Amenta & Kil, 2004 ;  Luo et al., 2009 ). Furthermore, they are only applicable when the specific physics equations are explicitly given and still hard to be generalized to incorporate other types of equations. As another direction to modeling physics-simulated data,  Long et al. (2018)  proposed PDE-Net which uncovers the underlying hidden PDEs and predicts the dynamics of complex systems.  Ruthotto & Haber (2018)  derived new CNNs: parabolic and hyperbolic CNNs based on ResNet ( He et al., 2016 ) architecture motivated by PDE theory. While  Long et al. (2018) ;  Ruthotto & Haber (2018)  are flexible to uncover hidden physics from the constrained kernels, it is still restrictive to a regular grid where the proposed constraints on the learnable filters are easily defined. The topic of reasoning physical dynamics of discrete objects has been actively studied ( Sanchez- Gonzalez et al., 2018 ;  Battaglia et al., 2016 ;  Chang et al., 2016 ) as the appearance of graph-based neural networks ( Kipf & Welling, 2017 ;  Santoro et al., 2017 ;  Gilmer et al., 2017 ). Although these models can handle sparsely located data points without explicitly given physics equations, they are purely data-driven so that the physics-inspired inductive bias for exploiting finite differences is not considered at all. In contrast, our method consists of physics-aware modules allowing efficiently leveraging the inductive bias to learn spatiotemporal data from the physics system. In this paper, we propose physics-aware difference graph networks (PA-DGN) whose architecture is motivated to leverage differences of sparsely available data from the physical systems. The differences are particularly important since most of the physics-related dynamic equations (e.g., Navier-Stokes equations) handle differences of physical quantities in spatial and temporal space instead of using the quantities directly. Inspired by the property, we first propose spatial difference layer (SDL) to efficiently learn the local representations by aggregating neighboring information in the sparse data points. The layer is based on graph networks (GN) as it easily leverages structural features to learn the localized representations and share the parameters for computing the localized features. Then, the layer is followed by recurrent graph networks (RGN) to predict the temporal difference which is another core component of physics-related dynamic equations. PA-DGN is applicable to various tasks and we provide two representative tasks; the approximation of directional derivatives and the prediction of graph signals.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel continual learning model with Additive Parameter Decomposition (APD) to tackle the problems of scalability, memory usage, computations, and order sensitivity. APD decomposes the network parameters at each layer of the target network into task-shared and sparse task-specific parameters with small mask vectors. Experiments on multiple datasets show that the proposed model is significantly superior to existing continual learning methods in terms of accuracy, efficiency, scalability, and order-robustness.",
        "Abstract": "While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, there are issues that remain to be tackled in order to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters  for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness.",
        "Introduction": "  INTRODUCTION Continual learning ( Thrun, 1995 ), or lifelong learning, is a learning scenario where a model is incrementally updated over a sequence of tasks, potentially performing knowledge transfer from earlier tasks to later ones. Building a successful continual learning model may lead us one step further towards developing a general artificial intelligence, since learning numerous tasks over a long-term time period is an important aspect of human intelligence. Continual learning is often formulated as an incremental / online multi-task learning that models complex task-to-task relationships, either by sharing basis vectors in linear models ( Kumar & Daume III, 2012 ;  Ruvolo & Eaton, 2013 ) or weights in neural networks ( Li & Hoiem, 2016 ). One problem that arises here is that as the model learns on the new tasks, it could forget what it learned for the earlier tasks, which is known as the problem of catastrophic forgetting. Many recent works in continual learning of deep networks ( Li & Hoiem, 2016 ;  Lee et al., 2017 ;  Shin et al., 2017 ;  Kirkpatrick et al., 2017 ;  Riemer et al., 2019 ;  Chaudhry et al., 2019 ) tackle this problem by introducing advanced regularizations to prevent drastic change of network weights. Yet, when the model should adapt to a large number of tasks, the interference between task-specific knowledge is inevitable with fixed network capacity. Recently introduced expansion-based approaches handle this problem by expanding the network capacity as they adapt to new tasks ( Rusu et al., 2016 ;  Fang et al., 2017 ;  Yoon et al., 2018 ;  Li et al., 2019 ). These recent advances have largely alleviated the catastrophic forgetting, at least with a small number of tasks. However, to deploy continual learning to real-world systems, there are a number of issues that should be resolved. First, in practical scenarios, the number of tasks that the model should train on may be large. In the lifelong learning setting, the model may even have to continuously train on an unlimited number of tasks. Yet, conventional continual learning methods have not been verified for their scalability to a large number of tasks, both in terms of effectiveness in the prevention of Published as a conference paper at ICLR 2020 catastrophic forgetting, and efficiency as to memory usage and computations (See  Figure 1 (a) , and (b)). Another important but relatively less explored problem is the problem of task order sensitivity, which describes the performance discrepancy with respect to the task arrival sequence (See  Figure 1 (c) ). The task order that the model trains on has a large impact on the individual task performance as well as the final performance, not only because of the model drift coming from the catastrophic forgetting but due to the unidirectional knowledge transfer from earlier tasks to later ones. This order-sensitivity could be highly problematic if fairness across tasks is important (e.g. disease diagnosis). To handle these practical challenges, we propose a novel continual learning model with Additive Parameter Decomposition (APD). APD decomposes the network parameters at each layer of the target network into task-shared and sparse task-specific parameters with small mask vectors. At each arrival of a task to a network with APD, which we refer to as APD-Net, it will try to maximally utilize the task-shared parameters and will learn the incremental difference that cannot be explained by the shared parameters using sparse task-adaptive parameters. Moreover, since having a single set of shared parameters may not effectively utilize the varying degree of knowledge sharing structure among the tasks, we further cluster the task-adaptive parameters to obtain hierarchically shared parameters (See  Figure 2 ). This decomposition of generic and task-specific knowledge has clear advantages in tackling the previously mentioned problems. First, APD will largely alleviate catastrophic forgetting, since learning on later tasks will have no effect on the task-adaptive parameters for the previous tasks, and will update the task-shared parameters only with generic knowledge. Secondly, since APD does not change the network topology as existing expansion-based approaches do, APD-Net is memory-efficient, and even more so with hierarchically shared parameters. It also trains fast since it does not require multiple rounds of retraining. Moreover, it is order-robust since the task-shared parameters can stay relatively static and will converge to a solution rather than drift away upon the arrival of each task. With the additional mechanism to retroactively update task-adaptive parameters, it can further alleviate the order-sensitivity from unidirectional knowledge transfer as well. We validate our methods on several benchmark datasets for continual learning while comparing against state-of-the-art continual learning methods to obtain significantly superior performance with minimal increase in network capacity while being scalable and order-robust. The contribution of this paper is threefold: • We tackle practically important and novel problems in continual learning that have been overlooked thus far, such as scalability and order robustness. • We introduce a novel framework for continual deep learning that effectively prevents catastrophic forgetting, and is highly scalable and order-robust, which is based on the decomposition of the network parameters into shared and sparse task-adaptive parameters with small mask vectors. • We perform extensive experimental validation of our model on multiple datasets against recent continual learning methods, whose results show that our method is significantly superior to them in terms of the accuracy, efficiency, scalability, as well as order-robustness.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on the energy-based latent variable model, which is a generative model used to characterize high-dimensional data. The model is expressed in terms of the joint distribution of a visible and a hidden random vector, and the parameter vector is estimated using the maximum likelihood method. Monte Carlo methods are used to approximate the gradient, and various optimization techniques are used to iteratively update the parameter estimate. However, this strategy is not without limitations, as MCMC estimators are typically biased on finite steps and require a long time to obtain an accurate gradient.",
        "Abstract": "The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing method. Our findings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models.",
        "Introduction": "  INTRODUCTION Energy-based latent variable models cover a broad class of generative models that are frequently used to characterize sophisticated distributions of high-dimensional data. Popular examples of this class include the restricted Boltzmann machines ( RBM, Smolensky, 1986 ;  Hinton, 2012 ), deep be- lief nets ( Hinton et al., 2006 ), and exponential family harmoniums ( Welling et al., 2005 ), among many others. Energy-based models are complementary to directed generative models such as the variational autoencoders ( Kingma & Welling, 2014 ), and have gained great success in synthesizing realistic data samples ( Xie et al., 2016 ; 2018b). They can also be combined with directed models to build more sophisticated structures ( Xie et al., 2018a ). In this article we focus on the energy-based latent variable model, whose general form can be expressed in terms of the joint distribution of a visible random vector, v ∈ V ⊂ R p , and a hidden or latent random vector, h ∈ H ⊂ R r , with the density function p(v, h; θ) = 1 Z(θ) exp{−E(v, h; θ)}, (1) where θ ∈ Θ is the unknown parameter vector, E(v, h; θ) is the energy function, and Z(θ) is a normalizing constant to ensure that p(v, h; θ) is a legitimate probability density or mass function. The model distribution, p v (v; θ), is defined to be the marginal distribution of p(v, h; θ). Similar to many other machine learning models, the standard approach to estimating the parameter vector θ is the maximum likelihood method. It can be shown that the derivative of the log-likelihood function can be expressed as the difference of two expectations, and hence Monte Carlo methods, especially the Markov chain Monte Carlo ( MCMC, Gilks et al., 1995 ), can be used to approximate the gradient. Various optimization techniques, such as the stochastic gradient method ( SG, Robbins & Monro, 1951 ;  Bottou, 2010 ), can then proceed to iteratively update the parameter estimate. This strategy, though elegant in theory, is not without limitations. In particular, MCMC estimators are typically consistent in the limiting case, but biased on finite steps, so one needs to run MCMC for a long time to obtain an accurate gradient, which would take tremendous amount of computing time.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel graph inference learning (GIL) framework for semi-supervised node classification on graphs. GIL builds structure relations between nodes to infer unknown node labels from those labeled nodes in an end-to-end way. The structure relations are well defined by jointly considering node attributes, between-node paths, and graph topological structures. A meta-learning procedure is introduced to optimize structure relations, which could be the first time for graph node classification. Comprehensive evaluations on four datasets demonstrate the superiority of GIL in contrast with other state-of-the-art methods.",
        "Abstract": "In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task.",
        "Introduction": "  INTRODUCTION Graph, which comprises a set of vertices/nodes together with connected edges, is a formal structural representation of non-regular data. Due to the strong representation ability, it accommodates many potential applications, e.g., social network ( Orsini et al., 2017 ), world wide data ( Page et al., 1999 ), knowledge graph ( Xu et al., 2017 ), and protein-interaction network ( Borgwardt et al., 2007 ). Among these, semi-supervised node classification on graphs is one of the most interesting also popular topics. Given a graph in which some nodes are labeled, the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works ( Brandes et al., 2008 ;  Zhou et al., 2004 ;  Zhu et al., 2003 ;  Yang et al., 2016 ;  Zhao et al., 2019 ) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations, it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information. With the progress of deep learning on grid-shaped images/videos ( He et al., 2016 ), a few of graph convolutional neural networks (CNN) based methods, including spectral ( Kipf & Welling, 2017 ) and spatial methods ( Niepert et al., 2016 ;  Pan et al., 2018 ;  Yu et al., 2018 ), have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations. Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters, they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs. Especially, in the case of few-shot learning, where a small number of training nodes are labeled, this kind of methods would drastically compromise the performance. For example, the Pubmed graph dataset ( Sen et al., 2008 ) consists of 19,717 nodes and 44,338 edges, but only 0.3% nodes are labeled for the semi-supervised node classification task. These aforementioned works usually boil down to a general classification task, where the model is learnt on a training set and selected by checking a validation set. However, they do not put great efforts on how to learn to infer from one node to another node on a topological graph, especially in the few-shot regime. In this paper, we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes, and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples. Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations. The between-node relations are structured as the integration of node attributes, connection paths, and graph topological structures. It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes, the consistency of local topological structures, and the between-node path reachability, as shown in  Fig. 1 . The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution ( Defferrard et al., 2016 ) for the sake of high-level feature extraction. For the between-node path reachability, we adopt the random walk algorithm to obtain the characteristics from a labeled reference node v i to a query unlabeled node v j in a given graph. Based on the computed node representation and between-node reachability, the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph. Inspired by the recent meta-learning strategy ( Finn et al., 2017 ), we learn to infer the structure relations from a training set to a validation set, which can benefit the generalization capability of the learned model. In other words, our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples, such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds: • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way. The structure relations are well defined by jointly considering node attributes, between-node paths, and graph topological structures. • To make the inference model better generalize to test nodes, we introduce a meta-learning procedure to optimize structure relations, which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora, Citeseer, and Pubmed) and one knowledge graph data (i.e., NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel Transferable Neural Architecture Search (T-NAS) method which is able to learn a meta-architecture that is able to adapt to a new task quickly through a few gradient steps. An efficient first-order approximation algorithm is proposed to optimize the whole search network based on gradient descent. Experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost.",
        "Abstract": "Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the transferability of NAS and conduct fast adaptation of neural architectures, we propose a novel Transferable Neural Architecture Search method based on meta-learning in this paper, which is termed as T-NAS. T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.",
        "Introduction": "  INTRODUCTION Deep neural networks have achieved huge successes in many machine learning tasks (Girshick, 2015Neural Architecture Search (NAS) (Pham et al., 2018; Liu et al., 2018b; Guo et al., 2019) is pro- posed to automatically search network structure for alleviating the complicated network design and heavy dependence on prior knowledge. More importantly, NAS has been proved to be effective and obtained the remarkable performance in image classification (Pham et al., 2018; Liu et al., 2018b), object detection (Ghiasi et al., 2019) and semantic segmentation (Chen et al., 2018; Liu et al., 2019). However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. As shown in  Figure 1 , we get the architecture-0 on a given dataset using a NAS method. Now, what if there exists a new task? This drives us to ask: how to get a suitable architecture for a new task in NAS? Generally, there exist two simple solutions in handling multiple tasks. One of them (S1) is to search an architecture for a new task from scratch but it is inefficient and not flexible for practical application scenarios. Another solution (S2) is to borrow architecture from the ones searched on other tasks but it might be not optimal for the new task. Therefore, it is urgently needed to study the transferability of NAS for large-scale model deployment in practical application. It should be Published as a conference paper at ICLR 2020 more desirable to learn a transferable architecture that can adapt to some new unseen tasks easily and quickly according to the previous knowledge. To this end, we propose a novel Transferable Neural Architecture Search (T-NAS) method (the bottom of  Figure 1 ). The starting point of T-NAS is inspired by recent meta-learning methods (Finn et al., 2017; Antoniou et al., 2019; Sun et al., 2019), especially Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017), where a model learns the meta-weights that are able to adapt to a new task through a few gradient steps. Push it forward, it is also possible to find a good initial point of network architecture for NAS. Therefore, the T-NAS learns a meta-architecture (transferable architecture) that is able to adapt to a new task quickly through a few gradient steps, which is more flexible than other NAS methods. Similar to MAML, such a good initial meta-architecture for adaptation should be more sensitive to changes in different tasks such that it can be easily transferred. It is worth mentioning that this is not the first work on the transferability of neural architecture. There are also some recent works that attempt to utilize the knowledge on neural architectures learned from previous tasks, such as Wong et al. (2018); Shaw et al. (2018). Specifically, Wong et al. (2018) proposes to transfer the architecture knowledge under a multi-task learning perspective, where the number of tasks is fixed during training phase, and it cannot do a fast adaption for a new task. In contrast, our model is able to make the adaption fast and the number of tasks is unlimited during training. The difference between our model and Shaw et al. (2018) is also obvious, where Shaw et al. (2018) is based on Bayesian inference but our model is based on gradient-based meta-learning. The quantitative comparison with Shaw et al. (2018) can be found in  Table 3 . Generally, architecture structure cannot be trained independently regardless of network weights (Liu et al., 2018b; Pham et al., 2018). Analogously, the training of meta-architecture is also associated with meta-weights. Therefore, the meta-architecture and meta-weights need to be optimized jointly across different tasks, which is a typical bilevel optimization problem (Liu et al., 2018b). In order to solve the costly bilevel optimization in T-NAS, we propose an efficient first-order approximation algorithm to update meta-architecture and meta-weights together. After the whole model is opti- mized, given a new task, we can get the network architecture structure suitable for the specific task with a few gradient steps from meta-architecture and meta-weights. At last, the decoded discrete architecture is used for the final architecture evaluation. To demonstrate the effectiveness of T-NAS, we conduct extensive experiments on task-level prob- lems due to amounts of tasks. Specifically, we split the experiments into two parts: few-shot learn- ing setting and supervised learning setting. For few-shot learning, T-NAS achieves state-of-the-art performance on multiple datasets (Omniglot, Mini-Imagenet, Fewshot-CIFAR100) compared with previous methods and other NAS-based methods. As for supervised learning, a 200-shot 50-query 10-way experiment setting is designed on the Mini-Imagenet dataset. Compared with the searched architectures from scratch for new given tasks, T-NAS achieves comparable performance but with 50x less searching cost. Our main contributions are summarized as follows: Published as a conference paper at ICLR 2020 • We propose a novel Transferable Neural Architecture Search (T-NAS). T-NAS can learn a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which is more flexible than other NAS methods. • We give the formulation of T-NAS and analyze the difference between T-NAS and other NAS methods. Further, to solve the bilevel optimization, we propose an efficient first-order approximation algorithm to optimize the whole search network based on gradient descent. • Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to symbolic superoptimization that is independent of human knowledge. We propose a reinforcement learning based method that autonomously discovers simplifying equivalences from scratch. Our method is able to learn a set of equivalent transformation actions from scratch, and then use them to simplify symbolic expressions. Experiments on a variety of datasets demonstrate that our method can achieve comparable performance with existing methods, while being independent of human knowledge.",
        "Abstract": "Deep  symbolic superoptimization refers to the task of applying deep learning methods to simplify symbolic expressions.   Existing approaches either perform supervised training on human-constructed datasets that defines equivalent expression pairs, or apply reinforcement learning with human-defined equivalent trans-formation actions.  In short,  almost all existing methods rely on human knowledge to define equivalence, which suffers from large labeling cost and learning bias, because it is almost impossible to define and comprehensive equivalent set. We thus propose HISS, a reinforcement learning framework for symbolic super-optimization that keeps human outside the loop.  HISS introduces a tree-LSTM encoder-decoder network with attention to ensure tractable learning.   Our experiments show that HISS can discover more simplification rules than existing human-dependent methods, and can learn meaningful embeddings for symbolic expressions, which are indicative of equivalence.",
        "Introduction": "  INTRODUCTION Superoptimization refers to the task of simplifying and optimizing over a set of machine instruc- tions, or code ( Massalin, 1987 ;  Schkufza et al., 2013 ), which is a fundamental problem in computer science. As an important direction in superoptimization, symbolic expression simplification, or symbolic superoptimization, aims at transforming symbolic expression to a simpler form in an ef- fective way, so as to avoid unnecessary computations. Symbolic superoptimization is an important component in compilers, e.g. LLVM and Halide, and it also has a wide application in mathematical engines including Wolfram 2 , Matlab, and Sympy. Over recent years, applying deep learning methods to address symbolic superoptimization has at- tracted great attention. Despite their variety, existing algorithms can be roughly divided into two categories. The first category is supervised learning, i.e. to learn a mapping between the input expressions and the output simplified expressions from a large number of human-constructed ex- pression pairs ( Arabshahi et al., 2018 ;  Zaremba & Sutskever, 2014 ). Such methods rely heavily on a human-constructed dataset, which is time- and labor-consuming. What is worse, such systems are highly susceptible to bias, because it is generally very hard to define a minimum and comprehensive axiom set for training. It is highly possible that some forms of equivalence are not covered in the training set, and fail to be recognized by the model. In order to remove the dependence on human annotations, the second category of methods leverages reinforcement learning to autonomously dis- cover simplifying equivalence ( Chen et al., 2018 ). However, to make the action space tractable, such systems still rely on a set of equivalent transformation actions defined by human beings, which again suffers from the labeling cost and learning bias. In short, the existing neural symbolic superoptimization algorithms all require human input to define equivalences. It would have benefited from improved efficiency and better simplification if there were algorithms independent of human knowledge. In fact, symbolic superoptimization should have been a task that naturally keeps human outside the loop, because it directly operates on machine code, whose consumers and evaluators are machines, not humans.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates and develops optimization techniques to accelerate training large deep neural networks, focusing on approaches based on variants of Stochastic Gradient Descent (SGD). Recent works have demonstrated that up to certain minibatch sizes, linear scaling of the learning rate with minibatch size can be used to speed up training. However, an adaptive learning rate mechanism for large batch learning is needed. Variants of SGD using layerwise adaptive learning rates have been proposed to address this problem, with the most successful being the LARS algorithm. This paper studies and develops new approaches specifically tailored to the large batch setting.",
        "Abstract": "Training large deep neural networks on massive datasets is  computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by  employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance.  By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes.",
        "Introduction": "  INTRODUCTION With the advent of large scale datasets, training large deep neural networks, even using computation- ally efficient optimization methods like Stochastic gradient descent (SGD), has become particularly challenging. For instance, training state-of-the-art deep learning models like BERT and ResNet-50 takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively ( Devlin et al., 2018 ;  He et al., 2016 ). Thus, there is a growing interest to develop optimization solutions to tackle this critical issue. The goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks, mostly focusing on approaches based on variants of SGD. Methods based on SGD iteratively update the parameters of the model by moving them in a scaled (negative) direction of the gradient calculated on a minibatch. However, SGD's scalability is limited by its inherent sequential nature. Owing to this limitation, traditional approaches to improve SGD training time in the context of deep learning largely resort to distributed asynchronous setup (Dean et al., 2012;  Recht et al., 2011 ). However, the implicit staleness introduced due to the asynchrony limits the parallelization of the approach, often leading to degraded performance. The feasibility of computing gradient on large minibatches in parallel due to recent hardware advances has seen the resurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous SGD. However, naïvely increasing the batch size typically results in degradation of generalization performance and reduces computational benefits ( Goyal et al., 2017 ). Synchronous SGD on large minibatches benefits from reduced variance of the stochastic gradients used in SGD. This allows one to use much larger learning rates in SGD, typically of the order square root of the minibatch size. Surprisingly, recent works have demonstrated that up to certain minibatch sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the Published as a conference paper at ICLR 2020 training  Goyal et al. (2017) . These works also elucidate two interesting aspects to enable the use of linear scaling in large batch synchronous SGD: (i) linear scaling of learning rate is harmful during the initial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be used initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size. Using these tricks,  Goyal et al. (2017)  was able to drastically reduce the training time of ResNet-50 model from 29 hours to 1 hour using a batch size of 8192. While these works demonstrate the feasibility of this strategy for reducing the wall time for training large deep neural networks, they also highlight the need for an adaptive learning rate mechanism for large batch learning. Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this problem. The most successful in this line of research is the LARS algorithm ( You et al., 2017 ), which was initially proposed for training RESNET. Using LARS, ResNet-50 can be trained on ImageNet in just a few minutes! However, it has been observed that its performance gains are not consistent across tasks. For instance, LARS performs poorly for attention models like BERT. Furthermore, theoretical understanding of the adaptation employed in LARS is largely missing. To this end, we study and develop new approaches specially catered to the large batch setting of our interest.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes Adversarial Variational Inference and Learning (AdVIL), a black-box inference and learning method for Markov random fields (MRFs). AdVIL introduces a variational encoder to infer the latent variables and a variational decoder for the MRF, which provides an estimate of the negative log-likelihood of the MRF in the form of an approximate contrastive free energy. AdVIL is evaluated in various undirected generative models, including restricted Boltzmann machines (RBM), deep Boltzmann machines (DBM), and Gaussian restricted Boltzmann machines (GRBM), on several real datasets. Results show that AdVIL provides a tighter estimate of the log partition function and achieves better log-likelihood results than the black-box NVIL method, and can deal with a broader family of MRFs without model-specific analysis.",
        "Abstract": "We propose a black-box algorithm called {\\it Adversarial Variational Inference and Learning} (AdVIL)  to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results. ",
        "Introduction": "  INTRODUCTION Markov random fields (MRFs) find applications in a variety of machine learning areas (Krähenbühl & Koltun, 2011;  Salakhutdinov & Larochelle, 2010 ;  Lafferty et al., 2001 ). In particular, one famous example is conditional random fields ( Lafferty et al., 2001 ), a conditional version of MRFs that was developed to address the limitations (e.g., local dependency and label bias) of directed models for sequential data (e.g., hidden Markov models and other discriminative Markov models based on directed graphical models). However, the inference and learning of general MRFs are challenging due to the presence of a global normalizing factor, i.e. partition function, especially when latent variables are present. Extensive efforts have been devoted to developing approximate methods. On one hand, sample-based methods ( Neal, 1993 ) and variational approaches ( Jordan et al., 1999 ;  Welling & Sutton, 2005 ;  Salakhutdinov & Larochelle, 2010 ) are proposed to infer the latent variables. On the other hand, extensive work ( Meng & Wong, 1996 ;  Neal, 2001 ;  Hinton, 2002 ;  Tieleman, 2008 ;  Wainwright et al., 2005 ;  Wainwright & Jordan, 2006 ) has been done to estimate the partition function. Among these methods, contrastive divergence ( Hinton, 2002 ) is proven effective in certain types of models. Most of the existing methods highly depend on the model structure and require model-specific analysis in new applications, which makes it important to develop black-box inference and learning methods. Previous work ( Ranganath et al., 2014 ;  Schulman et al., 2015 ) shows the ability to automatically infer the latent variables and obtain gradient estimate in directed models. However, there is no black-box learning method for undirected models except the recent work of NVIL ( Kuleshov & Ermon, 2017 ). NVIL introduces a variational distribution and derives an upper bound of the partition function in a general MRF, in the same spirit as amortized inference ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ;  Mnih & Gregor, 2014 ) for directed models. NVIL has several advantages over existing methods, including the ability of black-box learning, tracking the partition function during training and getting approximate samples efficiently during testing. However, NVIL also comes with two disadvantages: (1) it leaves the inference problem of MRFs unsolved 1 and only trains simple MRFs with tractable Published as a conference paper at ICLR 2020 posteriors, and (2) the upper bound of the partition function can be underestimated ( Kuleshov & Ermon, 2017 ), resulting in sub-optimal solutions on high-dimensional data. We propose Adversarial Variational Inference and Learning (AdVIL) to relieve some headache of learning an MRF model. AdVIL is a black-box inference and learning method that partly solves the two problems of NVIL and retains the advantages of NVIL at the same time. First, AdVIL introduces a variational encoder to infer the latent variables, which provides an upper bound of the free energy. Second, AdVIL introduces a variational decoder for the MRF, which provides a lower bound of the log partition function. The two variational distributions provide an estimate of the negative log-likelihood of the MRF. On one hand, the estimate is in an intuitive form of an approximate contrastive free energy, which is expressed in terms of the expected energy and the (conditional) entropy of the corresponding variational distribution. On the other hand, similar to GAN ( Goodfellow et al., 2014 ), the estimate is a minimax optimization problem, which is solved by stochastic gradient descent (SGD) in an alternating manner. Theoretically, our algorithm is convergent if the variational decoder approximates the model well. This motivates us to introduce an auxiliary variable to enhance the flexibility of the variational decoder, whose entropy is approximated by the third variational trick. We evaluate AdVIL in various undirected generative models, including restricted Boltzmann machines (RBM) ( Ackley et al., 1985 ), deep Boltzmann machines (DBM) ( Salakhutdinov & Hinton, 2009 ), and Gaussian restricted Boltzmann machines (GRBM) ( Hinton & Salakhutdinov, 2006 ), on several real datasets. We empirically demonstrate that (1) compared to the black-box NVIL ( Kuleshov & Ermon, 2017 ) method, AdVIL provides a tighter estimate of the log partition function and achieves much better log-likelihood results; and (2) compared to contrastive divergence based methods ( Hinton, 2002 ;  Welling & Sutton, 2005 ), AdVIL can deal with a broader family of MRFs without model-specific analysis and obtain better results when the model structure gets complex as in DBM.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a generic pre-training framework for visual-linguistic tasks, which is based on a multi-modal transformer network. The proposed framework is designed to learn generic feature representations that can effectively aggregate and align visual and linguistic information. The pre-trained model is then finetuned for a variety of visual-linguistic tasks, such as image captioning, visual question answering, and visual commonsense reasoning. Experiments on these tasks demonstrate the effectiveness of the proposed pre-training framework.",
        "Abstract": "We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark.",
        "Introduction": "  INTRODUCTION Pre-training of generic feature representations applicable to a variety of tasks in a domain is a hall- mark of the success of deep networks. Firstly in computer vision, backbone networks designed for and pre-trained on ImageNet (Deng et al., 2009) classification are found to be effective for improv- ing numerous image recognition tasks. Recently in natural language processing (NLP), Transformer networks (Vaswani et al., 2017) pre-trained with \"masked language model\" (MLM) objective (De- vlin et al., 2018) on large language corpus excel at a variety of NLP tasks. Meanwhile, for tasks at the intersection of vision and language, such as image captioning (Young et al., 2014; Chen et al., 2015; Sharma et al., 2018), visual question answering (VQA) (Antol et al., 2015; Johnson et al., 2017; Goyal et al., 2017; Hudson & Manning, 2019), visual commonsense reasoning (VCR) (Zellers et al., 2019; Gao et al., 2019), there lacks such pre-trained generic feature representations. The previous practice is to combine base networks pre-trained for image recognition and NLP respectively in a task-specific way. The task-specific model is directly finetuned for the specific target task, without any generic visual-linguistic pre-training. The task-specific model may well suffer from overfitting when the data for the target task is scarce. Also, due to the task-specific model design, it is difficult to benefit from pre-training, where the pre-training task may well be different from the target. There lacks a common ground for studying the feature design and pre- training of visual-linguistic tasks in general. In the various network architectures designed for different visual-linguistic tasks, a key goal is to effectively aggregate the multi-modal information in both the visual and linguistic domains. For ex- ample, to pick the right answer in the VQA task, the network should empower integrating linguistic information from the question and the answers, and aggregating visual information from the input image, together with aligning the linguistic meanings with the visual clues. Thus, we seek to derive generic representations that can effectively aggregate and align visual and linguistic information.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a posterior sampling algorithm for two-player zero-sum imperfect information games (TEGI) with the technique of counterfactual regret minimization (CFR). The proposed algorithm uses samples from posterior distributions of the environment to compute interaction strategies, which are then used to interact with the environment to collect data. Theoretical analysis shows that the proposed algorithm can provably converge to an approximate Nash Equilibrium at a rate of O( √ log T /T ). Empirical results demonstrate the effectiveness of the proposed algorithm.",
        "Abstract": "Posterior sampling for reinforcement learning (PSRL) is a useful framework for making decisions in an unknown environment.  PSRL maintains a posterior distribution of the environment and then makes planning on the environment sampled from the posterior distribution. Though PSRL works well on single-agent reinforcement learning problems, how to apply PSRL to multi-agent reinforcement learning problems is relatively unexplored. In this work, we extend PSRL to two-player zero-sum extensive-games with imperfect information (TEGI), which is a class of multi-agent systems. More specifically, we combine PSRL with counterfactual regret minimization (CFR), which is the leading algorithm for TEGI with a known environment. Our main contribution is a novel design of interaction strategies. With our interaction strategies, our algorithm provably converges to the Nash Equilibrium at a rate of $O(\\sqrt{\\log T/T})$. Empirical results show that our algorithm works well.",
        "Introduction": "  INTRODUCTION Reinforcement Learning (RL) (Sutton & Barto, 2018) provides a framework for decision-making problems in an unknown environment, such as robotics control. In an RL problem, agents improve their strategies by gaining information from iterative interactions with the environment. One typical target in designing RL algorithms is to reduce the number of interactions needed to find good strate- gies. Thus, how to reduce the number of samples by designing efficient interaction strategies is one of the key challenges in RL. Posterior sampling for RL (PSRL) (Strens, 2000) provides a useful framework for deciding how to interact with the environment. PSRL originates from the famous bandit algorithm Thompson Sampling (Russo et al., 2018), which uses samples from posterior distributions of bandit parameters to calculate current policy. PSRL also maintains a posterior distribution for the underlying envi- ronment and uses an environment which is sampled from this posterior to compute its interaction strategies. The interaction strategies are then used to interact with the environment to collect data. The design of the interaction strategies depends on the specific property of the task. For example, in a single-agent RL (SARL) problem, PSRL takes the strategy with the maximum expected reward on the sampled environment as the interaction strategy ( Osband et al., 2013 ). Theoretical and empirical results (Osband & Van Roy, 2016) both demonstrate that PSRL is one of the near-optimal methods for SARL. Moreover, although PSRL is a Bayesian-style algorithm, empirical evaluation ( Chapelle & Li, 2011 ) and theoretical analysis on the multi-armed bandit problems ( Agrawal & Goyal, 2017 ) suggest that it also enjoys good performance for a problem with fixed parameters. However, applying PSRL to multi-agent RL (MARL) tasks requires additional design on the interac- tion strategies. This is because the goal of MARL is quite different from that of SARL. In an MARL problem, each agent still aims to maximize its own reward, but the reward of an agent's strategy re- lies not only on the environment, but also on the strategies of other agents. Therefore, in MARL, the goal of learning is generally referred to finding a Nash Equilibrium (NE) where no agent is willing Published as a conference paper at ICLR 2020 to deviate its strategy individually. So we should design the interaction strategies with which the agents can find or approximate the NE efficiently. More specifically, we consider the RL problems in imperfect information extensive games (Osborne & Rubinstein, 1994). Extensive games provide a unified model for sequential decision-making problems in which agents take actions in turn. Imperfect information here means that agents can keep their own private information, such as the private cards in poker games. Games with imperfect information are also fundamental to many practical applications such as economics and security. In particular, we concentrate on two-player zero-sum imperfect information games (TEGI) where there are two players gaining opposite rewards and a chance player to model the transitions of the environment. When the environment (i.e. the transition functions of the chance player and the reward functions) is known, counterfactual regret minimization (CFR) (Zinkevich et al., 2008) is the leading algorithm in approximating the NE in a TEGI. However, in the RL setting where the environment is unknown, CFR is not applicable. In this work, we present a posterior sampling algorithm for TEGIs with the technique of CFR. That is, we apply CFR to the environment sampled from the posterior distribution. Our main contribution is a novel design of interaction strategies for the RL problem of TEGIs. With the proposed strategies, we show that our algorithm can provably converge to an approximate NE at a rate of O( √ log T /T ). Empirical results show that our algorithm works well.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes two novel approaches to generate unrestricted adversarial examples via semantic transformation. Experiments are conducted to attack both image classification and image captioning models on large scale datasets (ImageNet and MSCOCO). Comprehensive user studies are conducted to show that when compared to other attacks, the generated adversarial examples appear more natural to humans despite their large perturbations. The proposed attacks are tested against several state of the art defenses and show that they are more transferable and harder to defend.",
        "Abstract": "Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation.  Such adversarial perturbations are usually restricted by bounding their $\\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact.  In this paper, we instead introduce \"unrestricted\" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model. We also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, we conduct comprehensive user studies to show that our generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks.",
        "Introduction": "  INTRODUCTION Machine learning (ML), especially deep neural networks (DNNs) have achieved great success in various tasks, including image recognition (Krizhevsky et al., 2012; He et al., 2016), speech processing (Hinton et al., 2012) and robotics training (Levine et al., 2016). However, recent literature has shown that these widely deployed ML models are vulnerable to adversarial examples - carefully crafted perturbations aiming to mislead learning models (Carlini & Wagner, 2017; Kurakin et al., 2016; Xiao et al., 2018b). The fast growth of DNNs based solutions demands in-depth studies on adversarial examples to help better understand potential vulnerabilities of ML models and therefore improve their robustness. To date, a variety of different approaches has been proposed to generate adversarial examples (Goodfellow et al., 2014b; Carlini & Wagner, 2017; Kurakin et al., 2016; Xiao et al., 2018a); and many of these attacks search for perturbation within a bounded L p norm in order to preserve their photorealism. However, it is known that the L p norm distance as a perceptual similarity metric is not ideal (Johnson et al., 2016; Isola et al., 2017). In addition, recent work show that defenses trained on L p bounded perturbation are not robust at all against new types of unseen attacks (Kang et al., 2019). Therefore, exploring diverse adversarial examples, especially those with \"unrestricted\" magnitude of perturbation has acquired a lot of attention in both academia and industries (Brown et al., 2018). Recent work based on generative adversarial networks (GANs) (Goodfellow et al., 2014a) have introduced unrestricted attacks (Song et al, 2018). However, these attacks are limited to datasets like MNIST, CIFAR and CelebA, and are usually unable to scale up to bigger and more complex datasets such as ImageNet. Xiao et al. (2018b) directly manipulated spatial pixel flow of an image to produce adversarial examples without L p bounded constraints on the perturbation. However, the attack does not explicitly control visual semantic representation. More recently, Hosseini & Poovendran (2018) manipulated hue and saturation of an image to create adversarial perturbations. However, these examples are easily distinguishable by human and are also not scalable to complex datasets. In this work, we propose unrestricted attack strategies that explicitly manipulate semantic visual representations to generate natural-looking adversarial examples that are \"far\" from the original image in tems of the L p norm distance. In particular, we manipulate color (cAdv) and texture (tAdv) to create realistic adversarial examples (see  Fig 1 ). cAdv adaptively chooses locations in an image to change their colors, producing adversarial perturbation that is usually fairly substantial, while tAdv utilizes the texture from other images and adjusts the instance's texture field using style transfer. These semantic transformation-based adversarial perturbations shed light upon the understanding of what information is important for DNNs to make predictions. For instance, in one of our case studies, when the road is recolored from gray to blue, the image gets misclassified to tench (a fish) although a car remains evidently visible (Fig. 2b). This indicates that deep learning models can easily be fooled by certain large scale patterns. In addition to image classifiers, the proposed attack methods can be generalized to different machine learning tasks such as image captioning ( Karpathy & Fei-Fei (2015)). Our attacks can either change the entire caption to the target (Chen et al., 2017; Xu et al., 2019) or take on more challenging tasks like changing one or two specific target words from the caption to a target. For example, in  Fig. 1 , \"stop sign\" of the original image caption is changed to \"cat sitting\" and \"umbrella is\" for cAdv and tAdv respectively. To ensure our \"unrestricted\" semantically manipulated images are natural, we conducted extensive user studies with Amazon Mechanical Turk. We also tested our proposed attacks on several state of the art defenses. Rather than just showing the attacks break these defenses (better defenses will come up), we aim to show that cAdv and tAdv are able to produce new types of adversarial examples. Experiments also show that our proposed attacks are more transferable given their large and structured perturbations (Papernot et al., 2016). Our semantic adversarial attacks provide further insights about the vulnerabilities of ML models and therefore encourage new solutions to improve their robustness. In summary, our contributions are: 1) We propose two novel approaches to generate \"unrestricted\" adversarial examples via semantic transformation; 2) We conduct extensive experiments to attack both image classification and image captioning models on large scale datasets (ImageNet and MSCOCO); 3) We show that our attacks are equipped with unique properties such as smooth cAdv perturbations and structured tAdv perturbations. 4) We perform comprehensive user studies to show that when compared to other attacks, our generated adversarial examples appear more natural to humans despite their large perturbations; 5) We test different adversarial examples against several state of the art defenses and show that the proposed attacks are more transferable and harder to defend.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper studies the generalization performance of non-convex stochastic optimization algorithms for supervised learning. We aim to prove upper bounds on the generalization error of a learning algorithm, which takes as input a sequence of data points sampled i.i.d. from an unknown data distribution and outputs a (possibly randomized) parameter configuration. We consider the case where the loss function used to measure generalization performance is different from the objective function used in the training process.",
        "Abstract": "Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning  theory.   In  this  paper,  we  obtain  generalization  error  bounds  for  learning general  non-convex  objectives,  which  has  attracted  significant  attention  in  recent years.   We develop a new framework,  termed Bayes-Stability,  for proving algorithm-dependent generalization error bounds.  The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability.  Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic gradient Langevin dynamics (SGLD) and several other noisy gradient methods (e.g., with momentum, mini-batch and acceleration, Entropy-SGD). Our result recovers (and is typically tighter than) a recent result in Mou et al. (2018) and improves upon the results in Pensia et al. (2018).  Our experiments demonstrate that our data-dependent bounds can distinguish randomly labelled data from normal data, which provides an explanation to the intriguing phenomena observed in Zhang et al. (2017a). We also study the setting where the total loss is the sum of a bounded loss and an additiona l`2 regularization term. We obtain new generalization bounds for the continuous Langevin dynamic in this setting by developing a new Log-Sobolev inequality for the parameter distribution at any time. Our new bounds are more desirable when the noise level of the processis not very small, and do not become vacuous even when T tends to infinity.",
        "Introduction": "  INTRODUCTION Non-convex stochastic optimization is the major workhorse of modern machine learning. For in- stance, the standard supervised learning on a model class parametrized by R d can be formulated as the following optimization problem: min w∈R d E z∼D [F (w, z)] , where w denotes the model parameter, D is an unknown data distribution over the instance space Z, and F : R d × Z → R is a given objective function which may be non-convex. A learning algorithm takes as input a sequence S = (z 1 , z 2 , . . . , z n ) of n data points sampled i.i.d. from D, and outputs a (possibly randomized) parameter configurationŵ ∈ R d . A fundamental problem in learning theory is to understand the generalization performance of learn- ing algorithms-is the algorithm guaranteed to output a model that generalizes well to the data distribution D? Specifically, we aim to prove upper bounds on the generalization error err gen (S) = L(ŵ, D) − L(ŵ, S), where L(ŵ, D) = Ez∼D[L(ŵ, z)] and L(ŵ, S) = 1 n n i=1 L(ŵ, z i ) are the population and empirical losses, respectively. We note that the loss function L (e.g., the 0/1 loss) could be different from the objective function F (e.g., the cross-entropy loss) used in the training process (which serves as a surrogate for the loss L).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new certified robust training method, CROWN-IBP, which combines the efficiency of interval bound propagation (IBP) and the tightness of a linear relaxation based verification bound, CROWN. CROWN-IBP provides flexibility for exploiting the strengths of both IBP and convex relaxation based verifiable training methods, and is able to outperform state-of-the-art methods for training verifiable neural networks with ∞ robustness under all settings on MNIST and CIFAR-10 datasets.",
        "Abstract": "Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based bound, CROWN, in a backward bounding pass. CROWN-IBP is computationally efficient and consistently outperforms IBP baselines on training verifiably robust neural networks. We conduct large scale experiments on MNIST and CIFAR datasets, and outperform all previous linear relaxation and bound propagation based certified defenses in L_inf robustness.\nNotably, we achieve 7.02% verified test error on MNIST at epsilon=0.3, and 66.94% on CIFAR-10 with epsilon=8/255.",
        "Introduction": "  INTRODUCTION The success of deep neural networks (DNNs) has motivated their deployment in some safety-critical environments, such as autonomous driving and facial recognition systems. Applications in these areas make understanding the robustness and security of deep neural networks urgently needed, especially their resilience under malicious, finely crafted inputs. Unfortunately, the performance of DNNs are often so brittle that even imperceptibly modified inputs, also known as adversarial examples, are able to completely break the model ( Goodfellow et al., 2015 ;  Szegedy et al., 2013 ). The robustness of DNNs under adversarial examples is well-studied from both attack (crafting powerful adversarial examples) and defence (making the model more robust) perspectives ( Athalye et al., 2018 ;  Carlini & Wagner, 2017a ;b;  Goodfellow et al., 2015 ;  Madry et al., 2018 ;  Papernot et al., 2016 ;  Xiao et al., 2019b ;  2018b ;c;  Eykholt et al., 2018 ;  Chen et al., 2018 ;  Xu et al., 2018 ;  Zhang et al., 2019b ). Recently, it has been shown that defending against adversarial examples is a very difficult task, especially under strong and adaptive attacks. Early defenses such as distillation ( Papernot et al., 2016 ) have been broken by stronger attacks like C&W (Carlini & Wagner, 2017b). Many defense methods have been proposed recently ( Guo et al., 2018 ;  Song et al., 2017 ;  Buckman et al., 2018 ;  Ma et al., 2018 ;  Samangouei et al., 2018 ;  Xiao et al., 2018a ; 2019a), but their robustness improvement cannot be certified - no provable guarantees can be given to verify their robustness. In fact, most of these uncertified defenses become vulnerable under stronger attacks ( Athalye et al., 2018 ;  He et al., 2017 ). Several recent works in the literature seeking to give provable guarantees on the robustness perfor- mance, such as linear relaxations ( Wong & Kolter, 2018 ;  Mirman et al., 2018 ;  Wang et al., 2018a ;  Dvijotham et al., 2018b ;  Weng et al., 2018 ;  Zhang et al., 2018 ), interval bound propagation ( Mirman et al., 2018 ;  Gowal et al., 2018 ), ReLU stability regularization ( Xiao et al., 2019c ), and distributionally Published as a conference paper at ICLR 2020 robust optimization ( Sinha et al., 2018 ) and semidefinite relaxations ( Raghunathan et al., 2018a ;  Dvijotham et al. ). Linear relaxations of neural networks, first proposed by  Wong & Kolter (2018) , is one of the most popular categories among these certified defences. They use the dual of linear programming or several similar approaches to provide a linear relaxation of the network (referred to as a \"convex adversarial polytope\") and the resulting bounds are tractable for robust optimization. However, these methods are both computationally and memory intensive, and can increase model training time by a factor of hundreds. On the other hand, interval bound propagation (IBP) is a simple and efficient method for training verifiable neural networks ( Gowal et al., 2018 ), which achieved state-of-the-art verified error on many datasets. However, since the IBP bounds are very loose during the initial phase of training, the training procedure can be unstable and sensitive to hyperparameters. In this paper, we first discuss the strengths and weakness of existing linear relaxation based and interval bound propagation based certified robust training methods. Then we propose a new certified robust training method, CROWN-IBP, which marries the efficiency of IBP and the tightness of a linear relaxation based verification bound, CROWN ( Zhang et al., 2018 ). CROWN-IBP bound propagation involves a IBP based fast forward bounding pass, and a tight convex relaxation based backward bounding pass (CROWN) which scales linearly with the size of neural network output and is very efficient for problems with low output dimensions. Additional, CROWN-IBP provides flexibility for exploiting the strengths of both IBP and convex relaxation based verifiable training methods. The efficiency, tightness and flexibility of CROWN-IBP allow it to outperform state-of-the-art methods for training verifiable neural networks with ∞ robustness under all settings on MNIST and CIFAR- 10 datasets. In our experiment, on MNIST dataset we reach 7.02% and 12.06% IBP verified error under ∞ distortions = 0.3 and = 0.4, respectively, outperforming the state-of-the-art baseline results by IBP (8.55% and 15.01%). On CIFAR-10, at = 2 255 , CROWN-IBP decreases the verified error from 55.88% (IBP) to 46.03% and matches convex relaxation based methods; at a larger , CROWN-IBP outperforms all other methods with a noticeable margin.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the query-efficiency of black-box attack methods. We propose a novel query-efficient black-box attack algorithm, which utilizes the implicit information between two sequential iterations to enhance the query-efficiency. We evaluate our proposed method on various datasets and models, and the results demonstrate that our method can significantly reduce the number of queries while maintaining the attack success rate.",
        "Abstract": "Black-box attack methods aim to infer suitable attack patterns to targeted DNN models by only using output feedback of the models and the corresponding input queries. However, due to lack of prior and inefficiency in leveraging the query and feedback information, existing methods are mostly query-intensive for obtaining effective attack patterns. In this work, we propose a meta attack approach that is capable of attacking a targeted  model with much fewer queries. Its high query-efficiency stems from effective utilization of  meta learning approaches in learning generalizable prior abstraction from the previously observed attack patterns and exploiting  such prior to help infer attack patterns from only a few queries and outputs. Extensive experiments on MNIST, CIFAR10 and tiny-Imagenet demonstrate that our meta-attack method can remarkably reduce the number of model queries without sacrificing the attack performance. Besides, the obtained meta attacker is not restricted to a particular model but can be used easily with a fast adaptive ability to attack a variety of models. Our code will be released to the public.",
        "Introduction": "  INTRODUCTION Despite the great success in various tasks, deep neural networks (DNNs) are found to be suscepti- ble to adversarial attacks and often suffer dramatic performance degradation in front of adversarial examples, even if only tiny and invisible noise is imposed on the input ( Szegedy et al., 2014 ). To in- vestigate the safety and robustness of DNNs, many adversarial attack methods have been developed, which apply to either a white-box ( Goodfellow et al., 2015 ;  Moosavi-Dezfooli et al., 2016 ;  Carlini & Wagner, 2017 ;  Madry et al., 2018 ) or a black-box setting ( Papernot et al., 2017 ;  Brendel et al., 2018 ;  Narodytska & Kasiviswanathan, 2017 ). In the white-box attack setting, the target model is transparent to the attacker and imperceptible adversarial noise can be easily crafted to mislead this model by leveraging its gradient information ( Goodfellow et al., 2015 ). In contrast, in the black- box setting, the structure and parameters of the target DNN model are invisible, and the adversary can only access the input-output pair in each query. With a sufficient number of queries, black-box methods utilize the returned information to attack the target model generally by estimating gradient ( Chen et al., 2017 ;  Ilyas et al., 2018a ;  Narodytska & Kasiviswanathan, 2017 ;  Cheng et al., 2019 ). Black-box attack is more feasible in realistic scenarios than white-box attack but it is much more query-intensive. Such a drawback is largely attributed to the fact that returned information for each queried example is sparse and limited. During inferring attack patterns, existing black-box methods simply integrate the information between two sequential iterations brutally and ignore the implicit but profound message, thus not fully exploiting the returned information. Although query-efficient algorithms for generating attack examples are very meaningful in practice ( Ilyas et al., 2018a ), how to enhance query-efficiency for black-box attack remains underexplored.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt the effective receptive field (ERF) while leaving receptive field untouched. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with developed theory and perform favorably against prior works that adapt during runtime.",
        "Abstract": "Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the *effective* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.",
        "Introduction": "  INTRODUCTION The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data (Gibson, 1950). Humans are remarkably adept at making abstractions of the world (Hudson & Manning, 2019); we see in raw visual signals, abstract semantics away from deformation, and form concepts. Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing (Zhang, 2019). However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in Figure 1(a, b). In effect, this consumes large model capacity and data modes (Shelhamer et al., 2019). We argue that the awareness of deformations emerges from adaptivity - the ability to adapt at run- time (Kanazawa et al., 2016; Jia et al., 2016; Li et al., 2019). Modeling of geometric transformations has been a constant pursuit for vision researchers over decades (Lowe et al., 1999; Lazebnik et al., 2006; Jaderberg et al., 2015; Dai et al., 2017). A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that Published as a conference paper at ICLR 2020 (c) deformable conv. is representative of this direction is Deformable Convolution (Dai et al., 2017; Zhu et al., 2019). As shown in Figure 1(c), it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the \"theoretical receptive field\", that defines which input pixels can contribute to the final output. However, theoretical re- ceptive field does not measure how much impact an input pixel actually has. On the other hand, Luo et al. (2016) propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolu- tional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in Figure 1(d), DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two vari- ants - the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a Computation Reallocation Neural Architecture Search (CR-NAS) framework to automatically design the computation allocation of backbone for object detectors. CR-NAS reallocates the engaged computation cost in a more efficient way by introducing a two-level reallocation space to reallocate the computation across different resolution and spatial position. Experiments show that CR-NAS offers improvements for both fast mobile model and accurate model, such as ResNet, MobileNetV2, ResNeXt, on the COCO dataset. Furthermore, the discovered models show great transferability over other detection neck/head, other datasets, and other vision tasks.",
        "Abstract": "The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.",
        "Introduction": "  INTRODUCTION Object detection is one of the fundamental tasks in computer vision. The backbone feature extractor is usually taken directly from classification literature ( Girshick, 2015 ;  Ren et al., 2015 ;  Lin et al., 2017a ;  Lu et al., 2019 ). However, comparing with classification, object detection aims to know not only what but also where the object is. Directly taking the backbone of classification network for object detectors is sub-optimal, which has been observed in  Li et al. (2018) . To address this issue, there are many approaches either manually or automatically modify the backbone network.  Chen et al. (2019)  proposes a neural architecture search (NAS) framework for detection backbone to avoid expert efforts and design trails. However, previous works rely on the prior knowledge for classification task, either inheriting the backbone for classification, or designing search space similar to NAS on classification. This raises a natural question: How to design an effective backbone dedicated to detection tasks? To answer this question, we first draw a link between the Effective Receptive Field (ERF) and the computation allocation of backbone. The ERF is only small Gaussian-like factor of theoretical receptive field (TRF), but it dominates the output ( Luo et al., 2016 ). The ERF of image classification task can be easily fulfilled, e.g. the input size is 224×224 for the ImageNet data, while the ERF of object detection task need more capacities to handle scale variance across the instances, e.g. the input size is 800×1333 and the sizes of objects vary from 32 to 800 for the COCO dataset.  Lin et al. (2017a)  allocates objects of different scales into different feature resolutions to capture the appropriate ERF in each stage. Here we conduct an experiment to study the differences between the ERF of several FPN features. As shown in  Figure 1 , we notice the allocation of computation across different resolutions has a great impact on the ERF. Furthermore, appropriate computation allocation across spacial position ( Dai et al., 2017 ) boost the performance of detector by affecting the ERF. Based on the above observation, in this paper, we aim to automatically design the computation allocation of backbone for object detectors. Different from existing detection NAS works ( Ghiasi et al., 2019 ;  Ning Wang & Shen, 2019 ) which achieve accuracy improvement by introducing higher computation complexity, we reallocate the engaged computation cost in a more efficient way. We propose computation reallocation NAS (CR-NAS) to search the allocation strategy directly on the detection task. A two-level reallocation space is conducted to reallocate the computation across different resolution and spatial position. In stage level, we search for the best strategy to distribute the computation among different resolution. In operation level, we reallocate the computation by introducing a powerful search space designed specially for object detection. The details about search space can be found in Sec. 3.2. We propose a hierarchical search algorithm to cope with the complex search space. Typically in stage reallocation, we exploit a reusable search space to reduce stage-level searching cost and adapt different computational requirements. Extensive experiments show the effectiveness of our approach. Our CR-NAS offers improvements for both fast mobile model and accurate model, such as ResNet ( He et al., 2016 ), MobileNetV2 ( Sandler et al., 2018 ), ResNeXt ( Xie et al., 2017 ). On the COCO dataset, our CR-ResNet50 and CR-MobileNetV2 can achieve 38.3% and 33.9% AP, outperforming the baseline by 1.9% and 1.7% respectively without any additional computation budget. Furthermore, we transfer our CR-ResNet and CR-MobileNetV2 into the another ERF-sensitive task, instance segmentation, by using the Mask RCNN ( He et al., 2017 ) framework. Our CR-ResNet50 and CR-MobileNetV2 yields 1.3% and 1.2% COCO segmentation AP improvement over baseline. To summarize, the contributions of our paper are three-fold: • We propose computation reallocation NAS(CR-NAS) to reallocate engaged computation resources. To our knowledge, we are the first to dig inside the computation allocation across different resolution. • We develop a two-level reallocation space and hierarchical search paradigm to cope with the complex search space. Typically in stage reallocation, we exploit a reusable model to reduce stage-level searching cost and adapt different computational requirements. • Our CR-NAS offers significant improvements for various types of networks. The discov- ered models show great transferablity over other detection neck/head, e.g. NAS-FPN ( Cai & Vasconcelos, 2018 ), other dataset, e.g. PASCAL VOC ( Everingham et al., 2015 ) and other vision tasks, e.g. instance segmentation ( He et al., 2017 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a self-supervised methodology for jointly training a keypoint detector and its associated descriptor for robotic systems. This method is used to detect salient keypoints in images and re-identify them in a wide range of scenarios, which requires invariance properties to lighting effects, viewpoint changes, scale, time of day, etc. This method is compared to handcrafted image features such as SIFT and ORB, which have been shown to be limited in performance when compared to learned alternatives.",
        "Abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.",
        "Introduction": "  INTRODUCTION Detecting interest points in RGB images and matching them across views is a fundamental capa- bility of many robotic systems. Tasks such Simultaneous Localization and Mapping (SLAM) ( Ca- dena et al., 2016 ), Structure-from-Motion (SfM) ( Agarwal et al., 2010 ) and object detection as- sume that salient keypoints can be detected and re-identified in a wide range of scenarios, which requires invariance properties to lighting effects, viewpoint changes, scale, time of day, etc. How- ever, these tasks still mostly rely on handcrafted image features such as SIFT ( Lowe et al., 1999 ) or ORB ( Rublee et al., 2011 ), which have been shown to be limited in performance when compared to learned alternatives ( Balntas et al., 2017 ). Deep learning methods have revolutionized many computer vision applications including 2D/3D object detection ( Lang et al., 2019 ;  Tian et al., 2019 ), semantic segmentation ( Li et al., 2018 ;  Kirillov et al., 2019 ), human pose estimation ( Sun et al., 2019 ), etc. However, most learning algorithms need supervision and rely on labels which are often expensive to acquire. Moreover, supervising interest point detection is unnatural, as a human annotator cannot readily identify salient regions in images as well as key signatures or descriptors, which would allow their re-identification. Self- supervised learning methods have gained in popularity recently, being used for tasks such as depth regression ( Guizilini et al., 2019 ), tracking ( Vondrick et al., 2018 ) and representation learning ( Wang et al., 2019 ;  Kolesnikov et al., 2019 ). Following  DeTone et al. (2018b)  and  Christiansen et al. (2019) , we propose a self-supervised methodology for jointly training a keypoint detector as well as its associated descriptor.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Ensembling is a popular machine learning technique that combines the outputs of several models to achieve better performance than any of its members. Deep neural networks trained with different random seeds can converge to very different local minima, resulting in models that make independent errors. Ensembles of neural networks benefit from this observation to achieve better performance. Despite their success, ensembles are limited in practice due to their expensive computational and memory costs. This paper explores the use of ensembles for lifelong learning, where memory is a major bottleneck.",
        "Abstract": "\nEnsembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble’s cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable.\nIn this paper, we propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks",
        "Introduction": "  INTRODUCTION Ensembling is one of the oldest tricks in machine learning literature (Hansen & Salamon, 1990). By combining the outputs of several models, an ensemble can achieve better performance than any of its members. Many researchers demonstrate that a good ensemble is one where the ensemble's members are both accurate and make independent errors (Perrone & Cooper, 1992; Maclin & Opitz, 1999). In neural networks, SGD (Bottou, 2003) and its variants such as Adam (Kingma & Ba, 2014) are the most common optimization algorithm. The random noise from sampling mini-batches of data in SGD-like algorithms and random initialization of the deep neural networks, combined with the fact that there is a wide variety of local minima solutions in high dimensional optimization problem (Ge et al., 2015; Kawaguchi, 2016; Wen et al., 2019), results in the following observation: deep neural networks trained with different random seeds can converge to very different local minima although they share similar error rates. One of the consequence is that neural networks trained with different random seeds will usually not make all the same errors on the test set, i.e. they may disagree on a prediction given the same input even if the model has converged (Fort et al., 2019). Ensembles of neural networks benefit from the above observation to achieve better performance by averaging or majority voting on the output of each ensemble member (Xie et al., 2013; Huang et al., 2017). It is shown that ensembles of models perform at least as well as its individual members and diverse ensemble members lead to better performance (Krogh & Vedelsby, 1995). More recently, Lakshminarayanan et al. (2017) showed that deep ensembles give reliable predictive uncertainty estimates while remaining simple and scalable. A further study confirms that deep ensembles generally Published as a conference paper at ICLR 2020 achieves the best performance on out-of-distribution uncertainty benchmarks (Ovadia et al., 2019; Gustafsson et al., 2019) compared to other methods such as MC-dropout (Gal & Ghahramani, 2015). Despite their success on benchmarks, ensembles are limited in practice due to their expensive com- putational and memory costs, which increase lin- early with the ensemble size in both training and testing. Computation-wise, each ensemble mem- ber requires a separate neural network forward pass of its inputs. Memory-wise, each ensemble member requires an independent copy of neural network weights, each up to millions (sometimes billions) of parameters. This memory requirement also makes many tasks beyond supervised learning prohibitive. For example, in lifelong learning, a natural idea is to use a separate ensemble member for each task, adaptively growing the total number of parameters by creating a new independent set of weights for each new task. No previous work achieves competitive performance on lifelong learning via ensemble methods, as memory is a major bottleneck.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to hard-label black-box adversarial attack, which is the most challenging and practical attack setting. We propose a single-query sign oracle to evaluate the sign of directional derivative of Cheng's formulation, and based on this technique we develop a novel optimization algorithm called Sign-OPT for hard-label black-box attack. We theoretically prove and empirically demonstrate the significant reduction in the number of queries required for hard-label black box attack, and show that the proposed algorithm consistently reduces the query count by 5-10 times across different models and datasets, suggesting a practical and query-efficient robustness evaluation tool.",
        "Abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ",
        "Introduction": "  INTRODUCTION It has been shown that neural networks are vulnerable to adversarial examples ( Szegedy et al., 2016 ;  Goodfellow et al., 2015 ;  Carlini & Wagner, 2017 ;  Athalye et al., 2018 ). Given a victim neural network model and a correctly classified example, an adversarial attack aims to compute a small perturbation such that with this perturbation added, the original example will be misclassified. Many adversarial attacks have been proposed in the literature. Most of them consider the white-box setting, where the attacker has full knowledge about the victim model, and thus gradient based optimization can be used for attack. Popular Examples include C&W ( Carlini & Wagner, 2017 ) and PGD ( Madry et al., 2017 ) attacks. On the other hand, some more recent attacks have considered the probability black-box setting where the attacker does not know the victim model's structure and weights, but can iteratively query the model and get the corresponding probability output. In this setting, although gradient (of output probability to the input layer) is not computable, it can still be estimated using finite differences, and algorithms many attacks are based on this ( Chen et al., 2017 ;  Ilyas et al., 2018a ;  Tu et al., 2019 ;  Jun et al., 2018 ). In this paper, we consider the most challenging and practical attack setting - hard-label black-box setting - where the model is hidden to the attacker and the attacker can only make queries and get the corresponding hard-label decisions (e.g., predicted labels) of the model. A commonly used algorithm proposed in this setting, also called Boundary attack ( Brendel et al., 2017 ), is based on random walks on the decision surface, but it does not have any convergence guarantee. More recently,  Cheng et al. (2019)  showed that finding the minimum adversarial perturbation in the hard-label setting can be reformulated as another optimization problem (we call this Cheng's formulation in this paper). This Published as a conference paper at ICLR 2020 new formulation enjoys the benefit of having a smooth boundary in most tasks and the function value is computable using hard-label queries. Therefore, the authors of ( Cheng et al., 2019 ) are able to use standard zeroth order optimization to solve the new formulation. Although their algorithm converges quickly, it still requires large number of queries (e.g., 20,000) for attacking a single image since every function evaluation of Cheng's formulation has to be computed using binary search requiring tens of queries. In this paper, we follow the same optimization formulation of ( Cheng et al., 2019 ) which has the advantage of smoothness, but instead of using finite differences to estimate the magnitude of directional derivative, we propose to evaluate its sign using only a single query. With this single-query sign oracle, we design novel algorithms for solving the Cheng's formulation, and we theoretically prove and empirically demonstrate the significant reduction in the number of queries required for hard-label black box attack. Our contribution are summarized below: • Novelty in terms of adversarial attack. We elucidate an efficient approach to compute the sign of directional derivative of Cheng's formulation using a single query, and based on this technique we develop a novel optimization algorithm called Sign-OPT for hard-label black-box attack. • Novelty in terms of optimization. Our method can be viewed as a new zeroth order optimization algorithm that features fast convergence of signSGD. Instead of directly taking the sign of gradient estimation, our algorithm utilizes the scale of random direction. This make existing analysis inappropriate to our case, and we provide a new recipe to prove the convergence of this new optimizer. • We conduct comprehensive experiments on several datasets and models. We show that the proposed algorithm consistently reduces the query count by 5-10 times across different models and datasets, suggesting a practical and query-efficient robustness evaluation tool. Furthermore, on most datasets our algorithm can find an adversarial example with smaller distortion compared with previous approaches.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes iFlow, a framework for deep latent-variable models which allows for recovery of the true latent representations from which the observed data originates. iFlow unifies identifiability with normalizing flows, allowing for latent-variable inference and likelihood evaluation in an exact and efficient manner. Theoretical guarantees are provided on the recovery of the true latent representations, and experiments on synthetic data are used to validate the advantages of iFlow over prior approaches.",
        "Abstract": "Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence between variational approximate posterior and the true posterior, however, iVAE has to maximize the evidence lower bound (ELBO) of the marginal likelihood, leading to suboptimal solutions in both theory and practice. In contrast, we propose an identifiable framework for estimating latent representations using a flow-based model (iFlow). Our approach directly maximizes the marginal likelihood, allowing for theoretical guarantees on identifiability, thereby dispensing with variational approximations. We derive its optimization objective in analytical form, making it possible to train iFlow in an end-to-end manner. Simulations on synthetic data validate the correctness and effectiveness of our proposed method and demonstrate its practical advantages over other existing methods.",
        "Introduction": "  INTRODUCTION A fundamental question in representation learning relates to identifiability: under which condition is it possible to recover the true latent representations that generate the observed data? Most exist- ing likelihood-based approaches for deep generative modelling, such as Variational Autoencoders (VAE) ( Kingma & Welling, 2013 ) and flow-based models ( Kobyzev et al., 2019 ), focus on per- forming latent-variable inference and efficient data synthesis, but do not address the question of identifiability, i.e. recovering the true latent representations. The question of identifiability is closely related to the goal of learning disentangled representations ( Bengio et al., 2013 ). While there is no canonical definition for this term, we adopt the one where individual latent units are sensitive to changes in single generative factors while being relatively invariant to nuisance factors ( Bengio et al., 2013 ). A good representation for human faces, for ex- ample, should encompass different latent factors that separately encode different attributes including gender, hair color, facial expression, etc. By aiming to recover the true latent representation, identifi- able models also allow for principled disentanglement; this suggests that rather than being entangled in disentanglement learning in a completely unsupervised manner, we go a step further towards iden- tifiability, since existing literature on disentangled representation learning, such as β-VAE ( Higgins et al., 2017 ), β-TCVAE ( Chen et al., 2018 ), DIP-VAE ( Kumar et al., 2017 ) and FactorVAE ( Kim & Mnih, 2018 ), are neither general endeavors to achieve identifiability; nor do they provide theoretical guarantees on recovering the true latent sources. Recently,  Khemakhem et al. (2019)  introduced a theory of identifiability for deep generative models, based upon which the authors proposed an identifiable variant of VAEs called iVAE, to learn the distribution over latent variables in an identifiable manner. However, the downside of learning such an identifiable model within the VAE framework lies in the intractability of KL divergence between the approximate posterior and the true posterior. Consequently, in both theory and practice, iVAE inevitably leads to a suboptimal solution, which renders the learned model far less identifiable. In this paper, aiming at avoiding such a pitfall, we propose to learn an identifiable generative model through flows (short for normalizing flows ( Tabak et al., 2010 ;  Rezende & Mohamed, 2015 )). A nor- malizing flow is a transformation of a simple probability distribution (e.g. a standard normal) into a more complex probability distribution by a composition of a series of invertible and differentiable mappings ( Kobyzev et al., 2019 ). Hence, they can be exploited to effectively model complex prob- ability distributions. In contrast to VAEs relying on variational approximations, flow-based models allow for latent-variable inference and likelihood evaluation in an exact and efficient manner, making themselves a perfect choice for achieving identifiability. To this end, unifying identifiablity with flows, we propose iFlow, a framework for deep latent- variable models which allows for recovery of the true latent representations from which the observed data originates. We demonstrate that our flow-based model makes it possible to directly maximize the conditional marginal likelihood and thus achieves identifiability in a rigorous manner. We pro- vide theoretical guarantees on the recovery of the true latent representations, and show experiments on synthetic data to validate the theoretical and practical advantages of our proposed formulation over prior approaches.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces extreme tensoring, a family of generic modifications to any second-moment-based adaptive optimizer, which can be used to attain the benefits of adaptive preconditioning without significant memory overhead. We provide a regret analysis to quantify how extreme tensoring competes provably with full-memory AdaGrad in the online convex optimization framework. We also demonstrate, in a large-scale language modeling setting, that an optimizer requires very little additional memory to benefit from adaptive preconditioning. Furthermore, the inherent flexibility of our method enables us to conduct the first empirical study of the tradeoff between training convergence and memory in the optimizer.",
        "Abstract": "State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \\emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a large-scale NLP model, we reduce the optimizer memory overhead by three orders of magnitude, without degrading performance.",
        "Introduction": "  INTRODUCTION Among the most influential and important optimization techniques in machine learning are adaptive learning-rate methods, otherwise known as diagonal-matrix adaptive preconditioning. Essentially all of the most-commonly used incarnations of adaptive preconditioning (AdaGrad, Adam, RMSprop, Adadelta, etc.) accumulate second-moment estimators of each coordinate of the gradient, then scale the parameter updates by the square roots of these accumulators. These methods come with an overhead memory cost of storing these accumulators, thereby doubling the memory consumption. In the regime where model size encroaches upon the same order of magnitude as the total amount of RAM, a need has arisen to view memory as a limited resource in large-scale optimization. We address the question of whether the benefits of adaptive preconditioning can be attained with- out significant memory overhead. To this end, we introduce extreme tensoring, a family of generic modifications to any second-moment-based adaptive optimizer. Our method uses a compressed pre- conditioner which takes the form of a tensor product of arbitrary order, with simple updates, without necessarily requiring the parameters to be tensor-shaped. In our regret analysis, we quantify how extreme tensoring competes provably with full-memory AdaGrad in the online convex optimization framework, with a multiplicative data-dependent constant that can be measured empirically. In a large-scale language modeling setting, we demonstrate that an optimizer requires very little additional memory to benefit from adaptive preconditioning. Furthermore, the inherent flexibility Published as a conference paper at ICLR 2020 of our method enables us to conduct, to the first of our knowledge, the first empirical study of the tradeoff between training convergence and memory in the optimizer.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents LAMOL, a novel method for lifelong language learning (LLL) on a stream of natural language processing (NLP) tasks. LAMOL is a language modeling approach that uses a single model to address multiple NLP tasks by generating an answer based on the context and the question. To prevent catastrophic forgetting, the model generates pseudo-samples of the previous task to be replayed later. Experimental results show that LAMOL outperforms baselines and other state-of-the-art methods by a considerable margin and approaches the multitasking upper bound within 2-3%. Furthermore, the paper proposes adding task-specific tokens during pseudo-sample generation to evenly split the generated samples among all previous tasks. This extension stabilizes LLL and is particularly useful when training on a large number of tasks. The paper also analyzes how different amounts of pseudo-samples affect the final performance of LAMOL.\n\nAbstract: This paper presents LAMOL, a novel method for lifelong language learning (LLL) on a stream of natural language processing (NLP) tasks. LAMOL is a language modeling approach that uses a single model to address multiple NLP tasks by generating an answer based on the context and the question. To prevent catastrophic forgetting, the model generates pseudo-samples of the previous task to be replayed later. Experimental results show that LAMOL outperforms baselines and other state-of-the-art methods by a considerable margin and approaches the multitasking upper bound within 2-3%. Furthermore, the paper proposes adding task-specific tokens during pseudo-sample generation to evenly split the generated samples among all previous tasks. This extension stabilizes LLL and is particularly useful when training on a large number of tasks. The paper also analyzes how different amounts of pseudo-samples affect the final performance of LAMOL.",
        "Abstract": "Most research on lifelong learning applies to images or games, but not language.\nWe present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling.\nLAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity.\nSpecifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples.\nWhen the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task.\nThe results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. \nOverall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound.\nThe source code is available at https://github.com/jojotenya/LAMOL.",
        "Introduction": "  INTRODUCTION The current dominant paradigm for machine learning is to run an algorithm on a given dataset to produce a trained model specifically for a particular purpose; this is isolated learning ( Chen & Liu, 2016 , p. 150). In isolated learning, the model is unable to retain and accumulate the knowledge it has learned before. When a stream of tasks are joined to be trained sequentially, isolated learning faces catastrophic forgetting ( McCloskey & Cohen, 1989 ) due to a non-stationary data distribution that biases the model (left figure of  Figure 1 ). In contrast, lifelong learning is designed to address a stream of tasks by accumulating interconnected knowledge between learned tasks and retaining the performance of those tasks. A human easily achieves lifelong learning, but this is nontrivial for a machine; thus lifelong learning is a vital step toward artificial general intelligence. In this paper, we focus on lifelong language learning, where a machine achieves lifelong learning on a stream of natural language processing (NLP) tasks. To the best of our knowledge, lifelong language learning has been studied in only a few instances; for sentiment analysis ( Chen et al., 2015b ;  Xia et al., 2017 ), conversational agents ( Lee, 2017 ), word representation learning ( Xu et al., 2018 ), sentence representation learning ( Liu et al., 2019 ), text classification, and question answer- ing ( d'Autume et al., 2019 ). However, in all previous work, the tasks in the stream are essentially the same task but in different domains. To achieve lifelong language learning on fundamentally different tasks, we propose LAMOL - LAnguage MOdeling for Lifelong language learning. It has been shown that many NLP tasks can be considered question answering (QA) ( Bryan McCann & Socher, 2018 ). Therefore, we address multiple NLP tasks with a single model by training a language model (LM) that generates an answer based on the context and the question. Treating QA as language modeling is beneficial because the LM can be pre-trained on a large number of sentences without any labeling ( Radford et al., 2019 ); however, this does not directly solve the problem of LLL. If we train an LM on a stream of tasks, catastrophic forgetting still occurs. However, as an LM is intrinsically a text generator, we can use it to answer questions while generating pseudo-samples of the previous task to be replayed later. LAMOL is inspired by the data-based approach for LLL in which a generator learns to generate samples in previous tasks (middle of  Figure 1 ) ( Hanul Shin & Kim, 2017 ;  Kemker & Kanan, 2017 ). In contrast to previous approaches, LAMOL needs no extra generator (right of  Figure 1 ). LAMOL is also similar to multitask training, but the model itself generates data from previous tasks instead of using real data. Our main contributions in this paper are: • We present LAMOL, a simple yet effective method for LLL. Our method has the advantages of no requirements in terms of extra memory or model capacity. We also do not need to know how many tasks to train in advance and can always train on additional tasks when needed. • Experimental results show that our methods outperform baselines and other state-of-the-art meth- ods by a considerable margin and approaches the multitasking upper bound within 2-3%. • Furthermore, we propose adding task-specific tokens during pseudo-sample generation to evenly split the generated samples among all previous tasks. This extension stabilizes LLL and is partic- ularly useful when training on a large number of tasks. • We analyze how different amounts of pseudo-samples affect the final performance of LAMOL, considering results both with and without the task-specific tokens. • We open-source our code to facilitate further LLL research.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Deep neural networks are vulnerable to adversarial examples, which are deliberately crafted input samples that can mislead the networks to produce an output drastically different from what is expected. Recent studies have shown that adversarial robustness and accuracy are at odds with each other, suggesting an alternative path of designing a network whose adversarial examples are evasive rather than eliminated. This paper discusses existing defense mechanisms that hide the network's gradient information, and points out their vulnerability.",
        "Abstract": "We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model’s gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal is theoretically rationalized. We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the network training. This understanding is also empirically backed. We test k-WTA activation on various network structures optimized by a training method, be it adversarial training or not. In all cases, the robustness of k-WTA networks outperforms that of traditional networks under white-box attacks.",
        "Introduction": "  INTRODUCTION In the tremendous success of deep learning techniques, there is a grain of salt. It has become well- known that deep neural networks can be easily fooled by adversarial examples ( Szegedy et al., 2014 ). Those deliberately crafted input samples can mislead the networks to produce an output drastically different from what we expect. In many important applications, from face recognition authorization to autonomous cars, this vulnerability gives rise to serious security concerns ( Barreno et al., 2010 ;  2006 ;  Sharif et al., 2016 ;  Thys et al., 2019 ). Attacking the network is straightforward. Provided a labeled data item (x, y), the attacker finds a perturbation x perceptually similar to x but misleading enough to cause the network to output a label different from y. By far, the most effective way of finding such a perturbation (or adversarial example) is by exploiting the gradient information of the network with respect to its input: the gradient indicates how to perturb x to trigger the maximal change of y. The defense, however, is challenging. Recent studies showed that adversarial examples always exist if one tends to pursue a high classification accuracy-adversarial robustness seems at odds with the accuracy ( Tsipras et al., 2018 ;  Shafahi et al., 2019a ;  Su et al., 2018a ;  Weng et al., 2018 ;  Zhang et al., 2019 ). This intrinsic difficulty of eliminating adversarial examples suggests an alternative path: can we design a network whose adversarial examples are evasive rather than eliminated? Indeed, along with this thought is a series of works using obfuscated gradients as a defense mechanism ( Athalye et al., 2018 ). Those methods hide the network's gradient information by artificially discretizing the input ( Buckman et al., 2018 ; Lin et al., 2019) or introducing certain randomness to the input ( Xie et al., 2018a ; Guo et al., 2018) or the network structure ( Dhillon et al., 2018 ;  Cohen et al., 2019 ) (see more discussion in Sec. 1.1). Yet, the hidden gradient in those methods can still be approximated, and as recently pointed out by  Athalye et al. (2018) , those methods remain vulnerable.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper explores the possibility of pruning a network prior to training in order to improve computational efficiency. Recent work has suggested that this goal is achievable, with the Lottery Ticket Hypothesis (LTH) and Single-shot Network Pruning (SNIP) algorithms providing methods for pruning at initialization time. This paper examines the potential of these algorithms for reducing the computational cost of training large networks.",
        "Abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.",
        "Introduction": "  INTRODUCTION Deep neural networks exhibit good optimization and generalization performance in the overpa- rameterized regime ( Zhang et al., 2016 ;  Neyshabur et al., 2019 ;  Arora et al., 2019 ;  Zhang et al., 2019b ), but both training and inference for large networks are computationally expensive. Network pruning ( LeCun et al., 1990 ;  Hassibi et al., 1993 ;  Han et al., 2015b ;  Dong et al., 2017 ;  Zeng & Urtasun, 2019 ;  Wang et al., 2019 ) has been shown to reduce the test-time resource requirements with minimal performance degradation. However, as the pruning is typically done to a trained network, these methods don't save resources at training time. Moreover, it has been argued that it is hard to train sparse architectures from scratch while maintaining comparable performance to their dense counterparts ( Han et al., 2015a ;  Li et al., 2016 ). Therefore, we ask: can we prune a network prior to training, so that we can improve computational efficiency at training time?  Recently, Frankle & Carbin (2019)  shed light on this problem by proposing the Lottery Ticket Hypothesis (LTH), namely that there exist sparse, trainable sub-networks (called \"winning tickets\") within the larger network. They identify the winning tickets by taking a pre-trained network and removing connections with weights smaller than a pre-specificed threshold. They then reset the remaining weights to their initial values, and retrain the sub-network from scratch. Hence, they showed that the pre-trained weights are not necessary, only the pruned architecture and the corresponding initial weight values. Nevertheless, like traditional pruning methods, the LTH approach still requires training the full-sized network in order to identify the sparse sub-networks. Can we identify sparse, trainable sub-networks at initialization? This would allow us to exploit sparse computation with specified hardware for saving computation cost. (For instance,  Dey et al. (2019)  demonstrated 5x efficiency gains for training networks with pre-specified sparsity.) At first glance, a randomly initialized network seems to provide little information that we can use to judge the importance of individual connections, since the choice would seem to depend on complicated training dynamics. However, recent work suggests this goal is attainable.  Lee et al. (2018)  proposed the first algorithm for pruning at initialization time: Single-shot Network Pruning (SNIP), which uses a connection sensitivity criterion to prune weights with both small magnitude and small gradients.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Batch Normalization (BN) is a popular technique for training neural networks, however it is challenging to utilize BN when batch size is extremely small. To address this issue, this paper proposes a novel normalization method called Moving Average Batch Normalization (MABN). MABN replaces batch statistics with moving average statistics and utilizes a modified normalization form to reduce the number of batch statistics, centralize the weights of convolution kernels, and utilize renormalizing strategy. Experiments on multiple vision public datasets and tasks show that MABN with small batch size can achieve comparable performance as BN with regular batch size and has same inference consumption as vanilla BN.",
        "Abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.",
        "Introduction": "  INTRODUCTION Batch Normalization (BN) (Ioffe & Szegedy, 2015) is one of the most popular techniques for train- ing neural networks. It has been widely proven effective in many applications, and become the indispensable part of many state of the art deep models. Despite the success of BN, it's still challenging to utilize BN when batch size is extremely small 1 . The batch statistics with small batch size are highly unstable, leading to slow convergence during training and bad performance during inference. For example, in detection or segmentation tasks, the batch size is often limited to 1 or 2 per GPU due to the requirement of high resolution inputs or complex structure of the model. Directly computing batch statistics without any modification on each GPU will make performance of the model severely degrade. To address such issues, many modified normalization methods have been proposed. They can be roughly divided into two categories: some of them try to improve vanilla BN by correcting batch statistics ( Ioffe, 2017 ;  Singh & Shrivastava, 2019 ), but they all fail to completely restore the perfor- mance of vanilla BN; Other methods get over the instability of BN by using instance-level normal- ization ( Ulyanov et al., 2016 ;  Ba et al., 2016 ;  Wu & He, 2018 ), therefore models can avoid the affect Published as a conference paper at ICLR 2020 of batch statistics. This type of methods can restore the performance in small batch cases to some extent. However, instance-level normalization hardly meet industrial or commercial needs so far, for this type of methods have to compute instance-level statistics both in training and inference, which will introduce additional nonlinear operations in inference procedure and dramatically increase con- sumption  Shao et al. (2019) . While vanilla BN uses the statistics computed over the whole training data instead of batch of samples when training finished. Thus BN is a linear operator and can be merged with convolution layer during inference procedure. Figure 1(a) shows with ResNet-50 ( He et al., 2016 ), instance-level normalization almost double the inference time compared with vanilla BN. Therefore, it's a tough but necessary task to restore the performance of BN in small batch training without introducing any nonlinear operations in inference procedure. In this paper, we first analysis the formulation of vanilla BN, revealing there are actually not only 2 but 4 batch statistics involved in normalization during forward propagation (FP) as well as backward propagation (BP). The additional 2 batch statistics involved in BP are associated with gradients of the model, and have never been well discussed before. They play an important role in regularizing gradients of the model during BP. In our experiments (see  Figure 2 ), variance of the batch statistics associated with gradients in BP, due to small batch size, is even larger than that of the widely- known batch statistics (mean, variance of feature maps). We believe the instability of batch statistics associated with gradients is one of the key reason why BN performs poorly in small batch cases. Based on our analysis, we propose a novel normalization method named Moving Average Batch Normalization (MABN). MABN can completely get over small batch issues without introducing any nonlinear manipulation in inference procedure. The core idea of MABN is to replace batch statistics with moving average statistics. We substitute batch statistics involved in BP and FP with different type of moving average statistics respectively, and theoretical analysis is given to prove the benefits. However, we observed directly using moving average statistics as substitutes for batch statistics can't make training converge in practice. We think the failure takes place due to the occasional large gradients during training, which has been mentioned in  Ioffe (2017) . To avoid training collapse, we modified the vanilla normalization form by reducing the number of batch statistics, centralizing the weights of convolution kernels, and utilizing renormalizing strategy. We also theoretically prove the modified normalization form is more stable than vanilla form. MABN shows its effectiveness in multiple vision public datasets and tasks, including Ima- geNet ( Russakovsky et al., 2015 ), COCO ( Lin et al., 2014 ). All results of experiments show MABN with small batch size (1 or 2) can achieve comparable performance as BN with regular batch size (see Figure 1(b)). Besides, it has same inference consumption as vanilla BN (see Figure 1(a)). We also conducted sufficient ablation experiments to verify the effectiveness of MABN further.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces BERTSCORE, a language generation evaluation metric based on pre-trained BERT contextual embeddings. BERTSCORE computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings, and is designed to address two common pitfalls in n-gram-based metrics. Experiments demonstrate that BERTSCORE correlates highly with human evaluations for machine translation and image captioning tasks, and is more robust to adversarial examples than other metrics.",
        "Abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics. ",
        "Introduction": "  INTRODUCTION Automatic evaluation of natural language generation, for example in machine translation and caption generation, requires comparing candidate sentences to annotated references. The goal is to evaluate semantic equivalence. However, commonly used methods rely on surface-form similarity only. For example, BLEU ( Papineni et al., 2002 ), the most common machine translation metric, simply counts n-gram overlap between the candidate and the reference. While this provides a simple and general measure, it fails to account for meaning-preserving lexical and compositional diversity. In this paper, we introduce BERTSCORE, a language generation evaluation metric based on pre- trained BERT contextual embeddings ( Devlin et al., 2019 ). BERTSCORE computes the similarity of two sentences as a sum of cosine similarities between their tokens' embeddings. BERTSCORE addresses two common pitfalls in n-gram-based metrics ( Banerjee & Lavie, 2005 ). First, such methods often fail to robustly match paraphrases. For example, given the reference peo- ple like foreign cars, BLEU and METEOR ( Banerjee & Lavie, 2005 ) incorrectly give a higher score to people like visiting places abroad compared to consumers prefer imported cars. This leads to performance underestimation when semantically-correct phrases are penalized because they differ from the surface form of the reference. In contrast to string matching (e.g., in BLEU) or matching heuristics (e.g., in METEOR), we compute similarity using contextualized token embeddings, which have been shown to be effective for paraphrase detection ( Devlin et al., 2019 ). Second, n-gram mod- els fail to capture distant dependencies and penalize semantically-critical ordering changes ( Isozaki et al., 2010 ). For example, given a small window of size two, BLEU will only mildly penalize swapping of cause and effect clauses (e.g. A because B instead of B because A), especially when the arguments A and B are long phrases. In contrast, contextualized embeddings are trained to effectively capture distant dependencies and ordering. We experiment with BERTSCORE on machine translation and image captioning tasks using the outputs of 363 systems by correlating BERTSCORE and related metrics to available human judg- ments. Our experiments demonstrate that BERTSCORE correlates highly with human evaluations. In machine translation, BERTSCORE shows stronger system-level and segment-level correlations with human judgments than existing metrics on multiple common benchmarks and demonstrates Published as a conference paper at ICLR 2020 strong model selection performance compared to BLEU. We also show that BERTSCORE is well-correlated with human annotators for image captioning, surpassing SPICE, a popular task- specific metric ( Anderson et al., 2016 ). Finally, we test the robustness of BERTSCORE on the adversarial paraphrase dataset PAWS ( Zhang et al., 2019 ), and show that it is more ro- bust to adversarial examples than other metrics. The code for BERTSCORE is available at https://github.com/Tiiiger/bert score.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the expressivity and optimization of deep neural networks (DNNs). It is well known that a standard deep feedforward network with piecewise linear activations can partition the input space into many linear regions, where different linear functions are fitted. Studies have shown that the number of the linear regions increases more quickly with the depth of the DNN than with the width. Batch normalization and dropout are techniques used to alleviate optimization difficulties, such as the vanishing/exploding gradient problem and the shattered gradients problem. This paper seeks to answer the question of how these techniques affect the trained model and the reasons behind their success.",
        "Abstract": "A deep neural networks (DNN) with piecewise linear activations can partition the input space into numerous small linear regions, where different linear functions are fitted. It is believed that the number of these regions represents the expressivity of a DNN. This paper provides a novel and meticulous perspective to look into DNNs: Instead of just counting the number of the linear regions, we study their local properties, such as the inspheres, the directions of the corresponding hyperplanes, the decision boundaries, and the relevance of the surrounding regions. We empirically observed that different optimization techniques lead to completely different linear regions, even though they result in similar classification accuracies. We hope our study can inspire the design of novel optimization techniques, and help discover and analyze the behaviors of DNNs.",
        "Introduction": "  INTRODUCTION In the past few decades, deep neural networks (DNNs) have achieved remarkable success in various difficult tasks of machine learning ( Krizhevsky et al., 2012 ;  Graves et al., 2013 ;  Goodfellow et al., 2014 ;  He et al., 2016 ;  Silver et al., 2017 ;  Devlin et al., 2019 ). Albeit the great progress DNNs have made, there are still many problems which have not been thoroughly studied, such as the expressivity and optimization of DNNs. High expressivity is believed to be one of the most important reasons for the success of DNNs. It is well known that a standard deep feedforward network with piecewise linear activations can partition the input space into many linear regions, where different linear functions are fitted ( Pascanu et al., 2014 ;  Montufar et al., 2014 ). More specifically, the activation states are in one-to-one correspon- dence with the linear regions, i.e., all points in the same linear region activate the same nodes of the DNN, and hence the hidden layers serve as a series of affine transformations of these points. As approximating a complex curvature usually requires many linear regions ( Poole et al., 2016 ), the expressivity of a DNN is highly relevant to the number of the linear regions. Studies have shown that the number of the linear regions increases more quickly with the depth of the DNN than with the width ( Montufar et al., 2014 ;  Poole et al., 2016 ;  Arora et al., 2018 ).  Serra et al. (2018)  detailed the trade-off between the depth and the width, which depends on the number of neurons and the size of the input. However, a deep network usually leads to difficulties in opti- mization, such as the vanishing/exploding gradient problem ( Bengio et al., 1994 ;  Hochreiter, 1998 ) and the shattered gradients problem ( Balduzzi et al., 2017 ). Batch normalization (BN) can alleviate these by repeatedly normalizing the outputs to zero-mean and unit standard deviation, so that the scale of the weights can no longer affect the gradients through the layers ( Ioffe & Szegedy, 2015 ). Another difficulty is that the high complexity caused by the depth can easily result in overfitting.  Srivastava et al. (2014)  proposed dropout to reduce overfitting, which allows a DNN to randomly drop some nodes during training and work like an ensemble of several thin networks during testing. Despite the empirical benefits of these techniques, their effects on the trained model and the reasons behind their success are still unclear. Previous studies focused on explaining why these techniques Published as a conference paper at ICLR 2020 can help the optimization during training ( Wager et al., 2013 ;  Santurkar et al., 2018 ;  Bjorck et al., 2018 ). Different from theirs, our study is trying to answer the following question:",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes the SCALOR model, a probabilistic temporal generative model for unsupervised learning of object-oriented representation in temporal scenes. SCALOR significantly improves scalability in terms of object density, allowing it to model videos with nearly one hundred moving objects along with a dynamic background on synthetic and natural datasets. It also introduces a parallel discovery model with superior discovery capacity and performance, reducing the time complexity of processing each image from O(N) to O(1).",
        "Abstract": "Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the ﬁrst unsupervised object representation model shown to work for natural scenes containing several tens of moving objects.",
        "Introduction": "  INTRODUCTION Unsupervised structured representation learning for visual scenes is a key challenge in machine learning. When a scene is properly decomposed into meaningful entities such as foreground objects and background, we can benefit from numerous advantages of abstract symbolic representation. These include interpretability, sample efficiency, the ability of reasoning and causal inference, as well as compositionality and transferability for better generalization. In addition to symbols, another essential dimension is time. Objects, agents, and spaces all operate under the governance of time. Without accounting for temporal developments, it is often much harder if not impossible to discover certain relationships in a scene. Among a few methods that have been proposed for unsupervised learning of object-oriented repre- sentation in temporal scenes, SQAIR (Kosiorek et al., 2018) is by far the most complete model. As a probabilistic temporal generative model, it can learn object-wise structured representation while modeling underlying stochastic temporal transitions in the observed data. Introducing the propagation-discovery model, SQAIR can also handle dynamic scenes where objects may disap- pear or be introduced in the middle of a sequence. Although SQAIR provides promising ideas and shows the potential of this important direction, a few key challenges remain, limiting its applicability merely to synthetic toy tasks that are far simpler than typical natural scenes. The first and foremost limitation is scalability. Sequentially processing every object in an image, SQAIR has a fundamental limitation in scaling up to scenes with a large number of objects. As such, the state-of-the-art remains at the level of modeling videos containing only a few objects, such as MNIST digits, per image. Considering the complexity of typical natural scenes as well as the importance of scalable unsupervised object perception for applications such as self-driving systems, it is thus a challenge of the highest priority to scale robustly to scenes with a large number of objects. Scaling up the object-attention capacity is an important problem because it allows us to maximize the modern parallel computation that can maximize search capacity. This is contrary to humans, who can attend only to a few objects at a time in a time-consuming sequential manner. The Published as a conference paper at ICLR 2020 AlphaGo system (Silver et al., 2017) is an example demonstrating the power of such parallel search (attention) beyond human capacity. The second limitation is that previous models including SQAIR lack any form of background mod- eling and thus only cope with scenes without background, whereas natural scenes usually have a dynamic background. Thus, a temporal generative model that can deal with dynamic backgrounds along with many foreground objects is an important step toward natural video scene understanding. In this paper, we propose a model called SCALable Sequential Object-Oriented Representation (SCALOR). SCALOR resolves the aforementioned key limitations and hence can model complex videos with several tens of moving objects along with dynamic backgrounds, eventually making the model applicable to natural videos. In SCALOR, we achieve scalability with respect to the object density by parallelizing both the propagation and discovery processes, reducing the time complexity of processing each image from O(N ) to O(1), with N being the number of objects in an image. We also observe that the sequential object processing in SQAIR, which is based on an RNN, not only increases the computation time but also deteriorates discovery performance. To this end, we propose a parallel discovery model with superior discovery capacity and performance. SCALOR can also be regarded as a generative tracking model since it not only detects object trajectories but is also able to predict trajectories into the future. In our experiments, we demonstrate that SCALOR can model videos with nearly one hundred moving objects along with a dynamic background on synthetic datasets. Furthermore, we showcase the ability of SCALOR to operate on natural-scene videos containing tens of objects with a dynamic background. The contributions of this work are: (i) We propose the SCALOR model, which significantly im- proves (two orders of magnitude) the scalability in terms of object density. It is applicable to nearly a hundred objects while providing more efficient computation time than SQAIR. (ii) We propose parallelizing the propagation-discovery process by introducing the propose-reject model, reducing the time complexity to O(1). (iii) SCALOR can model scenes with a dynamic background. (iv) SCALOR is the first probabilistic model demonstrating its working on a significantly more complex task, i.e., natural scenes containing tens of objects as well as background.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Variational Auto-Encoder (VAE) has shown promise in learning interpretable and semantically meaningful representations of data, however, it has not been able to fully utilize the depth of neural networks like its supervised counterparts. This paper presents a progressive training strategy, termed pro-VLAE, which grows the hierarchical latent representations from different depths of the inference and generation model, learning from high- to low-levels of abstractions as the capacity of the model architecture grows. Quantitative studies demonstrate the ability of pro-VLAE to improve disentanglement on two benchmark data sets using three disentanglement metrics. Qualitative and quantitative evidence is presented that pro-VLAE is able to first learn the most abstract representations and then progressively disentangle existing factors or learn new factors at lower levels of abstraction, improving disentangling of hierarchical representations in the process.",
        "Abstract": "Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of “starting small”, we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new  representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark datasets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to our knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations.",
        "Introduction": "  INTRODUCTION Variational auto-encoder (VAE), a popular deep generative model (DGM), has shown great promise in learning interpretable and semantically meaningful representations of data ( Higgins et al. (2017) ;  Chen et al. (2018) ;  Kim & Mnih (2018) ;  Gyawali et al. (2019) ). However, VAE has not been able to fully utilize the depth of neural networks like its supervised counterparts, for which a fundamental cause lies in the inherent conflict between the bottom-up inference and top-down generation process ( Zhao et al. (2017) ;  Li et al. (2016) ): while the bottom-up abstraction is able to extract high-level representations helpful for discriminative tasks, the goal of generation requires the preservation of all generative factors that are likely at different abstraction levels. This issue was addressed in recent works by allowing VAEs to generate from details added at different depths of the network, using either memory modules between top-down generation layers ( Li et al. (2016) ), or hierarchical latent representations extracted at different depths via a variational ladder autoencoder ( VLAE, Zhao et al. (2017) ). However, it is difficult to learn to extract and disentangle all generative factors at once, especially at different abstraction levels. Inspired by human cognition system,  Elman (1993)  suggested the im- portance of \"starting small\" in two aspects of the learning process of neural networks: incremental input in which a network is trained with data and tasks of increasing complexity, and incremental memory in which the network capacity undergoes developmental changes given fixed external data and tasks - both pointing to an incremental learning strategy for simplifying a complex final task. Indeed, the former concept of incremental input has underpinned the success of curriculum learning ( Bengio et al. (2015) ). In the context of DGMs, various stacked versions of generative adversarial networks (GANs) have been proposed to decompose the final task of high-resolution image gener- ation into progressive sub-tasks of generating small to large images ( Denton et al. (2015) ;  Zhang Published as a conference paper at ICLR 2020 et al. (2018) ). The latter aspect of \"starting small\" with incremental growth of network capacity is less explored, although recent works have demonstrated the advantage of progressively growing the depth of GANs for generating high-resolution images ( Karras et al. (2018) ; Wang et al. (2018)). These works, so far, have focused on progressive learning as a strategy to improve image generation. We are motivated to investigate the possibility to use progressive learning strategies to improve learn- ing and disentangling of hierarchical representations. At a high level, the idea of progressively or sequentially learning latent representations has been previously considered in VAE. In  Gregor et al. (2015) , the network learned to sequentially refine generated images through recurrent networks. In  Lezama (2019) , a teacher-student training strategy was used to progressively increase the number of latent dimensions in VAE to improve the generation of images while preserving the disentangling ability of the teacher model. However, these works primarily focus on progressively growing the capacity of VAE to generate, rather than to extract and disentangle hierarchical representations. In comparison, in this work, we focus on 1) progressively growing the capacity of the network to extract hierarchical representations, and 2) these hierarchical representations are extracted and used in generation from different abstraction levels. We present a simple progressive training strategy that grows the hierarchical latent representations from different depths of the inference and generation model, learning from high- to low-levels of abstractions as the capacity of the model architecture grows. Because it can be viewed as a progressive strategy to train the VLAE presented in  Zhao et al. (2017) , we term the presented model pro-VLAE. We quantitatively demonstrate the ability of pro-VLAE to improve disentanglement on two benchmark data sets using three disentanglement metrics, including a new metric we proposed to complement the metric of mutual information gap (MIG) previously presented in  Chen et al. (2018) . These quantitative studies include comprehensive comparisons to β-VAE ( Higgins et al. (2017) ), VLAE ( Zhao et al. (2017) ), and the teacher-student strategy as presented in ( Lezama (2019) ) at different values of the hyperparameter β. We further present both qualitative and quantitative evidence that pro-VLAE is able to first learn the most abstract representations and then progressively disentangle existing factors or learn new factors at lower levels of abstraction, improving disentangling of hierarhical representations in the process.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes TREMBA, a black-box attack method that utilizes the information of a pretrained source network to improve the query efficiency of black-box attack on a target network. TREMBA produces adversarial perturbations with high level semantic patterns, which are effective across different networks, resulting in much lower queries on MNIST and ImageNet especially for the targeted attack that has low transferability. TREMBA can be applied to SOTA defended models and increases success rate by approximately 10% while reducing the number of queries by more than 50%.",
        "Abstract": "We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate.",
        "Introduction": "  INTRODUCTION The wide adoption of neural network models in modern applications has caused major security concerns, as such models are known to be vulnerable to adversarial examples that can fool neural networks to make wrong predictions ( Szegedy et al., 2014 ). Methods to attack neural networks can be divided into two categories based on whether the parameters of the neural network are assumed to be known to the attacker: white-box attack and black-box attack. There are several approaches to find adversarial examples for black-box neural networks. The transfer-based attack methods first pretrain a source model and then generate adversarial examples using a standard white-box attack method on the source model to attack an unknown target network ( Goodfellow et al., 2015 ;  Madry et al., 2018 ;  Carlini & Wagner, 2017 ;  Papernot et al., 2016a ). The score-based attack requires a loss-oracle, which enables the attacker to query the target network at multiple points to approximate its gradient. The attacker can then apply the white-box attack techniques with the approximated gradient ( Chen et al., 2017 ;  Ilyas et al., 2018a ;  Tu et al., 2018 ). A major problem of the transfer-based attack is that it can not achieve very high success rate. And transfer-based attack is weak in targeted attack. On the contrary, the success rate of score-based attack has only small gap to the white-box attack but it requires many queries. Thus, it is natural to combine the two black-box attack approaches, so that we can take advantage of a pretrained white-box source neural network to perform more efficient search to attack an unknown target black-box model. In fact, in the recent NeurIPS 2018 Adversarial Vision Challenge ( Brendel et al., 2018 ), many teams transferred adversarial examples from a source network as the starting point to carry out black-box boundary attack ( Brendel et al., 2017 ). N Attack also used a regression network as initialization in the score-based attack ( Li et al., 2019a ). The transferred adversarial example could be a good starting point that lies close to the decision boundary for the target network and accelerate further optimization. P-RGF ( Cheng et al., 2019 ) used the gradient information from the source model to accelerate searching process. However, gradient information is localized and sometimes it is misleading. In this paper, we push the idea of using a pretrained white-box source network to guide black-box attack significantly further, by proposing a method called TRansferable EMbedding based Black-box Attack (TREMBA). TREMBA contains two stages: (1) train an encoder-decoder that can effectively generate adversarial perturbations for the source network with a low-dimensional embedding space; (2) apply NES (Natural Evolution Strategy) of ( Wierstra et al., 2014 ) to the Published as a conference paper at ICLR 2020 low-dimensional embedding space of the pretrained generator to search adversarial examples for the target network. TREMBA uses global information of the source model, capturing high level semantic adversarial features that are insensitive to different models. Unlike noise-like perturbations, such perturbations would have much higher transferablity across different models. Therefore we could gain query efficiency by performing queries in the embedding space. We note that there have been a number of earlier works on using generators to produce adversarial perturbations in the white-box setting ( Baluja & Fischer, 2018 ;  Xiao et al., 2018 ;  Wang & Yu, 2019 ). While black-box attacks were also considered there, they focused on training generators with dynamic distillation. These early approaches required many queries to fine-tune the classifier for different target networks, which may not be practical for real applications. While our approach also relies on a generator, we train it as an encoder-decoder that produces a low-dimensional embedding space. By applying a standard black-box attack method such as NES on the embedding space, adversarial perturbations can be found efficiently for a target model. It is worth noting that the embedding approach has also been used in AutoZOOM ( Tu et al., 2018 ). However, it only trained the autoencoder to reconstruct the input, and it did not take advantage of the information of a pretrained network. Although it also produces structural perturbations, these perturbations are usually not suitable for attacking regular networks and sometimes its performance is even worse than directly applying NES to the images ( Cheng et al., 2019 ;  Guo et al., 2019 ). TREMBA, on the other hand, tries to learn an embedding space that can efficiently generate adversarial perturbations for a pretrained source network. Compared to AutoZOOM, our new method produces adversarial perturbation with high level semantic features that could hugely affect arbitrary target networks, resulting in significantly lower number of queries. We summarize our contributions as follows: 1. We propose TREMBA, an attack method that explores a novel way to utilize the information of a pretrained source network to improve the query efficiency of black-box attack on a target network. 2. We show that TREMBA can produce adversarial perturbations with high level semantic patterns, which are effective across different networks, resulting in much lower queries on MNIST and ImageNet especially for the targeted attack that has low transferablity. 3. We demonstrate that TREMBA can be applied to SOTA defended models ( Madry et al., 2018 ;  Xie et al., 2018 ). Compared with other black-box attacks, TREMBA increases success rate by approximately 10% while reduces the number of queries by more than 50%.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a general learning paradigm called Evolutionary Population Curriculum (EPC) to scale up the number of agents exponentially in multi-agent reinforcement learning (MARL). EPC is based on a self-attention based architecture which can generalize to an arbitrary number of agents with a fixed number of parameters, and an evolutionary selection process which helps address the misalignment of learning goals across stages and improves the agents' performance in the target environment. We illustrate the empirical benefits of EPC by implementing it on a popular MARL algorithm, MADDPG, and experimenting on three challenging environments. Results show that EPC outperforms baseline approaches by a large margin on all these environments as the number of agents grows even exponentially, and can improve the stability of the training procedure.",
        "Abstract": "In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely, EPC maintains multiple sets of agents in each stage, performs mix-and-match and fine-tuning over these sets and promotes the sets of agents with the best adaptability to the next stage. We implement EPC on a popular MARL algorithm, MADDPG, and empirically show that our approach consistently outperforms baselines by a large margin as the number of agents grows exponentially. The source code and videos can be found at https://sites.google.com/view/epciclr2020.",
        "Introduction": "  INTRODUCTION Most real-world problems involve interactions between multiple agents and the problem becomes significantly harder when there exist complex cooperation and competition among agents. Inspired by the tremendous success of deep reinforcement learning (RL) in single-agent applications, such as Atari games (Mnih et al., 2013), robotics manipulation (Levine et al., 2016), and navigation (Zhu et al., 2017; Wu et al., 2018; Yang et al., 2019), it has become a popular trend to apply deep RL techniques into multi-agent applications, including communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Mordatch & Abbeel, 2018), traffic light control (Wu et al., 2017), physical combats (Bansal et al., 2018), and video games (Liu et al., 2019; OpenAI, 2018). A fundamental challenge for multi-agent reinforcement learning (MARL) is that, as the number of agents increases, the problem becomes significantly more complex and the variance of policy gradients can grow exponentially (Lowe et al., 2017). Despite the advances on tackling this challenge via actor-critic methods (Lowe et al., 2017; Foerster et al., 2018), which utilize decentralized actors and centralized critics to stabilize training, recent works still scale poorly and are mostly restricted to less than a dozen agents. However, many real-world applications involve a moderately large population of agents, such as algorithmic trading (Wellman et al., 2005), sport team competition (Hausknecht & Stone, 2015), and humanitarian assistance and disaster response (Meier, 2015), where one agent should collaborate and/or compete with all other agents. When directly applying the existing MARL algorithms to complex games with a large number of agents, as we will show in Sec. 5.3, the agents may fail to learn good strategies and end up with little interaction with other agents even when collaboration is significantly beneficial. Yang et al. (2018) proposed a provably-converged mean- field formulation to scale up the actor-critic framework by feeding the state information and the average value of nearby agents' actions to the critic. However, this formulation strongly relies on the assumption that the value function for each agent can be well approximated by the mean of local * Equal contribution † Equal advising Published as a conference paper at ICLR 2020 pairwise interactions. This assumption often does not hold when the interactions between agents become complex, leading to a significant drop in the performance. In this paper, we propose a general learning paradigm called Evolutionary Population Curriculum (EPC), which allows us to scale up the number of agents exponentially. The core idea of EPC is to progressively increase the population of agents throughout the training process. Particularly, we divide the learning procedure into multiple stages with increasing number of agents in the environment. The agents first learn to play in simpler scenarios with less agents and then leverage these experiences to gradually adapt to later stages with more agents and ultimately our desired population. There are two key components in our curriculum learning paradigm. To process the varying number of agents during the curriculum procedure, the policy/critic needs to be population-invariant. So, we choose a self-attention (Vaswani et al., 2017) based architecture which can generalize to an arbitrary number of agents with a fixed number of parameters. More importantly, we introduce an evolutionary selection process, which helps address the misalignment of learning goals across stages and improves the agents' performance in the target environment. Intuitively, our within-stage MARL training objective only incentivizes agents to overfit a particular population in the current stage. When moving towards a new stage with a larger population, the successfully trained agents may not adapt well to the scaled environment. To mitigate this issue, we maintain multiple sets of agents in each stage, evolve them through cross-set mix-and-match and parallel MARL fine-tuning in the scaled environment, and select those with better adaptability to the next stage. EPC is RL-algorithm agnostic and can be potentially integrated with most existing MARL algorithms. In this paper, we illustrate the empirical benefits of EPC by implementing it on a popular MARL algorithm, MADDPG (Lowe et al., 2017), and experimenting on three challenging environments, including a predator-prey-style individual survival game, a mixed cooperative-and-competitive bat- tle game, and a fully cooperative food collection game. We show that EPC outperforms baseline approaches by a large margin on all these environments as the number of agents grows even exponen- tially. We also demonstrate that our method can improve the stability of the training procedure.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents two communication protocols, DEMAB and DELB, for distributed learning of stochastic multi-armed bandits and stochastic linear bandits. The protocols are designed to minimize communication cost while maintaining near-optimal performance, that is, regret comparable to the optimal regret of a single agent in M T interactions with the bandit instance. The amount of transmitted data per agent in DEMAB and DELB is independent of T, and is logarithmic with respect to other parameters.",
        "Abstract": "We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear bandits, we propose a protocol that achieves near-optimal regret and has communication cost of order $O\\left(\\left(Md+d\\log \\log d\\right)\\log T\\right)$, which has only logarithmic dependence on $T$.",
        "Introduction": "  INTRODUCTION Bandit learning is a central topic in online learning, and has various real-world applications, including clinical trials ( Wang, 1991 ), model selection ( Maron & Moore, 1994 ) and recommendation systems ( Agarwal et al., 2009 ;  Li et al., 2010 ;  Abe et al., 2003 ). In many tasks using bandit algorithms, it is appealing to employ more agents to learn collaboratively and concurrently in order to speed up the learning process. In many other tasks, the sequential decision making is distributed by nature. For instance, multiple spatially separated labs may be working on the same clinical trial. In such distributed applications, communication between agents is critical, but may also be expensive or time-consuming. Another example is a recommendation system deployed on multiple servers to handle high demand. Since the communication between servers may cause service latency, it would be desirable to design communication strategies without communicating too much. This motivates us to consider efficient protocols for distributed learning in bandit problems. A straightforward communication protocol for bandit learning is immediate sharing: each agent shares every new sample immediately with others. Under this scheme, agents can have good collaborative behaviors close to that in a centralized setting. However, the amount of communicated data is directly proportional to the total size of collected samples. When the bandit is played for a long timescale, the cost of communication would render this scheme impractical. A natural question to ask is: How Published as a conference paper at ICLR 2020 much communication is actually needed for near-optimal performance? In this work, we show that the answer is somewhat surprising: The required communication cost has almost no dependence on the time horizon. In this paper, we consider the distributed learning of stochastic multi-armed bandits (MAB) and stochastic linear bandits. There are M agents interacting with the same bandit instance in a syn- chronized fashion. In time steps t = 1, · · · , T , each agent pulls an arm and observes the associated reward. Between time steps, agents can communicate via a server-agent network. Following the typical formulation of single-agent bandit learning, we consider the task of regret minimization ( Lai et al., 1987 ;  Dani et al., 2008 ;  Bubeck et al., 2012 ). The total regret of all agents is used as the performance criterion of a communication protocol. The communication cost is measured by the total amount of data communicated in the network. Our goal is to minimize communication cost while maintaining near-optimal performance, that is, regret comparable to the optimal regret of a single agent in M T interactions with the bandit instance. For multi-armed bandits, we propose the DEMAB protocol, which achieves near-optimal regret. The amount of transmitted data per agent in DEMAB is independent of T , and is logarithmic with respect to other parameters. For linear bandits, we propose the DELB protocol, which achieves near-optimal regret, and has communication cost with at most logarithmic dependence on T .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel expansion-based approach for task-free continual learning (CL) called Continual Neural Dirichlet Process Mixture (CN-DPM). CN-DPM is inspired by the Mixture of Experts (MoE) and consists of a set of neural experts, which are expanded in a principled way built upon the Bayesian nonparametrics. Experiments on MNIST, SVHN, and CIFAR 10/100 show that CN-DPM successfully performs multiple types of CL tasks, including image classification and generation.",
        "Abstract": "Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.",
        "Introduction": "  INTRODUCTION Humans consistently encounter new information throughout their lifetime. The way the information is provided, however, is vastly different from that of conventional deep learning where each mini- batch is iid-sampled from the whole dataset. Data points adjacent in time can be highly correlated, and the overall distribution of the data can shift drastically as the training progresses. Continual learning (CL) aims at imitating incredible human's ability to learn from a non-iid stream of data without catastrophically forgetting the previously learned knowledge. Most CL approaches ( Aljundi et al., 2018 ; 2017; Lopez-Paz & Ranzato, 2017;  Kirkpatrick et al., 2017 ;  Rusu et al., 2016 ;  Shin et al., 2017 ;  Yoon et al., 2018 ) assume that the data stream is explicitly divided into a sequence of tasks that are known at training time. Since this assumption is far from realistic, task-free CL is more practical and demanding but has been largely understudied with only a few exceptions of ( Aljundi et al., 2019a ;b). In this general CL, not only is explicit task definition unavailable but also the data distribution gradually shifts without a clear task boundary. Meanwhile, existing CL methods can be classified into three different categories ( Parisi et al., 2019 ): regularization, replay, and expansion methods. Regularization and replay approaches address the catastrophic forgetting by regularizing the update of a specific set of weights or replaying the previ- ously seen data, respectively. On the other hand, the expansion methods are different from the two approaches in that it can expand the model architecture to accommodate new data instead of fixing it beforehand. Therefore, the expansion methods can bypass catastrophic forgetting by preventing pre-existing components from being overwritten by the new information. The critical limitation of prior expansion methods, however, is that the decisions of when to expand and which resource to use heavily rely on explicitly given task definition and heuristics. In this work, our goal is to propose a novel expansion-based approach for task-free CL. Inspired by the Mixture of Experts (MoE) ( Jacobs et al., 1991 ), our model consists of a set of experts, each of which is in charge of a subset of the data in a stream. The model expansion (i.e., adding more Published as a conference paper at ICLR 2020 experts) is governed by the Bayesian nonparametric framework, which determines the model com- plexity by the data, as opposed to the parametric methods that fix the model complexity before training. We formulate the task-free CL as an online variational inference of Dirichlet process mix- ture models consisting of a set of neural experts; thus, we name our approach as the Continual Neural Dirichlet Process Mixture (CN-DPM) model. We highlight the key contributions of this work as follows. • We are one of the first to propose an expansion-based approach for task-free CL. Hence, our model not only prevents catastrophic forgetting but also applies to the setting where no task definition and boundaries are given at both training and test time. Our model named CN-DPM consists of a set of neural network experts, which are expanded in a principled way built upon the Bayesian nonparametrics that have not been adopted in general CL research. • Our model can deal with both generative and discriminative tasks of CL. With several benchmark experiments of CL literature on MNIST, SVHN, and CIFAR 10/100, we show that our model successfully performs multiple types of CL tasks, including image classifi- cation and generation.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the effectiveness of adaptive gradient algorithms in training deep neural networks and generative adversarial networks (GANs). It compares the performance of adaptive gradient algorithms such as Ada-grad and Adam to non-adaptive methods such as Stochastic Gradient Descent (SGD) on benchmark datasets. Results show that adaptive gradient methods often find a solution with worse performance than SGD in supervised deep learning tasks, while Adam is important for GAN training and replacing it with non-adaptive methods would significantly deteriorate the performance.",
        "Abstract": "Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging  this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\\epsilon^{-4})$ complexity for finding $\\epsilon$-first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \\emph{improved} adaptive complexity $O\\left(\\epsilon^{-\\frac{2}{1-\\alpha}}\\right)$, where $\\alpha$ characterizes the growth rate of the cumulative stochastic gradient and $0\\leq \\alpha\\leq 1/2$. To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.",
        "Introduction": "  INTRODUCTION Adaptive gradient algorithms ( Duchi et al., 2011 ;  Tieleman & Hinton, 2012 ;  Kingma & Ba, 2014 ;  Reddi et al., 2019 ) are very popular in training deep neural networks due to their computational efficiency and minimal need for hyper-parameter tuning ( Kingma & Ba, 2014 ). For example, Ada- grad ( Duchi et al., 2011 ) automatically adjusts the learning rate for each dimension of the model parameter according to the information of history gradients, while its computational cost is almost the same as Stochastic Gradient Descent (SGD). However, in supervised deep learning (for ex- ample, image classification tasks using a deep convolutional neural network), there is not enough evidence showing that adaptive gradient methods converge faster than its non-adaptive counterpart (i.e., SGD) on benchmark datasets. For example, it is argued in ( Wilson et al., 2017 ) that adaptive gradient methods often find a solution with worse performance than SGD. Specifically,  Wilson et al. (2017)  observed that Adagrad has slower convergence than SGD in terms of both training and testing error, while using VGG ( Simonyan & Zisserman, 2014 ) on CIFAR10 data. GANs ( Goodfellow et al., 2014 ) are a popular class of generative models. In a nutshell, they consist of a generator and a discriminator, both of which are defined by deep neural networks. The gen- erator and the discriminator are trained under an adversarial cost, corresponding to a non-convex non-concave min-max problem. GANs are known to be notoriously difficult to train. In practice, Adam ( Kingma & Ba, 2014 ) is the defacto optimizer used for GAN training. The common op- timization strategy is to alternatively update the discriminator and the generator ( Arjovsky et al., Published as a conference paper at ICLR 2020 2017 ;  Gulrajani et al., 2017 ). Using Adam is important in GAN training, since replacing it with non-adaptive methods (e.g. SGD) would significantly deteriorate the performance. This paper stud- ies and attempts to answer the following question:",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the use of differential privacy to improve outlier/novelty detection and defense against backdoor attacks. It discusses how deep learning models with millions of parameters tend to remember too much and can easily overfit to rare training samples. It then explains how differential privacy can be used to \"hide\" certain input data from the output, and how it implies stability, such that the model learned by the algorithm is insensitive to the removal or replacement of an arbitrary point in the training dataset. Finally, it discusses how differential privacy can potentially be leveraged to improve the identification of outliers.",
        "Abstract": "Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as “outliers” that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy could improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.",
        "Introduction": "  INTRODUCTION Given a dataset where most of the samples are from a certain distribution, outlier detection aims to detect the minorities in the dataset that are far from the distribution, while the goal of novelty detection is to detect newly observed data samples that do not fit the distribution. On the other hand, poisoning examples that are intentionally added by attackers to achieve backdoor attacks could be treated as one type of \"outliers\" in the training dataset. Using machine learning for outlier/novelty detection is typically to train a model that learns the distribution where the training data samples are drawn from, and the final trained model could give a high anomaly score for the outliers/novelties that deviate from the same distribution. In both cases, the machine learning model is not supposed to learn from the outliers in the training dataset. Unfortunately, deep learning models that contain millions of parameters tend to remember too much (Song et al. [2017]), and can easily overfit to rare training samples (Carlini et al. [2018]). Protecting data privacy has been a major concern in many applications, because sensitive data are being collected and analyzed. Differential privacy has been proposed to \"hide\" certain input data from the output; that is, by looking at the statistical results calculated from input data, one cannot tell whether the input data contain a certain record or not. The way of applying differential privacy is to add random noise to the input data or the data analysis procedure, such that the output difference caused by the input difference can be hidden by the noise. A known fact is that differential privacy implies stability (Kasiviswanathan et al. [2011]). Particularly, a differentially private learning algorithm is stable in the sense that the model learned by the algorithm is insensitive to the removal or replacement of an arbitrary point in the training dataset (Bousquet & Elisseeff [2002]). When the training dataset contains a handful of outliers, the output model of a stable learning algorithm should be close to the one trained on the clean portion of the training set. Intuitively, compared with the model trained on contaminated dataset, the one trained on clean data could be better at distinguishing outliers from normal data. Therefore, differential privacy can potentially be leveraged to improve the Published as a conference paper at ICLR 2020 identification of outliers. This motivates us to apply differential privacy to anomaly detection and defense against backdoor attacks.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents Neural Logic Inductive Learning (NLIL), a differentiable ILP method that combines the strengths of neural and logic-based computation to learn first-order logic rules that explain data. NLIL is highly efficient and expressive, and is able to search for rules up to 10 times longer than the state-of-the-art methods while remaining 3 times faster. NLIL is applied to the Visual Genome dataset to learn explanations for 150 object classes over 1M entities, and is shown to have comparable predictive power as densely supervised models while maintaining interpretability.",
        "Abstract": "The capability of making interpretable and self-explanatory decisions is essential for developing responsible machine learning systems. In this work, we study the learning to explain the problem in the scope of inductive logic programming (ILP). We propose Neural Logic Inductive Learning (NLIL), an efficient differentiable ILP framework that learns first-order logic rules that can explain the patterns in the data. In experiments, compared with the state-of-the-art models, we find NLIL is able to search for rules that are x10 times longer while remaining x3 times faster. We also show that NLIL can scale to large image datasets, i.e. Visual Genome, with 1M entities.",
        "Introduction": "  INTRODUCTION The recent years have witnessed the growing success of deep learning models in a wide range of applications. However, these models are also criticized for the lack of interpretability in its behav- ior and decision making process (Lipton, 2016; Mittelstadt et al., 2019), and for being data-hungry. The ability to explain its decision is essential for developing a responsible and robust decision sys- tem (Guidotti et al., 2019). On the other hand, logic programming methods, in the form of first-order logic (FOL), are capable of discovering and representing knowledge in explicit symbolic structure that can be understood and examined by human (Evans & Grefenstette, 2018). In this paper, we investigate the learning to explain problem in the scope of inductive logic pro- gramming (ILP) which seeks to learn first-order logic rules that explain the data. Traditional ILP methods (Galárraga et al., 2015) rely on hard matching and discrete logic for rule search which is not tolerant for ambiguous and noisy data (Evans & Grefenstette, 2018). A number of works are proposed for developing differentiable ILP models that combine the strength of neural and logic- based computation (Evans & Grefenstette, 2018; Campero et al., 2018; Rocktäschel & Riedel, 2017; Payani & Fekri, 2019; Dong et al., 2019). Methods such as ∂ILP (Evans & Grefenstette, 2018) are referred to as forward-chaining methods. It constructs rules using a set of pre-defined templates and evaluates them by applying the rule on background data multiple times to deduce new facts that lie in the held-out set (related works available at Appendix A). However, general ILP problem involves several steps that are NP-hard: (i) the rule search space grows exponentially in the length of the rule; (ii) assigning the logic variables to be shared by predicates grows exponentially in the number of ar- guments, which we refer as variable binding problem; (iii) the number of rule instantiations needed for formula evaluation grows exponentially in the size of data. To alleviate these complexities, most works have limited the search length to within 3 and resort to template-based variable assignments, limiting the expressiveness of the learned rules (detailed discussion available at Appendix B). Still, most of the works are limited in small scale problems with less than 10 relations and 1K entities. On the other hand, multi-hop reasoning methods (Guu et al., 2015; Lao & Cohen, 2010; Lin et al., 2015; Gardner & Mitchell, 2015; Das et al., 2016) are proposed for the knowledge base (KB) com- pletion task. Methods such as NeuralLP (Yang et al., 2017) can answer the KB queries by searching for a relational path that leads from the subject to the object. These methods can be interpreted in the ILP domain where the learned relational path is equivalent to a chain-like first-order rule. Compared to the template-based counterparts, methods such as NeuralLP is highly efficient in variable binding and rule evaluation. However, they are limited in two aspects: (i) the chain-like rules represent a subset of the Horn clauses, and are limited in expressing complex rules such as those shown in  Fig- ure 1 ; (ii) the relational path is generated while conditioning on the specific query, meaning that the learned rule is only valid for the current query. This makes it difficult to learn rules that are globally consistent in the KB, which is an important aspect of a good explanation. In this work, we propose Neural Logic Inductive Learning (NLIL), a differentiable ILP method that extends the multi-hop reasoning framework for general ILP problem. NLIL is highly efficient and expressive. We propose a divide-and-conquer strategy and decompose the search space into 3 subspaces in a hierarchy, where each of them can be searched efficiently using attentions. This enables us to search for x10 times longer rules while remaining x3 times faster than the state-of-the- art methods. We maintain the global consistency of rules by splitting the training into rule generation and rule evaluation phase, where the former is only conditioned on the predicate type that is shared globally. And more importantly, we show that a scalable ILP method is widely applicable for model expla- nations in supervised learning scenario. We apply NLIL on Visual Genome (Krishna et al., 2016) dataset for learning explanations for 150 object classes over 1M entities. We demonstrate that the learned rules, while maintaining the interpretability, have comparable predictive power as densely supervised models, and generalize well with less than 1% of the data.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new approach to network pruning that improves the inference efficiency of deep neural networks. Traditional pruning methods consist of dense network training followed with pruning and fine-tuning iterations, while the proposed approach conducts the network pruning during the training process. This approach avoids the expensive pruning and fine-tuning iterations, and addresses the three problems of time-consuming training, sub-optimal pruned networks, and non-sparse pruned networks.",
        "Abstract": "We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and efﬁciency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.",
        "Introduction": "  INTRODUCTION Despite the impressive success that deep neural networks have achieved in a wide range of chal- lenging tasks, the inference in deep neural networks is highly memory-intensive and computation- intensive due to the over-parameterization of deep neural networks. Network pruning (LeCun et al. (1990);  Han et al. (2015) ;  Molchanov et al. (2017) ) has been recognized as an effective approach to improving the inference efficiency in resource-limited scenarios. Traditional pruning methods consist of dense network training followed with pruning and fine-tuning iterations. To avoid the expensive pruning and fine-tuning iterations, many sparse training meth- ods (Mocanu et al., 2018;  Bellec et al., 2017 ; Mostafa & Wang, 2019;  Dettmers & Zettlemoyer, 2019 ) have been proposed, where the network pruning is conducted during the training process. However, all these methods suffer from following three problems:",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper studies the convergence behaviour of gradient algorithms for bilinear zero-sum games, with a focus on Jacobi and Gauss-Seidel style updates. We provide a comprehensive analysis of the convergence rate of these algorithms, and prove a Stein-Rosenberg type theorem for Jacobi and GS updates. We also discuss the implications of our results for min-max optimization and generative adversarial networks.",
        "Abstract": "Min-max formulations have attracted great attention in the ML community due to the rise of deep generative models and adversarial methods, while understanding the dynamics of gradient algorithms for solving such formulations has remained a grand challenge. As a first step, we restrict to bilinear zero-sum games and give a systematic analysis of popular gradient updates, for both simultaneous and alternating versions. We provide exact conditions for their convergence and find the optimal parameter setup and convergence rates. In particular, our results offer formal evidence that alternating updates converge \"better\" than simultaneous ones.",
        "Introduction": "  INTRODUCTION Min-max optimization has received significant attention recently due to the popularity of generative adversarial networks (GANs) ( Goodfellow et al., 2014 ), adversarial training ( Madry et al., 2018 ) and reinforcement learning ( Du et al., 2017 ;  Dai et al., 2018 ), just to name some examples. Formally, given a bivariate function f (x, y), we aim to find a saddle point (x * , y * ) such that Since the beginning of game theory, various algorithms have been proposed for finding saddle points ( Arrow et al., 1958 ;  Dem'yanov & Pevnyi, 1972 ;  Gol'shtein, 1972 ;  Korpelevich, 1976 ;  Rockafellar, 1976 ;  Bruck, 1977 ;  Lions, 1978 ;  Nemirovski & Yudin, 1983 ;  Freund & Schapire, 1999 ). Due to its recent resurgence in ML, new algorithms specifically designed for training GANs were proposed ( Daskalakis et al., 2018 ;  Kingma & Ba, 2015 ;  Gidel et al., 2019b ;  Mescheder et al., 2017 ). However, due to the inherent non-convexity in deep learning formulations, our current understanding of the convergence behaviour of new and classic gradient algorithms is still quite limited, and existing analysis mostly focused on bilinear games or strongly-convex-strongly-concave games ( Tseng, 1995 ;  Daskalakis et al., 2018 ;  Gidel et al., 2019b ;  Liang & Stokes, 2019 ;  Mokhtari et al., 2019b ). Non- zero-sum bilinear games, on the other hand, are known to be PPAD-complete ( Chen et al., 2009 ) (for finding approximate Nash equilibria, see e.g.  Deligkas et al. (2017) ). In this work, we study bilinear zero-sum games as a first step towards understanding general min-max optimization, although our results apply to some simple GAN settings ( Gidel et al., 2019a ). It is well-known that certain gradient algorithms converge linearly on bilinear zero-sum games ( Liang & Stokes, 2019 ;  Mokhtari et al., 2019b ;  Rockafellar, 1976 ;  Korpelevich, 1976 ). These iterative algorithms usually come with two versions: Jacobi style updates or Gauss-Seidel (GS) style. In a Jacobi style, we update the two sets of parameters (i.e., x and y) simultaneously whereas in a GS style we update them alternatingly (i.e., one after the other). Thus, Jacobi style updates are naturally amenable to parallelization while GS style updates have to be sequential, although the latter is usually found to converge faster (and more stable). In numerical linear algebra, the celebrated Stein-Rosenberg theorem ( Stein & Rosenberg, 1948 ) formally proves that in solving certain linear systems, GS updates converge strictly faster than their Jacobi counterparts, and often with a larger set of convergent instances. However, this result does not readily apply to bilinear zero-sum games. Our main goal here is to answer the following questions about solving bilinear zero-sum games: • When exactly does a gradient-type algorithm converge? • What is the optimal convergence rate by tuning the step size or other parameters? • Can we prove something similar to the Stein-Rosenberg theorem for Jacobi and GS updates?",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents an approach for learning efficient computational mappings between multiple related datasets from different sources or domains. Optimal transport (OT) is used as a principled analytical framework to align heterogeneous datasets, and has been increasingly applied to problems in domain adaptation and transfer learning. However, a major challenge in practice is that the appropriate cost function is often unknown. This paper proposes a method for learning the cost function from data, which can then be used to find the global optimal transport.",
        "Abstract": "Learning to align multiple datasets is an important problem with many applications, and it is especially useful when we need to integrate multiple experiments or correct for confounding. Optimal transport (OT) is a principled approach to align  datasets, but a key challenge in applying OT is that we need to specify a cost function that accurately captures how the two datasets are related. Reliable cost functions are typically not available and practitioners often resort to using hand-crafted or Euclidean cost even if it may not be appropriate. In this work, we investigate how to learn the cost function using a small amount of side information which is often available. The side information we consider captures subset correspondence---i.e. certain subsets of points in the two data sets are known to be related. For example, we may have some images labeled as cars in both datasets; or we may have a common annotated cell type in single-cell data from two batches. We develop an end-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm and effectively learns the suitable cost function from side information. On systematic experiments in images, marriage-matching and single-cell RNA-seq, our method substantially outperform state-of-the-art benchmarks. ",
        "Introduction": "  INTRODUCTION In many applications, we have multiple related datasets from different sources or domains, and learning efficient computational mappings between these datasets is an important problem ( Long et al., 2017 ;  Zamir et al., 2018 ). For example, we might have single-cell RNA-Seq datasets generated for the same tissue type from two different labs. Since data come from the same type of tissue, we would like to map cells between the two datasets to merge them, so that we could analyze them jointly. However, there are often complex nonlinear batch artifacts generated by the different labs. Moreover the cells are not paired-for each cell measured in the first lab, there is not an identical clone in the second lab. How to integrate or align these two datasets is therefore a challenging problem. Optimal transport (OT) is a principled analytical framework to align heterogeneous datasets ( San- tambrogio, 2015 ). It has been increasingly applied to problems in domain adaptation and transfer learning ( Seguy et al., 2017 ;  Genevay et al., 2017 ;  Courty et al., 2017b ;  Li et al., 2019 ). Optimal transport is an approach for taking two datasets, and computing a mapping between them in the form of a \"transport plan\" γ. The mapping is optimal in the sense that among all reasonable mappings (precisely defined in Section 2), it minimizes the cost of aligning the two datasets. The transport cost is given by the user and encodes expert knowledge about how datasets relate to each other. For example, if the expert believes that one data Y is essentially data X with added Gaussian noise, then Euclidean cost could be natural. If the cost is correctly specified, then there are powerful methods for finding the global optimal transport (Villani, 2008). A major challenge in practice, e.g. for single-cell RNA-seq, is that we and experts do not know what cost is appropriate. Users often resort to using Euclidean or other hand-crafted cost functions, which could give misleading mappings.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes two new methods, Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM), to improve the transferability of adversarial examples. Experiments on the ImageNet dataset show that these methods can attack both normally trained models and adversarially trained models with higher attack success rates than existing baseline attacks. The results also demonstrate that the proposed methods can generate adversarial examples with higher transferability than state-of-the-art gradient-based attacks.",
        "Abstract": "Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid \"overfitting” on the white-box model being attacked and generate more transferable adversarial examples. NI-FGSM and SIM can be naturally integrated to build a robust gradient-based attack to generate more transferable adversarial examples against the defense models. Empirical results on ImageNet dataset demonstrate that our attack methods exhibit higher transferability and achieve higher attack success rates than state-of-the-art gradient-based attacks.",
        "Introduction": "  INTRODUCTION Deep learning models have been shown to be vulnerable to adversarial examples ( Goodfellow et al., 2014 ; Szegedy et al., 2014), which are generated by applying human-imperceptible perturbations on benign input to result in the misclassification. In addition, adversarial examples have an intriguing property of transferability, where adversarial examples crafted by the current model can also fool other unknown models. As adversarial examples can help identify the robustness of models ( Arnab et al., 2018 ), as well as improve the robustness of models by adversarial training ( Goodfellow et al., 2014 ), learning how to generate adversarial examples with high transferability is important and has gained increasing attentions in the literature ( Liu et al., 2016 ;  Dong et al., 2018 ; Xie et al., 2019;  Dong et al., 2019 ; Wang et al., 2019). Several gradient-based attacks have been proposed to generate adversarial examples, such as one- step attacks ( Goodfellow et al., 2014 ) and iterative attacks ( Kurakin et al., 2016 ;  Dong et al., 2018 ). Under the white-box setting, with the knowledge of the current model, existing attacks can achieve high success rates. However, they often exhibit low success rates under the black-box setting, es- pecially for models with defense mechanism, such as adversarial training ( Madry et al., 2018 ;  Song Published as a conference paper at ICLR 2020 et al., 2019 ) and input modification ( Liao et al., 2018 ;  Xie et al., 2018 ). Under the black-box setting, most existing attacks fail to generate robust adversarial examples against defense models. In this work, by regarding the adversarial example generation process as an optimization process, we propose two new methods to improve the transferability of adversarial examples: Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). • Inspired by the fact that Nesterov accelerated gradient (Nesterov, 1983) is superior to momentum for conventionally optimization (Sutskever et al., 2013), we adapt Nesterov accelerated gradient into the iterative gradient-based attack, so as to effectively look ahead and improve the transfer- ability of adversarial examples. We expect that NI-FGSM could replace the momentum iterative gradient-based method ( Dong et al., 2018 ) in the gradient accumulating portion and yield higher performance. • Besides, we discover that deep learning models have the scale-invariant property, and propose a Scale-Invariant attack Method (SIM) to improve the transferability of adversarial examples by optimizing the adversarial perturbations over the scale copies of the input images. SIM can avoid \"overfitting\" on the white-box model being attacked and generate more transferable adversarial examples against other black-box models. • We found that combining our NI-FGSM and SIM with existing gradient-based attack methods (e.g., diverse input method (Xie et al., 2019)) can further boost the attack success rates of adver- sarial examples. Extensive experiments on the ImageNet dataset (Russakovsky et al., 2015) show that our methods attack both normally trained models and adversarially trained models with higher attack success rates than existing baseline attacks. Our best attack method, SI-NI-TI-DIM (Scale-Invariant Nes- terov Iterative FGSM integrated with translation-invariant diverse input method), reaches an average success rate of 93.5% against adversarially trained models under the black-box setting. For further demonstration, we evaluate our methods by attacking the latest robust defense methods ( Liao et al., 2018 ;  Xie et al., 2018 ;  Liu et al., 2019 ;  Jia et al., 2019 ;  Cohen et al., 2019 ). The results show that our attack methods can generate adversarial examples with higher transferability than state-of-the- art gradient-based attacks.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a method for addressing the domain shift problem in few-shot classification by integrating feature-wise transformation layers into the feature encoder. A learning-to-learn algorithm is developed to optimize the hyper-parameters of the feature-wise transformation layers. Experiments show that the proposed feature-wise transformation layers can effectively improve the generalization ability of metric-based models to unseen domains, and further performance improvement is achieved with the learning-to-learn scheme.",
        "Abstract": "Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. To capture variations of the feature distributions under different domains, we further apply a learning-to-learn approach to search for the hyper-parameters of the feature-wise transformation layers. We conduct extensive experiments and ablation studies under the domain generalization setting using five few-shot classification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. Experimental results demonstrate that the proposed feature-wise transformation layer is applicable to various metric-based models, and provides consistent improvements on the few-shot classification performance under domain shift.",
        "Introduction": "  INTRODUCTION Few-shot classification ( Lake et al., 2015 ) aims to recognize instances from novel categories (query instances) with only few labeled examples in each class (support examples). Among various recent approaches for addressing the few-shot classification problem, metric-based meta-learning meth- ods ( Garcia & Bruna, 2018 ;  Sung et al., 2018 ;  Vinyals et al., 2016 ;  Snell et al., 2017 ;  Oreshkin et al., 2018 ) have received considerable attention due to their simplicity and effectiveness. In general, metric-based few-shot classification methods make the prediction based on the similarity between the query image and support examples. As illustrated in  Figure 1 , metric-based approaches con- sist of 1) a feature encoder and 2) a metric function. Given an input task consisting of few labeled images (the support set) and unlabeled images (the query set) from novel classes, the encoder first extracts the image features. The metric function then takes the features of both the labeled and unlabeled images as input and predicts the category of the query images. Despite the success of recognizing novel classes sampled from the same domain as in the training stage (e.g., , both train- ing and testing are on mini-ImageNet classes), Chen et al. ( Chen et al., 2019a ) recently raise the issue that existing metric-based approaches often do not generalize well to categories from different domains. The generalization ability to unseen domains, however, is of critical importance due to the difficulty to construct large training datasets for rare classes (e.g., , recognizing rare bird species in a fine-grained classification setting). As a result, understanding and addressing the domain shift problem for few-shot classification is of great interest. To alleviate the domain shift issue, numerous unsupervised domain adaptation techniques have been proposed ( Pan & Yang, 2010 ;  Chen et al., 2018 ;  Tzeng et al., 2017 ). These methods focus on adapting the classifier of the same category from the source to the target domain. Building upon the domain adaptation formulation, Dong and Xing ( Dong & Xing, 2018 ) relax the constraint and transfer knowledge across domains for recognizing novel category in the one-shot setting. However, unsupervised domain adaptation approaches assume that numerous unlabeled images are available in the target domain during training. In many cases, this assumption may not be realistic. For example, the cost and efforts of collecting numerous images of rare bird species can be prohibitively high. On the other hand, domain generalization methods have been developed (Blanchard et al., 2011;  Li et al., 2019 ) to learn classifiers that generalize well to multiple unseen domains without requiring the access to data from those domains. Yet, existing domain generalization approaches aim at recognizing instance from the same category in the training stage. In this paper, we tackle the domain generalization problem for recognizing novel category in the few-shot classification setting. As shown in Figure 1(c), our key observation is that the distributions of the image features extracted from the tasks in different domains are significantly different. As a result, during the training stage, the metric function may overfit to the feature distributions encoded only from the seen domains and thus fail to generalize to unseen domains. To address the issue, we propose to integrate feature-wise transformation layer to modulate the feature activations with affine transformations into the feature encoder. The use of these feature-wise transformation layers allows us to simulate various distributions of image features during the training stage, and thus improve the generalization ability of the metric function in the testing phase. Nevertheless, the hyper-parameters of the feature-wise transformation layers may require meticulous hand-tuning due to the difficulty to model the complex variation of the image feature distributions across various domains. In light of this, we develop a learning-to-learn algorithm to optimize the proposed feature-wise transformation layers. The core idea is to optimize the feature-wise transformation layers so that the model can work well on the unseen domains after training the model using the seen domains. We make the source code and datasets public available to simulate future research in this field. We make the following three contributions in this work: • We propose to use feature-wise transformation layers to simulate various image feature dis- tributions extracted from the tasks in different domains. Our feature-wise transformation Published as a conference paper at ICLR 2020 layers are method-agnostic and can be applied to various metric-based few-shot classifica- tion approaches for improving their generalization to unseen domains. • We develop a learning-to-learn method to optimize the hyper-parameters of the feature- wise transformation layers. In contrast to the exhaustive parameter hand-tuning process, the proposed learning-to-learn algorithm is capable of finding the hyper-parameters for the feature-wise transformation layers to capture the variation of image feature distribution across various domains. • We evaluate the performance of three metric-based few-shot classification models (includ- ing MatchingNet ( Vinyals et al., 2016 ), RelationNet ( Sung et al., 2018 ), and Graph Neural Networks ( Garcia & Bruna, 2018 )) with extensive experiments under the domain general- ization setting. We show that the proposed feature-wise transformation layers can effec- tively improve the generalization ability of metric-based models to unseen domains. We also demonstrate further performance improvement with our learning-to-learn scheme for learning the feature-wise transformation layers.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel method of using mode connectivity in loss landscapes to study and improve adversarial robustness of deep neural networks (DNNs) against different types of threats. The proposed method is demonstrated to be effective in repairing backdoored or error-injected DNNs, and in providing a geometric interpretation of the \"no free lunch\" hypothesis in adversarial robustness. Experiments on different DNN architectures and datasets show the effectiveness of the proposed method in understanding and improving adversarial robustness.",
        "Abstract": "Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness.  Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models.  We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models.  A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided.  Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness.",
        "Introduction": "  INTRODUCTION Recent studies on mode connectivity show that two independently trained deep neural network (DNN) models with the same architecture and loss function can be connected on their loss landscape using a high-accuracy/low-loss path characterized by a simple curve ( Garipov et al., 2018 ;  Gotmare et al., 2018 ;  Draxler et al., 2018 ). This insight on the loss landscape geometry provides us with easy access to a large number of similar-performing models on the low-loss path between two given models, and  Garipov et al. (2018)  use this to devise a new model ensembling method. Another line of recent research reveals interesting geometric properties relating to adversarial robustness of DNNs ( Fawzi et al., 2017 ; 2018;  Wang et al., 2018b ;  Yu et al., 2018 ). An adversarial data or model is defined to be one that is close to a bonafide data or model in some space, but exhibits unwanted or malicious behavior. Motivated by these geometric perspectives, in this study, we propose to employ mode connectivity to study and improve adversarial robustness of DNNs against different types of threats. A DNN can be possibly tampered by an adversary during different phases in its life cycle. For example, during the training phase, the training data can be corrupted with a designated trigger pattern associated with a target label to implant a backdoor for trojan attack on DNNs ( Gu et al., 2019 ;  Liu et al., 2018 ). During the inference phase when a trained model is deployed for task-solving, prediction-evasive attacks are plausible (Biggio & Roli, 2018;  Goodfellow et al., 2015 ;  Zhao et al., 2018 ), even when the model internal details are unknown to an attacker ( Chen et al., 2017 ;  Ilyas et al., 2018 ;  Zhao et al., 2019a ). In this research, we will demonstrate that by using mode connectivity in loss landscapes, we can repair backdoored or error-injected DNNs. We also show that mode Published as a conference paper at ICLR 2020 connectivity analysis reveals the existence of a robustness loss barrier on the path connecting regular and adversarially-trained models. We motivate the novelty and benefit of using mode connectivity for mitigating training-phase ad- versarial threats through the following practical scenario: as training DNNs is both time- and resource-consuming, it has become a common trend for users to leverage pre-trained models released in the public domain 2 . Users may then perform model fine-tuning or transfer learning with a small set of bonafide data that they have. However, publicly available pre-trained models may carry an unknown but significant risk of tampering by an adversary. It can also be challenging to detect this tampering, as in the case of a backdoor attack 3 , since a backdoored model will behave like a regular model in the absence of the embedded trigger. Therefore, it is practically helpful to provide tools to users who wish to utilize pre-trained models while mitigating such adversarial threats. We show that our proposed method using mode connectivity with limited amount of bonafide data can repair backdoored or error-injected DNNs, while greatly countering their adversarial effects. Our main contributions are summarized as follows: • For backdoor and error-injection attacks, we show that the path trained using limited bonafide data connecting two tampered models can be used to repair and redeem the attacked models, thereby resulting in high-accuracy and low-risk models. The performance of mode connectivity is significantly better than several baselines including fine-tuning, training from scratch, pruning, and random weight perturbations. We also provide technical explanations for the effectiveness of our path connection method based on model weight space exploration and similarity analysis of input gradients for clean and tampered data. • For evasion attacks, we use mode connectivity to study standard and adversarial-robustness loss landscapes. We find that between a regular and an adversarially-trained model, training a path with standard loss reveals no barrier, whereas the robustness loss on the same path reveals a barrier. This insight provides a geometric interpretation of the \"no free lunch\" hypothesis in adversarial robustness ( Tsipras et al., 2019 ;  Dohmatob, 2018 ;  Bubeck et al., 2019 ). We also provide technical explanations for the high correlation observed between the robustness loss and the largest eigenvalue of the input Hessian matrix on the path. • Our experimental results on different DNN architectures (ResNet and VGG) and datasets (CIFAR- 10 and SVHN) corroborate the effectiveness of using mode connectivity in loss landscapes to understand and improve adversarial robustness. We also show that our path connection is resilient to the considered adaptive attacks that are aware of our defense. To the best of our knowledge, this is the first work that proposes using mode connectivity approaches for adversarial robustness.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the effectiveness of self-training, a semi-supervised learning method, in sequence generation tasks such as machine translation and text summarization. Through empirical evaluation and ablation analysis, the paper finds that self-training can significantly improve the performance of the baseline model, and that perturbing the input and hidden states with noise is a crucial ingredient to prevent self-training from falling into the same local optimum as the base model. Experiments on machine translation and text summarization tasks demonstrate the effectiveness of noisy self-training.",
        "Abstract": "Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a noisy version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin.",
        "Introduction": "  INTRODUCTION Deep neural networks often require large amounts of labeled data to achieve good performance. However, acquiring labels is a costly process, which motivates research on methods that can ef- fectively utilize unlabeled data to improve performance. Towards this goal, semi-supervised learn- ing ( Chapelle et al., 2009 ) methods that take advantage of both labeled and unlabeled data are a natural starting point. In the context of sequence generation problems, semi-supervised approaches have been shown to work well in some cases. For example, back-translation ( Sennrich et al., 2015 ) makes use of the monolingual data on the target side to improve machine translation systems, latent variable models ( Kingma et al., 2014 ) are employed to incorporate unlabeled source data to facilitate sentence compression ( Miao & Blunsom, 2016 ) or code generation ( Yin et al., 2018 ). In this work, we revisit a much older and simpler semi-supervised method, self-training ( ST, Scudder (1965) ), where a base model trained with labeled data acts as a \"teacher\" to label the unannotated data, which is then used to augment the original small training set. Then, a \"student\" model is trained with this new training set to yield the final model. Originally designed for classification problems, common wisdom suggests that this method may be effective only when a good fraction of the predictions on unlabeled samples are correct, otherwise mistakes are going to be reinforced ( Zhu & Goldberg, 2009 ). In the field of natural language processing, some early work have successfully applied self-training to word sense disambiguation ( Yarowsky, 1995 ) and parsing ( McClosky et al., 2006 ;  Reichart & Rappoport, 2007 ;  Huang & Harper, 2009 ). However, self-training has not been studied extensively when the target output is natural language. This is partially because in language generation applications (e.g. machine translation) hypotheses are often very far from the ground-truth target, especially in low-resource settings. It is natural to Published as a conference paper at ICLR 2020 Train a new model f θ on S ∪ L 6: until convergence or maximum iterations are reached ask whether self-training can be useful at all in this case. While  Ueffing (2006)  and  Zhang & Zong (2016)  explored self-training in statistical and neural machine translation, only relatively limited gains were reported and, to the best of our knowledge, it is still unclear what makes self-training work. Moreover,  Zhang & Zong (2016)  did not update the decoder parameters when using pseudo parallel data noting that \"synthetic target parts may negatively influence the decoder model of NMT\". In this paper, we aim to answer two questions: (1) How does self-training perform in sequence generation tasks like machine translation and text summarization? Are \"bad\" pseudo targets indeed catastrophic for self-training? (2) If self-training helps improving the baseline, what contributes to its success? What are the important ingredients to make it work? Towards this end, we first evaluate self-training on a small-scale machine translation task and em- pirically observe significant performance gains over the supervised baseline (§3.2), then we perform a comprehensive ablation analysis to understand the key factors that contribute to its success (§3.3). We find that the decoding method to generate pseudo targets accounts for part of the improvement, but more importantly, the perturbation of hidden states - dropout ( Hinton et al., 2012 ) - turns out to be a crucial ingredient to prevent self-training from falling into the same local optimum as the base model, and this is responsible for most of the gains. To understand the role of such noise in self-training, we use a toy experiment to analyze how noise effectively propagates labels to nearby inputs, sometimes helping correct incorrect predictions (§4.1). Motivated by this analysis, we pro- pose to inject additional noise by perturbing also the input. Comprehensive experiments on machine translation and text summarization tasks demonstrate the effectiveness of noisy self-training.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes Precision Gating (PG), a novel end-to-end trainable method for dual-precision execution of deep neural networks (DNNs). PG enables DNN computation with lower average bitwidth than other state-of-the-art quantization methods, and has the potential to reduce DNN execution costs in both commodity and dedicated hardware. Experiments on CNNs and LSTMs show that PG achieves significant compute reduction and accuracy improvement compared to the baseline counterparts. PG also enables the same sparsity during back-propagation as forward propagation, which dramatically reduces the computational cost for both passes.",
        "Abstract": "We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks.  PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy.  The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss.  Our experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4× less compute on ImageNet. PG furthermore applies to RNNs. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7× computational cost reduction on LSTM on the Penn Tree Bank dataset.",
        "Introduction": "  INTRODUCTION In recent years, deep neural networks (DNNs) have demonstrated excellent performance on many computer vision and language modeling tasks such as image classification, semantic segmentation, face recognition, machine translation, and image captioning ( Krizhevsky et al., 2012 ;  He et al., 2016a ;  Ronneberger et al., 2015 ;  Chen et al., 2016 ;  Zhao et al., 2018 ;  Schroff et al., 2015 ;  Luong et al., 2015 ;  Vaswani et al., 2017 ). One evident trend in DNN design is that as researchers strive for better accuracy, both the model size and the number of DNN layers have drastically increased over time ( Xu et al., 2018 ). At the same time, there is a growing demand to deploy deep learning technology in edge devices such as mobile phones, VR/AR glasses, and drones ( Wu et al., 2019 ). The limited computational, memory, and energy budgets on these devices impose major challenges for the deployment of large DNN models at the edge. DNN quantization is an important technique for improving the hardware efficiency of DNN execu- tion ( Zhao et al., 2017 ). Numerous studies have shown that full-precision floating-point computation is not necessary for DNN inference - quantized fixed-point models produce competitive results with a small or zero loss in prediction accuracy ( Lin et al., 2016 ;  He et al., 2016b ;  Zhou et al., 2016 ; 2017). In some cases, quantization may even improve model generalization by acting as a form of regularization. Existing studies mainly focus on static quantization, in which the precision of each weight and activation is fixed prior to inference ( Hubara et al., 2017 ;  He et al., 2016b ). Along this line of work, researchers have explored tuning the bitwidth per layer ( Wu et al., 2018b ;  Wang et al., 2019 ;  Dong et al., 2019 ) as well as various types of quantization functions ( Wang et al., 2018 ;  Cour- bariaux et al., 2016 ;  Li et al., 2016 ;  Zhou et al., 2016 ). However, static DNN quantization methods cannot exploit input-dependent characteristics, where certain features can be computed in a lower precision during inference as they contribute less to the classification result for the given input. For Published as a conference paper at ICLR 2020 example, in computer vision tasks, the pixels representing the object of interest are typically more important than the background pixels. In this paper, we reduce the inefficiency of a statically quantized DNN via precision gating (PG), which computes most features with low-precision arithmetic operations and only updates few impor- tant features to a high precision. More concretely, PG first executes a DNN layer in a low precision and identifies the output features with large magnitude as important features. It then computes a sparse update to increase the precision of those important output features. Intuitively, small val- ues make a small contribution to the DNN's output; thus approximating them in a low precision is reasonable. Precision gating enables dual-precision DNN execution at the granularity of each indi- vidual output feature, and therefore greatly reducing the average bitwidth and computational cost of the DNN. We further introduce a differentiable gating function which makes PG applicable to a rich variety of network models. Experimental results show that PG achieves significant compute reduction and accuracy improve- ment on both CNNs and LSTMs. Compared to the baseline CNN counterparts, PG obtains 3.5% and 0.6% higher classification accuracy with up to 4.5× and 2.4× less computational cost for CIFAR-10 and ImageNet, respectively. On LSTM, compared to 8-bit uniform quantization PG boosts perplex- ity per word (PPW) by 1.2% with 2.8× less compute on the Penn Tree Bank (PTB) dataset. Our contributions are as follows: 1. We propose precision gating (PG), which to the best of our knowledge is the first end-to- end trainable method that enables dual-precision execution of DNNs. PG is applicable to a wide variety of CNN and LSTM models. 2. PG enables DNN computation with lower average bitwidth than other state-of-the-art quan- tization methods. By employing a low-cost gating scheme, PG has the potential to reduce DNN execution costs in both commodity and dedicated hardware. 3. PG achieves the same sparsity during back-propagation as forward propagation, which dramatically reduces the computational cost for both passes. This is in stark contrast to prior dynamic DNN optimization methods that focus only on reducing the inference cost.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a new pruning approach to obtain sparse neural networks with state-of-the-art test accuracy. The proposed compression scheme uses a new saliency criterion to identify important weights in the network throughout training and propose candidate masks. The algorithm not only evolves the pruned sparse model alone, but jointly also a (closely related) dense model that is used to correct for pruning errors during training. This results in better generalization properties on a wide variety of tasks, with no need for tuning of additional hyperparameters or retraining of the sparse model.",
        "Abstract": "Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and  (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and further that their performance surpasses all previously proposed pruning schemes (that come without feedback mechanisms).",
        "Introduction": "  INTRODUCTION Highly overparametrized deep neural networks show impressive results on machine learning tasks. However, with the increase in model size comes also the demand for memory and computer power at inference stage-two resources that are scarcely available on low-end devices. Pruning techniques have been successfully applied to remove a significant fraction of the network weights while preserv- ing test accuracy attained by dense models. In some cases, the generalization of compressed networks has even been found to be better than with full models ( Han et al., 2015 ; 2017;  Mocanu et al., 2018 ). The sparsity of a network is the number of weights that are identically zero, and can be obtained by applying a sparsity mask on the weights. There are several different approaches to find sparse models. For instance, one-shot pruning strategies find a suitable sparsity mask by inspecting the weights of a pretrained network ( Mozer & Smolensky, 1989 ;  LeCun et al., 1990 ;  Han et al., 2017 ). While these algorithms achieve a substantial size reduction of the network with little degradation in accuracy, they are computationally expensive (training and refinement on the dense model), and they are outperformed by algorithms that explore different sparsity masks instead of a single one. In dynamic pruning methods, the sparsity mask is readjusted during training according to different criteria ( Mostafa & Wang, 2019 ;  Mocanu et al., 2018 ). However, these methods require fine-tuning of many hyperparameters. We propose a new pruning approach to obtain sparse neural networks with state-of-the-art test accuracy. Our compression scheme uses a new saliency criterion that identifies important weights in the network throughout training to propose candidate masks. As a key feature, our algorithm not only evolves the pruned sparse model alone, but jointly also a (closely related) dense model that is used in a natural way to correct for pruning errors during training. This results in better generalization properties on a wide variety of tasks, since the simplicity of the scheme allows us further to study it from a theoretical point of view, and to provide further insights and interpretation. We do not require tuning of additional hyperparameters, and no retraining of the sparse model is needed (though can further improve performance).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to sparse dictionary learning (SDL) that exploits the blessing of dimensionality. It proposes a formulation which can efficiently recover the sparse signal matrix X once and for all, and provides an alternative derivation of the \"Matching, Stretching, and Projection\" (MSP) algorithm. The MSP algorithm is shown to be experimentally efficient and effective, and can be applied to a more general setting of maximizing a convex function over any compact set.",
        "Abstract": "Recently, the $\\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \\cite{zhai2019a} has proved surprisingly efficient and effective.  This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\\em principal}, {\\em independent}, or {\\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive to small noise, but it is also robust to outliers and resilient to sparse corruptions. We provide statistical justification for such inherently nice properties. To corroborate the theoretical analysis, we also provide extensive and compelling experimental evidence with both synthetic data and real images.",
        "Introduction": "  INTRODUCTION The explosion of massive amounts of high-dimensional data has become the modern-day norm for a large number of scientific and engineering disciplines and hence presents a daunting challenge for both computation and learning. Rising to this challenge, sparse dictionary learning (SDL) provides a potent framework in representation learning that exploits the blessing of dimensionality: real data tends to lie in or near some low-dimensional subspaces or manifolds, even though the ambient dimension is often extremely large (e.g. the number of raw pixels in an image). More specifically, SDL (Olshausen & Field (1997); Mairal et al. (2008; 2012; 2014); Spielman et al. (2012); Sun et al. (2015); Bai et al. (2018); Qu et al. (2019)) concerns the problem of learning a compact, sparse representation from raw, unlabelled data: given a data matrix Y = [y 1 , y 2 , . . . , y p ] ∈ R n×p that contains p n-dimensional samples, one aims to find a linear transformation (i.e. a dictionary) D ∈ R n×m and an associated maximally sparse representation X = [x 1 , x 2 , . . . , x p ] ∈ R m×p that satisfies Motivated by this practical significance, there has been a growing surge of interest recently (e.g. Rambhatla et al. (2019); Bai et al. (2018); Gilboa et al. (2018); Nguyen et al. (2018); Chatterji & Bartlett (2017); Mensch et al. (2016)) that aims to tackle SDL. In attempts to recover the sparse signals X, these existing work adopt an 0 - or 1 -penalty function to promote the underlying sparsity and give various optimization algorithms for the resulting objectives (some of those are heuristics while a few others have theoretical convergence guarantees). Although these penalty Published as a conference paper at ICLR 2020 functions are indeed sparsity-promoting, the resulting optimization problems must be solved one row at a time, hence resulting as many optimization problems as the ambient dimension n. Consequently, 0 - or 1 -based objectives result only in local methods (i.e. cannot yield the entire solution at once) and hence entail prohibitive computational burden. Another prominent approach in SDL is Sum- of-Squares (SOS), proposed by and articulated in a series of recent work (Barak et al. (2015); Ma et al. (2016); Schramm & Steurer (2017)). The key idea there is to utilize the properties of higher order SOS polynomials to correctly recover one column of the dictionary at a time, for which there are m columns in total. However, the computational complexity of these recovery methods are quasi-polynomial, thus again resulting in large computational expense. Very recently, in the complete dictionary learning 1 setting, a novel global approach has been sug- gested in Zhai et al. (2019a;b) that presents a formulation which can efficiently recover the sparse signal matrix X once and for all. In particular, Zhai et al. (2019b) has shown that if the generative model for Y = D o X o ∈ R n×p satisfies that D o ∈ O(n; R) is orthonormal and X o ∈ R n×p is Bernoulli-Gaussian sparse, 2 then maximizing the 4 -norm 3 of AY over O(n; R): max A 1 4 AY 4 4 subject to A ∈ O(n; R) (or AA * = I), (2) is able to find the ground truth dictionary D o up to an arbitrary signed permutation. Moreover, Zhai et al. (2019b) has proposed the simple \"Matching, Stretching, and Projection\" (MSP) algorithm, which is shown to be experimentally efficient and effective, for solving the program in equation 2: In this paper, we here give an alternative (arguably simpler and more revealing) derivation of the MSP algorithm (3). Consider the Lagrangian formulation of the constrained optimization problem given in equation 2. The necessary condition of critical points, ∇ A 1 4 AY 4 4 = ∇ A Λ, AA * − I for some Lagrangian multipliers Λ, implies: As the optimization is over the orthogonal group O(n; R), restricting the condition in equation 4 onto the orthogonal group yields a necessary condition for any critical point A: 4 Hence the critical point A can be viewed as a \"fixed point\" of the map: P O(n;R) ((·)Y ) •3 Y * from O(n; R) to itself. The MSP algorithm in equation 3 is to find the fixed point(s) of this map. Notice that the orthonormal constraint A ∈ O(n; R) in equation 2 can be viewed as enforcing the orthogonality of n unit vectors simultaneously. So, more flexibly and generally, one may choose to compute any k, for 1 ≤ k ≤ n, leading orthonormal bases of D o by solving the program: max W 1 4 W * Y 4 4 subject to W ∈ St(n, k; R) ⊂ R n×k , (6) where St(n, k; R) is the Stiefel manifold. 5 The orthogonal group O(n; R) and the unit sphere S n−1 can be viewed as two special cases of the Stiefel manifold St(n, k; R), with k = n and k = 1, re- spectively. In some specific tasks such as dictionary learning and blind deconvolution, optimization over the unit sphere has been widely practiced, such as in Sun et al. (2015); Bai et al. (2018); Zhang et al. (2018); Kuo et al. (2019). The more general setting of maximizing a convex function over any compact set also has been studied by Journée et al. (2010) in the context of sparse PCA, which has provided convergence guarantees for this class of programs.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a detection mechanism for adversarial examples that can withstand adaptive attacks. The mechanism partitions the input space into subspaces based on the original classifier's decision boundary, and then performs clean/adversarial example classification in the subspaces. Empirically, the proposed approach improves the state-of-the-art mean L2 distortion from 3.68 to 5.65 on MNIST dataset, and from 1.1 to 1.5 on CIFAR10 dataset. Additionally, the paper studies powerful and versatile generative classification models derived from the detection framework and demonstrates their competitive performances over discriminative robust classifiers.",
        "Abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we propose a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, we train K binary classifiers where the i-th binary classifier is used to distinguish between clean data of class i and adversarially perturbed samples of other classes. At test time, we first use a trained classifier to get the predicted label (say k) of the input, and then use the k-th binary classifier to determine whether the input is a clean sample (of class k) or an adversarially perturbed example (of other classes). We further devise a generative approach to detecting/classifying adversarial examples by interpreting each binary classifier as an unnormalized density model of the class-conditional data. We provide comprehensive evaluation of the above adversarial example detection/classification methods, and demonstrate their competitive performances and compelling properties. Code is available at https://github.com/xuwangyin/GAT-Generative-Adversarial-Training",
        "Introduction": "  INTRODUCTION Deep neural networks have become the staple of modern machine learning pipelines, achieving state- of-the-art performance on extremely difficult tasks in various applications such as computer vision (He et al., 2016), speech recognition (Amodei et al., 2016), machine translation (Vaswani et al., 2017), robotics (Levine et al., 2016), and biomedical image analysis (Shen et al., 2017). Despite their outstanding performance, these networks are shown to be vulnerable against various types of adversarial attacks, including evasion attacks (aka, inference or perturbation attacks) (Szegedy et al., 2013; Goodfellow et al., 2014; Carlini & Wagner, 2017b; Su et al., 2019) and poisoning attacks (Liu et al., 2017; Shafahi et al., 2018). These vulnerabilities in deep neural networks hinder their deployment in sensitive domains including, but not limited to, health care, finances, autonomous driving, and defense-related applications and have become a major security concern. Due to the mentioned vulnerabilities, there has been a recent surge toward designing defense mech- anisms against adversarial attacks (Gu & Rigazio, 2014; Jin et al., 2015; Papernot et al., 2016b; Published as a conference paper at ICLR 2020 Bastani et al., 2016; Madry et al., 2017; Sinha et al., 2018), which has in turn motivated the de- sign of stronger attacks that defeat the proposed defenses (Goodfellow et al., 2014; Kurakin et al., 2016b;a; Carlini & Wagner, 2017b; Xiao et al., 2018; Athalye et al., 2018; Chen et al., 2018; He et al., 2018). Besides, the proposed defenses have been shown to be limited and often not effective and easy to overcome (Athalye et al., 2018). Alternatively, a large body of work has focused on detection of adversarial examples (Bhagoji et al., 2017; Feinman et al., 2017; Gong et al., 2017; Grosse et al., 2017; Metzen et al., 2017; Hendrycks & Gimpel, 2017; Li & Li, 2017; Xu et al., 2017; Pang et al., 2018; Roth et al., 2019; Bahat et al., 2019; Ma et al., 2018; Zheng & Hong, 2018; Tian et al., 2018). While training robust classifiers focuses on maintaining performance in presence of adversarial examples, adversarial detection only cares for detecting such examples. The majority of the current detection mechanisms focus on non-adaptive threats, for which the attacks are not specifically tuned/tailored to bypass the detection mechanism, and the attacker is oblivious to the detection mechanism. In fact, Carlini & Wagner (2017a) and Athalye et al. (2018) showed that the detection methods presented in (Bhagoji et al., 2017; Feinman et al., 2017; Gong et al., 2017; Grosse et al., 2017; Metzen et al., 2017; Hendrycks & Gimpel, 2017; Li & Li, 2017; Ma et al., 2018), are significantly less effective than their claims under adaptive attacks. Overall, current solutions are mostly heuristic approaches that cannot provide performance guarantees. In this paper we propose a detection mechanism that can with- stand adaptive attacks. The idea is to partition the input space into subspaces based on the original classifier's decision bound- ary, and then perform clean/adversarial example classification the subspaces. The binary classifier in each subspace is trained to dis- tinguish in-class samples from adversarially perturbed samples of other classes. At inference time, we first use the original classi- fier to get an input sample's predicted labelk, and then use thê k-th binary classifier to identify whether the input is a clean sample (of classk) or an adversarially perturbed sample (of other classes).  Fig. 1  provides a schematic illustration of the proposed approach. Our specific contributions are: (1) We develop a principled adversarial example detection method that can withstand adaptive attacks. Empirically, our best models improve previous state-of-the-art mean L 2 distortion from 3.68 to 5.65 on MNIST dataset, and from 1.1 to 1.5 on CIFAR10 dataset. (2) We study powerful and versatile generative classification models derived from our detection framework and demonstrate their competitive performances over discriminative robust classifiers. While discriminative robust classifiers are vulnerable to rubbish examples, inputs that have confident predictions under our models have interpretable features.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper demonstrates that deep models trained for seemingly simple objectives can unintentionally learn privacy- and bias-sensitive attributes that are not part of the specified objective, a phenomenon known as overlearning. We discuss the consequences of overlearning, including the potential for inference-time privacy violations and the inadequacy of privacy regulations that rely on explicit enumeration of learned attributes. We also develop a new de-censoring technique to extract information from censored representations and analyze where and why overlearning happens.",
        "Abstract": "``\"Overlearning'' means that a model trained for a seemingly simple\nobjective implicitly learns to recognize attributes and concepts that are\n(1) not part of the learning objective, and (2) sensitive from a privacy\nor bias perspective.  For example, a binary gender classifier of facial\nimages also learns to recognize races, even races that are\nnot represented in the training data, and identities.\n\nWe demonstrate overlearning in several vision and NLP models and analyze\nits harmful consequences.  First, inference-time representations of an\noverlearned model reveal sensitive attributes of the input, breaking\nprivacy protections such as model partitioning.  Second, an overlearned\nmodel can be \"`re-purposed'' for a different, privacy-violating task\neven in the absence of the original training data.\n\nWe show that overlearning is intrinsic for some tasks and cannot be\nprevented by censoring unwanted attributes.  Finally, we investigate\nwhere, when, and why overlearning happens during model training.",
        "Introduction": "  INTRODUCTION We demonstrate that representations learned by deep models when training for seemingly simple objectives reveal privacy- and bias-sensitive attributes that are not part of the specified objective. These unintentionally learned concepts are neither finer-, nor coarse-grained versions of the model's labels, nor statistically correlated with them. We call this phenomenon overlearning. For example, a binary classifier trained to determine the gender of a facial image also learns to recognize races (including races not represented in the training data) and even identities of individuals. Overlearning has two distinct consequences. First, the model's inference-time representation of an input reveals the input's sensitive attributes. For example, a facial recognition model's representa- tion of an image reveals if two specific individuals appear together in it. Overlearning thus breaks inference-time privacy protections based on model partitioning ( Osia et al., 2018 ;  Chi et al., 2018 ;  Wang et al., 2018 ). Second, we develop a new, transfer learning-based technique to \"re-purpose\" a model trained for benign task into a model for a different, privacy-violating task. This shows the inadequacy of privacy regulations that rely on explicit enumeration of learned attributes. Overlearning is intrinsic for some tasks, i.e., it is not possible to prevent a model from learning sensitive attributes. We show that if these attributes are censored ( Xie et al., 2017 ;  Moyer et al., 2018 ), the censored models either fail to learn their specified tasks, or still leak sensitive information. We develop a new de-censoring technique to extract information from censored representations. We also show that overlearned representations enable recognition of sensitive attributes that are not present in the training data. Such attributes cannot be censored using any known technique. This shows the the inadequacy of censoring as a privacy protection technology. To analyze where and why overlearning happens, we empirically show how general features emerge in the lower layers of models trained for simple objectives and conjecture an explanation based on the complexity of the training data.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents V4D, a general and flexible framework for video-level representation learning. V4D is composed of two critical designs: (1) holistic sampling strategy, and (2) 4D convolutional interaction. The holistic sampling strategy uniformly samples a sequence of short-term units covering the whole video, and the 4D convolutional interaction captures inter-clip interaction to enhance the representation power of the original clip-level 3D CNNs. V4D is evaluated on three video action recognition benchmarks, Mini-Kinetics, Kinetics-400 and Something-Something-V1, and achieves competitive performance and evident performance improvement over its 3D counterparts.",
        "Abstract": "Most existing 3D CNN structures for video representation learning are clip-based methods, and do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, namely V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, as well as preserving 3D spatio-temporal representations with residual connections. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.",
        "Introduction": "  INTRODUCTION 3D convolutional neural networks (3D CNNs) and their variants ( Ji et al., 2010 ;  Tran et al., 2015 ;  Carreira & Zisserman, 2017 ;  Qiu et al., 2017 ;  Wang et al., 2018b ) provide a simple extension from 2D counterparts for video representation learning. However, due to practical issues such as memory consumption and computational cost, these models are mainly used for clip-level feature learning instead of learning from the whole video. The clip-based methods randomly sample a short clip (e.g., 32 frames) from a video for representation learning, and calculate prediction scores for each clip independently. The prediction scores of all clips are simply averaged to yield the video-level prediction. These clip-based models often ignore the video-level structure and long-range spatio- temporal dependency during training, as they only sample a small portion of the entire video. In fact, in some cases, it could be difficult to identify an action correctly by only using partial observation. Meanwhile, simply averaging the prediction scores of all clips could be sub-optimal during inference. To overcome this issue, Temporal Segment Network (TSN) ( Wang et al., 2016 ) was proposed. TSN uniformly samples multiple clips from the entire video, and the average scores are used to guide back-propagation during training. Thus TSN is a video-level representation learning framework. However, the inter-clip interaction and video-level fusion in TSN is only performed at very late stage, which fails to capture finer temporal structures. In this paper, we propose a general and flexible framework for video-level representation learning, called V4D. As shown in  Figure 1 , to model long-range dependency in a more efficient way, V4D is composed of two critical designs: (1) holistic sampling strategy, and (2) 4D convolutional interaction. We first introduce a video-level sampling strategy by uniformly sampling a sequence of short-term units covering the whole video. Then we model long-range spatio-temporal dependency by designing a unique 4D residual block. Specifically, we present a 4D convolutional operation to capture inter-clip Published as a conference paper at ICLR 2020 interaction, which could enhance the representation power of the original clip-level 3D CNNs. The 4D residual blocks could be easily integrated into the existing 3D CNNs to perform long-range modeling hierarchically, which is more efficient than TSN. We also design a specific video-level inference algorithm for V4D. Finally, we verify the effectiveness of V4D on three video action recognition benchmarks, Mini-Kinetics ( Xie et al., 2018 ), Kinetics-400 ( Carreira & Zisserman, 2017 ) and Something-Something-V1 ( Goyal et al., 2017 ). Our V4D achieves very competitive performance on the three benchmarks, and obtains evident performance improvement over its 3D counterparts.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the implicit bias of gradient descent and gradient flow for homogeneous neural networks. We prove that gradient descent and gradient flow converge to the direction of a KKT point or even the max-margin direction, under various assumptions including the convergence of loss and gradient directions. We also discuss the implications of our results for deep learning.",
        "Abstract": "In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.",
        "Introduction": "  INTRODUCTION A major open question in deep learning is why gradient descent or its variants, are biased towards solutions with good generalization performance on the test set. To achieve a better understanding, previous works have studied the implicit bias of gradient descent in different settings. One simple but insightful setting is linear logistic regression on linearly separable data. In this setting, the model is parameterized by a weight vector w, and the class prediction for any data point x is determined by the sign of w x. Therefore, only the direction w/ w 2 is important for making prediction.  Soudry et al. (2018a ;b);  Ji & Telgarsky (2018 ; 2019c);  Nacson et al. (2019c)  investigated this prob- lem and proved that the direction of w converges to the direction that maximizes the L 2 -margin while the norm of w diverges to +∞, if we train w with (stochastic) gradient descent on logistic loss. Interestingly, this convergent direction is the same as that of any regularization path: any se- quence of weight vectors {w t } such that every w t is a global minimum of the L 2 -regularized loss L(w) + λt 2 w 2 2 with λ t → 0 ( Rosset et al., 2004 ). Indeed, the trajectory of gradient descent is also pointwise close to a regularization path ( Suggala et al., 2018 ). The aforementioned linear logistic regression can be viewed as a single-layer neural network. A nat- ural and important question is to what extent gradient descent has similiar implicit bias for modern deep neural networks. For theoretical analysis, a natural candidate is to consider homogeneous neu- ral networks. Here a neural network Φ is said to be (positively) homogeneous if there is a number L > 0 (called the order) such that the network output Φ(θ; x), where θ stands for the parameter and x stands for the input, satisfies the following: It is important to note that many neural networks are homogeneous ( Neyshabur et al., 2015a ;  Du et al., 2018 ). For example, deep fully-connected neural networks or deep CNNs with ReLU or Published as a conference paper at ICLR 2020 LeakyReLU activations can be made homogeneous if we remove all the bias terms, and the order L is exactly equal to the number of layers. In ( Wei et al., 2019 ), it is shown that the regularization path does converge to the max-margin di- rection for homogeneous neural networks with cross-entropy or logistic loss. This result suggests that gradient descent or gradient flow may also converges to the max-margin direction by assuming homogeneity, and this is indeed true for some sub-classes of homogeneous neural networks. For gra- dient flow, this convergent direction is proven for linear fully-connected networks ( Ji & Telgarsky, 2019a ). For gradient descent on linear fully-connected and convolutional networks, ( Gunasekar et al., 2018b ) formulate a constrained optimization problem related to margin maximization and prove that gradient descent converges to the direction of a KKT point or even the max-margin di- rection, under various assumptions including the convergence of loss and gradient directions. In an independent work, ( Nacson et al., 2019a ) generalize the result in ( Gunasekar et al., 2018b ) to smooth homogeneous models (we will discuss this work in more details in Section 2).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces deep convolutional neural networks (CNNs) and their applications in machine learning and pattern recognition tasks. It then discusses various methods of reducing the parameter space of CNNs, such as pruning, weight sharing, quantization, and low-rank and sparse representation of filters. Finally, it examines methods of improving the inference speed of CNNs.",
        "Abstract": "We present a novel method of compression of deep Convolutional Neural Networks (CNNs) by weight sharing through a new representation of convolutional filters. The proposed method reduces the number of parameters of each convolutional layer by learning a $1$D vector termed Filter Summary (FS). The convolutional filters are located in FS as overlapping $1$D segments, and nearby filters in FS share weights in their overlapping regions in a natural way. The resultant neural network based on such weight sharing scheme, termed Filter Summary CNNs or FSNet, has a FS in each convolution layer instead of a set of independent filters in the conventional convolution layer. FSNet has the same architecture as that of the baseline CNN to be compressed, and each convolution layer of FSNet has the same number of filters from FS as that of the basline CNN in the forward process. With compelling computational acceleration ratio, the parameter space of FSNet is much smaller than that of the baseline CNN. In addition, FSNet is quantization friendly. FSNet with weight quantization leads to even higher compression ratio without noticeable performance loss. We further propose Differentiable FSNet where the way filters share weights is learned in a differentiable and end-to-end manner. Experiments demonstrate the effectiveness of FSNet in compression of CNNs for computer vision tasks including image classification and object detection, and the effectiveness of DFSNet is evidenced by the task of Neural Architecture Search.",
        "Introduction": "  INTRODUCTION Deep Convolutional Neural Networks (CNNs) have achieved stunning success in various machine learning and pattern recognition tasks by learning highly semantic and discriminative representation of data ( LeCun et al., 2015 ). Albeit the power of CNNs, they are usually over-parameterized and of large parameter space, which makes it difficult for deployment of CNNs on mobile platforms or other platforms with limited storage. Moreover, the large parameter space of CNNs encourages researchers to study regularization methods that prevent overfitting ( Srivastava et al., 2014 ). In the recently emerging architecture such as Residual Network ( He et al., 2016 ) and Densely Connected Network ( Huang et al., 2017 ), most parameters concentrate on convolution filters, which are used to learn deformation invariant features in the input volume. The deep learning community has developed several compression methods of reducing the parameter space of filters and the entire neural network, such as pruning ( Luo et al., 2017 ;  Li et al., 2017 ;  Anwar et al., 2017 ), weight sharing and quantization ( Han et al., 2016 ;  Tung & Mori, 2018 ;  Park et al., 2017 ) and low-rank and sparse representation of the filters ( Ioannou et al., 2016 ;  Yu et al., 2017 ). Quantization based methods ( Tung & Mori, 2018 ;  Park et al., 2017 ) may not substantially improve the execution time of CNNs, and many methods ( Lebedev & Lempitsky, 2016 ;  Zhang et al., 2018 ) have also been proposed to improve the inference speed of CNNs.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the lottery ticket phenomenon, which occurs when small, sparse sub-networks can be found in over-parameterized deep neural networks (DNNs) which, when trained in isolation, can achieve similar or even greater performance than the original, highly over-parameterized network. We evaluate whether the lottery ticket phenomenon is merely an artifact of supervised image classification with feed-forward convolutional neural networks, or whether it generalizes to other domains, architectural paradigms, and learning regimes (e.g., environments with reward signals). We find that lottery tickets are present in both natural language processing (NLP) and reinforcement learning (RL) tasks, and are able to find masks and initializations which enable a Transformer Big model to train from scratch to achieve 99% the BLEU score of the unpruned model on the Newstest'14 machine translation task while using only a third of the parameters. These results demonstrate that the lottery ticket phenomenon is a general property of deep neural networks, and highlight their potential for practical applications.",
        "Abstract": "The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",
        "Introduction": "  INTRODUCTION The lottery ticket phenomenon (Frankle & Carbin, 2019; Frankle et al., 2019;  Zhou et al., 2019 ) occurs when small, sparse sub-networks can be found in over-parameterized deep neural networks (DNNs) which, when trained in isolation, can achieve similar or even greater performance than the original, highly over-parameterized network. This phenomenon suggests that over-parameterization in DNN training is beneficial primarily due to proper initialization rather than regularization during the training process itself ( Allen-Zhu et al., 2019 ; 2018;  Du & Lee, 2018 ;  Du et al., 2019 ;  Neyshabur et al., 2014 ;  2019 ). However, despite extensive experiments in Frankle & Carbin (2019) and Frankle et al. (2019), it remains unclear whether the lottery ticket phenomenon is an intrinsic feature of DNN behavior or whether it is dependent on other factors such as supervised learning, network architecture, specific tasks (e.g., image classification), the bias in the dataset, or artifacts from the optimization algorithm itself. As discussed in Frankle & Carbin (2019) and  Liu et al. (2019) , large learning rates severely damage the lottery ticket effect, and for larger models (such as VGG and ResNets) and datasets (e.g., ImageNet), heuristics like learning rate warmup (Frankle & Carbin, 2019) and late rewinding (Frankle et al., 2019) are needed to induce high performance and reliable winning tickets. Recent work has also questioned the effectiveness of the lottery ticket hypothesis, raising concerns about the generality of this phenomenon ( Liu et al., 2019 ;  Gale et al., 2019 ). In this work, we address the question of whether the lottery ticket phenomenon is merely an artifact of supervised image classification with feed-forward convolutional neural networks, or whether Published as a conference paper at ICLR 2020 this phenomenon generalizes to other domains, architectural paradigms, and learning regimes (e.g., environments with reward signals). Many natural language processing (NLP) models feature complex gating mechanics paired with recurrent dynamics, either of which may significantly impact the optimization process and, consequently, the lottery ticket phenomenon. Furthermore, prior work has suggested that this phenomenon is not present in Transformer models ( Gale et al., 2019 ), calling the broad applicability of lottery tickets into question. In reinforcement learning (RL), the data distribution shifts as the agent learns from often reward signals, significantly modifying the optimization process and the resultant networks. Pre-trained feature extractors have proven successful in computer vision ( Kornblith et al., 2019 ;  Razavian et al., 2014 ;  Yosinski et al., 2014 ), but in RL, agents often fail to generalize even to extremely similar situations ( Raghu et al., 2018 ;  Lanctot et al., 2017 ;  Cobbe et al., 2018 ;  Ruderman et al., 2019 ). To answer this question, we evaluate whether the lottery ticket hypothesis holds for NLP and RL, both of which are drastically different from traditional supervised image classification. To demonstrate the lottery ticket phenomenon, we ask whether sparsified subnetworks initialized as winning tickets outperform randomly initialized subnetworks at convergence. We note that, though desirable, we do not require that subnetworks match the performance of the full network, as originally stated in Frankle et al. (2019). We exclude this requirement because we are primarily interested in whether appropriate initialization impacts the performance of subnetwork training, consistent with the revised definition of the lottery ticket hypothesis in Frankle & Carbin (2019). For NLP, we evaluate language modelling on Wikitext-2 with LSTMs ( Merity et al., 2017 ) and machine translation on the WMT'14 English-German translation task with Transformers ( Vaswani et al., 2017 ). For RL, we evaluate both classic control problems and Atari games ( Bellemare et al., 2013 ). Perhaps surprisingly, we found that lottery tickets are present in both NLP and RL tasks. In NLP, winning tickets were present both in recurrent LSTMs trained on language modeling and in Transformers ( Vaswani et al., 2017 ) trained on a machine translation task while in RL, we observed winning tickets in both classic control problems and Atari games (though with high variance). Notably, we are able to find masks and initializations which enable a Transformer Big model to train from scratch to achieve 99% the BLEU score of the unpruned model on the Newstest'14 machine translation task while using only a third of the parameters. Together, these results demonstrate that the lottery ticket phenomenon is a general property of deep neural networks, and highlight their potential for practical applications.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel method to protect the privacy of data processed by on-device machine learning. The method tweaks a conventional neural network into a complex-valued one, such that intermediate-layer features are released without sacrificing input privacy. The proposed method is designed to preserve the piecewise linearity of features, while preventing an adversary from recovering the exact input or inferring the correct output. The neural network is trained using the original data without additional human supervision, and is efficient to conduct inference.",
        "Abstract": "Previous studies have found that an adversary attacker can often infer unintended input information from intermediate-layer features. We study the possibility of preventing such adversarial inference, yet without too much accuracy degradation. We propose a generic method to revise the neural network to boost the challenge of inferring input attributes from features, while maintaining highly accurate outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the output from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method significantly diminishes the adversary's ability in inferring about the input while largely preserves the resulting accuracy.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have shown superb capabilities to process massive volume of data, and local devices such as mobile phones, medical equipment, Internet of Things (IoT) devices have become major data entry points in recent years. Although on-device machine learning has exhibited various advantages, it usually burdens thin devices with overwhelming computational overhead. Yet offloading data or processed features to a cloud operator would put the individual privacy at risk. For example, if a user has a virtual assistant at home, it should not worry about its private data being collected and processed by an untrusted cloud operator. The operator, on the other hand, should not be able to recover original signals or their interpretation. However, as shown in the previous literature ( Dosovitskiy & Brox (2016) ; Zeiler & Fergus (2014);  Ma- hendran & Vedaldi (2015) ;  Shokri et al. (2017) ;  Ganju et al. (2018) ;  Melis et al. (2019) ), intermediate- layer features face many privacy threats, where the adversary either reconstructs the input or infers unintended properties about the input. Hence, on-device processing encounters a dilemma, i.e. while we expect intermediate-layer features to yield high accuracy, we certainly would not want sensitive information to be leaked. Therefore, we propose a novel method which tweaks a conventional neural network into a complex- valued one, such that intermediate-layer features are released without sacrificing input privacy too much. More precisely, we turn the original real-valued features into complex-valued ones, rotate these features by a random angle, and feed them to the cloud for further processing. We face two significant challenges in the design. First, the tweaked features have to be correctly handled by the DNN on the cloud. Although network structures vary depending on the input types, it is desired that the piecewise linearity of features to be preserved, so that the resulting accuracy does not degrade too much compared with the original features. Second, an adversary attacker who intercepts the features in the middle should not be able to recover the exact input, nor can it figure out the correct output. Most importantly, the neural network is supposed to be trained using the original data without additional human supervision, and is efficient to conduct inference.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the temporal difference (TD) learning algorithm with linear function approximation, which is used to obtain the expected long-term reward of a given policy in reinforcement learning. We analyze the finite sample analysis of TD and two approaches to reduce the variance: batch TD and Variance Reduced TD (VRTD). We address the questions of whether and how variance reduction can help to improve the convergence accuracy over vanilla TD, and the overall computational complexity of VRTD.",
        "Abstract": "Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by \\cite{korda2015td}, which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in \\cite{korda2015td}, and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Furthermore, the variance error (for both i.i.d.\\ and Markovian sampling) and the bias error (for Markovian sampling) of VRTD are significantly reduced by the batch size of variance reduction in comparison to those of vanilla TD. As a result, the overall computational complexity of VRTD to attain a given accurate solution outperforms that of TD under Markov sampling and outperforms that of TD under i.i.d.\\ sampling for a sufficiently small conditional number.",
        "Introduction": "  INTRODUCTION In reinforcement learning (RL), policy evaluation aims to obtain the expected long-term reward of a given policy and plays an important role in identifying the optimal policy that achieves the maximal cumulative reward over time Bertsekas and Tsitsiklis (1995); Dayan and Watkins (1992);  Rummery and Niranjan (1994) . The temporal difference (TD) learning algorithm, originally proposed by  Sutton (1988) , is one of the most widely used policy evaluation methods, which uses the Bellman equation to iteratively bootstrap the estimation process and continually update the value function in an incremental way. In practice, if the state space is large or infinite, function approximation is often used to find an approximate value function efficiently. Theoretically, TD with linear function approximation has been shown to converge to the fixed point solution with i.i.d. samples and Markovian samples in  Sutton (1988) ;  Tsitsiklis and Van Roy (1997) . The finite sample analysis of TD has also been studied in  Bhandari et al. (2018) ;  Dalal et al. (2018a) ;  Cai et al. (2019) ;  Srikant and Ying (2019) . Since each iteration of TD uses one or a mini-batch of samples to estimate the mean of the pseudo- gradient 1 , TD learning usually suffers from the inherent variance, which substantially degrades the convergence accuracy. Although a diminishing stepsize or very small constant stepsize can reduce the variance  Bhandari et al. (2018) ;  Srikant and Ying (2019) , they also slow down the convergence significantly. Two approaches have been proposed to reduce the variance. The first approach is the so-called batch TD, which takes a fixed sample set and transforms the empirical mean square projected Bellman error (MSPBE) into an equivalent convex-concave saddle-point problem  Du et al. (2017) . Due to the finite-sample nature of such a problem, stochastic variance reduction techniques for conventional optimization can be directly applied here to reduce the variance. In particular,  Du et al. (2017)  showed that SVRG  Johnson and Zhang (2013)  and  SAGA Defazio et al. (2014)  can be applied to improve Published as a conference paper at ICLR 2020 the performance of batch TD algorithms, and  Peng et al. (2019)  proposed two variants of SVRG to further save the computation cost. However, the analysis of batch TD does not take into account the statistical nature of the training samples, which are generated by a MDP. Hence, there is no guarantee of such obtained solutions to be close to the fixed point of TD learning. The second approach is the so-called TD with centering (CTD) algorithm proposed in  Korda and La (2015) , which introduces the variance reduction idea to the original TD learning algorithm. For the sake of better reflecting its major feature, we refer to CTD as Variance Reduced TD (VRTD) throughout this paper. Similarly to the SVRG in  Johnson and Zhang (2013) , VRTD has outer and inner loops. The beginning of each inner-loop (i.e. each epoch) computes a batch of sample pseudo- gradients so that each subsequent inner loop iteration modifies only one sample pseudo-gradient in the batch pseudo-gradients to reduce the variance. The main difference between VRTD and batch TD is that VRTD applies the variance reduction directly to TD learning rather than to a transformed optimization problem in batch TD. Though  Korda and La (2015)  empirically verified that VRTD has better convergence accuracy than vanilla TD learning, some technical errors in the analysis in  Korda and La (2015)  have been pointed out in follow up studies  Dalal et al. (2018a) ;  Narayanan and Szepesvári (2017) . Furthermore, as we discuss in Section 3, the technical proof in  Korda and La (2015)  regarding the convergence of VRTD also has technical errors so that their results do not correctly characterize the impact of variance reduction on TD learning. Given the recent surge of interest in the finite time analysis of the vanilla TD  Dalal et al. (2018a) ;  Bhandari et al. (2018) ;  Dalal et al. (2018b) ;  Srikant and Ying (2019) , it becomes imperative to reanalyze the VRTD and accurately understand whether and how variance reduction can help to improve the convergence accuracy over vanilla TD. Towards this end, this paper specifically addresses the following central questions. • For i.i.d. sampling, it has been shown in  Dalal et al. (2018a) ;  Bhandari et al. (2018)  that vanilla TD converges only to a neighborhood of the fixed point for a constant stepsize and suffers from a constant error term caused by the variance of the stochastic pseudo-gradient at each iteration. For VRTD, does the variance reduction help to reduce such an error and improve the accuracy of convergence? How does the error depend on the variance reduction parameter, i.e., the batch size for variance reduction? • For Markovian sampling, it has been shown in  Bhandari et al. (2018)  that the convergence of vanilla TD further suffers from a bias error due to the correlation among samples in addition to the variance error as in i.i.d. sampling. Does VRTD, which was designed to have reduced variance, also enjoy reduced bias error? If so, how does the bias error depend on the batch size for variance reduction? • For both i.i.d. and Markovian sampling, to attain an -accurate solution, what is the overall computational complexity (the total number of computations of pseudo-gradients) of VRTD, and does VRTD have a reduced overall computational complexity compared to TD?",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel regularization technique for Generative Adversarial Networks (GANs) called consistency regularization. This technique is simple to use and surprisingly effective, and is less computationally expensive than prior techniques. Experiments show that it works across a large range of GAN variants and datasets, and can lead to new state-of-the-art results as measured by Frechet Inception Distance. The proposed technique can also further boost the performance of existing GAN models.",
        "Abstract": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization—a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
        "Introduction": "  INTRODUCTION Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ) have recently demonstrated impressive results on image-synthesis benchmarks ( Radford et al., 2016 ;  Zhang et al., 2017 ;  Miyato & Koyama, 2018 ;  Zhang et al., 2018 ;  Brock et al., 2018 ;  Karras et al., 2019 ). In the original setting, GANs are composed of two neural networks trained with competing goals: the generator is trained to synthesize realistic samples to fool the discriminator and the discriminator is trained to distinguish real samples from fake ones produced by the generator. One major problem with GANs is the instability of the training procedure and the general sensitivity of the results to various hyperparameters ( Salimans et al., 2016 ). Because GAN training implicitly requires finding the Nash equilibrium of a non-convex game in a continuous and high dimensional parameter space, it is substantially more complicated than standard neural network training. In fact, formally characterizing the convergence properties of the GAN training procedure is mostly an open problem ( Odena, 2019 ). Previous work ( Arjovsky & Bottou, 2017 ;  Miyato et al., 2018a ;  Odena et al., 2017 ;  Chen et al., 2019 ;  Wei et al., 2018 ) has shown that interventions focused on the discriminator can mitigate stability issues. Most successful interventions fall into two categories, normalization and regularization. Spectral normalization is the most effective normalization method, in which weight matrices in the discriminator are divided by an approximation of their largest singular value. For regularization,  Gulrajani et al. (2017)  penalize the gradient norm of straight lines between real data and generated data.  Roth et al. (2017)  propose to directly regularize the squared gradient norm for both the training data and the generated data. DRAGAN ( Kodali et al., 2017 ) introduces another form of gradient penalty where the gradients at Gaussian perturbations of training data are penalized. One may anticipate simultaneous regularization and normalization could improve sample quality. However, most of these gradient based regularization methods either provide marginal gains or fail to introduce any improvement when normalization is used ( Kurach et al., 2019 ), which is also observed in our experiments. These regularization methods and spectral normalization are motivated by controlling Lipschitz constant of the discriminator. We suspect this might be the reason that applying both does not lead to overlaid gain. In this paper, we examine a technique called consistency regularization ( Bachman et al., 2014 ;  Saj- jadi et al., 2016 ;  Laine & Aila, 2016 ; Zhai et al., 2019;  Xie et al., 2019 ;  Hu et al., 2017 ) in contrast to gradient-based regularizers. Consistency regularization is widely used in semi-supervised learning to ensure that the classifier output remains unaffected for an unlabeled example even it is augmented in semantic-preserving ways. In light of this intuition, we hypothesize a well-trained discrimina- tor should also be regularized to have the consistency property, which enforces the discriminator to be unchanged by arbitrary semantic-preserving perturbations and to focus more on semantic and structural changes between real and fake data. Therefore, we propose a simple regularizer to the dis- criminator of GAN: we augment images with semantic-preserving augmentations before they are fed into the GAN discriminator and penalize the sensitivity of the discriminator to those augmentations. This technique is simple to use and surprisingly effective. It is as well less computationally expen- sive than prior techniques. More importantly, in our experiments, consistency regularization can always further improve the model performance when spectral normalization is used, whereas the performance gains of previous regularization methods diminish in such case. In extensive ablation studies, we show that it works across a large range of GAN variants and datasets. We also show that simply applying this technique on top of existing GAN models leads to new state-of-the-art results as measured by Frechet Inception Distance ( Heusel et al., 2017 ). In summary, our contributions are summarized as follows: • We propose consistency regularization for GAN discriminators to yield a simple, effective regularizer with lower computational cost than gradient-based regularization methods. • We conduct extensive experiments with different GAN variants to demonstrate that our technique interacts effectively with spectral normalization. Our consistency regularized GAN (CR-GAN) achieves the best FID scores for unconditional image generation on both CIFAR-10 and CelebA. • We show that simply applying the proposed technique can further boost the performance of state-of-the-art GAN models. We improve FID scores for conditional image generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a novel approach for off-policy estimation in reinforcement learning, which does not require that the off-policy data come from the stationary distribution or that the behavior policy be known. We formulate the off-policy estimation problem into one of solving for the fixed point of an operator, and develop a new algorithm with generalization bounds. We empirically demonstrate the effectiveness of our method on several classic control benchmarks, showing that it is effective even if the off-policy data has not reached the stationary distribution.",
        "Abstract": "Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible.  Recently, \\citet{liu18breaking} proposed an approach that avoids the curse of horizon suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data being collected by a known behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a \"backward flow\" operator and show that the fixed point solution gives the desired importance ratios of stationary distributions between the target and behavior policies.  We analyze its asymptotic consistency and finite-sample\ngeneralization. Experiments on benchmarks verify the effectiveness of our proposed approach.\n",
        "Introduction": "  INTRODUCTION As reinforcement learning (RL) is increasingly applied to crucial real-life problems like robotics, recommendation and conversation systems, off-policy estimation becomes even more critical. The task here is to estimate the average long-term reward of a target policy, given historical data collected by (possibly unknown) behavior policies. Since the reward and next state depend on what action the policy chooses, simply averaging rewards in off-policy data does not estimate the target policy's long-term reward. Instead, proper correction must be made to remove the bias in data distribution. One approach is to build a simulator that mimics the reward and next-state transitions in the real world, and then evaluate the target policy against the simulator (e.g.,  Fonteneau et al., 2013 ; Ie et al., 2019). While the idea is natural, building a high-fidelity simulator could be extensively challenging in numerous domains, such as those that involve human interactions. Another approach is to use propensity scores as importance weights, so that we could use the weighted average of rewards in off-policy data as a suitable estimate of the average reward of the target policy. The latter approach is more robust, as it does not require modeling assumptions about the real world's dynamics. It often finds more success in short-horizon problems like contextual bandits, but its variance often grows exponentially in the horizon, a phenomenon known as \"the curse of horizon\" ( Liu et al., 2018 ). To address this challenge,  Liu et al. (2018)  proposed to solve an optimization problem of a minimax nature, whose solution directly estimates the desired propensity score of states under the stationary distribution, avoiding an explicit dependence on horizon. While their method is shown to give more accurate predictions than previous algorithms, it is limited in several important ways: • The method requires that data be collected by a known behavior policy. In practice, however, such data are often collected over an extended period of time by multiple, unknown behavior policies. For example, observational healthcare data typically contain patient records, whose treatments were provided by different doctors in multiple hospitals, each following potentially different procedures that are not always possible to specify explicitly. • The method requires that the off-policy data reach the stationary distribution of the behavior policy. In reality, it may take a very long time for a trajectory to reach the stationary distribution, which may be impractical due to various reasons like costs and missing data. In this paper, we introduce a novel approach for the off-policy estimation problem that overcome these drawbacks. The main contributions of our work are three-fold: • We formulate the off-policy estimation problem into one of solving for the fixed point of an operator. Different from the related, and similar, Bellman operator that goes forward in time, this operator is backward in time. • We develop a new algorithm, which does not have the aforementioned limitations of  Liu et al. (2018) , and analyze its generalization bounds. Specifically, the algorithm does not require that the off-policy data come from the stationary distribution, or that the behavior policy be known. • We empirically demonstrate the effectiveness of our method on several classic control benchmarks. In particular, we show that, unlike  Liu et al. (2018) , our method is effective even if the off-policy data has not reached the stationary distribution. In the next section, we give a brief overview of recent and related works. We then move to describing the problem setting that we have used in the course of the paper and our off-policy estimation approach. Finally, we present several experimental results to show the effectiveness of our method.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper explores the theoretical research of Graph Neural Networks (graph NNs) and their expressive power. It examines the characterization of Deep Learning (DL) model expressive power, and the ability of deep and non-linear architectures to enhance representation power. It also discusses the issue of over-smoothing and the need for theoretical analysis of graph NN expressive power.",
        "Abstract": "Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity.\nOur strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes.\nOur theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\\H{o}s -- R\\'{e}nyi graph.\nWe show that when the Erd\\H{o}s -- R\\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ``information loss\" in the limit of infinite layers with high probability.\nBased on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.",
        "Introduction": "  INTRODUCTION Motivated by the success of Deep Learning (DL), several attempts have been made to apply DL mod- els to non-Euclidean data, particularly, graph-structured data such as chemical compounds, social networks, and polygons. Recently, Graph Neural Networks (graph NNs) ( Duvenaud et al., 2015 ;  Li et al., 2016 ;  Gilmer et al., 2017 ;  Hamilton et al., 2017 ;  Kipf & Welling, 2017 ;  Nguyen et al., 2017 ;  Schlichtkrull et al., 2018 ;  Battaglia et al., 2018 ;  Xu et al., 2019 ;  Wu et al., 2019a ) have emerged as a promising approach. However, despite their practical popularity, theoretical research of graph NNs has not been explored extensively. The characterization of DL model expressive power, i.e., to identify what function classes DL mod- els can (approximately) represent, is a fundamental question in theoretical research of DL. Many studies have been conducted for Fully Connected Neural Networks (FNNs) ( Cybenko, 1989 ;  Hornik, 1991 ;  Hornik et al., 1989 ;  Barron, 1993 ;  Mhaskar, 1993 ;  Sonoda & Murata, 2017 ;  Yarotsky, 2017 ) and Convolutional Neural Networks (CNNs) ( Petersen & Voigtlaender, 2018 ;  Zhou, 2018 ;  Oono & Suzuki, 2019 ). For such models, we have theoretical and empirical justification that deep and non- linear architectures can enhance representation power ( Telgarsky, 2016 ;  Chen et al., 2018b ;  Zhou & Feng, 2018 ). However, for graph NNs, several papers have reported that node representations go indistinguishable (known as over-smoothing) and prediction performances severely degrade when we stack many layers ( Kipf & Welling, 2017 ;  Wu et al., 2019b ;  Li et al., 2018 ). Besides,  Wu et al. (2019a)  reported that graph NNs achieved comparable performance even if they removed interme- diate non-linear functions. These studies posed a question about the current architecture and made us aware of the need for the theoretical analysis of the graph NN expressive power.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a stochastic model, BasisGAN, for multi-mode conditional image generation. The model exploits the known observation that a well-trained deep network can converge to significantly different sets of parameters across multiple trainings, and introduces stochastic convolutional layers with filter generators to directly map an input condition to diverse output images. Theoretical arguments are provided to support the simplification of replacing stochastic filter generation with basis generation, and experiments show that the proposed BasisGAN is a simple yet effective solution to multi-mode conditional image generation.",
        "Abstract": "While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. One of these questions is how to efficiently achieve proper diversity and sampling of the multi-mode data space. To address this, we introduce BasisGAN, a stochastic conditional multi-mode image generator. By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we learn a plug-and-played basis generator to stochastically generate basis elements, with just a few hundred of parameters, to fully embed stochasticity into convolutional filters. By sampling basis elements instead of filters, we dramatically reduce the cost of modeling the parameter space with no sacrifice on either image diversity or fidelity. To illustrate this proposed plug-and-play framework, we construct variants of BasisGAN based on state-of-the-art conditional image generation networks, and train the networks by simply plugging in a basis generator, without additional auxiliary components, hyperparameters, or training objectives. The experimental success is complemented with theoretical results indicating how the perturbations introduced by the proposed sampling of basis elements can propagate to the appearance of generated images.",
        "Introduction": "  INTRODUCTION Conditional image generation networks learn mappings from the condition domain to the image do- main by training on massive samples from both domains. The mapping from a condition, e.g., a map, to an image, e.g., a satellite image, is essentially one-to-many as illustrated in  Figure 1 . In other words, there exists many plausible output images that satisfy a given input condition, which motivates us to explore multi-mode conditional image generation that produces diverse images con- ditioned on one single input condition. One technique to improve image generation diversity is to feed the image generator with an addi- tional latent code in the hope that such code can carry information that is not covered by the input condition, so that diverse output images are achieved by decoding the missing information conveyed through different latent codes. However, as illustrated in the seminal work  Isola et al. (2017) , encod- ing the diversity with an input latent code can lead to unsatisfactory performance for the following reasons. While training using objectives like GAN loss  Goodfellow et al. (2014) , regularizations like L1 loss  Isola et al. (2017)  and perceptual loss  Wang et al. (2018)  are imposed to improve both visual fidelity and correspondence to the input condition. However, no similar regularization is im- posed to enforce the correspondence between outputs and latent codes, so that the network is prone to ignore input latent codes in training, and produce identical images from an input condition even with different latent codes. Several methods are proposed to explicitly encourage the network to take into account input latent codes to encode diversity. For example,  Mao et al. (2019)  explicitly max- imizes the ratio of the distance between generated images with respect to the corresponding latent codes; while Zhu et al. (2017b) applies an auxiliary network for decoding the latent codes from the generative images. Although the diversity of the generative images is significantly improved, these methods experience drawbacks. In  Mao et al. (2019) , at least two samples generated from the same condition are needed for calculating the regularization term, which multiplies the memory footprint while training each mini-batch. Auxiliary network structures and training objectives in Zhu et al. (2017b) unavoidably increase training difficulty and memory footprint. These previously proposed methods usually require considerable modifications to the underlying framework. In this paper, we propose a stochastic model, BasisGAN, that directly maps an input condition to diverse output images, aiming at building networks that model the multi-mode intrinsically. The proposed method exploits a known observation that a well-trained deep network can converge to significantly different sets of parameters across multiple trainings, due to factors such as different parameter initializations and different choices of mini-batches. Therefore, instead of treating a con- ditional image generation network as a deterministic function with fixed parameters, we propose modeling the filter in each convolutional layer as a sample from filter space, and learning the corre- sponding filter space using a tiny network for efficient and diverse filter sampling. In  Ghosh et al. (2018) , parameter non-uniqueness is used for multi-mode image generation by training several gen- erators with different parameters simultaneously as a multi-agent solution. However, the maximum modes of  Ghosh et al. (2018)  are restricted by the number of agents, and the replication increases memory as well as computational cost. Based on the above parameters non-uniqueness property, we introduce into a deep network stochastic convolutional layers, where filters are sampled from learned filter spaces. Specifically, we learn the mapping from a simple prior to the filter space us- ing neural networks, here referred to as filter generators. To empower a deterministic network with multi-mode image generation, we divide the network into a deterministic sub-model and a stochastic sub-model as shown in  Figure 1 , where standard convolutional layers and stochastic convolutional layers with filter generators are deployed, respectively. By optimizing an adversarial loss, filter gen- erators can be jointly trained with a conditional image generation network. In each forward pass, filters at stochastic layers are sampled by filter generators. Highly diverse images conditioned on the same input are achieved by jointly sampling of filters in multiple stochastic convolutional layers. However, filters of a convolutional layer are usually high-dimensional while being together written as one vector, which makes the modeling and sampling of a filter space highly costly in practice in terms of training time, sampling time, and filter generator memory footprint. Based on the low-rank property observed from sampled filters, we decompose each filter as a linear combination of a small set of basis elements  Qiu et al. (2018) , and propose to only sample low-dimensional spatial basis elements instead of filters. By replacing filter generators with basis generators, the proposed method becomes highly efficient and practical. Theoretical arguments are provided on how perturbations introduced by sampling basis elements can propagate to the appearance of generated images. The proposed BasisGAN introduces a generalizable concept to promote diverse modes in the con- ditional image generation. As basis generators act as plug-and-play modules, variants of BasisGAN can be easily constructed by replacing in various state-of-the-art conditional image generation net- Published as a conference paper at ICLR 2020 works the standard convolutional layers by stochastic layers with basis generators. Then, we directly train them without additional auxiliary components, hyperparameters, or training objectives on top of the underlying models. Experimental results consistently show that the proposed BasisGAN is a simple yet effective solution to multi-mode conditional image generation. We further empirically show that the inherent stochasticity introduced by our method allows training without paired sam- ples, and the one-to-many image-to-image translation is achieved using a stochastic auto-encoder where stochasticity prevents the network from learning a trivial identity mapping. Our contributions are summarized as follows: • We propose a plug-and-played basis generator to stochastically generate basis elements, with just a few hundred of parameters, to fully embed stochasticity into network filters. • Theoretic arguments are provided to support the simplification of replacing stochastic filter generation with basis generation. • Both the generation fidelity and diversity of the proposed BasisGAN with basis generators are validated extensively, and state-of-the-art performances are consistently observed.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an analysis of neural architecture search (NAS) methods based on weight sharing, which are used to reduce computation cost. Through comprehensive experiments and analysis, the paper answers key questions about the accuracy and stability of found architectures, and the impact of weight sharing on these metrics. The paper also explores partial weight sharing methods, which reduce the degree of weight sharing and improve the performance and stability of found architectures. The main contributions of this paper are the definition of new metrics for evaluating NAS methods, the delivery of interesting observations and insights, and the exploration of partial weight sharing methods.",
        "Abstract": "With the success of deep neural networks, Neural Architecture Search (NAS) as a way of automatic model design has attracted wide attention. As training every child model from scratch is very time-consuming, recent works leverage weight-sharing to speed up the model evaluation procedure. These approaches greatly reduce computation by maintaining a single copy of weights on the super-net and share the weights among every child model. However, weight-sharing has no theoretical guarantee and its impact has not been well studied before. In this paper, we conduct comprehensive experiments to reveal the impact of weight-sharing: (1) The best-performing models from different runs or even from consecutive epochs within the same run have significant variance; (2) Even with high variance, we can extract valuable information from training the super-net with shared weights; (3) The interference between child models is a main factor that induces high variance; (4) Properly reducing the degree of weight sharing could effectively reduce variance and improve performance.",
        "Introduction": "  INTRODUCTION Learning to design neural architectures automatically has aroused wide interests recently due to its success in many different machine learning tasks. One stream of neural architectures search (NAS) methods is based on reinforcement learning (RL) ( Zoph & Le, 2016 ;  Zoph et al., 2018 ;  Tan et al., 2019 ), where a neural architecture is built from actions and its performance is used as reward. This approach usually demands considerable computation power - each search process takes days with hundreds of GPUs. Population based algorithms ( Gaier & Ha, 2019 ;  Liang et al., 2018 ;  Jaderberg et al., 2017 ) are another popular approach for NAS, new trials could inherit neural architecture from better performing ones as well as their weights, and mutate the architecture to explore better ones. It also has high computation cost. To speed up the search process, a family of methods attracts increasing attention with greatly reduced computation ( Pham et al., 2018 ;  Liu et al., 2018c ;  Bender et al., 2018 ). Instead of training every child model, they build a single model, called super-net, from neural architecture search space, and maintain a single copy of weights on the super-net. Several training approaches have been proposed on this model, e.g., training with RL controller ( Pham et al., 2018 ), training by applying dropout ( Bender et al., 2018 ) or architecture weights on candidate choices ( Liu et al., 2018c ). In these approaches, weight-sharing is the key for the speedup. However, weight sharing has no theoretical guarantee and its impact has not been well studied before. The directions of improving such methods would be more clear if some key questions had been answered: 1) How far is the accuracy of found architecture from the best one within search space? 2) Could the best architecture be stably found in multiple runs of search process? 3) How does weight sharing affect the accuracy and stability of the found architecture? In this paper, we answer the above-mentioned questions using comprehensive experiments and analysis. To understand the behavior of weight sharing approaches, we use a small search space, which makes it possible to have ground truth for comparison. It is a simplified NAS problem, therefore, making it easy to show the ability of the NAS algorithms with weight sharing. As a result, we find that the rank of child models is very unstable in different runs of the search process, and also very different from ground truth. In fact, the instability 1 commonly exists not only in different runs, Under review as a conference paper at ICLR 2020 but also in consecutive training epochs within the same run. Also worthy of note, in spite of high variance, we can extract statistical information from the variance, the statistics can be innovatively leveraged to prune search space and improve the search result. To further understand where the variance comes from, we record and analyze more metric data from the experiments. It is witnessed that some child models have interference with each other, and the degree of this interference varies depending on different child models. At the very end of the super-net training, training each child model in one mini-batch can make this model be the best performing one on the validation data. Based on the insights, we further explore partial weight sharing, that is, each child model could selectively share weights with others, rather than all of them sharing the same copy of weights. It can be seen as reduced degree of weight sharing. One method we have explored is sharing weights of common prefix layers among child models. Another method is to cluster child models into groups, each of which shares a copy of weights. Experiment results show that partial weight sharing makes the rank of child models more stable and becomes closer to ground truth. It implies that with proper degree or control of weight sharing, better child models can be more stably found. To summarize, our main contributions are as follows: • We define new metrics for evaluating the performance of the NAS methods based on weight sharing, and propose a down-scaled search space which makes it possible to have a deeper analysis by comparing it with ground truth. • We design various experiments, and deliver some interesting observations and insights. More importantly, we reveal that valuable statistics can be extracted from training the super-net, which can be leveraged to improve performance. • We take a step further to explain the reasons of high variance. Then we use decreased degree of weight sharing, which shows lower variance and better performance, to support the reasoning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the issue of mode collapse in generative adversarial networks (GANs) and proposes a toolkit to detect it. It also examines potential remedies for calibrating the GAN's learned distribution to alleviate the mode collapse. The underlying reasons for mode collapse are discussed, ranging from data imbalance to the training difficulty of GANs.",
        "Abstract": "Generative adversarial networks (GANs) nowadays are capable of producing im-ages of incredible realism.   One concern raised is whether the state-of-the-artGAN’s learned distribution still suffers from mode collapse. Existing evaluation metrics for image synthesis focus on low-level perceptual quality. Diversity tests of samples from GANs are usually conducted qualitatively on a small scale. In this work, we devise a set of statistical tools, that are broadly applicable to quantitatively measuring the mode collapse of GANs. Strikingly, we consistently observe strong mode collapse on several state-of-the-art GANs using our toolset.  We analyze possible causes, and for the first time present two simple yet effective “black-box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data.",
        "Introduction": "  INTRODUCTION Generative adversarial networks (GANs) ( Goodfellow et al., 2014 ) have demonstrated unprecedented power for various image generation tasks. However, GANs have also been suffering from generation bias and/or loss of diversity. The underlying reasons could be compound, ranging from the data imbalance to the training difficulty of GANs, and more: • First of all, the training data for GANs, especially for the typical unconditional/unsupervised generation tasks ( Karras et al., 2017 ; 2018), might possess various subject or attribute imbalances. As a result, GANs trained with them might be further biased towards the denser areas, similarly to the classifier bias towards the majority class in imbalanced classification. • More intrinsically, even when the training dataset \"looks\" balanced, training GANs is notoriously more unstable (sometimes even uncontrollable) than training classifiers, potentially constituting another source of mode collapse. One most common hurdle of GANs is the loss of diversity due to mode collapse ( Goodfellow, 2016 ), wherein the generator concentrates too large a probability mass on a few modes of the true distribution. Another widely reported issue, known as co-variate shift ( Santurkar et al., 2017 ), could be viewed as a nuanced version of mode collapse. This paper seeks to explore: do the state-of-the-art GANs still suffer from mode collapse? Can we have a toolkit to detect that? And if the mode collapse happens, is there any \"easy and quick\" remedy for calibrating the GAN's learned distribution to alleviate the mode collapse?",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes GraphNVP, a novel framework for molecular graph generation based on invertible normalizing flow. GraphNVP is equipped with two latent representations for a molecular graph: one for the graph structure represented by an adjacency tensor, and one for node (atom) attributes. We introduce two types of reversible flows that work for the aforementioned two latent representations of graphs. We also develop a novel two-step generation process to sample a graph. The full reversibility of our model on graphs contributes to two major benefits: a simple architecture and precise log-likelihood maximization.",
        "Abstract": "We propose GraphNVP, an invertible flow-based molecular graph generation model. Existing flow-based models only handle node attributes of a graph with invertible maps. In contrast, our model is the first invertible model for the whole graph components: both of dequantized node attributes and adjacency tensor are converted into latent vectors through two novel invertible flows. This decomposition yields the exact likelihood maximization on graph-structured data. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and(ii) node attributes. We empirically demonstrate that our model and the two-step generation efficiently generates valid molecular graphs with almost no duplicated molecules, although there are no domain-specific heuristics ingrained in the model. We also confirm that the sampling (generation) of graphs is faster in magnitude than other models in our implementation. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties",
        "Introduction": "  INTRODUCTION Generation of molecules with certain desirable properties is a crucial problem in computational drug discovery. Recently, deep learning approaches are being actively studied for generating promising candidate molecules quickly. Earlier models ( Kusner et al., 2017 ;  Gómez-Bombarelli et al., 2018 ) depend on a string-based representation of molecules. However, recent models ( Jin et al., 2018 ;  You et al., 2018a ;  De Cao & Kipf, 2018 ) directly work on molecular graph representations and record impressive experimental results. In these studies, either variational autoencoder (VAE) ( Kingma & Welling, 2014 ) or generative adversarial network (GAN) ( Goodfellow et al., 2014 ;  Radford et al., 2015 ) are used mainly to learn mappings between the graphs and their latent vector representations. In this paper, we propose GraphNVP, yet another framework for molecular graph generation based on the invertible normalizing flow, which was mainly adopted for image generation tasks ( Dinh et al., 2017 ;  Kingma & Dhariwal, 2018 ). To capture distributions of irregular graph structure of molecules into a latent representation, we propose a novel two-step generation scheme. Specifically, GraphNVP is equipped with two latent representations for a molecular graph: first for the graph structure represented by an adjacency tensor, and second for node (atom) attributes. We introduce two types of reversible flows that work for the aforementioned two latent representations of graphs. Recent work by  Liu et al. (2019)  proposes a flow-based invertible model for transforming the node attribute matrix. However, they use a non-invertible encoder for transforming the adjacency tensor making the complete model non-invertible. Our model is the first fully invertible model for the whole graph components: both adjacency tensor and node attributes are converted into latent vectors through two novel invertible flows. To sample a graph, we develop a novel two-step generation process. During the generation process, GraphNVP first generates the graph structure. Then node attributes are generated according to this structure. This two-step generation enables efficient generation of valid molecular graphs. The full reversibility of our model on graphs contributes to two major benefits: a simple architecture and precise log-likelihood maximization. A major advantage of invertible models is that we do not need to design a separate decoder for sample generation: new graph samples can be generated by simply feeding a latent vector into the same model but in the reverse order.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a meta algorithm called BOSH-attack to evaluate the robustness of deep neural networks and other complex or discontinuous models, such as decision trees and detection-based defense models. BOSH-attack combines Bayesian optimization and iterative local updates to explore important solution space and find a much smaller adversarial perturbation efficiently. Experiments demonstrate that BOSH-attack can consistently boost existing decision-based attacks to find better examples with much smaller perturbation, and reduce the computation cost by 10x compared to the naive approach.",
        "Abstract": "Adversarial example generation becomes a viable method for evaluating the robustness of a machine learning model. In this paper, we consider hard-label black- box attacks (a.k.a. decision-based attacks), which is a challenging setting that generates adversarial examples based on only a series of black-box hard-label queries. This type of attacks can be used to attack discrete and complex models, such as Gradient Boosting Decision Tree (GBDT) and detection-based defense models. Existing decision-based attacks based on iterative local updates often get stuck in a local minimum and fail to generate the optimal adversarial example with the smallest distortion. To remedy this issue, we propose an efficient meta algorithm called BOSH-attack, which tremendously improves existing algorithms through Bayesian Optimization (BO) and Successive Halving (SH). In particular, instead of traversing a single solution path when searching an adversarial example, we maintain a pool of solution paths to explore important regions. We show empirically that the proposed algorithm converges to a better solution than existing approaches, while the query count is smaller than applying multiple random initializations by a factor of 10.",
        "Introduction": "  INTRODUCTION It has been shown that machine learning models, including deep neural networks, are vulnerable to adversarial examples ( Goodfellow et al., 2014 ;  Szegedy et al., 2013 ;  Chen et al., 2017a ). Therefore, evaluating the robustness of a given model becomes crucial for security sensitive applications. In order to evaluate the robustness of deep neural networks, researchers have developed \"attack algo- rithms\" to generate adversarial examples that can mislead a given neural network while being as close as possible to the original example ( Goodfellow et al., 2014 ;  Moosavi-Dezfooli et al., 2016 ;  Carlini & Wagner, 2017b ;  Chen et al., 2017b ). Most of these attack methods are based on maxi- mizing a loss function with a gradient-based optimizer, where the gradient is either computed by back-propagation (in the white-box setting) or finite-difference estimation (in the soft-label black- box setting). Although these methods work well on standard neural networks, when it comes to complex or even discontinuous models, such as decision trees and detection-based defense models, they cannot be directly applied because the gradient is not available. Hard-label black-box attacks, also known as decision-based attacks, consider the most difficult but realistic setting where the attacker has no information about the model structure and parameters, and the only valid operation is to query the model to get the corresponding decision-based (hard-label) output ( Brendel et al., 2017 ). This type of attacks can be used as a \"universal\" way to evaluate robustness of any given models, no matter continuous or discrete. For instance,  Cheng et al. (2018) ;  Chen et al. (2019a)  have applied decision-based attacks for evaluating robustness of Gradient Boost- ing Decision Trees (GBDT) and random forest. Current decision-based attacks, including  Brendel et al. (2017) ;  Cheng et al. (2018) ;  Chen et al. (2019b) ;  Cheng et al. (2019) , are based on iterative local updates - starting from an initial point on the decision surface, they iteratively move the points along the surface until reaching a local minimum (in terms of distance to the original example). The update is often based on gradient estimation or some other heuristics. However, the local update nature makes these methods sensitive to the starting point. As we demonstrate in Figure 1(a), the perturbation of converged adversarial examples for a neural network are quite different for different initialization configurations, and this phenomenon becomes more severe when it comes to discrete models such as GBDTs (see Figure 1(b)). This makes decision-based attacks converge to a sub- Under review as a conference paper at ICLR 2020 optimal perturbation. As a result, the solution cannot really reflect the robustness of the targeted model. To overcome these difficulties and make decision-based attacks better reflect the robustness of mod- els, we propose a meta algorithm called BOSH-attack that consistently boosts the solution quality of existing iterative local update based attacks. Our main idea is to combine Bayesian optimization, which finds solution closer to global optimum but suffers from high computation cost, with iterative local updates, which converges fast but often get stuck in local minimum. Specifically, given a deci- sion based attack A, our algorithm maintains a pool of solutions and at each iteration we run A for m steps on each solution. The proposed Bayesian Optimization resampling (BO) and Successive Halving (SH) are then used to explore important solution space based on current information and cut out unnecessary solution paths. Our contributions are summarized below: 1. We conduct thorough experiments to show that current decision-based attacks often con- verge to a local optimum, thus further improvements are required. 2. Based on the idea of Bayesian optimization and successive halving, we design a meta algo- rithm to boost the performance of current decision-based attack algorithms and encourage them to find a much smaller adversarial perturbation efficiently. 3. Comprehensive experiments demonstrate that BOSH-attack can consistently boost existing decision-based attacks to find better examples with much smaller perturbation. In addition to the standard neural network models, we also test our algorithms on attacking discrete GBDT models and detector-based defense models. Moreover, our algorithm can reduce the computation cost by 10x compared to the naive approach.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new metric of Generative Adversarial Networks (GANs) and an algorithm based on this metric. The metric, called duality gap, is an upper bound of traditional metrics and can be used to measure the difference between the generated distribution and the true data distribution. The paper also establishes a generalization error bound under the new metric and shows that the empirical duality gap can be viewed as the loss function for GANs. Experiments demonstrate that the proposed algorithm outperforms state-of-the-art algorithms.",
        "Abstract": "Generative Adversarial Networks (GANs) are powerful, but difficult to understand and train because  GANs is a min-max problem. This paper understand GANs with duality gap that comes from game theorem and show that duality gap can be a kind of metric to evolution the difference between the true data distribution and the distribution generated by generator with given condition. And train the networks using duality gap can get some better results. Furthermore, the paper calculates the generalization bound of duality gap to estimate the help design the neural networks and select the sample size.\n",
        "Introduction": "  INTRODUCTION In the past few years, Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ) are im- pactful because it has shown lots of great results for many AI tasks, such as image generation, dialogue generation, and images inpainting ( Abadi & G Andersen, 2016 ;  Goodfellow, 2016 ;  Ho & Ermon, 2016 ). Differing from other unsupervised learning methods for model generation that con- centrate on the hard optimization of the measure of distribution fit such as the maximum likelihood method, GANs, which are a kind of methods of implicit models ( Mohamed & Lakshminarayanan, 2017 ;  Tran et al., 2017 ), can be seen as a game between two networks, the generator and the dis- criminator. Training GANs will improve the two networks' capability synchronously. Denote the discriminator as f and a generator as g. The objective of GANs is where p data is the true data distribution and p z is the standard Gaussian distribution. Here, the goal of f is to discriminate the difference between two distributions and the goal of g is to generate a distribution with the Gaussian noise. Therefore the problem of GANs is a min-max problem. The minimization problem is to search for the optimal discriminator f that can distinguish two distributions as much as possible and the maximization problem is to find the optimal generator g such that the discriminator can not find the difference. So the GAN is just like a game between these two players. This is in general a challenging task to find the best solution because it may be not a concave-convex min-max optimization. This means that the objective, denoted by V (f, g), may not be a convex function when fixing f and not a concave function when fixing g. The first major problem of GANs is how to measure the difference between the generated distribu- tion and the true data distribution. It means that there is no an unanimous metric to represent the difference between the true data distribution and the generated distribution ( Borji, 2018 ). Different metrics have achieved different performances on the different benchmark datasets, although many state-of-the-art models can show similar results ( Lucic et al., 2017 ). It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by hu- man eyes. Another problem is the convergence of the training algorithm of GANs, especially the global convergence. It means that if the original generator and discriminator are random, it is diffi- cult to confirm that the generator and discriminator can converge to the ideal conclusion by training with given data. So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets. Although it can be proved that the generator and discriminator can converge to the local Nash equilibrium under some strong assumptions ( Martin et al., 2017 ), many GAN algorithms can not converge globally ( Gemp & Mahadeven, 2019 ), In this paper, our main contributions are: Under review as a conference paper at ICLR 2020 • We propose a new metric of GANs and prove that the metric can be an upper bound of the traditional metrics. • We establish a generalization error bound under the new metric and show that the empirical metric can be viewed as the loss function for GANs. • We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms. The remainder of this paper is organized as follows. In Section 2, some related work are reviewed. Section 3 gives the new metric named duality gap that can be seen as an upper bound of traditional metrics. In Section 4, we establish a generalization error bound under the new metric and show that the empirical duality gap can be viewed as the loss function for GANs. Section 5 and 6 provide the new algorithm and some experimental results. Finally, we give our conclusions and future work.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new model of Gaussian Conditional Random Fields for binary classification (GCRFBC). The model assumes that discrete outputs are conditionally independent conditioned on continuous latent variables which follow a multivariate Gaussian distribution. Two different inference and learning approaches are proposed and tested on both synthetic and real-world datasets. Results indicate that the empirical Bayes approach (GCRFBCb) better exploits output dependence structure, and both GCRFBC models outperformed other models in terms of predictive performance and computation time.",
        "Abstract": "In this paper, a Gaussian conditional random field model for structured binary classification (GCRFBC) is proposed. The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. The model representation of GCRFBC is extended by latent variables which yield some appealing properties. Thanks to the GCRF latent structure, the model becomes tractable, efficient, and open to improvements previously applied to GCRF regression. Two different forms of the algorithm are presented: GCRFBCb (GCRGBC - Bayesian) and GCRFBCnb (GCRFBC - non-Bayesian). The extended method of local variational approximation of sigmoid function is used for solving empirical Bayes in GCRFBCb variant, whereas MAP value of latent variables is the basis for learning and inference in the GCRFBCnb variant. The inference in GCRFBCb is solved by Newton-Cotes formulas for one-dimensional integration. Both models are evaluated on synthetic data and real-world data. It was shown that both models achieve better prediction performance than relevant baselines. Advantages and disadvantages of the proposed models are discussed.",
        "Introduction": "  INTRODUCTION Increased quantity and variety of sources of data with correlated outputs, so called structured data, created an opportunity for exploiting additional information between dependent outputs to achieve better prediction performance. One of the most successful probabilistic models for structured out- put classification problems are conditional random fields (CRF) ( Sutton & McCallum, 2006 ). The main advantages of CRFs lie in their discriminatory nature, resulting in the relaxation of indepen- dence assumptions and the label bias problem that are present in many graphical models. Aside of many advantages, CRFs also have many drawbacks mostly resulting in high computational cost or intractability of inference and learning. A wide range of different approaches of tackling these problems has been proposed, and they motivate our work, too. One of the popular methods for structured regression based on CRFs - Gausian conditional random fields (GCRF) - has the form of multivariate Gaussian distribution ( Radosavljevic et al., 2010 ). The main assumption of the model is that the relations between outputs are presented in quadratic form. It has convex loss function and, consequently, efficient inference and learning, and expensive sampling methods are not used. In this paper, a new model of Gaussian conditional random fields for binary classification is pro- posed (GCRFBC). GCRFBC builds upon regression GCRF model which is used to define latent variables over which output dependencies are defined. The model assumes that discrete outputs y i are conditionally independent conditioned on continuous latent variables z i which follow a distribu- tion modeled by a GCRF. That way, relations between discrete outputs are not expressed directly. Two different inference and learning approaches are proposed in this paper. The first one is based on evaluating empirical Bayes by marginalizing latent variables (GCRFBCb), whereas MAP value of latent variables is the basis for learning and inference in the second model (GCRFBCnb). In order to derive GCRFBCb model and its learning procedure the variational approximation of Bayesian logistic regression ( Jaakkola & Jordan, 2000 ) is generalized. Compared to CRFs and structured SVM classifiers, the GCRFBC models have some appealing properties: Under review as a conference paper at ICLR 2020 • The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. Thanks to the GCRF latent structure, the model becomes tractable, efficient and open to improvements previously applied to GCRF regression mod- els. • Defining correlations directly between discrete outputs may introduce unnecessary noise to the model ( Tan et al., 2010 ). This problem can be solved by defining structured relations on a latent continuous variable space. • In case that unstructured predictors are unreliable, which is signaled by their large variance (diagonal elements in the covariance matrix), it is simple to marginalize over latent variable space and obtain better results. GCRFBC model is relying on the assumption that the underlying distribution of latent variables is multivariate normal distribution, due to that in the case when this distribution cannot be fitted well to the data (e.g. when the distribution of latent variables is multimodal) the model will not perform as well as it is expected. The proposed models are experimentally tested on both synthetic and real-world datasets in terms of predictive performance and computation time. In experiments with synthetic datasets, the results clearly indicate that the the empirical Bayes approach (GCRFBCb) better exploits output dependence structure, more so as the variance of the latent variables increases. We also tested both approaches on real-world datasets of predicting ski lift congestion, gene function classification, classification of music according to emotion and highway congestion. Both GCRFBC models outperformed ridge logistic regression, lasso logistic regression, neural network, random forest, and structured SVM classifiers, demonstrating that the proposed models can exploit output dependencies in a real-world setting.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel method to leverage the lead bias of news articles to conduct large-scale pretraining of summarization models. The idea is to use the top few sentences of a news article as the target summary and use the rest as the content. The goal of the pretrained model is to generate an abstractive summary given the content. The model is pretrained on a three-year collection of online news articles and evaluated on five benchmark news summarization datasets. Results show that the pretrained model achieves a remarkable performance on various target datasets without any finetuning and outperforms existing baselines like pointer-generator network.",
        "Abstract": "Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabelled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. For example, the pretrained model without finetuning outperforms pointer-generator network on CNN/DailyMail dataset. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset.",
        "Introduction": "  INTRODUCTION The goal of text summarization is to condense a piece of text into a shorter version that contains the salient information. Due to the prevalence of news articles and the need to provide succinct sum- maries for readers, a majority of existing datasets for summarization come from the news domain ( Hermann et al., 2015 ;  Sandhaus, 2008 ;  Narayan et al., 2018 ). However, according to journalistic conventions, the most important information in a news report usually appears near the beginning of the article ( Kedzie et al., 2018 ;  Jung et al., 2019 ). While it facilitates faster and easier understanding of the news for readers, this lead bias causes undesirable consequences for summarization models. The output of these models is inevitably affected by the positional information of sentences. Fur- thermore, the simple baseline of using the top few sentences as summary can achieve a stronger performance than many sophisticated models ( See et al., 2017 ). It can take a lot of effort for models to overcome the lead bias  Kedzie et al. (2018) . Additionally, most existing summarization models are fully supervised and require time and labor- intensive annotations to feed their insatiable appetite for labeled data. For example, the New York Times Annotated Corpus ( Sandhaus, 2008 ) contains 1.8 million news articles, with 650,000 sum- maries written by library scientists. Therefore, some recent work ( Gusev, 2019 ) explores the effect of domain transfer to utilize datasets other than the target one. But this method may be affected by the domain drift problem and still suffers from the lack of labelled data. The recent promising trend of pretraining models ( Devlin et al., 2018 ;  Radford et al., 2018 ) proves that a large quantity of data can be used to boost NLP models' performance. Therefore, we put forward a novel method to leverage the lead bias of news articles in our favor to conduct large-scale pretraining of summarization models. The idea is to leverage the top few sentences of a news article as the target summary and use the rest as the content. The goal of our pretrained model is to generate an abstractive summary given the content. Coupled with careful data filtering and cleaning, the lead bias can provide a delegate summary of sufficiently good quality, and it immediately renders the large quantity of unlabeled news articles corpus available for training news summarization models. We employ this pretraining idea on a three-year collection of online news articles. We conduct thorough data cleaning and filtering. For example, to maintain a quality assurance bar for using Under review as a conference paper at ICLR 2020 leading sentences as the summary, we compute the ratio of overlapping non-stopping words between the top 3 sentences and the rest of the article. As a higher ratio implies a closer semantic connection, we only keep articles for which this ratio is higher than a threshold. We end up with 21.4M articles based on which we pretrain a transformer-based encoder-decoder summarization model. We conduct thorough evaluation of our models on five benchmark news summarization datasets. Our pretrained model achieves a remarkable performance on various target datasets without any finetuning. This shows the effectiveness of leveraging the lead bias to pretrain on large-scale news data. We further finetune the model on target datasets and achieve better results than a number of strong baseline models. For example, the pretrained model without finetuning obtains state-of-the-art results on DUC-2003 and DUC-2004. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset ( Narayan et al., 2018 ). Human evaluation results also show that our models outperform existing baselines like pointer-generator network. The rest of paper is organized as follows. We introduce related work in news summarization and pretraining in Section 2. We describe the details of pretraining using lead bias in Section 3. We introduce the transformer-based summarization model in Section 4. We show the experimental results in Section 5 and conclude the paper in Section 6.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to 3D location and orientation detection of vehicles using deep convolutional networks. The proposed method is capable of detecting non-fixed numbers of keypoints, and can be used to infer vehicle trajectories containing 3D coordinate information without the need for radar or 3D detection technology. The method is efficient and ensures real-time performance, and is able to constrain the quantitative changes of keypoints through auxiliary target information. The results of the proposed method are compared to existing methods, showing improved accuracy and information richness.",
        "Abstract": "We present a method to infer 3D location and orientation of vehicles on a single image. To tackle this problem, we optimize the mapping relation between the vehicle’s wheel grounding point on the image and the real location of the wheel in the 3D real world coordinate. Here we also integrate three task priors, including a ground plane constraint and vehicle wheel grounding point position, as well as a small projection error from the image to the ground plane. And a robust light network for grounding point detection in autopilot is proposed based on the vehicle and wheel detection result. In the light grounding point detection network, the DSNT key point regression method is used for balancing the speed of convergence and the accuracy of position, which has been proved more robust and accurate compared with the other key point detection methods. With more, the size of grounding point detection network is less than 1 MB, which can be executed quickly on the embedded environment. The code will be available soon.",
        "Introduction": "  INTRODUCTION 3D location and orientation detection is a basic but challenging problem in computer vision, which focuses on the prediction accuracy of visible and invisible points. It has been applied in many ways, including human action recognition, human-computer interaction, recently popular object detection and so on. In our application scenario, we define the point of wheel contacting with the ground as the keypoint in the vehicle instance. This paper mainly solves the problem of non-fixed number of vehicle keypoint detection, which is the basis of vehicle automatic driving perception technology. Recent researches have shown that deep convolutional network has powerful ability in information acquisition and image processing. Advanced network structures, such as Hourglass ( Newell et al., 2016 ), HRNet ( Sun et al., 2019 ) etc., usually have multi-scale architectures in critical point detection tasks. Location and orientation estimation tasks based on above networks with efficient transposed convolution structure can effectively solve the problem of invisible points in inference. Because they effectively combine the context information in different receptive field to ensure the high- level semantic information and high resolution information fusion at the same time. The fusion in inference process provides a rich multi-level information. This is also an important method to improve the detection accuracy of keypoints of fixed quantity. Different from the current keypoint detection tasks, we aim to solve the problem of non-fixed number of keypoint detection. Due to the influence of shooting angle and distance, the purpose of keypoints of vehicles visible in the sample is not fixed and fluctuates violently. To adapt to the application scenes and avoid the disastrous consequences caused by the potential wrong inference of invisible points, we only forecast visible points in the image. We adopt the top-down keypoint detection strategy and put forward a novel detection process, con- straining the location information of the keypoint through wheel detection. Meanwhile, the wheel area also provides abundant pixel information for keypoint detection, including visual identifiable geometric and location information. After obtaining the grounding point information, we project the 2D point to 3D coordinate. Finally, we fuse and process multi-frame vehicle's location and orientation information to complete visual-only vehicle trajectory description. The inference process of vehicle trajectory proposed by us has the following innovation points: Under review as a conference paper at ICLR 2020 • We obtain the location and orientation in the 3D coordinate information of the vehicle by the way of keypoint detection, which doesn't need radar and 3D detection technology. It reduces the time cost greatly and ensures the real-time performance at the same time. • We directly constrain the quantitative changes of keypoints through auxiliary target infor- mation, ensuring the continuity of updating the model gradient. • We combine the information of multiple frames and infer the vehicle trajectory which con- tains location and orientation in the 3D coordinate information of the vehicle, with high information richness.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the exact learning dynamics of deep linear networks under a spectrum of curvature-corrected update rules. It is shown that deep linear networks capture the essential nonlinear relationship between network's input-output maps and their parameters, and exhibit comparable learning behavior to their nonlinear counterparts. The paper also investigates the convergence rate under various initial conditions, decoupled modes of convergence dynamics, and the implicit bias for regularization and resistance to overfitting.",
        "Abstract": "Deep neural networks exhibit complex learning dynamics due to the highly non-convex loss landscape, which causes slow convergence and vanishing gradient problems. Second order approaches, such as natural gradient descent, mitigate such problems by neutralizing the effect of potentially ill-conditioned curvature on the gradient-based updates, yet precise theoretical understanding on how such curvature correction affects the learning dynamics of deep networks has been lack- ing. Here, we analyze the dynamics of training deep neural networks under a generalized family of natural gradient methods that applies curvature corrections, and derive precise analytical solutions. Our analysis reveals that curvature corrected update rules preserve many features of gradient descent, such that the learning trajectory of each singular mode in natural gradient descent follows precisely the same path as gradient descent, while only accelerating the temporal dynamics along the path. We also show that layer-restricted approximations of natural gradient, which are widely used in most second order methods (e.g. K-FAC), can significantly distort the learning trajectory into highly diverging dynamics that significantly differs from true natural gradient, which may lead to undesirable net- work properties. We also introduce fractional natural gradient that applies partial curvature correction, and show that it provides most of the benefit of full curvature correction in terms of convergence speed, with additional benefit of superior numerical stability and neutralizing vanishing/exploding gradient problems, which holds true also in layer-restricted approximations.",
        "Introduction": "  INTRODUCTION Difficulty in training deep neural networks arises from the fact that the network's input-output map f θ (·) is nonlinearly related to its parameters θ. This causes non-convex loss landscape with prolifera- tion of saddle-points and poorly-conditioned curvature where gradient-based first order optimization methods perform poorly ( Martens, 2010 ;  Dauphin et al., 2014 ). Second order methods, such as nat- ural gradient descent ( Amari, 1998 ), compensate for the effect of curvature by using the distance metric intrinsic to the space of input-output maps to define the update steps ( Pascanu & Bengio, 2013 ;  Martens, 2014 ;  Bernacchia et al., 2018 ), rather than the parameter space. Recent advance- ments led to approximate implementations of these methods that prove efficient for practical scale applications ( Ba et al., 2016 ;  Grosse & Martens, 2016 ;  Martens et al., 2018 ;  Osawa et al., 2019 ). Despite their practical effectiveness, however, the exact nature of such curvature-corrected learning process remains largely unknown. Do curvature-corrected learning methods simply accelerate con- vergences towards the same minimum solutions as gradient descent, or do they impose implicit bias toward qualitatively different solutions? As a first step toward establishing theoretical understanding of these questions, we analyze the exact learning dynamics of deep linear networks under a spectrum of curvature-corrected update rules. Deep linear networks provide an excellent mathematical framework for developing insightful theo- retical understanding of the complex inner workings of deep nonlinear networks ( Goodfellow et al., 2016 ). Despite their simplicity, deep linear networks capture the essential nonlinear relationship be- tween network's input-output maps and their parameters, and exhibit comparable learning behavior to their nonlinear counterparts that can be exactly solved for rigorous analysis. Indeed, many recent works analyzed the learning trajectories of deep linear networks under gradient descent to compute the convergence rate under various initial conditions ( Arora et al., 2018a ;b;  Bartlett et al., 2019 ;  Du & Hu, 2019 ), revealed decoupled modes of convergence dynamics to explain the origin of multiple Under review as a conference paper at ICLR 2020 stage-like loss profiles ( Saxe et al., 2013 ), and showed the implicit bias for regularization ( Du et al., 2018 ; Arora et al., 2019) and resistance to overfitting ( Advani & Saxe, 2017 ;  Lampinen & Ganguli, 2018 ;  Poggio et al., 2018 ). Yet, it is uncertain whether these convergence properties generally apply for update rules beyond gradient descent.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes MLModelScope, a distributed design consisting of a specification and a runtime that enables repeatable, fair, and scalable evaluation and benchmarking of Machine Learning (ML) and Deep Learning (DL) models. The proposed specification is a text-based and encapsulates the model evaluation by defining its pre-processing, inference, post-processing pipeline steps and required software stack. The runtime system uses the evaluation specification along with user-defined hardware constraints as input to provision the evaluation, perform benchmarking, and generate reports. MLModelScope is framework/hardware agnostic, extensible, and customizable, and is equipped with command line, library, and web interfaces for ease of use. Experiments are showcased to compare different model pipelines, hardware, and frameworks.",
        "Abstract": "Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major \"pain point\" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking.  We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope's capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance.",
        "Introduction": "  INTRODUCTION The emergence of Machine Learning (ML) and Deep Learning (DL) within a wide array of application domains has ushered in a great deal of innovation in the form of new models and hardware/software (HW/SW) stacks (frameworks, libraries, compilers, and hardware accelerators) to support these models. Being able to evaluate and compare these innovations in a timely manner is critical for their adoption. These innovations are introduced at such a rapid pace ( Dean et al., 2018 ; arXiv ML Papers Statistics) that researchers are hard-pressed to study and compare them. As a result, there is an urging need by both research and industry for a scalable model/HW/SW evaluation platform. Evaluation platforms must maintain repeatability (the ability to reproduce a claim) and fairness (the ability to keep all variables constant and allow one to quantify and isolate the benefits of the target of interest). For ML/DL, repeatable and fair evaluation is challenging, since there is a tight coupling between model execution and the underlying HW/SW components. Model evaluation is a complex process where the model, dataset, evaluation method, and HW/SW stack must work in unison to maintain the accuracy and performance claims (e.g. latency, throughput, memory usage). To maintain repeatability, authors are encouraged to publish their code, containers, and write documentation which details the usage along with HW/SW requirements ( Mitchell et al., 2019 ; Reproducibility Checklist;  Dodge et al., 2019 ;  Lipton & Steinhardt, 2019 ;  Pineau et al., 2018 ). Often, the documentation miss details which make the results not reproducible. To perform a fair evaluation, evaluators have to manually normalize the underlying stack and delineate the codes to characterize performance or accuracy. This is a daunting endeavor. As a consequence, repeatable and fair evaluation is a \"pain-point\" within the community ( Gundersen et al., 2018 ;  Plesser, 2018 ;  Ghanta et al., 2018 ;  Hutson, 2018 ;  Li & Talwalkar, 2019 ;  Tatman et al., 2018 ; Reproducibility in Machine Learning; ICLR Reproducibility Challenge). Thus, an evaluation platform design must have a standard way to specify, provision, and introspect evaluations to guarantee repeatability and fairness. In this paper, we propose MLModelScope: a distributed design which consists of a specification and a runtime that enables repeatable, fair, and scalable evaluation and benchmarking. The proposed specification is a text-based and encapsulates the model evaluation by defining its pre-processing, inference, post-processing pipeline steps and required SW stack. The runtime system uses the evaluation specification along with user-defined HW constraints as input to provision the evalua- tion, perform benchmarking, and generate reports. More specifically, MLModelScope guarantees repeatable and fair evaluation by (1) defining a novel scheme to specify model evaluation which separates the entanglement of data/code/SW/HW; (2) defining common techniques to provision workflows with specified HW/SW stacks; and (3) providing a consistent benchmarking and reporting methodology. Through careful design, MLModelScope solves the design objectives while being framework/hardware agnostic, extensible, and customizable. In summary, this paper makes the following contributions: 1 we comprehensively discuss the complexity of model evaluation and describe prerequisites for a model evaluation platform. 2 We propose a model evaluation specification and an open-source, framework/hardware agnostic, extensible, and customizable distributed runtime design which consumes the specification to execute model evaluation and benchmarking at scale. 3 We implemented the design with support for Caffe, Caffe2, CNTK, MXNet, PyTorch, TensorFlow, TensorRT, and TFLite, running on ARM, Power, and x86 with CPU, GPU, and FPGA. 4 For ease of use, we equip MLModelScope with command line, library, and ready-made web interfaces which allows \"push-button\" model evaluation * . 5 We also add introspection capability in MLModelScope to analyze accuracy at different stages and capture latency and memory information at different levels of the HW/SW stack. 6 We showcase MLModelScope by running experiments which compare different model pipelines, hardware, and frameworks. Model evaluation is complex. Researchers that publish and share DL models can attest to that but are sometimes unaware of the full scope of this complexity. To perform repeatable and fair evaluation, we need to be cognizant of the HW/SW stack and how it affects the accuracy and performance of a model.  Figure 1  shows our classification of the HW/SW stack levels. Model level ( L1 ) evaluates a model by performing input pre-processing, model in- ference, and post-processing. The pre-processing stage transforms the user input into a form that the model expects. The model inference stage calls the framework's inference API on the processed input and produces an output. The post-processing stage transforms the model output to a form that can be viewed by a user or used to compute metrics. Framework level ( L2 ) performs model inference by executing the layers in the model graph using a framework such as TensorFlow, MXNet, or PyTorch. Layer level ( L3 ) executes a sequence of ML library calls for layers such as convolution, normalization, or softmax. ML Library level ( L4 ) invokes a chain of system library calls for functions in ML libraries such as cuDNN( Chetlur et al., 2014 ), MKL-DNN (MKL-DNN) or OpenBLAS ( Xianyi et al., 2014 ). And, last but not the least, at the hardware level ( L5 ), there are CPU/GPU instructions, disk, and network I/O events, and other low-level system operations through the entire model evaluation. All the HW/SW abstractions must work in unison to maintain the reported accuracy and performance claims. When things go awry, each level within the abstraction hierarchy can be suspect. Currently, model authors distribute models by publishing documentation and ad hoc scripts to public repositories such as GitHub. Due to the lack of specification, authors may under-specify or omit key aspects of model evaluation. This inhibits, or makes it difficult, for others to repeat their evaluations or validate their claims. Thus all aspects of the model evaluation must be captured by a evaluation platform to guarantee repeatability. To highlight this, consider the model evaluation pipeline at L1 . While the model inference stage is relatively straight forward, the pre- and post-processing stages are surprisingly subtle and can easily introduce discrepancies in the results. Some of the discrepancies might be \"silent errors\" - where the evaluation is correct for the majority of the inputs but is incorrect for a small number of cases. In general, accuracy errors due to under-specifying pre- and post-processing are difficult to identify and even more difficult to debug. In Section 4.1, we show the effects of under-specifying different operations in pre-processing on image classification models. The current practice of publishing models also causes a few challenges which must be addressed by a fair and scalable evaluation platform. First, any two ad hoc scripts do not adhere to a consistent evaluation API. The lack of a consistent API makes it difficult to evaluate models in parallel and, in turn, slows down the ability to quickly compare models across different HW/SW stacks. Second, ad Under review as a conference paper at ICLR 2019 hoc scripts tend to not clearly demarcate the stages of the model evaluation pipeline. This makes it hard to introspect and debug the evaluation. Furthermore, since an apple-to-apple comparison between models requires a fixed HW/SW stack, it is difficult to perform honest comparison between two shared models without modifying some ad hoc scripts. MLModelScope addresses these challenges through the careful design of a model evaluation specification and a distributed runtime as described in Section 3.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach for improving Batch Normalization (BN) with skewness reduction (BNSR) for training deep neural networks. BNSR introduces a nonlinear function to decrease the skewness of the feature distributions and increase the flexibility of the network. Experiments on CIFAR-100 and ImageNet datasets show that BNSR outperforms other normalization approaches, including Batch Normlization (BN), Layer Normlization (LN) and Instance Normlization (IN). This is the first work to consider skewness for normalization.",
        "Abstract": "Batch Normalization (BN) is a well-known technique used in training deep neural networks.\n    The main idea behind batch normalization is to normalize the features of the layers ($i.e.$, transforming them to have a mean equal to zero and a variance equal to one).\n    Such a procedure encourages the optimization landscape of the loss function to be smoother, and improve the learning of the networks for both speed and performance.\n    In this paper,\n    we demonstrate that the performance of the network can be improved,\n    if the distributions of the features of the output in the same layer are similar.\n    As normalizing based on mean and variance does not necessarily make the features to have the same distribution, we propose a new normalization scheme: Batch Normalization with Skewness Reduction (BNSR).\n    Comparing with other normalization approaches,\n    BNSR transforms not just only the mean and variance,\n    but also the skewness of the data.\n    By tackling this property of a distribution, we are able to make the output distributions of the layers to be further similar. The nonlinearity of BNSR may further improve the expressiveness of the underlying network.\n    Comparisons with other normalization schemes are tested on the CIFAR-100 and ImageNet datasets. Experimental results show that the proposed approach can outperform other state-of-the-arts that are not equipped with BNSR.",
        "Introduction": "  INTRODUCTION In recent years, deep neural networks have been applied to many visual computing tasks, such as image recognition ( Krizhevsky et al., 2012 ;  Huang et al., 2017 ), image super-resolution ( Tong et al., 2017 ), video-based activity recognition ( Feichtenhofer et al., 2016 ), etc.( Ronneberger et al., 2015 ;  Feichtenhofer et al., 2018 ), achieving promising results. These models are usually trained with stochastic gradient descent or its variants. State-of-the-art neural networks often have many layers, which means they have a lot of parameters to learn, leading to practical issues including long training time and high risk of overfitting. To facilitate learning with gradient descent, Batch Normalization (BN) was proposed in ( Ioffe & Szegedy, 2015 ), which has been found very effective in deep learning. A BN layer normalizes the batch input to zero mean and unit variance. (In practice, a BN layer learns a mapping that does not necessarily maintain the \"zero mean, unit variance\" property for the outputs. But that level of detail will not affect the validity of the discussion here.) This has been shown to improve the speed of convergence in training deep neural networks as well as improving the performance ( He et al., 2016 ), and hence BN has become one common component of many popular deep networks. We have discovered that, making the distributions of the features in the same layer more similar would make the network performs better. However, the standard BN procedure only normalizes the features to ensure that they have the same mean and variance. This does not necessarily make the distributions of the features in the same layer to become similar. For example, an exponential distribution can also have zero mean and unit variance. In other words, the standard BN, while performing normalization with respect to the mean and the variance, will not ensure the features of different layers to have similar distributions. Note that, the mean and the variance are only the first-order and second-order moments, respectively, for a distribution. To further encourage the distributions to become closer, we propose to introduce an extra dimension of normalization by mapping the data to ensure they have similar skewness. Skewness is a measure of the asymmetry of a Under review as a conference paper at ICLR 2020 distribution, and we hypothesize that including this measure will provide a much stronger constraint towards making these distributions become similar. From another point of view, modifying skewness requires nonlinear operations. Recent research ( Pascanu et al., 2013b ;  Montufar et al., 2014 ) has shown that deep neural networks are more ex- pressive while stacking up the nonlinear activation with more layers. The nonlinearity introduced for modifying skewness may further contribute to improving the network's capacity in approaching any desired input-output mapping (which is typically highly nonlinear), and thus making network learning more flexible. In this paper, we present a novel approach for improving BN with skewness reduction (BNSR) for training deep neural networks. We notice that, during training, our approach can make the feature distributions to be similar with fewer epochs. Also, we demonstrate that, it is more effective while applying BNSR on the layer with more dissimilar distributions of the features. We further com- pare our proposed method with other normalization schemes, including Batch Normlization (BN), Layer Normlization (LN) and Instance Normlization (IN) on CIFAR-100 and ImageNet datasets. Experimental results show that BNSR outperforms all of them. Our contributions are summarized as follows: • We propose a new batch normalization scheme. To our best knowledge, this is the first work to consider skewness for normalization. • The scheme introduces a nonlinear function, which not only decreases the skewness of the feature distributions, but also increases the flexibility of the network. • We demonstrate that our approach outperforms other normalization approach on visual recognition tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel model, Isomorphic Neural Network (ISONN), to address the challenges in graph representation learning and classification. ISONN consists of two components: a graph isomorphic feature extraction component and a classification component. The graph isomorphic feature extraction component automatically learns a group of subgraph templates of useful patterns from the input graph. The classification component projects the graph instances to their labels. Two variants of ISONN are proposed to guarantee the efficiency when dealing with large subgraphs.",
        "Abstract": "Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the ‘node-orderless’ property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep mod- els on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node- order constraint, we propose a novel model named Isomorphic Neural Network (ISONN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. ISONN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in ISONN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.",
        "Introduction": "  INTRODUCTION The graph structure is attracting increasing interests because of its great representation power on various types of data. Researchers have done many analyses based on different types of graphs, such as social networks, brain networks and biological networks. In this paper, we will focus on the binary graph classification problem, which has extensive applications in the real world. For example, one may wish to identify the social community categories according to the users' social interactions (Gao et al., 2017), distinguish the brain states of patients via their brain networks (Wang et al., 2017), and classify the functions of proteins in a biological interaction network (Hamilton et al., 2017). To address the graph classification task, many approaches have been proposed. One way to estimate the usefulness of subgraph features is feature evaluation criteria based on both labeled and unlabeled graphs (Kong & Yu, 2010). Some other works also proposed to design a pattern exploration approach based on pattern co-occurrence and build the classification model (Jin et al., 2009) or develop a boosting algorithm (Wu et al., 2014). However, such works based on BFS or DFS cannot avoid computing a large quantity of possible subgraphs, which causes high computational complexity though the explicit subgraphs are maintained. Recently, deep learning models are also widely used to solve the graph-oriented problems. Although some deep models like MPNN (Gilmer et al., 2017) and GCN (Kipf & Welling, 2016) learn implicit structural features, the explict structural information cannot be maintained for further research. Besides, most existing works on graph classification use the aggregation of the node features in graphs as the graph representation (Xu et al., 2018; Hamilton et al., 2017), but simply doing aggregation on the whole graph cannot capture the substructure precisely. While there are other models can capture the subgraphs, they often need more complex computation and mechanism (Wang et al., 2017; Narayanan et al., 2017) or need additonal node labels to find the subgraph strcuture (Gaüzere et al., 2012; Shervashidze et al., 2011). However, we should notice that when we deal with the graph-structured data, different node-orders will result in very different adjacency matrix representations for most existing deep models which take the adjacency matrices as input if there is no other information on graph. Therefore, compared with the original graph, matrix naturally poses a redundant constraint on the graph node-order. Such a node-order is usually unnecessary and manually defined. The different graph matrix represen- tations brought by the node-order differences may render the learning performance of the existing models to be extremely erratic and not robust. Formally, we summarize the encountered challenges in the graph classification problem as follows: • Explicit useful subgraph extraction. The existing works have proposed many discrimina- tive models to discover useful subgraphs for graph classification, and most of them require manual efforts. Nevertheless, how to select the contributing subgraphs automatically with- out any additional manual involvement is a challenging problem. • Graph representation learning. Representing graphs in the vector space is an important task since it facilitates the storage, parallelism and the usage of machine learning models for the graph data. Extensive works have been done on node representations (Grover & Leskovec, 2016; Lin et al., 2015; Lai et al., 2017; Hamilton et al., 2017), whereas learning the representation of the whole graph with clear interpretability is still an open problem requiring more explorations. • Node-order elimination for subgraphs. Nodes in graphs are orderless, whereas the ma- trix representations of graphs cast an unnecessary order on nodes, which also renders the features extracted with the existing learning models, e.g., CNN, to be useless for the graphs. For subgraphs, this problem also exists. Thus, how to break such a node-order constraint for subgraphs is challenging. • Efficient matching for large subgraphs. To break the node-order, we will try all possible node permutations to find the best permutation for a subgraph. Clearly, trying all possible permutaions is a combinatorial explosion problem, which is extremly time-comsuming for finding large subgraph templates. The problem shows that how to accelerate the proposed model for large subgraphs also needs to be solved. In this paper, we propose a novel model, namely Isomorphic Neural Network (ISONN) and its vari- ants, to address the aforementioned challenges in the graph representation learning and classification problem. ISONN is composed of two components: the graph isomorphic feature extraction compo- nent and the classification component, aiming at learning isomorphic features and classifying graph instances, respectively. In the graph isomorphic feature extraction component, ISONN automati- cally learns a group of subgraph templates of useful patterns from the input graph. ISONN makes use of a set of permutation matrices, which act as the node isomorphism mappings between the templates and the input graph. With the potential isomorphic features learned by all the permutation matrices and the templates, ISONN adopts one min-pooling layer to find the best node permutation for each template and one softmax layer to normalize and fuse all subgraph features learned by dif- ferent kernels, respectively. Such features learned by different kernels will be fused together and fed as the input for the classification component. ISONN further adopts three fully-connected layers as the classification component to project the graph instances to their labels. Moreover, to accelerate the proposed model when dealing with large subgraphs, we also propose two variants of ISONN to gurantee the efficiency.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel input-dependent variational dropout regularization for network sparsification. The proposed beta-Bernoulli dropout regularizer learns to generate Bernoulli dropout mask for each neuron with sparsity-inducing prior, and the dependent beta-Bernoulli dropout yields significantly more compact network than input-independent beta-Bernoulli dropout. Experiments on multiple public datasets show that the proposed regularization obtains high degree of sparsity without accuracy loss, and further perform run-time pruning for even less computational cost.",
        "Abstract": "While variational dropout approaches have been shown to be effective for network sparsification, they are still suboptimal in the sense that they set the dropout rate for each neuron without consideration of the input data. With such input independent dropout, each neuron is evolved to be generic across inputs, which makes it difficult to sparsify networks without accuracy loss. To overcome this limitation, we propose adaptive variational dropout whose probabilities are drawn from sparsity inducing beta-Bernoulli prior. It allows each neuron to be evolved either to be generic or specific for certain inputs, or dropped altogether. Such input-adaptive sparsity- inducing dropout allows the resulting network to tolerate larger degree of sparsity without losing its expressive power by removing redundancies among features. We validate our dependent variational beta-Bernoulli dropout on multiple public datasets, on which it obtains significantly more compact networks than baseline methods, with consistent accuracy improvements over the base networks.",
        "Introduction": "  INTRODUCTION One of the main obstacles in applying deep learning to large-scale problems and low-power com- puting systems is the large number of network parameters, as it can lead to excessive memory and computational overheads. To tackle this problem, researchers have explored network sparsification methods to remove unnecessary connections in a network, which is implementable either by weight pruning ( Han et al., 2016 ) or sparsity-inducing regularizations ( Wen et al., 2016 ). Recently, variational Bayesian approaches have shown to be useful for network sparsification, outperforming non-Bayesian counterparts. They take a completely different approach from the conventional methods that either uses thresholding or sparsity-inducing norms on parameters, and uses well-known dropout regularization instead. Specifically, these approaches use variational dropout ( Kingma et al., 2015 ) which adds in multiplicative stochastic noise to each neuron, as a means of obtaining sparse neural networks. Removal of unnecessary neurons could be done by either setting the dropout rate individually for each neuron with unbounded dropout rate ( Molchanov et al., 2017 ) or by pruning based on the signal-to-noise ratio ( Neklyudov et al., 2017 ). While these variational dropout approaches do yield compact networks, they are suboptimal in that the dropout rate for each neuron is learned completely independently of the given input data and labels. With input-independent dropout regularization, each neuron has no choice but to encode generic information for all possible inputs, since it does not know what input and tasks it will be given at evaluation time, as each neuron will be retained with fixed rate regardless of the input. Obtaining high degree of sparsity in such as setting will be difficult as dropping any of the neurons will result in information loss. For maximal utilization of the network capacity and thus to obtain a more compact model, however, each neuron should be either irreplaceably generic and used by all tasks, or highly specialized for a task such that there exists minimal redundancy among the learned representations. This goal can be achieved by adaptively setting the dropout probability for each input, such that some of the neurons are retained with high probability only for certain types of inputs and tasks. To this end, we propose a novel input-dependent variational dropout regularization for network sparsification. We first propose beta-Bernoulli dropout that learns to set dropout rate for each individual neuron, by generating the dropout mask from beta-Bernoulli prior, and show how to train it using variational inference. This dropout regularization is a proper way of obtaining a Bayesian neural network and also sparsifies the network, since beta-Bernoulli distribution is a sparsity-inducing Under review as a conference paper at ICLR 2020 prior. Then, we propose dependent beta-Bernoulli dropout, which is an input-dependent version of our variational dropout regularization. Such adaptive regularization has been utilized for general network regularization by a non-Bayesian and non-sparsity-inducing model ( Ba & Frey, 2013 ); yet, the increased memory and computational overheads that come from learning additional weights for dropout mask generation made it less appealing for generic network regularization. In our case of network sparsification, however, the over- heads at training time is more than rewarded by the reduced memory and computational requirements at evaluation time, thanks to the high degree of sparsification obtained in the final output model. We validate our dependent beta-Bernoulli variational dropout regularizer on multiple public datasets for network sparsification performance and prediction error, on which it obtains more compact network with substantially reduced prediction errors, when compared with both the base network and existing network sparsification methods. Further analysis of the learned dropout probability for each unit reveals that our input-adaptive variational dropout approach generates a clearly distinguishable dropout mask for each task, thus enables each task to utilize different sets of neurons for their specialization. Our contribution in this paper is threefold: • We propose beta-Bernoulli dropout, a novel dropout regularizer which learns to generate Bernoulli dropout mask for each neuron with sparsity-inducing prior, that obtains high degree of sparsity without accuracy loss. • We further propose dependent beta-Bernoulli dropout, which yields significantly more compact network than input-independent beta-Bernoulli dropout, and further perform run- time pruning for even less computational cost. • Our beta-Bernoulli dropout regularizations provide novel ways to implement a sparse Bayesian Neural Network, and we provide a variational inference framework for learning it.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes Hindsight Trust Region Policy Optimization (HTRPO), a methodology for efficiently training agents in sparse reward environments. HTRPO extends the effective and monotonically iterative policy optimization procedure within trust region to accommodate sparse reward environments. It estimates the objective function and the expectation of KL divergence between policies using generated hindsight data instead of on-policy data, and applies a f-divergence to approximate KL divergence. Experiments demonstrate that HTRPO can significantly improve the performance and sample efficiency in sparse reward scenarios while maintaining the learning stability, and can be neatly applied to both simple discrete tasks and continuous environments.",
        "Abstract": "As reinforcement learning continues to drive machine intelligence beyond its conventional boundary, unsubstantial practices in sparse reward environment severely limit further applications in a broader range of advanced fields. Motivated by the demand for an effective deep reinforcement learning algorithm that accommodates sparse reward environment, this paper presents Hindsight Trust Region Policy Optimization (HTRPO), a method that efficiently utilizes interactions in sparse reward conditions to optimize policies within trust region and, in the meantime, maintains learning stability. Firstly, we theoretically adapt the TRPO objective function, in the form of the expected return of the policy, to the distribution of hindsight data generated from the alternative goals. Then, we apply Monte Carlo with importance sampling to estimate KL-divergence between two policies, taking the hindsight data as input. Under the condition that the distributions are sufficiently close, the KL-divergence is approximated by another f-divergence. Such approximation results in the decrease of variance and alleviates the instability during policy update.  Experimental results on both discrete and continuous benchmark tasks demonstrate that HTRPO converges significantly faster than previous policy gradient methods. It achieves effective performances and high data-efficiency for training policies in sparse reward environments.",
        "Introduction": "  INTRODUCTION Reinforcement Learning has been a heuristic approach confronting a great many real-world prob- lems from playing complex strategic games ( Mnih et al., 2015 ;  Silver et al., 2016 ;  Justesen et al., 2019 ) to the precise control of robots( Levine et al., 2016 ; Mahler & Goldberg, 2017;  Quillen et al., 2018 ), in which policy gradient methods play very important roles( Sutton et al., 2000 ;  Deisenroth et al., 2013 ). Among them, the ones based on trust region including Trust Region Policy Opti- mization ( Schulman et al., 2015a ) and Proximal Policy Optimization ( Schulman et al., 2017 ) have achieved stable and effective performances on several benchmark tasks. Later on, they have been verified in a variety of applications including skill learning( Nagabandi et al., 2018 ), multi-agent control( Gupta et al., 2017 ), imitation learning( Ho et al., 2016 ), and have been investigated further to be combined with more advanced techniques( Nachum et al., 2017 ; Houthooft et al., 2016;  Heess et al., 2017 ). One unresolved core issue in reinforcement learning is efficiently training the agent in sparse reward environments, in which the agent is given a distinctively high feedback only upon reaching the desired final goal state. On one hand, generalizing reinforcement learning methods to sparse reward scenarios obviates designing delicate reward mechanism, which is known as reward shaping( Ng et al., 1999 ); on the other hand, receiving rewards only when precisely reaching the final goal states also guarantees that the agent can focus on the intended task itself without any deviation. Despite the extensive use of policy gradient methods, they tend to be vulnerable when dealing with sparse reward scenarios. Admittedly, policy gradient may work in simple and sufficiently rewarding environments through massive random exploration. However, since it relies heavily on the expected return, the chances in complex and sparsely rewarding scenarios become rather slim, which often makes it unfeasible to converge to a policy by exploring randomly. Recently, several works have been devoted to solving the problem of sparse reward, mainly applying either hierarchical reinforcement learning ( Kulkarni et al., 2016 ;  Vezhnevets et al., 2017 ;  Le et al., 2018 ;  Marino et al., 2019 ) or a hindsight methodology, including Hindsight Experience Replay Under review as a conference paper at ICLR 2020 ( Andrychowicz et al., 2017 ), Hindsight Policy Gradient ( Rauber et al., 2019 ) and their extensions ( Fang et al., 2019 ;  Levy et al., 2019 ). The idea of Hindsight Experience Replay(HER) is to regard the ending states obtained through the interaction under current policy as alternative goals, and therefore generate more effective training data comparing to that with only real goals. Such augmentation overcomes the defects of random exploration and allows the agent to progressively move towards intended goals. It is proven to be promising when dealing with sparse reward reinforcement learning problems. For Hindsight Policy Gradient(HPG), it introduces hindsight to policy gradient approach and im- proves sample efficiency in sparse reward environments. Yet, its learning curve for policy update still oscillates considerably. Because it inherits the intrinsic high variance of policy gradient meth- ods which has been widely studied in  Schulman et al. (2015b) ,  Gu et al. (2016)  and  Wu et al. (2018) . Furthermore, introducing hindsight to policy gradient methods would lead to greater vari- ance ( Rauber et al., 2019 ). Consequently, such exacerbation would cause obstructive instability during the optimization process. To design an advanced and efficient on-policy reinforcement learning algorithm with hindsight expe- rience, the main problem is the contradiction between on-policy data needed by the training process and the severely off-policy hindsight experience we can get. Moreover, for TRPO, one of the most significant property is the approximated monotonic converging process. Therefore, how these ad- vantages can be preserved when the agent is trained with hindsight data also remains unsolved. In this paper, we propose a methodology called Hindsight Trust Region Policy Optimization (HTRPO). Starting from TRPO, a hindsight form of policy optimization problem within trust region is theoretically derived, which can be approximately solved with the Monte Carlo estimator using severely off-policy hindsight experience data. HTRPO extends the effective and monotonically iter- ative policy optimization procedure within trust region to accommodate sparse reward environments. In HTRPO, both the objective function and the expectation of KL divergence between policies are estimated using generated hindsight data instead of on-policy data. To overcome the high variance and instability in KL divergence estimation, another f -divergence is applied to approximate KL divergence, and both theoretically and practically, it is proved to be more efficient and stable. We demonstrate that on several benchmark tasks, HTRPO can significantly improve the performance and sample efficiency in sparse reward scenarios while maintains the learning stability. From the experiments, we illustrate that HTRPO can be neatly applied to not only simple discrete tasks but continuous environments as well. Besides, it is verified that HTRPO can be generalized to different hyperparameter settings with little impact on performance level.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a unified and self-contained framework for certifying robustness in either 2 or ∞ norm by randomized smoothing. The framework is motivated by differential privacy and two types of robustness are presented: D ∞ robustness, which uses ∞-divergence to measure the distance between the probabilities of predictions on randomized natural samples and randomized adversarial samples, and D M R robustness, which uses the Maximal Relative Rényi (MR) divergence as the probability distance measurement. The paper demonstrates that the Gaussian mechanism is a near optimal choice for certifying D M R robustness in 2 norm, and the robust radius is O(1). It is also shown that an exponential mechanism is the optimal choice for certifying D ∞ robustness in ∞ norm, but the robust radius is only O(1/d). The paper further proves that the Gaussian mechanism is also a near optimal choice for certifying D M R robustness in ∞ norm, but the robust radius is O(1/ √ d log d). The largest robust ∞ radius that can be certified by randomized smoothing to achieve D M R robustness is upper bounded by O(1/ √ d).\n\nThis paper presents a unified and self-contained framework for certifying robustness in either 2 or ∞ norm by randomized smoothing. The framework is motivated by differential privacy and two types of robustness are presented: D ∞ robustness and D M R robustness. The paper demonstrates the (near) optimal mechanisms of the framework for certifying the 2 and ∞ - normed robustness, and provides an upper bound for the largest robust ∞ radius that can be certified by randomized smoothing to achieve D M R robustness.",
        "Abstract": "Randomized smoothing, which was recently proved to be a certified defensive technique, has received considerable attention due to its scalability to large datasets and neural networks. However, several important questions still remain unanswered in the existing frameworks, such as (i) whether Gaussian mechanism is an optimal choice for certifying $\\ell_2$-normed robustness, and (ii) whether randomized smoothing can certify $\\ell_\\infty$-normed robustness (on high-dimensional datasets like ImageNet). To answer these questions, we introduce a {\\em  unified} and {\\em self-contained} framework to study randomized smoothing-based certified defenses, where we mainly focus on the two most popular norms in adversarial machine learning, {\\em i.e.,} $\\ell_2$ and $\\ell_\\infty$ norm. We answer the above two questions by first demonstrating that Gaussian mechanism and  Exponential mechanism are the (near) optimal options to certify the $\\ell_2$ and $\\ell_\\infty$-normed robustness. We further show that the largest $\\ell_\\infty$ radius certified by randomized smoothing is upper bounded by $O(1/\\sqrt{d})$, where $d$ is the dimensionality of the data. This theoretical finding suggests that certifying $\\ell_\\infty$-normed robustness by randomized smoothing may not be scalable to high-dimensional data. The veracity of our framework and analysis is verified by extensive evaluations on CIFAR10 and ImageNet.",
        "Introduction": "  INTRODUCTION The past decade has witnessed tremendous success of deep learning in handling various learning tasks like image classification ( Krizhevsky et al., 2012 ), natural language processing ( Cho et al., 2014 ), and game playing ( Silver et al., 2016 ). Nevertheless, a major unresolved issue of deep learn- ing is its vulnerability to adversarial samples that are almost indistinguishable from natural samples to humans but can mislead deep neural networks (DNNs) to make wrong predictions with high confidence ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ). This phenomenon, referred to as adver- sarial attack, is considered to be one of the biggest threats to the deployment of many deep learning systems. Thus, a great deal of effort has been devoted to developing defensive techniques for it. However, the majority of the existing defenses are of heuristic nature (i.e., without any theoretical guarantees), implying that they may be ineffective against stronger attacks. Recent works ( He et al., 2017 ;  Athalye et al., 2018 ;  Uesato et al., 2018 ) have confirmed this concern, and showed that most of those heuristic defenses actually fail to defend stronger adaptive attacks. This forces us to shift our attentions to certifiable defenses as they can classify all the samples in a predefined neighborhood of the natural samples with a theoretically-guaranteed error bound. Among all existing certifiable defensive techniques, randomized smoothing emerges as the most popular one due to its scalability to large datasets and arbitrary networks. Remarkably, using the Gaussian mechanism for random- ized smoothing,  Cohen et al. (2019)  successfully certify 49% accuracy on the original ImageNet dataset under adversarial perturbations with 2 norm less than 0.5. Despite these successes, there are still several unanswered questions regarding randomized smoothing based certified defenses. One of such questions is, why should Gaussian noise be used for randomized smoothing to certify 2 -normed robustness, and is Gaussian mechanism the best option? Another important question is regarding the generalizability of this method to other norms, especially the ∞ norm. If randomized smoothing can be used to certify ∞ -normed robustness, what mechanism is the optimal choice? To shed light on the above questions, we propose in this paper a unified and self-contained frame- work for randomized smoothing-based certified defenses. We look at the problem from a differential privacy's point of view and present two types of robustness in this framework. One is motivated by Under review as a conference paper at ICLR 2020 -differential privacy ( -DP), which uses ∞-divergence to measure the distance between the prob- abilities of predictions on randomized natural samples and randomized adversarial samples and is therefore called D ∞ robustness. The other is inspired by -zero concentrated differential privacy ( -zCDP) that uses the Maximal Relative Rényi (MR) divergence as the probability distance mea- surement and is called D M R robustness. For both of them, we focus on certifying robustness in either 2 or ∞ norm by randomized smoothing. Specifically, our contributions are five-fold: 1. We propose a unified and self-contained framework for certifying D ∞ and/or D M R robust- ness in 2 and ∞ norms by randomized smoothing. 2. In our framework, we demonstrate that the Gaussian mechanism is a near optimal choice for certifying D M R robustness in 2 norm, and the robust radius is O(1). 3. We also prove that an exponential mechanism is the optimal choice for certifying D ∞ robustness in ∞ norm, but the robust radius is only O(1/d), making it unscalable to high- dimensional data. 4. We show that the Gaussian mechanism is also a near optimal choice for certifying D M R ro- bustness in ∞ norm, but the robust radius is O(1/ √ d log d), making it also hardly scalable to high-dimensional data. 5. The largest robust ∞ radius that can be certified by randomized smoothing to achieve D M R robustness is upper bounded by O(1/ √ d).  Table 1  summarizes the (near) optimal mechanisms of our framework for certifying the 2 and ∞ - normed robustness.",
        "label": 0
    },
    {
        "Summary": "\nAbstract: This paper proposes a novel exploration method called novelty-pursuit which bridges the intrinsically motivated goal exploration process and the maximum state entropy exploration. Novelty-pursuit performs in two stages: first, it selects a visited state with the least visitation counts as the goal to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. To tackle the problem of the curse of dimension and exhaustive storage when selecting the least visited states, we approximate the visitation counts via prediction errors given by Random Network Distillation. We also employ training techniques based on reward shaping and HER to accelerate training the goal-conditioned policy. We demonstrate the exploration efficiency of the proposed method and achieve better performance on environments from the maze, Mujoco tasks, to long-horizon video games of SuperMarioBros.",
        "Abstract": "Efficient exploration is essential to reinforcement learning in huge state space. Recent approaches to address this issue include the intrinsically motivated goal exploration process (IMGEP) and the maximum state entropy exploration (MSEE). In this paper, we disclose that goal-conditioned exploration behaviors in IMGEP can also maximize the state entropy, which bridges the IMGEP and the MSEE. From this connection, we propose a maximum entropy criterion for goal selection in goal-conditioned exploration, which results in the new exploration method novelty-pursuit. Novelty-pursuit performs the exploration in two stages: first, it selects a goal for the goal-conditioned exploration policy to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. We demonstrate the effectiveness of the proposed method in environments from simple maze environments, Mujoco tasks, to the long-horizon video game of SuperMarioBros. Experiment results show that the proposed method outperforms the state-of-the-art approaches that use curiosity-driven exploration.",
        "Introduction": "  INTRODUCTION Efficient exploration is important to learn a (near-) optimal policy for reinforcement learning (RL) in huge state space ( Sutton & Barto, 1998 ). Dithering strategies like epsilon-greedy, Gaussian action noise, and Boltzmann exploration are inefficient and require exponential interactions to explore the whole state space. In contrast, deep exploration ( Osband et al., 2016 ) overcomes this dilemma via temporally extended behaviors with a long-term vision. Recently, proposed methods include the in- trinsically motivated goal exploration process (IMGEP) ( Forestier et al., 2017 ), and maximum state entropy exploration (MSEE) ( Hazan et al., 2019 ). In particular, IMGEP selects interesting states from the experience buffer as goals for a goal-conditioned exploration policy. In this way, explo- ration behaviors are naturally temporally-extended via accomplishing self-generated goals. On the other hand, MSEE aims to search for a policy such that it maximizes the entropy of state distribution. In this way, the agent can escape from the local optimum caused by insufficient state exploration. In this paper, we show that the target of maximizing the support of state distribution (discovering new states) and maximizing the entropy of state distribution (unifying visited state distribution) can be both achieved by the goal-conditioned policy. From this connection, we propose an exploration method called novelty-pursuit. Abstractly, our method performs in two stages: first, it selects a visited state with the least visitation counts as the goal to reach the boundary of the explored region; then, it takes random actions to explore the non-explored region. An illustration can be seen in  Figure 1 . Intuitively, this process is efficient since the agent avoids exploring within the explored region. Besides, the exploration boundary will be expanded further as more and more new states are discovered. Finally, the agent will probably explore the whole state space to find the optimal policy. A naive implementation of the above strategies can lead to inefficient exploration and exploitation on complex environments. First, to tackle the problem of the curse of dimension and exhaustive stor- age when selecting the least visited states, we approximate the visitation counts via prediction errors given by Random Network Distillation ( Burda et al., 2019b ). Besides, we observe that previous methods used in IMGEP ( Forestier et al., 2017 ) are inefficient to train the goal-conditioned explo- ration policy. We employ training techniques based on reward shaping ( Ng et al., 1999 ) and HER ( Andrychowicz et al., 2017 ) to accelerate training the goal-conditioned policy. Finally, we addition- ally train an unconditioned exploitation policy to utilize samples collected by the goal-conditioned Under review as a conference paper at ICLR 2020 exploration policy with environment-specific rewards. Thus, the exploration and exploitation is de- coupled in our method. Our contributions are summarized as follows: (1) We disclose that goal-conditioned behaviors can also maximize the state entropy, which bridges the intrinsically motivated goal exploration process and the maximum state entropy explore. (2) We propose a method called novelty-pursuit from this connection and give practical implementations. (3) We demonstrate the exploration efficiency of the proposed method and achieve better performance on environments from the maze, Mujoco tasks, to long-horizon video games of SuperMarioBros.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel method for deep neural networks (DNNs) to flexibly change their size after training. The method factorizes a weight matrix in each layer into two low-rank matrices after training the DNNs via singular value decomposition (SVD). By changing the rank in each layer, the model can be scaled to an arbitrary size. The paper also introduces simple criteria that characterize the importance of each basis and layer, namely, the error- and complexity-based criteria. Experiments on image-classification tasks of the CIFAR-10/100 datasets using deep convolutional neural networks (CNNs) show that the proposed method exhibits better performance for up to approximately 75% compressed models than existing methods.",
        "Abstract": "Compressing deep neural networks (DNNs) is important for real-world applications operating on resource-constrained devices. However, it is difficult to change the model size once the training is completed, which needs re-training to configure models suitable for different devices. In this paper, we propose a novel method that enables DNNs to flexibly change their size after training. We factorize the weight matrices of the DNNs via singular value decomposition (SVD) and change their ranks according to the target size. In contrast with existing methods, we introduce simple criteria that characterize the importance of each basis and layer, which enables to effectively compress the error and complexity of models as little as possible. In experiments on multiple image-classification tasks, our method exhibits favorable performance compared with other methods.",
        "Introduction": "  INTRODUCTION As part of the great progress made in deep learning, deep neural network (DNN) models with higher performance have been proposed for various machine-learning tasks ( LeCun et al., 2015 ). However, these performance improvements require a higher number of parameters and greater computational complexity. Therefore, it is important to compress them without sacrificing the performance for running the models on resource-constrained devices.  Han et al. (2016)  reduced the memory requirement for devices by pruning and quantizing weight coefficients after training the models.  Howard et al. (2017) ;  Sandler et al. (2018) ;  Howard et al. (2019)  used factorized operations called depth-wise and point-wise convolutions in a proposal for light-weight models suited to mobile devices. However, these methods require pre-defined network structures and pruning the model weights after training. Recently, automated frameworks, such as the so-called neural architecture search (NAS) ( Zoph & Le, 2017 ), have been proposed.  Tan et al. (2019)  proposed a NAS method to accelerate the inference speed on smartphones by incorporating resource-related constraints into the objective function.  Stamoulis et al. (2019)  significantly reduced the search costs for NAS by applying a gradient-based search scheme with a superkernel that shares weights for multiple convolutional kernels. However, the models trained by these methods are dedicated to specific devices, and thus do not possess the ability to be reconfigured for use on different devices. In order to change the model size, it is necessary to re-train them according to the resources of the target devices. For example, it has been reported that the inference speed when operating the same model on different devices differs according to the computing performance and memory capacity of the hardware accelerator ( Ignatov et al., 2018 ). Therefore, it is desirable that the model size can be flexibly changed according to the resources of the target devices without re-training the model, which we refer to as scalability in this paper. To this end,  Yu et al. (2019)  introduced switchable batch normalization (BN) ( Ioffe & Szegedy, 2015 ), which switches BN layers according to pre-defined widths, and proposed \"slimmable\" net- works whose width can be changed after training. Moreover,  Yu & Huang (2019)  proposed univer- sally slimmable networks (US-Nets) that extend slimmable networks to arbitrary widths. However, since these methods directly reduce the width (i.e., dimensionality) in each layer, the principal com- ponents are not taken into account. In addition, they reduce the width uniformly across all layers, which ignores differences in the importance of different layers. In this paper, we propose a novel method that enables DNNs to flexibly change their size after training. We factorize a weight matrix in each layer into two low-rank matrices after training the DNNs via singular value decomposition (SVD). By changing the rank in each layer, our method can scale the model to an arbitrary size (Figure 1(a)). Our contributions are as follows. • We do not directly reduce the width but instead reduce the redundant basis in the column space of the weight matrix, which prevents the feature map in each layer from losing im- portant features. • We introduce simple criteria that characterize the importance of each basis and layer, namely, the error- and complexity-based criteria. These enable to effectively compress the error and complexity of the models as little as possible. • We facilitate the performance of low rank networks with the following methods: a learning procedure that simultaneously minimizes losses for both the full and low rank networks (Figure 1(b)), and the mean & variance correction for each BN layer according to the given rank. In the experiments on image-classification tasks of the CIFAR-10/100 ( Krizhevsky, 2009 ) datasets using deep convolutional neural networks (CNNs), our method exhibits better performance for up to approximately 75% compressed models than slimmable networks and US-Nets. In the following, we first describe the details of our method (Section 2) and briefly review related works (Section 3). Then, we give some experimental results (Section 4) and conclude the paper (Section 5).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel and efficient method, called stochastically controlled compositional gradient (SCCG), for solving composition problems involving a two-finite-sum structure. SCCG incorporates a stochastically controlled function to estimate the inner function G(x) and a stochastically controlled compositional gradient to estimate the ∇f (x). The proposed method improves query complexity over existing approaches and provides a mini-batch version for both strongly convex and non-convex functions. Theoretical analysis is provided to identify a bound on the size of the subsets used to estimate the gradient.",
        "Abstract": "We consider  composition problems of the form  $\\frac{1}{n}\\sum\\nolimits_{i= 1}^n F_i(\\frac{1}{n}\\sum\\nolimits_{j = 1}^n G_j(x))$. Composition optimization arises in many important machine learning applications: reinforcement learning, variance-aware learning, nonlinear embedding, and many others. Both gradient descent and stochastic gradient descent are straightforward solution, but both require to  compute $\\frac{1}{n}\\sum\\nolimits_{j = 1}^n{G_j( x )} $ in each single iteration, which is inefficient-especially when $n$ is large. Therefore, with the aim of significantly reducing the query complexity of such problems, we designed a stochastically controlled compositional gradient algorithm that incorporates two kinds of variance reduction techniques, and works in both strongly convex and non-convex settings. The strategy is also accompanied by a mini-batch version of the proposed method that improves query complexity with respect to the size of the mini-batch. Comprehensive experiments demonstrate the superiority of the proposed method over existing methods.",
        "Introduction": "  INTRODUCTION In this paper, we study the following composition minimization problem, min x∈R N f (x) def = F (G(x)) def = 1 n n i=1 Fi 1 n n j=1 Gj(x) , (1.1) where f : R N → R is differentiable and possibly non-convex, each F i : R M → R is a smooth function, each G i : R N → R M is a mapping function, both the numbers of F i 's and G j 's are assumed to be n for simplicity We call G(x):= 1 n n j=1 G j (x) the inner function, and F (w):= 1 n n i=1 F i (w) the outer function. Many machine learning problems can be cast as composition problems that include two finite-sum structures: reinforcement learning ( Sutton et al., 1998 ;  Wang et al., 2017 ;  Liu et al., 2016 ), variance-averse learning ( Lian et al., 2017 ), and nonlinear embedding ( Hinton & Roweis, 2003 ;  Dikmen et al., 2015 ). In particular, • (reinforcement learning) The S × S system of Bellman equations  Wang et al. (2017)  can be written as min x∈R S E[B]x − E[b] 2 , where E[B] = I − γP π , γ ∈ (0, 1) is a discount factor, P π is the transition probability under policy π, and E[b] is the expected state transition reward. This is one of key problems in reinforcement learning for evaluating the value of a policy π. • (risk-averse learning) The risk-averse learning  Lian et al. (2017)  aims to maximize the expected return while control the variance (or risk) in the meantime: min x − E a [h(x; a)] + λVar a [h(x; a)], where h(x; a) is the loss function including a random variable a, λ > 0 is a regularization parameter. • (nonlinear embedding) Stochastic nonlinear embedding  Hinton & Roweis (2003)  aims to map a group of points from a high dimensional space to a low dimensional space by minimizing the KL divergence. It is a non-convex composition optimization min x t KL(p ·|t q ·|t ) := t i p i|t log p i|t q i|t , (1.2) where p i|t and q i|t are the conditional probabilities w.r.p. {z i } n i=1 and {x i } n i=1 , Under review as a conference paper at ICLR 2020 p i|t = d(zt,zi) j =t d(zt,zj ) , q i|t = d(xt,xi) j =t d(xt,xj ) , where d(·, ·) is the dissimilar distance function between two samples. To solve the composition optimization including the finite-sum structure in (1.1), two most straight- forward approaches are the gradient descent (GD) and the stochastic gradient descent (SGD). How- ever, it is extremely expensive to scan all the inner functions (for both SGD and GD) as well as all the outer functions (for GD) in each iteration. However, note that, unlike solving common stochastic optimization problems, randomly sampling one inner function and one outer function does not give an unbiased estimate for the true gradient; that is, E i∼[n],j∼[n] [(∂G j (x)) T ∂F i (G(x))] = ∇f (x), whereG(x)is the estimation of G(x). The key to solving this composition objective is how to es- timate the value of G(x k ) and its Jacobian with high accuracy using only a few samples in each iteration. Recently, many stochastic optimization methods solving the composition problem have been devel- oped, such as the stochastic gradient based method and the variance-reduction based method. For example, stochastic compositional gradient descent (SCGD) ( Wang et al., 2017 ;  Liu et al., 2016 ) estimates the inner function G(x) by using an iterative weighted average of the past values of G(x), and then performs the stochastic quasi-gradient iteration. The advantage of this method is that con- vergence rate does not depend on n; however, it queries more samples to the desired point. Another set of approaches is based on variance reduction - for instance, compositional stochastic variance reduction gradient (Compositional-SVRG) ( Lian et al., 2017 ) estimates the inner function G(x) and the gradient of function f (x) by using the variance reduction technique; however, the derived linear convergence rate is related to n. Motivated by a few recent works ( Lei & Jordan, 2017 ;  Lei et al., 2017 ;  Allen-Zhu, 2017 ) that focus on the stochastically controlled gradient, we were inspired to look for a way to improve the query complexity and reduce the dependence on n to solve the composition optimization in (1.1). Hence, this paper presents a novel and more efficient method named stochastically controlled com- positional gradient (SCCG) for solving composition problems involving a two-finite-sum structure. The result is improved query complexity over existing approaches. Further, all results in this paper can be easily extended to cases where the number of F i and the number of G j are different. The main contributions of this article are summarized below. • We provide a stochastically controlled function to estimate the inner function G(x). In- spired by stochastically controlled stochastic gradient (SCSG) ( Lei & Jordan, 2017 ) that estimates the gradient, G(x) can also be estimated by using a snapshotx s , in which G(x s ) is not computed directly, but is estimated through a random subset from [n]. This is the first time that a stochastically controlled function has been incorporated into the process of estimating the inner function. We have also analyzed how the size of the subset might influence the query complexity for both strongly convex and non-convex functions. • We provide a stochastically controlled compositional gradient to estimate the ∇f (x). How- ever, there are two potential situations that could be encountered in the estimation process that can impede convergence. First, the expectation of the gradient is no longer an un- biased estimation; and, second, the gradient of f (x s ) at the snapshot is formed by two random subsets, which are used for the functions F i and G j respectively. Moreover, the biased gradient bring more difficulty in proving the convergence, which are greatly differ- ent from those encountered in ( Lei & Jordan, 2017 ;  Lian et al., 2017 ;  Lei et al., 2017 ). To address these scenarios, we have identified a bound on the size of the subsets that are used to estimate the gradient. The details of the analysis can be referred to Section 3.1 and 3.2. • A mini-batch version of the proposed algorithm is also provided for both strongly convex and non-convex functions. The corresponding query complexities are improved according to the size of the mini-batch. More information can be referred to Section 3.3.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the causes of the Graph Neural Networks' (GNNs) suspended animation problem, and proposes novel graph residual terms and a new graph neural network architecture, Graph Residual Neural Network (GRESNET), to resolve the problem. Experiments are conducted on several existing vanilla GNNs to demonstrate the effectiveness of GRESNET. Theoretic analyses are also provided to demonstrate its effectiveness from the norm-preservation perspective.",
        "Abstract": "The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the GRESNET (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes’ raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node’s representations between sequential layers. Detailed studies about the GRESNET framework for many existing GNNs, including GCN, GAT and LOOPYNET, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.",
        "Introduction": "  INTRODUCTION Graph neural networks (GNN), e.g., graph convolutional network (GCN) Kipf & Welling (2016) and graph attention network (GAT) Veličković et al. (2018), based on the approximated spectral graph convolutional operator Hammond et al. (2011), can learn the representations of the graph data effectively. Meanwhile, such GNNs have also received lots of criticism, since as these GNNs' ar- chitectures go deep, the models' performance will get degraded, which is similar to observations on other deep models (e.g., convolutional neural network) as reported in He et al. (2015). Meanwhile, different from the existing deep models, when the GNN model depth reaches a certain limit (e.g., depth ≥ 5 for GCN with the bias term disabled or depth ≥ 8 for GCN with the bias term enabled on the Cora dataset), the model will not respond to the training data any more and become not learn- able. Formally, we name such an observation as the GNNs' suspended animation problem, whereas the corresponding model depth is named as the suspended animation limit of GNNs. Here, we need to add a remark: to simplify the presentations in this paper, we will first take vanilla GCN as the base model example to illustrate our discoveries and proposed solutions in the method sections. Meanwhile, empirical tests on several other existing GNNs, e.g., GAT Veličković et al. (2018) and LOOPYNET Zhang et al. (2018), will also be studied in the experiment section of this paper. As illustrated in  Figure 1 , we provide the learning performance of the GCN model on the Cora dataset, where the learning settings (including train/validation/test sets partition, algorithm imple- mentation and fine-tuned hyper-parameters) are identical to those introduced in Kipf & Welling (2016). The GCN model with the bias term disable of seven different depths, i.e., GCN(1-layer)- GCN(7-layer), are compared. Here, the layer number denotes the sum of hidden and output layers, which is also equal to the number of spectral graph convolutional layers involved in the model. For instance, besides the input layer, GCN(7-layer) has 6 hidden layer and 1 output layer, both of which involve the spectral graph convolutional operations. According to the plots, GCN(2-layer) and GCN(3-layer) have comparable performance, which both outperform GCN(1-layer). Mean- while, as the model depth increases from 3 to 7, its learning performance on both the training set Under review as a conference paper at ICLR 2020 (a) Training Accuracy (b) Testing Accuracy and the testing set degrades greatly. It is easy to identify that such degradation is not caused by over- fitting the training data. What's more, much more surprisingly, as the model depth goes deeper to 5 or more, it will suffer from the suspended animation problem and does not respond to the training data anymore. (Similar phenomena can be observed for GCN (bias enabled) and GAT as illustrated by Figures 9 and 10 in the appendix of this paper, whose suspended animation limits are 8 and 5, respectively. Meanwhile, on LOOPYNET, we didn't observe such a problem as shown in Figure 11 in the appendix, and we will state the reasons in Section 6 in detail.) In this paper, we will investigate the causes of the GNNs' suspended animation problem, and analyze if such a problem also exists in all other GNN models or not. GNNs are very different from the tradi- tional deep learning models, since the extensive connections among the nodes render their learning process no longer independent but strongly correlated. Therefore, the existing solutions proposed to resolve such problems, e.g., residual learning methods used in ResNet for CNN He et al. (2015), cannot work well for GNNs actually. In this paper, several different novel graph residual terms will be studied for GNNs specially. Equipped with the new graph residual terms, we will further intro- duce a new graph neural network architecture, namely graph residual neural network (GRESNET), to resolve the observed problem. Instead of merely stacking the spectral graph convolution layers on each other, the extensively connected high-ways created in GRESNET allow the raw features or intermediate representations of the nodes to be fed into each layer of the model. We will study the effectiveness of the GRESNET architecture and those different graph residuals for several existing vanilla GNNs. In addition, theoretic analyses on GRESNET will be provided in this paper as well to demonstrate its effectiveness from the norm-preservation perspective. The remaining parts of this paper are organized as follows. In Section 2, we will introduce the related work of this paper. The suspended animation problem with the spectral graph convolutional operator will be discussed in Section 3, and the suspended animation limit will be analyzed in Section 4. Graph residual learning will be introduced in Section 5, whose effectiveness will be tested in Section 6. Finally, we will conclude this paper in Section 7.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on games with continuous action spaces, such as Generative Adversarial Networks (GANs) and the Colonel Blotto game. We develop a solution which significantly improves the status-quo for computing Nash equilibrium for multi-player games. We focus on mixed strategy Nash equilibria, which always exist under mild conditions, and present an efficient method to represent the mixed strategy. This method requires many variables in a continuous space and the corresponding probability distribution may not have a density function in closed-form.",
        "Abstract": "Nash equilibrium has long been a desired solution concept in multi-player games, especially for those on continuous strategy spaces, which have attracted a rapidly growing amount of interests due to advances in research applications such as the generative adversarial networks. Despite the fact that several deep learning based approaches are designed to obtain pure strategy Nash equilibrium, it is rather luxurious to assume the existence of such an equilibrium. In this paper, we present a new method to approximate mixed strategy Nash equilibria in multi-player continuous games, which always exist and include the pure ones as a special case. We remedy the pure strategy weakness by adopting the pushforward measure technique to represent a mixed strategy in continuous spaces. That allows us to generalize the Gradient-based Nikaido-Isoda (GNI) function to measure the distance between the players' joint strategy profile and a Nash equilibrium. Applying the gradient descent algorithm, our approach is shown to converge to a stationary Nash equilibrium under the convexity assumption on payoff functions, the same popular setting as in previous studies. \nIn numerical experiments, our method consistently and  significantly outperforms recent works on approximating Nash equilibrium for quadratic games, general blotto games, and GAMUT games.",
        "Introduction": "  INTRODUCTION Nash equilibrium ( Nash, 1950 ) is one of the most important solution concepts in game scenario with multiple rational participants. It plays an important role in theoretical analysis of games to guide rational decision-making processes in multi-agent systems. With the recent success of machine learning applications in games, it attracts even more research interests on applying machine learning technique for unsolved game theory problems, for example, computation of Nash equilibrium for multi-player games. In this paper, we focus on games with continuous action spaces, which include the famous application for Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ), as well as many important game types such as the colonel blotto game ( Gross & Wagner, 1950 ), Cournot competition ( R, 1996 ). We develop a solution which significantly improves the status-quo. There have been several successful approaches to compute Nash equilibrium for multi-player (mostly 2-player) continuous game ( Raghunathan et al., 2019 ;  Balduzzi et al., 2018 ;  Letcher et al., 2018 ). These works seek Nash equilibria corresponding to pure strategies, in which each player takes a specific action to achieve its best payoff given other players' actions. A major concern for such a solution concept is its possible non-existence. As a result, the convergences to a Nash equi- librium for these approaches were proven under the assumption for the existence of a pure strategy Nash equilibrium, which can hardly be checked in practice, and their applicability is limited to spe- cific types of games. On the contrary, it is known that mixed strategy Nash equilibria always exist under mild conditions. And note that any pure strategy Nash equilibrium is also a mixed strategy Nash equilibrium, which means the latter one is a much more desired solution concept. However, a key challenge that obstructs the study of computing a mixed strategy Nash equilibrium, especially for a continuous game, lies on how to design an efficient method to represent the mixed strategy. To be precise, a pure strategy can be represented by a single variable choosing from some region. But as a distribution on each player's action space, a mixed strategy with respect to the player is defined in a (subspace of) real space R. More generally, exact representation for a mixed strategy of a player usually requires many variables in a continuous space. In addition, the corresponding probability distribution may not have a density function in closed-form.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the use of deep learning in traditional numerical solvers of conservation laws. Conservation laws are fundamental laws of nature with broad applications in multiple fields such as physics, chemistry, biology, geology, and engineering. Solving the differential equations associated with conservation laws has been a major branch of computational mathematics, and a lot of effective methods have been proposed. Deep learning has the potential to improve these traditional algorithms and ease the usage of high-end methods.",
        "Abstract": "Conservation laws are considered to be fundamental laws of nature. It has broad application in many fields including physics, chemistry, biology, geology, and engineering. Solving the differential equations associated with conservation laws is a major branch in computational mathematics. Recent success of machine learning, especially deep learning, in areas such as computer vision and natural language processing, has attracted a lot of attention from the community of computational mathematics and inspired many intriguing works in combining machine learning with traditional methods. In this paper, we are the first to explore the possibility and benefit of solving nonlinear conservation laws using deep reinforcement learning. As a proof of concept, we focus on 1-dimensional scalar conservation laws. We deploy the machinery of deep reinforcement learning to train a policy network that can decide on how the numerical solutions should be approximated in a sequential and spatial-temporal adaptive manner. We will show that the problem of solving conservation laws can be naturally viewed as a sequential decision making process and the numerical schemes learned in such a way can easily enforce long-term accuracy. \nFurthermore, the learned policy network is carefully designed to determine a good local discrete approximation based on the current state of the solution, which essentially makes the proposed method a meta-learning approach.\nIn other words, the proposed method is capable of learning how to discretize for a given situation mimicking human experts. Finally, we will provide details on how the policy network is trained, how well it performs compared with some state-of-the-art numerical solvers such as WENO schemes, and how well it generalizes. Our code is released anomynously at \\url{https://github.com/qwerlanksdf/L2D}.",
        "Introduction": "  INTRODUCTION Conservation laws are considered to be one of the fundamental laws of nature, and has broad applications in multiple fields such as physics, chemistry, biology, geology, and engineering. For example, Burger's equation, a very classic partial differential equation (PDE) in conservation laws, has important applications in fluid mechanics, nonlinear acoustics, gas dynamics, and traffic flow. Solving the differential equations associated with conservation laws has been a major branch of computational mathematics (LeVeque, 1992;  2002 ), and a lot of effective methods have been proposed, from classic methods such as the upwind scheme, the Lax-Friedrichs scheme, to the advanced ones such as the ENO/WENO schemes ( Liu et al., 1994 ;  Shu, 1998 ), the flux-limiter methods ( Jerez Galiano & Uh Zapata, 2010 ), and etc. In the past few decades, these traditional methods have been proven successful in solving conservation laws. Nonetheless, the design of some of the high-end methods heavily relies on expert knowledge and the coding of these methods can be a laborious process. To ease the usage and potentially improve these traditional algorithms, machine learning, especially deep learning, has been recently incorporated into this field. For example, the ENO scheme requires lots of 'if/else' logical judgments when used to solve complicated system of equations or high-dimensional equations. This very much resembles the old-fashioned expert systems. The recent trend in artificial intelligence (AI) is to replace the expert systems by the so-called 'connectionism', e.g., deep neural networks, which leads to the recent bloom of AI. Therefore, it Under review as a conference paper at ICLR 2020 is natural and potentially beneficial to introduce deep learning in traditional numerical solvers of conservation laws.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Latently Invertible Autoencoder (LIA), a new generative model that utilizes an invertible network to bridge the encoder and the decoder of Variational Autoencoder (VAE) in a symmetric manner. LIA is capable of exact inference and generation with one architecture, and is not affected by the posterior collapse when the decoder is more complex. Experiments on FFHQ and LSUN datasets show that LIA achieves superior performance on inference and generation compared to state-of-the-art generative models.",
        "Abstract": "Deep generative models such as Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) play an increasingly important role in machine learning and computer vision. However, there are two fundamental issues hindering their real-world applications: the difficulty of conducting variational inference in VAE and the functional absence of encoding real-world samples in GAN. In this paper, we propose a novel algorithm named Latently Invertible Autoencoder (LIA) to address the above two issues in one framework. An invertible network and its inverse mapping are symmetrically embedded in the latent space of VAE. Thus the partial encoder first transforms the input into feature vectors and then the distribution of these feature vectors is reshaped to fit a prior by the invertible network. The decoder proceeds in the reverse order of the encoder's composite mappings. A two-stage stochasticity-free training scheme is designed to train LIA via adversarial learning, in the sense that the decoder of LIA is first trained as a standard GAN with the invertible network and then the partial encoder is learned from an autoencoder by detaching the invertible network from LIA.  Experiments conducted on the FFHQ face dataset and three LSUN datasets validate the effectiveness of LIA for inference and generation.",
        "Introduction": "  INTRODUCTION Deep generative models play a more and more important role in cracking challenges in computer vision as well as in other disciplines, such as high-quality image generation ( Isola et al., 2017 ;  Zhu et al., 2017 ;  Karras et al., 2018a ; b ;  Brock et al., 2018 ), text-to-speech transformation ( van den Oord et al., 2016a ; 2017), information retrieval ( Wang et al., 2017 ), 3D rendering ( Wu et al., 2016 ;  Eslami et al., 2018 ), and signal-to-image acquisition ( Zhu et al., 2018 ). Overall, the generative models fall into four categories: autoencoder and its most important variant of Variational Auto- Encoder (VAE) ( Kingma & Welling, 2013 ), auto-regressive models ( van den Oord et al., 2016b ;a), Generative Adversarial Network (GAN) ( Goodfellow et al., 2014 ), and normalizing flows (NF) ( Tabak & Vanden-Eijnden, 2010 ; Tabak & Turner, 2013;  Rezende & Mohamed, 2015 ). Here we compare these models through the perspective of data dimensionality reduction and recon- struction. To be formal, let x be a data point in the d x -dimensional observable space R dx and y be its corresponding low-dimensional representation in the feature space R dy . The general formulation of dimensionality reduction is f : R dx → R dy , x → y = f (x), where f (·) is the mapping function and d y d x . The manifold learning aims at requiring f under various constraints on y ( Tenenbaum1 et al., 2000 ;  Roweis & Saul, 2000 ). However, the sparsity of data points in high-dimensional space often leads to model overfitting, thus necessitating research on opposite mapping from y to x, i.e. g : R dy → R dx , y → x = g(y), where g(·) is the opposite mapping function with respect to f (·), to reconstruct the data. In general, the role of g(·) is a regularizer to f (·) or a generator to produce more data. The autoencoder is Under review as a conference paper at ICLR 2020 of mapping x f → y g →x. A common assumption in autoencoder is that the variables in low- dimensional space are usually sampled from a prior distribution P(z; θ) such as uniform or Gaussian. To differentiate from y, we let z represent the low-dimensional vector following the prior distribution. Thus we can write It is crucial to establish such dual maps z = f (x) and x = g(z). In the parlance of probability, the process of x → z = f (x) is called inference, and the other procedure of z → x = g(z) is called sampling or generation. VAE is capable of carrying out inference and generation in one framework by two collaborative functional modules. However, it is known that in many cases VAEs are only able to generate blurry images due to the imprecise variational inference. To see this, we write the approximation of the marginal log-likelihood log p(x) = log p(x|z)p(z)dz ≥ −KL[q(z|x)||p(z)] + E q [log p(x|z)], (1) where KL[q(z|x)||p(z)] is the Kullback-Leibler divergence with respect to posterior probability q(z|x) and prior p(z). This lower-bound log-likelihood usually produces imprecise inference. Furthermore, the posterior collapse frequently occurs when using more sophisticated decoder mod- els ( Bowman et al., 2015 ; Kingma et al., 2016). These two issues greatly limit the generation capability of the VAE. On the other hand, GAN is able to achieve photo-realistic generation re- sults ( Karras et al., 2018a ; b ). However, its critical limitation is the absence of the encoder f (x) for carrying inference on real images. Effort has been made on learning an encoder for GAN under the framework of VAE, however the previous two issues of learning VAE still exist. Normalizing flows can perform the exact inference and generation with one architecture by virtue of invertible networks ( Kingma & Dhariwal, 2018 ). But it requires the dimension d x of the data space to be identical to the dimension d z of the latent space, thus posing computational issues due to high complexity of learning deep flows and computing the Jacobian matrices. Inspired by recent success of GANs ( Karras et al., 2018a ; b ) and normalizing flows (Kingma et al., 2016;  Kingma & Dhariwal, 2018 ), we develop a new model called Latently Invertible Autoencoder (LIA). LIA utilizes an invertible network to bridge the encoder and the decoder of VAE in a symmetric manner. We summarize its key advantages as follows: • The symmetric design of the invertible network brings two benefits. The prior distribution can be exactly fitted from an unfolded feature space, thus significantly easing the inference problem. Besides, since the latent space is detached, the autoencoder can be trained without variational optimization thus there is no approximation here. • The two-stage adversarial learning decomposes the LIA framework into a Wasserstein GAN (only a prior needed) and a standard autoencoder without stochastic variables. Therefore the training is deterministic 2 , implying that the model will be not affected by the posterior collapse when the decoder is more complex or followed by additional losses such as the adversarial loss and the perceptual loss. • We compare LIA with state-of-the-art generative models on inference and generation/recon- struction. The experimental results on FFHQ and LSUN datasets show the LIA achieves superior performance on inference and generation.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel method, Global Momentum Compression (GMC), for sparse communication in distributed machine learning models. GMC combines memory gradient and momentum SGD to achieve sparse communication in distributed machine learning models, and theoretically proves the convergence rate of GMC for both convex and non-convex problems. Empirical results show that GMC can reduce the communication cost by approximately 100 fold with no loss of generalization accuracy, and can also achieve comparable (sometimes better) performance with comparable communication compression ratio, with an extra theoretical guarantee.",
        "Abstract": "With the rapid growth of data, distributed stochastic gradient descent~(DSGD) has been widely used for solving large-scale machine learning problems. Due to the latency and limited bandwidth of network, communication has become the bottleneck of DSGD when we need to train large scale models, like deep neural networks. Communication compression with sparsified gradient, abbreviated as \\emph{sparse communication}, has been widely used for reducing communication cost in DSGD. Recently, there has appeared one method, called deep gradient compression~(DGC), to combine memory gradient and momentum SGD for sparse communication. DGC has achieved promising performance in practice. However, the theory about the convergence of DGC is lack. In this paper, we propose a novel method, called \\emph{\\underline{g}}lobal \\emph{\\underline{m}}omentum \\emph{\\underline{c}}ompression~(GMC), for sparse communication in DSGD. GMC also combines memory gradient and momentum SGD. But different from DGC which adopts local momentum, GMC adopts global momentum. We theoretically prove the convergence rate of GMC for both convex and non-convex problems. To the best of our knowledge, this is the first work that proves the convergence of distributed momentum SGD~(DMSGD) with sparse communication and memory gradient. Empirical results show that, compared with the DMSGD counterpart without sparse communication, GMC can reduce the communication cost by approximately 100 fold without loss of generalization accuracy. GMC can also achieve comparable~(sometimes better) performance compared with DGC, with an extra theoretical guarantee.",
        "Introduction": "  INTRODUCTION Many machine learning models can be formulated as the following empirical risk minimization problem: min w∈R d F (w) := 1 n n i=1 f (w; ξ i ), (1) where w denotes the model parameter, ξ i denotes the ith training data, n is the number of training data, and d is the size of the model. For example, let ξ i = (a i , y i ), where a i denotes the feature of the ith training data and y i denotes the label. Then in logistic regression f (w; ξ i ) = log(1 + e −yia T i w ) + λ 2 w 2 , and in SVM f (w; ξ i ) = max(0, 1 − y i a T i w) + λ 2 w 2 . Many deep learning models can also be formulated as (1). One of the efficient ways to solve (1) is stochastic gradient descent (SGD) ( Robbins & Monro, 1951 ). In each iteration, SGD calculates one stochastic gradient ∇f (w; ξ i ) and updates w by w ← w−η∇f (w; ξ i ), or updates w with a mini-batch of stochastic gradients. Inspired by momentum and nesterov's accelerated gradient descent, momentum SGD (MSGD) ( Polyak, 1964 ;  Tseng, 1998 ;  Lan, 2012 ;  Kingma & Ba, 2015 ) has been proposed and widely used in machine learning. In practice, MSGD can achieve better performance than SGD ( Krizhevsky et al., 2012 ;  Sutskever et al., 2013 ). Many machine learning platforms like TensorFlow, PyTorch, and MXNet adopt MSGD as one of the optimization methods. With the rapid growth of data, distributed SGD (DSGD) ( Dekel et al., 2012 ;  Li et al., 2014b ) has attracted much attention since it can parallelly calculate a batch of stochastic gradients. DSGD can Under review as a conference paper at ICLR 2020 be formulated as follows: w t+1 = w t − η t p p k=1 g t,k , (2) where p is the number of workers, g t,k is the stochastic gradient or a mini-batch of stochastic gra- dients calculated by the kth worker at the tth iteration. DSGD can be implemented on distributed frameworks like parameter server and all-reduce framework. Each worker calculates g t,k and sends it to the server or other workers for updating w. Recently, more and more large models, such as deep learning models, are used in machine learning to improve the generalization ability. In large models, g t,k is a high dimensional vector. Due to the latency and limited bandwidth of network, commu- nication cost has become the bottleneck of traditional DSGD or distributed MSGD (DMSGD). For example, when we implement DSGD on parameter server, the server needs to receive p high dimen- sion vectors from workers, which will lead to communication traffic jam and make the convergence of DSGD slow. Hence, we need to compress g t,k to reduce the communication cost. Recently, researchers have proposed two main categories of communication compression techniques for reducing communication cost in DSGD and DMSGD. The first category is quantization ( Wen et al., 2017 ;  Alistarh et al., 2017 ;  Jiang & Agrawal, 2018 ). In machine learning problems, 32-bit float number is typically adopted for representation. Quantization methods quantize the value (gradient or parameter) representation from 32 bits to some low bit-width like 8 bits or 4 bits. Since the quantized gradients in most methods are an unbiased estimation of the original ones, the convergence rate of these methods has the same order of magnitude as that of DSGD, but slower due to the extra quantization variance. It is easy to find that the communication cost can be reduced by 31 fold in the ideal case. In practice, at least 4 bits should be adopted for representation in most cases to keep original accuracy. In these cases, the communication cost is reduced by 7 fold. The other category is based on sparsified gradient, which is called sparse communication. In sparse communication, after calculating the update vector g t,k at each iteration, each worker only sends a subset of coordinates in g t,k , denoted as S(g t,k ), to the server or other workers. Here, S(g t,k ) is a sparse vector and hence it can reduce the communication cost. In recent works ( Aji & Heafield, 2017 ;  Lin et al., 2018 ), each worker will typically remember those values which are not sent to the server, i.e., g t,k − S(g t,k ), and store it in the memory rather than dropping them. The g t,k − S(g t,k ) is called memory gradient and will be used to calculate the next update vector g t+1,k . This is intu- itively necessary because a subset of coordinates in the stochastic gradient can not reflect the real descent direction and can make errors with higher probability than the original stochastic gradi- ent. This memory gradient based sparse communication strategy has been widely adopted by recent communication compression methods and has achieved better performance than quantization meth- ods and other sparse communication methods without memory gradient. In these memory gradient based sparse communication methods, some are for vanilla SGD ( Aji & Heafield, 2017 ;  Alistarh et al., 2018 ;  Stich et al., 2018 ;  Karimireddy et al., 2019 ;  Tang et al., 2019 ). The convergence rate of vanilla SGD with sparse communication has been proved in ( Alistarh et al., 2018 ;  Stich et al., 2018 ;  Karimireddy et al., 2019 ;  Tang et al., 2019 ). Very recently, there has appeared one sparse commu- nication method for distributed MSGD (DMSGD), called deep gradient compression (DGC) ( Lin et al., 2018 ), which has achieved better performance than vanilla DSGD with sparse communication in practice. However, the theory about the convergence of DGC is still lack. Furthermore, although DGC uses momentum SGD, the momentum in DGC is calculated by each worker. Hence it is a local momentum without global information. In this paper, we propose a novel method, called global momentum compression (GMC), for sparse communication in DMSGD which includes DSGD as a special case. The main contributions of this paper are summarized as follows: • GMC combines memory gradient and momentum SGD to achieve sparse communication in DMSGD (DSGD). But different from DGC which adopts local momentum, GMC adopts global momentum. • We theoretically prove the convergence rate of GMC for both convex and non-convex prob- lems. To the best of our knowledge, this is the first work that proves the convergence of DMSGD with sparse communication and memory gradient. Under review as a conference paper at ICLR 2020 • Empirical results show that, compared with the DMSGD counterpart without sparse com- munication, GMC can reduce the communication cost by approximately 100 fold with no loss of generalization accuracy. • GMC can also achieve comparable (sometimes better) performance with comparable com- munication compression ratio, with an extra theoretical guarantee.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel deep reinforcement learning (DRL) method, Learning Transitional Skills (LTS), which can autonomously acquire primitive and transitional skills without extrinsic reward. The primitive skills are distinguishable and diverse, and the transitional skills can accomplish smooth transitions between primitive skills. Experiments on four simulated robotic tasks demonstrate the effectiveness of LTS in solving downstream tasks, performing the transition between primitive skills, and composing skills in a weighted way.",
        "Abstract": "By maximizing an information theoretic objective, a few recent methods empower the agent to explore the environment and learn useful skills without supervision. However, when considering to use multiple consecutive skills to complete a specific task, the transition from one to another cannot guarantee the success of the process due to the evident gap between skills. In this paper, we propose to learn transitional skills (LTS) in addition to creating diverse primitive skills without a reward function. By introducing an extra latent variable for transitional skills, our LTS method discovers both primitive and transitional skills by minimizing the difference of mutual information and the similarity of skills. By considering various simulated robotic tasks, our results demonstrate the effectiveness of LTS on learning both diverse primitive skills and transitional skills, and show its superiority in smooth transition of skills over the state-of-the-art baseline DIAYN.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (DRL) has shown its great effectiveness in learning various reward- driven skills in wide domains, such as performing robotic manipulation tasks (Levine et al. (2016)), playing video games (Mnih et al. (2015)), playing adversarial board games (Silver et al. (2016)) and implementing robot navigation in complex environments (Wang et al. (2018)). Nevertheless, for the majority of real applications, there is no reward in a long term until the agent reaches a goal state (Wu & Chen (2007)), especially in unseen environments. In such cases, DRL has difficulty in carrying out the tasks. By observing the human intelligence that can explore their surroundings and learn valuable skills without reward, a couple of prior works have been recently proposed to generate skills without supervision by incorporating intrinsic motivation into DRL methods (Barto (2013),Ryan & Deci (2000)). Diverse skills can be autonomously acquired without extrinsic reward by maximizing an information theoretic objective using a maximum entropy policy (DIAYN (Eysenbach et al. (2018)); VIC (Gregor et al. (2016)); DAS (Sharma et al. (2019))). Discovered skills may help the exploration in complex environments, and can also serve as primitive skills for hierarchical DRL. Particularly, a high-level meta-policy could be adopted in the hierarchical framework to choose corresponding low-level primitive skills to complete tasks in order. Although discovered skills are both distinguishable and diverse, it is still exceedingly difficult to integrate such skills for a complex task that requires smooth transitions between skills (Lee et al. (2018)). Take the basketball as an example: learning the passing, catching and shooting skills in an isolated way cannot guarantee to score in the court due to the possible failure in the process of tran- sitions between skills. To address this problem, we propose to further learn transitional skills (LTS), where discovered primitive skills, same as prior works (Eysenbach et al. (2018)), are distinguishable and as diverse as possible. More concretely, our LTS method learns both primitive and transitional skills by optimizing an in- formation theoretic objective, where extra transitional skills are generated to fill in the gap between diverse primitive skills. For such purpose, aside from using the latent variable on which we con- dition primitive skills, an extra latent variable is introduced on which we condition our transitional policy. Furthermore, a compensation term has to be considered in the objective function because the Under review as a conference paper at ICLR 2020 distinct between primitive skills and transitional skills will lead to the decline of multual informa- tion between the latent variable corresponding to primitive skills and states generated by transitional skills. This compensation considers the divergence of the latent variable corresponding to primitive skills and that for transitional skills. Different from learning primitive skills, our learning process considers two arbitrary primitive skills and multiple transitional skills between them, where both primitive and transitional skillls are un- known and to be learned. By maximizing the multual information with compensation, both primitive skills and transitional skills are discovered, which can be used to effectively learn downstream tasks. On four simulated robotic tasks, experimental results show that our LTS can discover both prim- itive skills and transitional skills, successfully perform the transition between primitive skills that are distinguishable, and achieve a better peformance in comparison to the state-of-the-art baseline DIAYN. The main contributions of our work can be summarized as follows. Our proposed LTS can learn both primitive and transitional skills without extrinsic reward, where the primitive skills are distinguish- able and diverse, and the transitional skills can accomplish smooth transitions between primitive skills. And extensive experiments are conducted, which demonstrates the effectiveness of our LTS method in solving downstream tasks, performing the transition between primitive skills as well as the weighted way to compose skills.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the possibility of designing new convolutional neural network (CNN) architectures which can be trained using standard optimization methods on standard benchmark datasets but by themselves enjoy robustness against adversarial examples. We introduce the concept of defectiveness into the CNNs, replacing standard convolutional layers with defective versions on a standard CNN and training the network in the standard way. Experimental results show that Defective CNN has superior defense performance than standard CNN against various attacks, and achieves state-of-the-art results against two transfer-based black-box attacks while maintaining high accuracy on clean test data.",
        "Abstract": "Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves state-of-the-art performance against transfer-based attacks without applying any adversarial training.",
        "Introduction": "  INTRODUCTION Deep learning ( LeCun et al., 2015 ), especially deep Convolutional Neural Network (CNN) ( LeCun et al., 1998 ), has led to state-of-the-art results spanning many machine learning fields ( He et al., 2016 ;  Ren et al., 2015 ). Despite the great success in numerous applications, recent studies show that deep CNNs are vulnerable to some well-designed input samples named as Adversarial Examples ( Szegedy et al., 2013 ;  Biggio et al., 2013 ). Take the task of image classification as an example, for almost every commonly used well-performed CNN, attackers are able to construct a small perturbation on an input image. The perturbation is almost imperceptible to humans but can make the model give a wrong prediction. The problem is serious as some designed adversarial examples can be transferred among different kinds of CNN architectures ( Papernot et al., 2016b ), which means a machine learning system can be easily attacked even if the attacker does not have access to the model parameters. There is a rapidly growing body of work on how to obtain a robust convolutional neural network, mainly based on adversarial training ( Szegedy et al., 2013 ;  Madry et al., 2017 ;  Goodfellow et al., 2015 ;  Huang et al., 2015 ). However, those methods need lots of extra computation to obtain adversarial examples at each time step and may tend to overfit the attacking method used in training ( Buckman et al., 2018 ). In this paper, different from most existing methods, we tackle the problem from another perspective. In particular, we explore the possibility of designing new CNN architectures which can be trained using standard optimization methods on standard benchmark datasets but by themselves enjoy robustness, without appealing to other techniques. Recently, studies ( Geirhos et al., 2017 ; 2018;  Baker et al., 2018 ) show that the predictions of CNNs mainly depend on the texture of objects but not the shape. Also,  Liu et al. (2018)  finds attack methods usually perturb patches to contain textural features of incorrect classes. They suggest that the wrong prediction by CNNs for adversarial examples comes from the change on the texture-level information. The small perturbation of adversarial examples will change the textures and eventually affect the features extracted by the CNNs. Therefore, a natural way to avoid adversarial examples is to let the CNN make prediction Under review as a conference paper at ICLR 2020 relying less on textures but more about other information which will not be severely affected by small perturbations, such as shape. In real practice, sometimes a camera might have mechanical failures which cause the output image to have many defective pixels (such pixels are always black in all images). Nonetheless, humans can still recognize objects in the image with defective pixels but have to classify the objects by other information as some local textural information is missing. Motivated by this, we introduce the concept of defectiveness into the convolutional neural networks: We call a neuron a defective neuron if its output value is fixed to zero no matter what input signal is received, and a convolutional layer a defective convolutional layer if it contains defective neurons. Before training, we replace the standard convolutional layers with the defective version on a standard CNN and train the network in the standard way. As defective neurons of the defective convolutional layer contain no information and are very different from their spatial neighbors, the textural information cannot be accurately extracted from the bottom defective layers to top layers. Therefore, we destroy local textural information to a certain extent and prompt the neural network to learn more other information for classification. We call the architecture deployed with defective convolutional layers as Defective CNN. We find that applying the defective convolutional layers to the bottom 1 layers of the network and introducing various patterns for defective neurons arrangement across channels are crucial for robustness. According to the experimental results, we find • Standard CNN consistently works better than Defective CNN on manipulated images in which patches are randomly relocated. This justifies our proposal: Defective CNN makes predictions relying less on textural information but more on shape information, and thus preserving local textural information but destroying shape information of images would more hurt its performance. Furthermore, the adversarial examples generated by Defective CNNs appear to have semantic shapes (See  Figure 1  and Appendix B). • Experimental results show that Defective CNN has superior defense performance than standard CNN against the decision-based attack, transfer-based attacks, additive Gaussian noise, and grey-box attacks. • Using the standard training method, Defective CNN achieves state-of-the-art results against two transfer-based black-box attacks while maintaining high accuracy on clean test data. This suggests that the proposed architecture may be practical for real-world tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces deep neural networks (DNNs) and their vulnerability to adversarial examples, and discusses the need for effective defensive strategies to ensure the safety of deep models in real-world applications. It then examines the two main challenges for adversarial detection: (1) extracting discriminative information to train the detector, and (2) ensuring the detector is robust to different data sources and attack methods.",
        "Abstract": "Adversarial examples have been well known as a serious threat to deep neural\nnetworks (DNNs). To ensure successful and safe operations of DNNs on realworld tasks, \nit is urgent to equip DNNs with effective defense strategies. In this\nwork, we study the detection of adversarial examples, based on the assumption\nthat the output and internal responses of one DNN model for both adversarial and\nbenign examples follow the generalized Gaussian distribution (GGD), but with\ndifferent parameters (i.e., shape factor, mean, and variance). GGD is a general\ndistribution family to cover many popular distributions (e.g., Laplacian, Gaussian,\nor uniform). It is more likely to approximate the intrinsic distributions of internal\nresponses than any specific distribution. Besides, since the shape factor is more\nrobust to different databases rather than the other two parameters, we propose\nto construct discriminative features via the shape factor for adversarial detection,\nemploying the magnitude of Benford-Fourier coefficients (MBF), which can be\neasily estimated using responses. Finally, a support vector machine is trained\nas the adversarial detector through leveraging the MBF features. Through the\nKolmogorov-Smirnov (KS) test, we empirically verify that: 1) the posterior vectors \nof both adversarial and benign examples follow GGD; 2) the extracted MBF features \nof adversarial and benign examples follow different distributions. Extensive \nexperiments in terms of image classification demonstrate that the proposed \ndetector is much more effective and robust on detecting adversarial examples \nof different crafting methods and different sources, in contrast to state-of-the-art \nadversarial detection methods.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have achieved a remarkable success in many important applications, such as image classification, face recognition, object detection, etc. In the meanwhile, DNNs have been shown to be very vulnerable to adversarial examples. However, many real-world scenarios have very restrictive requirements about the robustness of DNNs, such as face verification for login, or semantic segmentation in autonomous driving. Adversarial examples are a serious threat to the applications of DNNs to these important tasks. Since many kinds of adversarial attack methods have been proposed to fool DNNs, it is more urgent to equip effective defensive strategies to ensure the safety of deep models in real-world applications. However, defense seems to be more challenging than attack, as it has to face adversarial examples from unknown crafting methods and unknown data sources. Typical defensive strategies include adversarial training, adversarial de-noising, and adversarial detection. Compared to the former two strategies, adversarial detection is somewhat more cost-effective, as it often does not need to re-train or modify the original DNN model. There are two main challenges for adversarial detection. (1) The adversarial examples are designed to camouflage themselves to be close to the corresponding benign examples in the input space. Then, where and how to extract the discriminative information to train the detector? (2) The data sources and the generating methods of adversarial examples are often inaccessible to the detector. In this case, the detector can be stably effective across different data sources and different attack meth- ods? In other words, a good adversarial detector is required to be not only effective to distinguish adversarial and benign examples, but also robust to different data sources and attack methods.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel end-to-end model, Deep Graph Spectral Evolution Network (GSEN), to optimally fit the graph evolution process by the composition of newly-developed generalized graph kernels. GSEN is highly expressive and efficient, with a parameter and memory complexity that is independent of the graph size and a time complexity that is linear to the graph size. Extensive experiments on several synthetic and real-world datasets demonstrate the superior accuracy and efficiency of GSEN compared to existing deep generative models and models based on graph signal processing.",
        "Abstract": "Characterizing the underlying mechanism of graph topological evolution from a source graph to a target graph has attracted fast increasing attention in the deep graph learning domain. However, there lacks expressive and efficient that can handle global and local evolution patterns between source and target graphs. On the other hand, graph topological evolution has been investigated in the graph signal processing domain historically, but it involves intensive labors to manually determine suitable prescribed spectral models and prohibitive difficulty to fit their potential combinations and compositions. To address these challenges, this paper proposes the deep Graph Spectral Evolution Network (GSEN) for modeling the graph topology evolution problem by the composition of newly-developed generalized graph kernels. GSEN can effectively fit a wide range of existing graph kernels and their combinations and compositions with the theoretical guarantee and experimental verification. GSEN has outstanding efficiency in terms of time complexity ($O(n)$) and parameter complexity ($O(1)$), where $n$ is the number of nodes of the graph. Extensive experiments on multiple synthetic and real-world datasets have demonstrated outstanding performance.",
        "Introduction": "  INTRODUCTION Understanding the mechanism of graph generation and evolution has significant importance in many applications, such as brain simulation, mobility network simulation, and social network modeling and intervention. Beyond the traditional methods from network science domain, graph generation and evolution have been attracting fast increase attention by deep graph generative models due to their great potential of learning the underlying known generation and evolution mechanism in an end-to-end fashion Simonovsky & Komodakis (2018). Based on deep graph generative models, the graph generation problem is considered as decoding a graph based on latent variables following some underlying distribution while graph evolution can be modeled as a mapping to a target graph topology given a source graph topology. Graph evolution based on deep graph learning is a very challenging problem and is still in its nascent stage because of the extremely high-dimension of the data. An ideal model should be able to capture both the local and global characteristics of source graph and be able to determine the existence or weight of potential edges for each pair of nodes. Models that are both expressive and efficient are in urgent demand. The domain which has investigated graph evolution for a long time is graph signal processing, where well-defined mathematical framework and various techniques such as graph wavelets and kernels that abstract the graph process in frequency domain have been hypothesized and verified in many applications. For example, Kunegis et al. have demonstrated that triangle-closing kernels fit very well to the evolution of graph spectrum during the \"befriending process\" in some social networks ( Leskovec et al. (2008) ). Most recently, neuroscience researchers found that the functional connec- tivity shares the same graph Fourier basis with structural connectivity in several special situations. Although graph signal processing allows powerful and concise models to characterize many graph evolution processes, they require to first determine the potentially suitable type of graph kernel and then fit the parameters of it. However, this raises up serious challenges: First, it is difficult to discover or select suitable kernel types for various applications. Graph kernels are proposed based on the analyses and abstraction of the prior knowledge on various graph phenomena. But until now, quite a lot of phenomena have not yet been analyzed or interpreted by human. For example, it is unclear whether and how the spectrum of resting-state functional connectivity transforms into task- specific functional connectivity in human brain  Hermundstad et al. (2013) . Moreover, for many Under review as a conference paper at ICLR 2020 sophisticated phenomena, the graph process typically involves the combination and composition of multiple graph processes corresponding to multiple kernels. For example, the evolution of social networks might involve not only the triangle closing process (i.e., two friends of a person tend to be friends) by triangle-closing kernels, but also could include the behavior diffusion process which can be characterized by diffusion kernels. Also, the involvement of different kernel process might be simultaneous or sequential, and hence prohibitively difficult to manually determine or combinatorially optimize. To address these challenges, this paper proposes a novel end-to-end model named Deep Graph Spectral Evolution Network (GSEN) to optimally fit the graph evolution process by the composition of newly-developed generalized graph kernels. The generalized graph kernels widely cover existing graph kernels as well as their combination and composition as special cases, and hence are able to fit them with outstanding expressiveness. In addition to this high expressiveness, GSEN is also highly concise in terms of small parameter complexity and time complexity for training. Specifically, the number of parameters and memory complexity of GSEN are independent of the graph size while the time complexity for the training of GSEN is linear to the graph size. This largely outperforms the state-of-the-art, which typically requires O(n 2 ) time complexity and memory complexity. Extensive experiments on several synthetic datasets and multiple real-world datasets in two domains have been conducted. The results demonstrate the superior accuracy of our GSEN over existing deep generative models for graph transformation and models based on graph signal processing. The higher efficiency of GSEN compared to existing deep generative models has also been verified.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a lightweight Transformer architecture, Group-Transformer, for character-level language modeling. Group-Transformer is inspired by group convolution approaches and reduces parameters and calculations in the proposed modules. To compensate for the information loss of inter-group correlations, two inter-group operations are added to the group attention layer and group feed-forward layer. Experiments on two benchmark datasets, enwik8 and text8, show that Group-Transformer with 6M parameters outperforms all LSTM-based models with under 35M parameters and has better performance than Transformers with a comparable number of parameters.",
        "Abstract": "Character-level language modeling is an essential but challenging task in Natural Language Processing. \nPrior works have focused on identifying long-term dependencies between characters and have built deeper and wider networks for better performance. However, their models require substantial computational resources, which hinders the usability of character-level language models in applications with limited resources. In this paper, we propose a lightweight model, called Group-Transformer, that reduces the resource requirements for a Transformer, a promising method for modeling sequence with long-term dependencies. Specifically, the proposed method partitions linear operations to reduce the number of parameters and computational cost. As a result, Group-Transformer only uses 18.2\\% of parameters compared to the best performing LSTM-based model, while providing better performance on two benchmark tasks, enwik8 and text8. When compared to Transformers with a comparable number of parameters and time complexity, the proposed model shows better performance. The implementation code will be available.",
        "Introduction": "  INTRODUCTION Character-level language modeling has become a core task in the field of natural language processing (NLP) such as classification ( Zhang et al., 2015 ), sequence tagging ( Guo et al., 2019a ), question answering ( He & Golub, 2016 ), and recognition ( Baek et al., 2019 ;  Hwang & Sung, 2016 ), with its simplicity on generating text and its adaptability to other languages. Along with the development of deep learning in NLP, using recurrent neural networks (RNNs) have been a standard way to solve the problem for many years. Recently, however, a new architecture, Transformer ( Vaswani et al., 2017 ), have shown promise in addressing this problem and have achieved breakthroughs in general language modeling ( Al-Rfou et al., 2019 ;  Dai et al., 2019 ). Though this technique has achieved incredible successes, it has led to the huge size of Transformer- based models due to building deeper and wider networks. Transformer-XL ( Dai et al., 2019 ) and GPT-2 ( Radford et al., 2019 ), for instance, contain 277M and 1542M parameters, respectively. This trend toward a large size model for performance is not suitable for edge device applications, which require small memory sizes, such as optical character reader (OCR) and speech to text (STT), and for auto-correction and auto-completion applications that need fast real-time responsiveness. To tackle this issue, choosing an appropriately efficient strategy becomes more crucial, especially in the real-world application which requires not only good performance but a lightweight model. In this paper, we introduce a lightweight transformer for character-level language modeling. Our method is one of the factorization methods in that it separates the standard linear layer in trans- former architecture using group-wise linear operation and makes sparse connectivity between linear transformations. The proposed model is referred to as Group-Transformer since it is inspired by the group convolution approaches ( Zhang et al., 2018 ;  Sandler et al., 2018 ) that have effectively compressed huge image processing models for usability on mobile devices. While the group strategy reduces parameters and calculations in the proposed modules, its mutually exclusive calculation for the multiple groups compromises performance, caused by the information loss of inter-group correlations. To compensate for this problem, we added two inter-group opera- tions that share a common feature over groups for the group attention layer and linking features in different groups for the group feed-forward layer. By modeling the inter-group information flows, Group-Transformer becomes performant as well as lightweight. We conducted extensive experiments on two benchmark datasets, enwik8 and text8, and found that Group-Transformer with 6M parameters outperformed all LSTM-based models with under 35M parameters. Furthermore, Group-Transformer shows better performance when compared against Transformers with a comparable number of parameters. We provide further analysis to identify the contributions of our proposed modules in detail. To the best of our knowledge, Group-Transformer is the first attempt to build a lightweight Transformer with the group strategy.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an analytical framework based on optimal transport theory to interpret the behavior of deep neural networks (DNNs). The framework uses Wasserstein divergence (W-distance) to measure the difference between the distribution of any layer and the target distribution. The paper provides theoretical analysis and experiments to demonstrate that the W-distance between the distribution of any layer and the target distribution decreases along the depth of a DNN, and that the W-distance between the distribution in an iteration and the target distribution decreases across the training iterations when introducing a loss in the layer. The proposed framework provides a different view of understanding and interpreting neural networks.",
        "Abstract": "Deep neural networks (DNNs) have achieved unprecedented practical success in many applications.\nHowever, how to interpret DNNs is still an open problem.\nIn particular, what do hidden layers behave is not clearly understood. \nIn this paper, relying on a teacher-student paradigm, we seek to understand the layer behaviors of DNNs by ``monitoring\" both across-layer and single-layer distribution evolution to some target distribution in the training. Here, the ``across-layer\" and ``single-layer\" considers the layer behavior \\emph{along the depth} and  a specific layer \\emph{along training epochs}, respectively. \nRelying on optimal transport theory, we employ the Wasserstein distance ($W$-distance)  to measure the divergence between the layer distribution and the target distribution. \nTheoretically, we prove that i) the $W$-distance of across layers to the target distribution tends to decrease along the depth. ii) the $W$-distance of a specific layer to the target distribution tends to decrease along training iterations. iii) \nHowever, a deep layer is not always better than a shallow layer for some samples. Moreover, our results helps to analyze the stability of layer distributions and explains why auxiliary losses helps the training of DNNs. Extensive experiments on real-world datasets justify our theoretical findings.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have been successfully applied in computer vision, such as image classification ( Chen et al., 2019b ;  Hsu et al., 2019 ), image generation ( Cao et al., 2019 ;  Brock et al., 2019 ;  Chrysos et al., 2019 ) and speech recognition ( Yeh et al., 2019 ;  Chen et al., 2019a ). Despite their success, the internal mechanism of DNNs is still a black box. In particular, understanding what do hidden layers do and how to achieve remarkable performance remain persistently elusive. To answer these questions, we seek to understand across-layer and single-layer behavior. For the across-layer behavior, we study the learning process by measuring the W-distance between the distribution of any layer and the target distribution. Recently, most methods only focus on final predictions in different tasks ( He et al., 2016 ). Due to the end-to-end training, interpreting each intermediate layer behaviors of a DNN, which, however, is still not clear. To provide interpretability, existing works try to produce a single prediction and observe the classification performance of each layer ( Papernot & McDaniel, 2018 ;  Szegedy et al., 2013 ;  Kaya et al., 2019 ). For example, ( Alain & Bengio, 2016 ) experimentally observe that the linear separability of features increases monotonically along the depth of a DNN. Unfortunately, there is no theoretical analysis to support the experimental finding. To understand the learning process of DNNs, one can explore the across-layer behavior by monitoring how the distributions propagate across different layers. However, how to open the internal mechanism of DNNs and investigate the across-layer behavior of a DNN remains an open question. For the single-layer behavior, we seek to measure the W-distance between the distribution in an iteration and the target distribution. It is important to analyze the distributional stability of one layer by measuring the change of the distributions. Recently, the distributional stability of a deep neural network attracts extensive attention ( Santurkar et al., 2018 ). Some studies try to visualize the training behavior of one layer by plotting the mean and variance of features ( Santurkar et al., 2018 ). Unfortunately, the visualizations are subjective and lack of necessary theoretical justifications. To this end, ( Sonoda & Murata, 2019 ) propose a transport analysis method and state that a denoising Autoencoder transports mass to decrease the Shannon entropy of the data distribution. However, the Under review as a conference paper at ICLR 2020 (a) Network forward propagation (b) Label distribution propagation (c) Label distribution method is limited and inflexible for analyzing a general case of deep neural networks. Therefore, it is very necessary and important to develop a new analytical method to interpret the stability of layers. In this paper, we apply optimal transport theory to analyze the behavior of distributions. Specifically, we exploit Wasserstein divergence (W-distance) to measure the difference between the distribution of any layer and the target distribution. By monitoring the change of the W-distance, we are able to study both across-layer and single-layer behaviors. Our contributions are summarized as follows. • We analyze the across-layer behavior and prove that the W-distance between the distribution of any layer and the target distribution decreases along the depth of a DNN. It means that every layer of the network can express the target distribution progressively. • We analyze the single-layer behavior and prove that for a specific layer, the W-distance between the distribution in an iteration and the target distribution decreases across the training iterations when introducing a loss in the layer. • Moreover, we provide experiments and theoretical justifications on these findings. The proposed analytical framework provides a different view of understanding and interpreting neural networks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel deep multi-input model for text classification. The model uses words, characters, and labels as inputs, and is designed to capture hierarchical, high-level, long distance, and global features from texts. The model is inspired by Densenet and uses a new attention mechanism to replace global pooling. Experiments on several public text classification datasets show that the proposed model outperforms all baseline models and has fewer parameters than similar works.",
        "Abstract": "Recently, deep learning has made extraordinary achievements in text classification. However, most of present models, especially convolutional neural network (CNN), do not extract long-range associations, global representations, and hierarchical features well due to their relatively shallow and simple structures. This causes a negative effect on text classification. Moreover, we find that there are many express methods of texts. It is appropriate to design the multi-input model to improve the classification effect. But most of models of text classification only use words or characters and do not use the multi-input model. Inspired by the above points and Densenet (Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017.), we propose a new text classification model, which uses words, characters, and labels as input. The model, which is a deep CNN with a novel attention mechanism, can effectively leverage the input information and solve the above issues of the shallow model. We conduct experiments on six large text classification datasets. Our model achieves the state of the art results on all datasets compared to multiple baseline models.",
        "Introduction": "  INTRODUCTION Text classification, including sentiment analysis ( Pang et al. (2002) ;  Yang & Cardie (2014) ), topic classification ( Tong & Koller (2002) ), and spam detection ( Jindal & Liu (2007) ), is an important subdomain of natural language processing (NLP). It is widely used in public opinion monitoring, advertising filtering, user and product analysis and other fields. In recent years, deep learning has been frequently used in text classification. Compared with traditional methods ( Wang & Manning (2012) ) that rely on hand-crafted features, deep learning methods are employed to learn features from texts by a variety of neural network structures, especially recurrent neural network (RNN) and convolutional neural network (CNN), and attention mechanisms. There are some examples of text classification in  Table 1 . Both the first and the second cases should be classified as the topic of science and technology, and the third case should be classified as the topic of animals. Most of key words or phrases of case1 and case2, which determine the category, are similar (e.g., NASA, moons, and Saturn), but the location of them is different.It can be well captured by CNN, because of the position invariance of CNN. However, the key words or phrases of case3 are scattered. The dependencies between them are long distance. Compared with case1 and case2, these key words or phrases are relatively hard to capture by straightforward or shallow models. And case3 contains a lot of geographical names, which make it possible to be misclassified as geographic topics. By the above examples, we find a problems in present models: These models are shallow, especially CNN. It limits the final classification effect. Specifically, texts have the hierarchical structure which includes characters, words, phrase, sentences, and documents. The shallow models are hard to capture the hierarchical, high-level, long distance or global features form texts, which is why the models are limited.. Nowadays, the amount of text information on the Internet is increasing rapidly. Thus, the model which can cope with texts of various lengths and levels is needed, especially deep models of complicated texts. Compared with the CNN models ( He et al. (2016) ;  Huang et al. (2017) ) of computer vision (CV), the CNN models of text classification are very shallow. It indicates that deeper models can be tried to improve the result of text classification, as they have done in CV. Moreover, most of models utilize words or character as the single input, but the multi-input model is well suited to text classification. The natural language has a variety of carriers or expressions, such as words and characters, which can be related to others. This is very helpful for designing multiple Under review as a conference paper at ICLR 2020 inputs. We think the multi-input model can obtain better results because it provides more useful information. Some related works ( Amplayo et al. (2018) ;  Xue & Li (2018) ) have been done in order to solve this problem, but most of works used multi-input models in some special subdomains such as user sentiment analysis. Those are not the general methods for text classification. In order to solve these problems, we propose a novel deep multi-input model for text classification. Our model uses words, characters, and labels as inputs. These are proved to be beneficial for text classification in previous works. Both the multi-input model and deep model have relatively more parameters and more expensive computing cost, and more effective feature extraction is needed on the multi-input model. Thus, we use a structure that is similar to Dense block of Densenet ( Huang et al. (2017) ) to alleviate the above problems, because it has narrow feature maps and can reduce the numbers of parameters. Moreover, we employ a new attention mechanism, which allows capturing of dependencies without regard to their distance in inputs, to replace the global pooling. It can strengthen feature extraction from several inputs. We conduct extensive experiments on several public text classification datasets. The results show that our model outperforms all baseline models on all datasets, and has fewer parameters in comparison to similar works. Our primary contributions are as follows: 1. We propose a novel multi-input model for text classification. Our model inputs include words, characters, and labels. Most of previous works only use a single input such as the word or use the multi-input model in certain subdomains like user sentiment analysis ( Amplayo et al. (2018) ). 2. Inspired by Densenet, we design a 42-layers network. It is deeper than most of present models. Compared with shallow and straightforward models, our model can extract more global or long-term features, and have fewer parameters. 3. We use a new attention mechanism in our model. It is used to replace the global pooling that commonly used in CNN. Compared with global pooling, our method can effectively utilize inputs information to capture key part more precisely. Compared with using the attention mechanism at the beginning of the model, this method can significantly reduce parameters and computation, because the feature maps of high layers are much smaller than that after embedding.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes ISBNet, a novel architecture combining advantages from both efficient network design and neural architecture search (NAS). ISBNet supports instance-level selective branching mechanism by introducing lightweight SelectionNets, which improves inference efficiency significantly by reducing redundant computation. ISBNet integrates gumbel-softmax to the branch selection process, which enables direct gradient descent optimization and is more tractable than RL-based methods. Experiments show that ISBNet is extremely efficient during inference and successfully selects only vital branches on a per-input basis. With a minor 1.07% accuracy decrease, ISBNet reduces the parameter size and FLOPs by 10x and 11.31x respectively comparing to the NAS searched high-performance architecture DARTS. Furthermore, with a tiny model of 0.57M parameters, ISBNet achieves much better accuracy while with only 8.03% and 30.60% inference time parameter size and FLOPs comparing to the expert-designed efficient network ShuffleNetV2 1.5x.",
        "Abstract": "Recent years have witnessed growing interests in designing efficient neural networks and neural architecture search (NAS). Although remarkable efficiency and accuracy have been achieved, existing expert designed and NAS models neglect the fact that input instances are of varying complexity and thus different amounts of computation are required. Inference with a fixed model that processes all instances through the same transformations would incur computational resources unnecessarily. Customizing the model capacity in an instance-aware manner is required to alleviate such a problem. In this paper, we propose a novel Instance-aware Selective Branching Network-ISBNet to support efficient instance-level inference by selectively bypassing transformation branches of insignificant importance weight. These weights are dynamically determined by a lightweight hypernetwork SelectionNet and recalibrated by gumbel-softmax for sparse branch selection. Extensive experiments show that ISBNet achieves extremely efficient inference in terms of parameter size and FLOPs comparing to existing networks. For example, ISBNet takes only 8.70% parameters and 31.01% FLOPs of the efficient network MobileNetV2 with comparable accuracy on CIFAR-10.",
        "Introduction": "  INTRODUCTION Deep convolutional neural networks (CNNs) ( He et al., 2016 ;  Zoph et al., 2018 ) have revolution- ized computer vision with increasingly larger and more sophisticated architectures. These model architectures have been designed and calibrated by domain experts with rich engineering experi- ence. To achieve good inference results, these models typically comprise hundreds of layers and contain tens of millions of parameters and consequently, consume substantial amounts of compu- tational resources for both training and inference. Recently, there has been a growing interest in efficient network design ( Howard et al., 2017 ;  Iandola et al., 2016 ;  Zhang et al., 2018 ;  Sandler et al., 2018 ) and neural architecture search (NAS) ( Zoph et al., 2018 ;  Real et al., 2018 ;  Liu et al., 2018b ), respectively with the objective of devising network architectures that are efficient during inference and automating the architecture design process. Many efficient architectures have indeed been designed in recent years. SqueezeNet ( Iandola et al., 2016 ) and MobileNet ( Howard et al., 2017 ) substantially reduce parameter size and computation cost in terms of FLOPs on mobile devices. More recent works such as MobileNetV2 ( Sandler et al., 2018 ) and ShuffleNetV2 ( Ma et al., 2018 ) further reduce the FLOPs. It is well recognized that devising these architectures is non-trivial and requires engineering expertise. Automating the architecture design process via neural architecture search (NAS) has attracted in- creasing attention in recent years. Mainstream NAS algorithms ( Zoph & Le, 2016 ;  Zoph et al., 2018 ;  Real et al., 2018 ) search for the network architecture iteratively. In each iteration, an archi- tecture is proposed by a controller, and then trained and evaluated. The evaluation performance is in turn exploited to update the controller. This process is incredibly slow because both the controller and each derived architecture require training. For instance, the reinforcement learning (RL) based controller NASNet ( Zoph et al., 2018 ) takes 1800 GPU days and the evolution algorithm based con- troller AmoebaNet ( Real et al., 2018 ) incurs 3150 GPU days to obtain the best architecture. Many acceleration methods ( Baker et al., 2017 ;  Liu et al., 2018a ;  Bender et al., 2018 ;  Pham et al., 2018 ) have been proposed to accelerate the search process, and more recent works ( Liu et al., 2018b ;  Under review as a conference paper at ICLR 2020 Xie et al., 2018 ;  Wu et al., 2018 ;  Cai et al., 2018 ) remove the controller and instead optimize the architecture selection and parameters together with gradient-based optimization algorithms. While both expert designed and NAS searched models have produced remarkable efficiency and prediction performance, they have neglected one critical issue that would affect inference effi- ciency. The architectures of these models are fixed during inference time and thus not adap- tive to the varying complexity of input instances. However, in real-world applications, there are only a small fraction of input instances requiring deep representations ( Wang et al., 2018 ;  Huang et al., 2017a ). Consequently, expensive computa- tional resources would be wasted if all instances are treated equally. Designing a model with suffi- cient representational power to cover the hard in- stances, and meanwhile a finer-grained control to provide just necessary computation dynamically for instance of varying difficulty is therefore es- sential. In this paper, we propose ISBNet to address the aforementioned issue with its building block Cell as illustrated in  Figure 1 . Following the widely adopted strategy in NAS ( Zoph et al., 2018 ;  Pham et al., 2018 ;  Liu et al., 2018b ;  Xie et al., 2018 ), the backbone network is a stack of L structurally iden- tical cells, receiving inputs from their two previous cells and each cell contains N inter-connected computational Nodes. The architecture of ISBNet deviates from the conventional wisdom of NAS which painstakingly search for the connection topology and the corresponding transformation op- eration of each connection. In ISBNet, each node is instead simply connected to its prescribed preceding node(s) and each connection transforms via a candidate set of B operations (branches). To allow for instance-aware inference control in the branch level, we integrate L lightweight hy- pernetworks SelectionNets, one for each cell to determine the importance weight of each branch. Gumbel-softmax ( Jang et al., 2016 ;  Maddison et al., 2016 ) is further introduced to recalibrate these weights, which enables efficient gradient-based optimization during training, and more importantly, leads to sparse branch selection during inference for efficiency. The contributions of ISBNet can be summarized as follows: • ISBNet is a general architecture framework combining advantages from both efficient net- work design and NAS, whose components are readily customizable. • ISBNet is a novel architecture supporting the instance-level selective branching mechanism by introducing lightweight SelectionNets, which improves inference efficiency significantly by reducing redundant computation. • ISBNet successfully integrates gumbel-softmax to the branch selection process, which en- ables direct gradient descent optimization and is more tractable than RL-based method. • ISBNet achieves state-of-the-art inference efficiency in terms of parameter size and FLOPs and inherently supports applications requiring fine-grained instance-level control. Our experiments show that ISBNet is extremely efficient during inference and successfully selects only vital branches on a per-input basis. In particular, with a minor 1.07% accuracy decrease, ISB- Net reduces the parameter size and FLOPs by 10x and 11.31x respectively comparing to the NAS searched high-performance architecture DARTS ( Liu et al., 2018b ). Furthermore, with a tiny model of 0.57M parameters, ISBNet achieves much better accuracy while with only 8.03% and 30.60% inference time parameter size and FLOPs comparing to the expert-designed efficient network Shuf- fleNetV2 1.5x ( Ma et al., 2018 ). We also conduct ablation studies and visualize the branch selection process to understand the proposed architecture better. The main results and findings are summa- rized in Sec 4.2 and Sec 4.3.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the optimization landscape of neural networks and the success of training neural networks through overparametrization. It is shown that with polynomial activation functions, simple optimization algorithms are guaranteed to find a solution that memorizes training data, even in the mildly overparametrized regime where the number of parameters is only a constant factor larger than the number of training data.",
        "Abstract": "It has been observed \\citep{zhang2016understanding} that deep neural networks can memorize: they achieve 100\\% accuracy on training data. Recent theoretical results explained such behavior in highly overparametrized regimes, where the number of neurons in each layer is larger than the number of training samples. In this paper, we show that neural networks can be trained to memorize training data perfectly in a mildly overparametrized regime, where the number of parameters is just a constant factor more than the number of training samples, and the number of neurons is much smaller.",
        "Introduction": "  INTRODUCTION In deep learning, highly non-convex objectives are optimized by simple algorithms such as stochastic gradient descent. There has been many theoretical analysis for the optimization landscape of neural networks(e.g.,  Brutzkus et al. (2017) ;  Brutzkus & Globerson (2017) ;  Ge et al. (2017b) ;  Wang et al. (2018) ), but even very simple two-layer networks have spurious local optima( Safran & Shamir, 2018 ). In practice, it was observed that neural networks are able to fit the training data perfectly, even when the data/labels are randomly corrupted( Zhang et al., 2017 ). Recently, a series of work ( Du et al. (2019) ;  Allen-Zhu et al. (2019c) ;  Chizat & Bach (2018) ;  Jacot et al. (2018) , see more references in Section 1.2) developed a theory of neural tangent kernels (NTK) that explains the success of training neural networks through overparametrization. Several results showed that if the number of neurons at each layer is much larger than the number of training samples, networks of different architectures (multilayer/recurrent) can all fit the training data perfectly. However, if one considers the number of parameters required for the current theoretical analysis, these networks are highly overparametrized. Consider fully connected networks for example. If a two-layer network has a hidden layer with r neurons, the number of parameters is at least rd where d is the dimension of the input. For deeper networks, if it has two consecutive hidden layers of size r, then the number of parameters is at least r 2 . All of the existing works require the number of neurons r per-layer to be at least the number of training samples n (in fact, most of them require r to be a polynomial of n). In these cases, the number of parameters can be at least nd or even n 2 for deeper networks -much larger than the number of training samples n. Therefore, a natural question is whether neural networks can fit the training data in the mildly overparametrized regime - where the number of parameters is only a constant factor larger than the number of training data. To achieve this, one would want to use a small number of neurons in each layer - n/d for a two-layer network and √ n for a three-layer network.  Yun et al. (2018)  showed such networks have enough capacity to memorize any training data. In this paper we show with polynomial activation functions, simple optimization algorithms are guaranteed to find a solution that memorizes training data.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new algorithm, OPTIMISTIC-AMSGRAD, which combines the ideas of adaptivity, momentum, and optimistic online learning to accelerate the training of deep neural networks. Theoretical analysis of the algorithm is provided, and experiments show that OPTIMISTIC-AMSGRAD improves upon the existing AMSGRAD algorithm in terms of training loss, testing loss, and classification accuracy.",
        "Abstract": "This paper considers a new variant of AMSGrad called Optimistic-AMSGrad. AMSGrad is a popular adaptive gradient based optimization algorithm that is widely used in training deep neural networks. The new variant assumes that mini-batch gradients in consecutive iterations have some underlying structure, which makes the gradients sequentially predictable. By exploiting the predictability and some ideas from Optimistic Online learning, the proposed algorithm can accelerate the convergence and also enjoys a tighter regret bound. We evaluate Optimistic-AMSGrad and AMSGrad in terms of various performance measures (i.e., training loss, testing loss, and classification accuracy on training/testing data), which demonstrate that Optimistic-AMSGrad improves AMSGrad.",
        "Introduction": "  INTRODUCTION Nowadays deep learning has been very successful in numerous applications, from robotics (e.g.,  Levine et al. (2017) ), computer vision (e.g.,  He et al. (2016) ;  Goodfellow et al. (2014) ), reinforcement learning (e.g.,  Mnih et al. (2013) ), to natural language processing (e.g.,  Graves et al. (2013) ). A common goal in these applications is learning quickly. It becomes a desired goal due to the presence of big data and/or the use of large neural nets. To accelerate the process, there are variety of training algorithms proposed in recent years, such as AMSGRAD ( Reddi et al. (2018) ), ADAM ( Kingma & Ba (2015) ), RMSPROP ( Tieleman & Hinton (2012) ), ADADELTA (Zeiler (2012)), and NADAM ( Dozat (2016) ), etc. All the prevalent algorithms for training deep nets mentioned above combine two ideas: the idea of adaptivity from ADAGRAD ( Duchi et al. (2011) ;  McMahan & Streeter (2010) ) and the idea of momentum from NESTEROV'S METHOD ( Nesterov (2004) ) or HEAVY BALL method ( Polyak (1964) ). ADAGRAD is an online learning algorithm that works well compared to the standard online gradient descent when the gradient is sparse. Its update has a notable feature: the effective learning rate is different for each dimension, depending on the magnitude of gradient in each dimension, which might help in exploiting the geometry of data and leading to a better update. On the other hand, NESTEROV'S METHOD or HEAVY BALL Method ( Polyak (1964) ) is an accelerated optimization algorithm whose update not only depends on the current iterate and current gradient but also depends on the past gradients (i.e., momentum). State-of-the-art algorithms like AMSGRAD ( Reddi et al. (2018) ) and ADAM ( Kingma & Ba (2015) ) leverage the ideas to accelerate training neural nets. In this paper, we propose an algorithm that goes further than the hybrid of the adaptivity and momentum approach. Our algorithm is inspired by OPTIMISTIC ONLINE LEARNING (see e.g.  Chiang et al. (2012) ;  Rakhlin & Sridharan (2013a ; b );  Syrgkanis et al. (2015) ;  Abernethy et al. (2018) ). OPTIMISTIC ONLINE LEARNING considers that a good guess of the loss function in each round is available and plays an action by utilizing the guess. By exploiting the guess, algorithms in OPTIMISTIC ONLINE LEARNING can enjoy a smaller regret than the ones without exploiting the guess. We combine the OPTIMISTIC ONLINE LEARNING idea with the adaptivity and the momentum ideas to design a new algorithm - OPTIMISTIC-AMSGRAD. We also provide a theoretical analysis of OPTIMISTIC-AMSGRAD. The proposed algorithm not only adapts to the informative dimensions, exhibits momentum, but also exploits a good guess of the next gradient to facilitate acceleration. We conduct experiments and show that OPTIMISTIC-AMSGRAD improves AMSGRAD in terms of various measures: training loss, testing loss, and classification accuracy on training/testing data over epochs.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the problem of learning a data representation in cases where no meaningful input representation or explicit similarity information exists. We propose a novel neural network architecture that can learn representations when only given the answers to a set of general triplet comparisons. Our approach opens doors into two different worlds: (1) using the power of DNNs to approximately solve an NP hard optimization problem with discrete input, and (2) providing the first, scalable approach for the ordinal embedding problem based on neural networks.",
        "Abstract": "In this paper, we discuss the fundamental problem of representation learning from a new perspective. It has been observed in many supervised/unsupervised DNNs that the final layer of the network often provides an informative representation for many tasks, even though the network has been trained to perform a particular task. The common ingredient in all previous studies is a low-level feature representation for items, for example, RGB values of images in the image context. In the present work, we assume that no meaningful representation of the items is given. Instead, we are provided with the answers to some triplet comparisons of the following form: Is item A more similar to item B or item C? We provide a fast algorithm based on DNNs that constructs a Euclidean representation for the items, using solely the answers to the above-mentioned triplet comparisons. This problem has been studied in a sub-community of machine learning by the name \"Ordinal Embedding\". Previous approaches to the problem are painfully slow and cannot scale to larger datasets. We demonstrate that our proposed approach is significantly faster than available methods, and can scale to real-world large datasets.\n\nThereby, we also draw attention to the less explored idea of using neural networks to directly, approximately solve non-convex, NP-hard optimization problems that arise naturally in unsupervised learning problems.",
        "Introduction": "  INTRODUCTION It has been widely recognized that deep neural networks (DNN) provide a powerful tool for represen- tation learning ( Bengio et al., 2013 ). Representations learned in an unsupervised fashion have been demonstrated to be useful in learning tasks such as classification ( Ranzato et al., 2007 ; 2008;  Hinton & Salakhutdinov, 2008 ;  Hinton et al., 2006 ;  Bengio et al., 2007 ). In the context of supervised learn- ing, representations are typically learned as by-products in neural networks ( Radford et al., 2015 ). For example in image classification, low level representations of inputs (e.g., rgb values) are fed to a network, together with class label information, the network is trained to perform some supervised classification. As a by-product it discovers a condensed data representation in the last hidden layers of the network that turns out to be surprisingly successful for other computer vision tasks such as ob- ject detection or semantic segmentation ( Girshick et al., 2014 ;  Kümmerer et al., 2014 ;  Long et al., 2015 ;  Ren et al., 2015 ). Subsequently, more direct mechanisms have been designed to explicitly learn data representations ( Mensink et al., 2012 ;  Bell & Bala, 2015 ;  Schroff et al., 2015 ). Again the raw data is fed to a network, this time together with some information about the similarity of objects, and then the network is trained to generate a meaningful data representation. Particularly relevant to our work is the field of contrastive representation learning ( Wang et al., 2014 ;  Hoffer & Ailon, 2015 ;  Cheng et al., 2016 ;  Ge, 2018 ;  Arora et al., 2019 ), where similarity information is provided in terms of contrastive triplets of points (x, x + , x − ): for a given point x, the point x + and x − are specifically chosen data points that are similar / dissimilar to x. Such approaches have been extremely successful on image and text data, and they are particularly elegant if the similarity information can be extracted from the raw data in some unsupervised manner. Examples are the case of word embeddings ( Mikolov et al., 2013 ), where words are considered similar if they occur within the same local neighborhood, or text representations ( Logeswaran & Lee, 2018 ), where two subsequent sentences are considered similar, or in computer vision ( Wang & Gupta, 2015 ) where pairs of image patches in subsequent frames of videos are considered similar. There are two main ingredients that seem unavoidable in all these approaches: First, one needs an explicit low level representation of the data to feed as input to the DNN architectures. Secondly, one needs information about the similarity of different data points that can be used to train the network parameters. This information can be provided by class membership labels in supervised approaches, or it is extracted from some unsupervised meta-data - for instance the temporal information for frames of video data. In our work, we investigate the problem of learning a data representation in cases where no mean- ingful input representation or explicit similarity information exists. Instead we assume that we are provided with the answers to a set of general triplet comparisons where, for any arbitrary triplet of items (x i , x j , x k ), we know whether x i is more similar to x j or x k . The problem of learning a representation based on such triplet comparisons is called ordinal embedding ( Agarwal et al., 2007 ;  van der Maaten & Weinberger, 2012 ;  Kleindessner & von Luxburg, 2014 ;  Terada & von Luxburg, 2014 ). This setting can be of advantage in many cases. Consider learning a representation for a large set of movies. There does not exist an obvious informative low level representation that can be used in a straight forward manner in a neural net, yet one can ask triplet comparisons from users of a movie database. An example of a triplet comparison on movies in shown in Figure 1a. Other use-cases also arise in more scientific contexts. For example, psychophysical scaling aims to find the functional relation of a physical stimulus and human perception. For example,  Aguilar et al. (2017)  conducted experiments with triplet comparisons to find out the relation between the angle of a tilted plane and the perceived angle of a human observer. In this study, a number of triplet questions are asked from human observers (see an example of triplet question in Figure 1b). Later, a one-dimensional embedding (representation) is learned for the perceived value of angle. A comprehensive study on the application of ordinal embedding in psychophysics is available at  Haghiri et al. (2019) . We design a novel neural network architecture that can learn representations when we are only given the answers to a set of triplet comparisons and no input representation exists. Our approach opens doors into two different worlds: (1) We use the power of DNNs to approximately solve an NP hard optimization problem with discrete input (point identifiers and binary comparisons). Its widely believed amongst neural net- work practitioners that the non-convex landscape of deep and wide neural networks is primarily free of sub-optimal local minima. This theory is supported under simplified assumptions by vari- ous theoretical findings ( Nguyen & Hein, 2017 ;  Choromanska et al., 2015 ;  Kawaguchi & Bengio, 2019 ;  Kawaguchi et al., 2019 ). These results provide a basis for the hypothesis that deep neural network models offer a tool that can make non-convex, NP-hard optimization problems \"tractable in practice\". In machine learning, this line of thinking is particularly interesting for unsupervised learning prob- lems such as clustering, dimensionality reduction or representation learning, where the typical ob- jective functions are discrete, often NP hard, or non-convex. The standard approach to solve such problems are convex relaxations. Albeit the resulting relaxed problem then can be solved exactly, there often does not exist any provable guarantee that relates the solution of the relaxed problem to the solution of the original problem. We believe that the ability of DNN's to directly (approximately) solve non-convex optimization functions provides an attractive alternative and has not really been explored to solve such problems in ML: very little emphasis has been placed in approaches that use neural networks not as learning machines but rather as toolboxes to solve optimization problems. The same point of view might be valuable for general, even discrete optimization problems. In our paper we demonstrate that DNNs can be successfully applied to such a problem. It will be interesting to see how more generic architectures can be designed to solve more generic discrete, non-convex, NP-Hard optimization problems. (2) We provide the first, scalable approach for the ordinal embedding problem based on neural networks. The problem of ordinal embedding has been studied extensively in the machine learning literature ( Agarwal et al., 2007 ;  van der Maaten & Weinberger, 2012 ;  Kleindessner & von Luxburg, 2014 ;  Terada & von Luxburg, 2014 ;  Jain et al., 2016 ). Optimizing the ordinal embedding objective by traditional means is notoriously difficult (either algorithms are very slow or lead to unsatisfactory results), and for computational reasons it is close to impossible to embed more than 10000 items. Thanks to the fast parallel computations of DNN training, our approach runs significantly faster than traditional methods (see Subsection 4.4).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel channel-level neural architecture search (CNAS) method that searches for a good architecture by gradient descent. CNAS uses the existing shape of search space (e.g., NASNet), but performs macro search. The resulting model is sparse in terms of channels and can be searched quickly due to its simplicity. Experiments on CIFAR-10 show that CNAS achieves 2.28% test error with 4.6 million parameters and autoaugment in 1.1 GPU days.",
        "Abstract": "There is growing interest in automating designing good neural network architectures. The NAS methods proposed recently have significantly reduced architecture search cost by sharing parameters, but there is still a challenging problem of designing search space. We consider search space is typically defined with its shape and a set of operations and propose a channel-level architecture search\\,(CNAS) method using only a fixed type of operation. The resulting architecture is sparse in terms of channel and has different topology at different cell. The experimental results for CIFAR-10 and ImageNet show that a fine-granular and sparse model searched by CNAS achieves very competitive performance with dense models searched by the existing methods.",
        "Introduction": "  INTRODUCTION Nowadays, deep neural networks (DNNs) are used extensively and successfully in many fields and applications such as computer vision, speech recognition, machine translation, and automated vehicles. Designing DNNs often requires significant architecture engineering, a large amount of trial and error by experts. Although transfer learning is widely used to save the efforts required for designing good architectures of DNNs from scratch, it is not always possible to use. Recently, there is growing interest in automating designing good neural network architectures ( 30 ;  31 ;  20 ;  24 ;  21 ;  15 ;  3 ;  28 ;  14 ;  2 ;  22 ;  7 ;  27 ;  4 ;  29 ;  10 ). Most of them can be categorized into reinforcement learning-based (RL) methods, evolutionary algorithm-based (EV) methods, hypernetwork-based (HY) methods, and gradient-based (GR) methods, in terms of the search algorithm. RL methods ( 30 ;  31 ;  20 ;  24 ) use a controller model that enumerates a bunch of candidate models, which are trained for a fixed number of epochs from scratch, and then, is updated using the validation accuracies of the candidate models evaluated on a validation set. To reduce the search space of candidate models, some of them ( 31 ;  20 ) assume each model is composed of multiple convolutional layers called cells having the same architecture and focuses on searching for the best cell architecture. For example, in NASNet ( 31 ), a cell is composed of five blocks, and each block composed of two operations, which are selected among a set of various convolution and pooling operations by the controller model. To reduce the search space, NASNet also transfers the learned architecture for a small dataset (e.g., CIFAR-10) to a large dataset (e.g., ImageNet). To optimize an architecture with less amount of computation, ENAS ( 20 ) exploits parameter (weight) sharing, which avoids training each candidate model from scratch by sharing the weights of candidate models. It constructs a large computational graph, where each subgraph represents the architecture of a candidate model, and the controller model is trained to search for a subgraph corresponding to a good candidate model. EV methods ( 23 ;  1 ;  12 ;  17 ;  26 ;  21 ;  15 ) also have been extensively studied. AmoebaNet ( 21 ) uses the same search space with NASNet, but searches a good cell architecture based on evolutionary algorithm instead of RL controller. The population is initialized with models with random architectures, and some models are sampled from the population. The model with the highest validation fitness within the samples is selected as the parent (i.e., exploitation), and a child having a mutation in terms of operations and skip connections is constructed from the parent (i.e., exploration). Hierarchical NAS ( 15 ) uses hierarchical representation for cell architecture where smaller graph motifs are used as building blocks to form larger motifs, instead of flat representation. Unfortunately, most of RL and EV methods, except ENAS, require an enormous amount of computing power for training thousands Under review as a conference paper at ICLR 2020 of child models. They usually need hundreds or thousands of GPU days for architecture search, which is almost impossible for a typical machine learning practitioner. HY methods and GR methods avoid such a large cost of architecture search by sharing parameters as in ENAS ( 20 ). HY methods ( 3 ;  28 ) bypass fully training candidate models by instead training an auxiliary model, a HyperNet ( 8 ), to dynamically and directly generate the weights of a candidate model. SMASH ( 3 ) generates an architecture of an entire network (i.e., macro search) in terms of the hyperparameters of filters (e.g., number, size) with fixing the type of operation in the HyperNet space, while GHN ( 28 ) generates an architecture of a cell (i.e., micro search) in terms of operations in the NAS search space. GR methods ( 22 ;  7 ;  27 ;  4 ;  29 ;  10 ) do not rely on controllers, evolutionary algorithm, and hypernet- works, but exploit gradient descent on network architectures, which can significantly improve the speed of NAS. They basically relax the search space to be continuous, so that the architecture can be optimized with respect to its validation set performance by gradient descent. Here, search space corresponds to a parent network, and a child network (subgraph) can be derived from the parent network by gradient descent. Most of GR methods focus on searching a good cell architecture in terms of operations and repeating the same architecture as in NASNet. After architecture search, they should usually re-train the candidate architecture snapshot from scratch using the training set due to inconsistency between the performance of derived child networks and converged parent networks. As described above, one of the major trends in NAS is exploiting the concept of parameter sharing through hypernetwork or gradient descent in order to reduce the cost (i.e., GPU days) of NAS. By parameter sharing, HY and GR methods can automatically optimize an architecture that can achieve the state-of-the-art performance on CIFAR-10 and ImageNet just within a few days (as summarized in  Table 4 ). However, there is still a challenging problem in the above architecture search methods: designing search space. In principle, the search space should be large and expressive enough to capture a diverse set of promising candidate models, and at the same time, should be small enough to train with the limited amount of resources and time ( 2 ). Some methods ( 24 ;  15 ;  22 ) addressed that defining search space is extremely important for the performance of neural architecture search. The problem about designing search space may not be solved at once. The search space of the existing NAS methods is typically defined with a shape of the overall network and a set of operations such as identity, normal convolution, separable convolution, average pooling, and max pooling. Many of them follow the NASNet search space for the shape of the network (i.e., stacking cells) and define their own set of operations. Since the number of possible types of operations for search space is limited due to the search cost, the set of operations used itself may have a large impact on the performance of architecture search. In this paper, we investigate the possibility of achieving competitive performance with the state-of-the- art architecture search methods with using a fixed type of operation. To achieve such a performance, we focus on the sparsity of a model. A candidate model in the existing methods has multiple types of operations connected with each other via skip connections, and each operation takes the entire feature maps (called channel) of certain previous nodes or cells as input and returns its entire resulting channels as output. Thus, the candidate model can be regarded as a dense model in term of input and output channels of the operations. We propose a channel-level neural architecture search (CNAS) method that regards channels as vertices and a single fixed operation as edges and searches for a good architecture by gradient descent. The resulting model is sparse in terms of channels. CNAS uses the existing shape of search space (e.g., NASNet), but performs macro search. Thus, the resulting architecture has different topology at different cells. In CNAS, the final sparse architecture can be searched quickly due to its simplicity, and at the same time, can compensate for the disadvantage of using homogeneous operation due to its sparsity. For CIFAR-10, CNAS searches for the architecture in 1.1 GPU days, which achieves 2.28% test error with 4.6 million parameters and autoaugment. The rest of the paper is organized as follows. Section 2 explains our method CNAS. Section 3 shows the experimental results, and Section 4 summarizes the characteristics of related methods. Section 5 concludes this paper.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a two-stage method for recognizing mathematical expressions (MEs) from real scenes. The method decouples the feature extraction process and the translation process, using YOLOv3 to locate and classify symbols and a seq2seq model to translate the feature vectors into LaTeX strings. The two-stage approach has better generalization ability than the end-to-end method and achieves higher recognition rate. Additionally, a method to automatically generate MEs images with position and classification information of the symbols is proposed. This paper is the first to use a seq2seq model to solve the structural analysis problem for MEs recognition and may accelerate progress in machine recognition of other two-dimensional languages.",
        "Abstract": "\nAlthough mathematical expressions (MEs) recognition have achieved great progress, the development of MEs recognition in real scenes is still unsatisfactory. Inspired by the recent work of neutral network, this paper proposes a novel two-stage approach which takes a printed mathematical expression image as input and generates LaTeX sequence as output. In the first stage, this method locates and recognizes the math symbols of input image by object detection algorithm. In the second stage, it translates math symbols with position information into LaTeX sequences by seq2seq model equipped with attention mechanism. In particular, the detection of mathematical symbols and the structural analysis of mathematical formulas are carried out separately in two steps, which effectively improves the recognition accuracy and enhances the generalization ability. The experiment demonstrates that the two-stage method significantly outperforms the end-to-end method. Especially, the ExpRate(expression recognition rate) of our model is 74.1%, 20.3 percentage points higher than that of the end-to-end model on the test data that doesn’t come from the same source as training data.",
        "Introduction": "  INTRODUCTION Mathematical expressions (MEs) play an essential role in math, physics and many other fields. Recognizing mathematical expressions is receiving increasing attentions for application in digiti- zation and retrieval of printed documents. The process of recognizing mathematical expressions is to convert mathematical expressions into LaTeX strings, which includes three stages: symbol segmentation, symbol recognition and structural analysis. We usually divide recognition of MEs into handwritten and printed domains. In the domain of printed MEs, researchers face three chal- lenges( Anderson (1967) ;  Belaid & Haton (1984) ): the complicated two-dimensional structures, var- ious styles of images in printed input and strong dependency on contextual information. Three major problems are involved in MEs recognition ( Zanibbi et al. (2012) ;  Mouchre et al. (2016) ): symbol segmentation, symbol recognition, structural analysis. These problems can be solved sequentially or globally.  Deng et al. (2016a)  proposed an end-to-end formula recognition method for images generated directly from LaTeX code. For their method, a CNN is applied to ex- tract visual features from the input images and then every row is encoded using a recurrent neural network (RNN). These encoded features are then used by an RNN decoder with a visual attention mechanism to produce final outputs. Otherwise, It is called non-homologous test data. The result demonstrated that the model had good performance on homologous test data. However, in practice we found that the model had poor performance on non-homologous test data, that is, the generalization ability of the above model is very weak. In real scenes, the images may be of great variety, like a wide range of backgrounds, as is shown in  Figure 1 . It is impossible to predict all input cases and the test data may diverge from what the system has seen before. Therefore, the method above is not appropriate to be applied to recognize images from real scenes. It seems a necessary task to design a model that has strong generalization ability to recognize real MEs images. To improve the generalization ability, we decouple the feature extraction process and the translation process. In the feature extraction process, we use YOLOv3( Redmon & Farhadi, 2018 ) to locate and classify the symbols of images. Then the class and location information of each symbol is vectorized, which is used as input for the seq2seq model and translated into LaTeX strings. The two-stage approach has several benefits. Firstly, changes in the input images styles would have no influence on the encoder-decoder model, and YOLOv3 has good generalization ability( Redmon et al., 2015 ;  Redmon & Farhadi, 2018 ), so the two-stage method would have better generalization ability than the end-to-end method( Deng et al., 2016b ). Secondly, the feature vectors composed by position and classification information are much more concise than the feature vectors extracted directly by the convolutional layers( Deng et al., 2016b ), which are easier to learn and get higher recognition rate. The main contributions of this paper can be summarized as: (1) A two-stage method for MEs recognition is proposed to decouple the feature extraction process and the translation process, which has better generalization ability and achieve better accuracy. (2) By concatenating position information and classification information into feature vectors, we successfully translate symbols with position information into LaTeX strings by the se- q2seq model. To the best of our knowledge, we use the seq2seq model to solve structural analysis problem for the first time and achieve satisfactory results, which may accelerate progress in machine recognition of other two-dimensional languages. (3) We propose a method to automatically generate MEs images with position and classifica- tion information of the symbols, which avoid expensive manual annotation.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a method, Uncertainties of Features in Each Layer (UFEL), for detecting out-of-distribution (OOD) samples in deep neural networks (DNNs). UFEL extracts the uncertainties of features close to the input and output layers and combines them for detecting OOD samples. Experiments demonstrate that UFEL can obtain state-of-the-art performance in several datasets and models, and is robust to hyperparameters.",
        "Abstract": "In this paper, we tackle the problem of detecting samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples, in classification. Many previous studies have attempted to solve this problem by regarding samples with low classification confidence as OOD examples using deep neural networks (DNNs). However, on difficult datasets or models with low classification ability, these methods incorrectly regard in-distribution samples close to the decision boundary as OOD samples. This problem arises because their approaches use only the features close to the output layer and disregard the uncertainty of the features. Therefore, we propose a method that extracts the uncertainties of features in each layer of DNNs using a reparameterization trick and combines them. In experiments, our method outperforms the existing methods by a large margin, achieving state-of-the-art detection performance on several datasets and classification models. For example, our method increases the AUROC score of prior work (83.8%) to 99.8% in DenseNet on the CIFAR-100 and Tiny-ImageNet datasets.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have achieved high performance in many classification tasks such as image classification ( Krizhevsky et al., 2012 ;  Simonyan & Zisserman, 2014 ), object detection ( Lin et al., 2017 ;  Redmon & Farhadi, 2018 ), and speech recognition ( Hinton et al., 2012 ;  Hannun et al., 2014 ). However, DNNs tend to make high confidence predictions even for samples that are not drawn from the training distribution, i.e., out-of-distribution (OOD) samples ( Hendrycks & Gimpel, 2016 ). Such errors can be harmful to medical diagnosis and automated driving. Because it is not generally possible to control the test data distribution in real-world applications, OOD samples are inevitably included in this distribution. Therefore, detecting OOD samples is important for ensuring the safety of an artificial intelligence system ( Amodei et al., 2016 ). There have been many previous studies ( Hendrycks & Gimpel, 2016 ;  Liang et al., 2017 ;  Lee et al., 2017 ;  DeVries & Taylor, 2018 ;  Lee et al., 2018 ;  Hendrycks et al., 2018 ) that have attempted to solve this problem by regarding samples that are difficult to classify or samples with low classification confidence as OOD examples using DNNs. Their approaches work well and they are computation- ally efficient. The limitation of these studies is that, when using difficult datasets or models with low classification ability, the confidence of inputs will be low, even if the inputs are in-distribution samples. Therefore, these methods incorrectly regard such in-distribution samples as OOD samples, which results in their poor detection performance ( Malinin & Gales, 2018 ), as shown in  Figure 1 . One cause of the abovementioned problem is that their approaches use only the features close to the output layer and the features are strongly related to the classification accuracy. Therefore, we use not only the features close to the output layer but also the features close to the input layer. We hypothesize that the uncertainties of the features close to the input layer are the uncertainties of the feature extraction and are effective for detecting OOD samples. For example, when using convolutional neural networks (CNNs), the filters of the convolutional layer close to the input layer extract features such as edges that are useful for in-distribution classification. In other words, in- distribution samples possess more features that convolutional filters react to than OOD samples. Therefore, the uncertainties of the features will be larger when the inputs are in-distribution samples. Another cause of the abovementioned problem is that their approaches disregard the uncertainty of the features close to the output layer. We hypothesize that the uncertainties of the latent features close Under review as a conference paper at ICLR 2020 Baseline ( Hendrycks & Gimpel, 2016 ) UFEL (ours) max softmax probability Baseline UFEL (ours) degree of uncertainty to the output layer are the uncertainties of classification and are also effective for detecting OOD samples. For example, in-distribution samples are embedded in the feature space close to the output layer to classify samples. In contrast, OOD samples have no fixed regions for embedding. Therefore, the uncertainties of the features of OOD samples will be larger than those of in-distribution samples. Based on the hypotheses, we propose a method that extracts the Uncertainties of Features in Each Layer (UFEL) and combines them for detecting OOD samples. Each uncertainty is easily estimated after training the discriminative model by computing the mean and the variance of their features using a reparameterization trick such as the variational autoencoder ( Kingma & Welling, 2013 ) and variational information bottleneck ( Alemi et al., 2016 ; 2018). Our proposal is agnostic to the model architecture and can be easily combined with any regular architecture with minimum modifications. We visualize the maximum values of output probability and the combined uncertainties of the latent features in the feature space of the penultimate layer in  Figure 1 . The combined uncertainties of the features discriminate the in-distribution and OOD images that are difficult to classify. For example, although the images that are surrounded by the red line are in-distribution samples, they have low maximum softmax probabilities and could be regarded as OOD samples in prior work. Meanwhile, their uncertainties are smaller than those of OOD samples and they are regarded as in-distribution samples in our method. In experiments, we validate the hypothesis demonstrating that each uncertainty is effective for de- tecting OOD examples. We also demonstrate that UFEL can obtain state-of-the-art performance in several datasets including CIFAR-100, which is difficult to classify, and models including LeNet5 with low classification ability. Moreover, UFEL is robust to hyperparameters such as the number of in-distribution classes and the validation dataset.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Adversarial Symmetric GAN (AS-GAN), a method which improves the robustness of discriminator in Generative Adversarial Networks (GANs) by introducing adversarial training on real samples into the classic GANs training framework. This method reduces the adversarial noise contained in the gradient, regularizes the capability of the discriminator, and improves training stability and convergence. Results from image generation tasks with DCGAN and ResNet architectures show consistent improvement of training stability and acceleration of convergence.",
        "Abstract": "Generative adversarial networks have achieved remarkable performance on various tasks but suffer from sensitivity to hyper-parameters, training instability, and mode collapse. We find that this is partly due to gradient given by non-robust discriminator containing non-informative adversarial noise, which can hinder generator from catching the pattern of real samples. Inspired by defense against adversarial samples, we introduce adversarial training of discriminator on real samples that does not exist in classic GANs framework to make adversarial training symmetric, which can balance min-max game and make discriminator more robust. Robust discriminator can give more informative gradient with less adversarial noise, which can stabilize training and accelerate convergence. We validate the proposed method on image generation tasks with varied network architectures quantitatively. Experiments show that training stability, perceptual quality, and diversity of generated samples are consistently improved with small additional training computation cost.",
        "Introduction": "  INTRODUCTION Generative adversarial networks (GANs) have been applied successfully in various research fields such as natural image modeling ( Radford et al., 2015 ), image translation ( Isola et al., 2016 ;  Zhu et al., 2017 ), cross-modal image generation ( Dash et al., 2017 ), image super-resolution ( Ledig et al., 2016 ), semi-supervised learning ( Odena, 2016 ) and sequential data modeling ( Mogren, 2016 ;  Yu et al., 2016 ). Different from explicit density estimation based models ( Kingma et al., 2014 ;  Oord et al., 2016 ;  Hinton, 2012 ), GANs are implicit generative models with two neural networks playing min-max game to find a map from random noise to target distribution, in which the generator tries to generate fake samples to fool discriminator and the discriminator tries to distinguish them from real samples ( Goodfellow et al., 2014 ). In original GANs formula, optimal discriminator measures the Jensen-Shannon divergence between real data distribution and generated distribution. The discrep- ancy measure can be generalized to f-divergence ( Nowozin et al., 2016 ) or replaced by earth-mover distance ( Arjovsky et al., 2017 ). Despite the success, GANs are notoriously difficult to train( Kodali et al., 2018 ;  Arjovsky & Bottou, 2017 ), which are very sensitive to hyper-parameters. When the support of these two distributions are approximately disjoint, gradient given by discriminator with standard objective may vanish, and training becomes unstable ( Arjovsky et al., 2017 ). More seri- ously, generated distribution can fail to cover the whole data distribution and collapse to a single mode in some cases ( Dumoulin et al., 2017 ;  Che et al., 2016 ). The condition of discriminator determines the training stability and performance to a great extent. On the one hand, representation capacity of discriminator realized by a neural network is not infinite. Meanwhile, the discriminator is usually not optimal to measure true discrepancy when trained in an alternative manner practically. On the other hand, discriminator as a classifier is also vulnerable to adversarial samples (Appendix D): benign samples added by imperceptible perturbation can mis- lead classifier to give wrong prediction ( Szegedy et al., 2014 )( Goodfellow et al., 2015 ). Adversarial samples can be easily crafted by gradient-based method such as Fast Gradient Sign Method (FGSM) ( Goodfellow et al., 2015 ) or Basic Iterative Method (BIM) ( Kurakin et al., 2017 ). It should be noted that the gradient given by discriminator that guides update of the generator is exactly the same as gradient used to craft adversarial samples of the discriminator. In other words, the gradi- ent contains uninformative adversarial noise which is imperceptible but can mislead the generator. However, generator can still generate meaningful samples in classic GANs training procedure. This Under review as a conference paper at ICLR 2020 is because discriminator is adversarially trained with diverse generated fake samples. Nevertheless, adversarial training on real samples does not exist in classic training framework. As a consequence, training will become unstable when generated distribution approximates target distribution because the gradient given by non-robust discriminator around real samples contains more adversarial noise. To this end, we introduce adversarial training on real samples into classic GANs training framework to further improve the robustness of discriminator, which can reduce adversarial noise contained in gradient. It can be proved by results shown in  Figure 1  empirically that the noise in gradient of adversarially trained discriminator is partly eliminated. Meanwhile, our proposed method can regularize the capability discriminator by performing adversarial training both on real samples and fake samples to alleviate training collapse. We validate the proposed method on image generation tasks with widely adopted DCGAN ( Radford et al., 2015 ) and ResNet ( He et al., 2015 ;  Gulrajani et al., 2017 ) architecture, which shows consistent improvement of training stability and acceleration of convergence. To our best knowledge, this is the first work to consider GANs from the perspective of adversarial samples, besides which we make GANs training scheme symmetric and improve per- formance efficiently with acceptable computation cost. We term the proposed method as adversarial symmetric GAN (AS-GAN).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an efficient hierarchical meta-reinforcement learning (MGHRL) algorithm to tackle complex problems with large state and action spaces or sparse reward settings. MGHRL is built on top of the architecture of PEARL and a two level hierarchy inspired by HAC. Evaluation on simulated robotics environments shows the superiority of MGHRL to state-of-the-art meta-RL and hierarchical RL methods in sparse reward settings. MGHRL focuses on meta learning the overall strategy for different tasks, providing a simpler and better way for meta RL than directly learning the detailed solution.",
        "Abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.",
        "Introduction": "  INTRODUCTION Deep Reinforcement Learning (DRL) has recently shown a great success on a wide range of tasks, ranging from games ( Mnih et al., 2015 ) to robotics control ( Levine et al., 2016 ;  Bengio & LeCun, 2016 ). However, for more complex problems with larger state and action spaces or sparse reward set- tings, traditional DRL methods hardly works. Hierarchical reinforcement learning (HRL) in which multiple layers of policies are trained to learn to operate on different levels of temporal abstraction, has long held the promise to learn such difficult tasks ( Dayan & Hinton, 1992 ;  Parr & Russell, 1997 ;  Barto & Mahadevan, 2003 ). By decomposing a complex problem into subproblems, HRL signifi- cantly reduces the difficulty of solving specific task. Learning multiple levels of policies in parallel is challenging due to non-stationary state transition functions. Recent HRL approaches ( Nachum et al., 2018 ;  Levy et al., 2019 ) use states as goals directly, allowing simple and fast training of the lower layer. Human intelligence is remarkable for their fast adaptation to many new situations using the knowl- edge learned from past experience. However, agents trained by conventional DRL methods men- tioned above can only learn one separate policy per task, failing to generalize to new tasks without additional large amount of training data. Meta reinforcement learning (meta-RL) addresses such problems by learning how to learn. Given a number of tasks with similar structures, meta-RL methods enable agents learn such structure from previous experience on many tasks. Thus when encountering a new task, agents can quickly adapt to it with only a small amount of experience. Most current meta-RL methods leverage experience from previous tasks to adapt to new tasks by directly learn the policy parameters over primitive action space. ( Finn et al., 2017 ;  Rakelly et al., 2019 ). Such approaches suffer from two problems: (i) For complex tasks which requires sophisti- cated control strategies, it would be quite inefficient to directly learn such policy with one nonlinear function approximator and the adaptation to new tasks is prone to be inaccurate. This problem can become more severe in spare reward settings. (ii) When the task distribution is much wider (riding bicycle as meta-train task and riding motorcycle as meta-test task), these methods can hardly be effective since primitive action execution mechanism is entirely different although they may share Under review as a conference paper at ICLR 2020 a similar high-level strategy. Moreover, existing current meta-RL methods perform badly in sparse reward settings, which are quite common in real world. In this paper, we aim at tackling the problems mentioned above by proposing an efficient hierarchical meta-RL method that realizes meta learning high-level goal generation and leaves the learning of low-level policy for independent RL. Intuitively, this is quite similar to how a human being behaves: we usually transfer the overall understanding of similar tasks rather than remember specific actions. Our meta goal-generation framework is built on top of the architecture of PEARL ( Rakelly et al., 2019 ) and a two level hierarchy inspired by HAC ( Levy et al., 2019 ). Our evaluation on several simulated robotics environments (Plappert et al., 2018) shows the superiority of MGHRL to state- of-the-art meta-RL and hierarchical RL methods in sparse reward settings. Generally, our contributions are as follows: • We propose an algorithm that achieves efficient meta reinforcement learning on challenging robotics environments with sparse reward settings and outperforms other leading methods. • Similar to the way humans leverage past experience to learn new complex tasks, our al- gorithm focuses on meta learning the overall strategy for different tasks, which provides a much simpler and better way for meta RL comparing with directly learning the detailed solution. Since we focus on meta goal-generation and leave the low level policy for independent learning, we believe our algorithm can still accelerate the acquisition of new tasks sampled from much wider task distributions. For example, to learn tasks such as riding bicycles and riding a motorcycle, the two primitive action execution mechanism are entirely different but the two learning process still share similar high-level structures. Through meta goal-generation learning, we expect our method can still accelerate the acquisition of such tasks. We leave these for future work to explore.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a Spectral-based Nonlocal (SNL) block as an efficient, simple, and generic component for capturing long-range spatial-temporal dependencies with deep neural networks. The proposed SNL block is a generalization of the classical nonlocal blocks and is more robust and can degrade into the NL and NS with specific assumptions. The proposed SNL and generalized SNL (gSNL) blocks have outperformed other nonlocal blocks across both image and video classification tasks with a clear-cut improvement.",
        "Abstract": "The nonlocal network is designed for capturing long-range spatial-temporal dependencies in several computer vision tasks. Although having shown excellent performances, it needs an elaborate preparation for both the number and position of the building blocks. In this paper, we propose a new formulation of the nonlocal block and interpret it from the general graph signal processing perspective, where we view it as a fully-connected graph filter approximated by Chebyshev polynomials. The proposed nonlocal block is more efficient and robust, which is a generalized form of existing nonlocal blocks (e.g. nonlocal block, nonlocal stage). Moreover, we give the stable hypothesis and show that the steady-state of the deeper nonlocal structure should meet with it. Based on the stable hypothesis,  a full-order approximation of the nonlocal block is derived for consecutive connections. Experimental results illustrate the clear-cut improvement and practical applicability of the generalized nonlocal block on both image and video classification tasks.",
        "Introduction": "  INTRODUCTION Capturing the long-range spatial-temporal dependencies is crucial for the Deep Convolutional Neu- ral Networks (CNNs) to extract discriminate features in vision tasks such as image and video clas- sification. However, the traditional convolution operator only focuses on processing local neighbor- hood at a time. This makes the CNNs need to go deeper with convolutional operations to enlarge the receptive fields, which lead to higher computation and memory. Moreover, going deeper cannot always increase the effective receptive fields due to the Gaussian distribution of the kernel weight (Luo et al. (2016)). To eliminate this limitation, some recent works focus on designing the network architecture with wider and well-designed modules to catch the long-range dependencies such as (Peng et al. (2017), Chen et al. (2017), Zhao et al. (2017)). Although having larger receptive fields, these modules still need to be applied recursively to catch the dependencies of the pairs in large distances. Inspired by the classical non-local means method in image denoising, Wang et al. (2018) proposes the nonlocal neural network which uses the nonlocal (NL) block to concern the \"full-range\" de- pendencies in only one module by exploring the correlations between each position and all other positions. In the NL block, the affinity matrix is first computed to represent the correlations between each position pair. Then the weight means of features are calculated based on the affinity matrix to refine the feature representation. Finally, the residual connection is added to the refined feature map. Due to its simplicity and effectiveness, the nonlocal block has been widely used in image and video classification (Wang et al. (2018); Yue et al. (2018); Tao et al. (2018); Chen et al. (2018)), image segmentation (Huang et al. (2018); Yue et al. (2018); Wang et al. (2018)) and person re-identification (Liao et al. (2018); Zhang et al. (2019)) recently. However, due to the complexity of the affinity matrix, the nonlocal block 1 needs much more com- putational effort and is sensitive to its number and position in the neural network (Tao et al. (2018)). Some works solve the first problem by simplifying the calculation of the affinity matrix such as Huang et al. (2018), He et al. (2019), Yue et al. (2018), Chen et al. (2018). Only a few works try to solve the second problem which limits the robustness of the nonlocal network 2 . Tao et al. (2018) Under review as a conference paper at ICLR 2020 proposes the nonlocal stage (NS) block which concerns the diffusion nature and maintains the same affinity matrix for all the nonlocal units in the NS block. Comparing with the NL block, the NS block is insensitive to the numbers and allows deeper nonlocal structure. However, the deeper nonlocal structure of NS block increases the complexity and do not have a remarkable improvement. In this work, we focus on elaborating a robust nonlocal block which is more flexible when using in the neural network. We prove that the nonlocal operator in the nonlocal block is equivalent to the Chebyshev-approximated fully-connected graph filter with irrational constraints that limits its liberty for learning. To remove these irrational constraints, we propose the Spectral-based Nonlocal (SNL) block which is more robust and can degrade into the NL and NS with specific assumptions. We also prove that the deeper nonlocal structure satisfies the stable hypothesis with the help of steady- state analysis. Based on this hypothesis, we give the full-order approximated spectral nonlocal (gSNL) block which is well-performed for deeper nonlocal structure. Finally, we add our proposed nonlocal blocks into the deep network and evaluate them on the image and video classification tasks. Experiments show that the networks with our proposed blocks are more robust and have a higher accuracy than using other types of nonlocal blocks. To summarize, our contributions are threefold: • We propose a spectral nonlocal (SNL) block as an efficient, simple, and generic component for capturing long-range spatial-temporal dependencies with deep neural networks, which is a generalization of the classical nonlocal blocks. • We propose the stable hypothesis, which can enable the deeper nonlocal structure without an elaborate preparation for both the number and position of the building blocks. We further extend SNL into generalized SNL (gSNL), which can enable multiple nonlocal blocks to be plugged into the existing computer vision architectures with stable learning dynamics. • Both SNL and gSNL have outperformed other nonlocal blocks across both image and video classification tasks with a clear-cut improvement.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a unified generative adversarial framework, NoiGAN, to learn noise-aware knowledge graph embedding. The framework consists of two main components, a noise-aware KGE model to learn robust representations of knowledge and an adversarial learning framework for error detection. The KGE model and error detection model benefit from each other, with the error detection model preparing reliable data for the KGE model to improve the quality of embedding it learns, and the KGE model providing a promising reasoning model for the error detection model to better distinguish noisy triples from the correct one. Experiments demonstrate that NoiGAN is superior to existing state-of-the-art algorithms, with the KGE model and GAN alternately and iteratively boosting performance in terms of both knowledge graph completion and noise detection.",
        "Abstract": "Knowledge graph has gained increasing attention in recent years for its successful applications of numerous tasks. Despite the rapid growth of knowledge construction, knowledge graphs still suffer from severe incompletion and inevitably involve various kinds of errors. Several attempts have been made to complete knowledge graph as well as to detect noise. However, none of them considers unifying these two tasks even though they are inter-dependent and can mutually boost the performance of each other. In this paper, we proposed to jointly combine these two tasks with a unified Generative Adversarial Networks (GAN) framework to learn noise-aware knowledge graph embedding. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms both in regard to knowledge graph completion and error detection. ",
        "Introduction": "  INTRODUCTION Knowledge graph, as a well-structured effective representation of knowledge, plays a pivotal role in many real-world applications such as web search ( Graupmann et al., 2005 ), question answer- ing( Hao et al., 2017 ; Yih et al., 2015), and personalized recommendation ( Zhang et al., 2016 ). It is constructed by extracting information as the form of triple from unstructured text using infor- mation extraction systems. Each triple (h, r, t) represents a relation r between a head entity h and a tail entity t. Recent years have witnessed extensive construction of knowledge graph, such as Freebase ( Bollacker et al., 2008 ), DBPedia ( Auer et al., 2007 ), and YAGO ( Suchanek et al., 2007 ). However, these knowledge graphs suffer from severe sparsity as we can never collect all the infor- mation. Moreover, due to the huge volumes of web resources, the task to construct knowledge graph usually involves automatic mechanisms to avoid human supervision and thus inevitably introduces many kinds of errors, including ambiguous, conflicting and erroneous and redundant information. To address these shortcomings, various methods for knowledge graph refinement have been pro- posed, whose goals can be arguably classified into two categories: (1) knowledge graph completion, the task to add missing knowledge to the knowledge graph, and (2) error detection, the task to iden- tify incorrect triples in the knowledge graph. Knowledge graph embedding (KGE) currently hold the state-of-the-art in knowledge graph completion for their promising results (Bordes et al., 2013;  Yang et al., 2014 ). Nonetheless, they highly rely on high quality training data and thus are lack of robustness to noise (Pujara et al., 2017). Error detection in knowledge graph is a challenging problem due to the difficulty of obtaining noisy data. Reasoning based methods are the most widely used methods for this task (Paulheim, 2017). Without the guidance of noisy data, they detect er- rors by performing reasoning over the knowledge graph to determine the correctness of a triple. A rich ontology information is required for such kind of methods and thus impede its application for real-world knowledge graphs. Existing works consider knowledge graph embedding and error detection independently whereas these two tasks are inter-dependent and can greatly influence each other. On one hand, error de- tection model is extremely useful to prepare reliable data for knowledge graph embedding. On the other hand, high quality embedding learned by KGE model provides a basis for reasoning to iden- tify noisy data. Inspired by the recent advances of generative adversarial deep models ( Goodfellow et al., 2014 ), in this paper, we proposed to jointly combine these two tasks with a unified GAN frame- work, known as NoiGAN, to learn noise-aware knowledge graph embedding. In general, NoiGAN consists of two main components, a noise-aware KGE model to learn robuster representation of Under review as a conference paper at ICLR 2020 knowledge and an adversarial learning framework for error detection. During the training, noise- aware KGE model takes the confidence score learned by GAN as guidance to eliminate the noisy data from the learning process whereas the GAN requires that KGE model continuously provides high quality embedding as well as credible positive examples to model the discriminator and the generator. Cooperation between the two components drives both to improve their capability. The main contributions of this paper are summarized as follows: • We propose a unified generative adversarial framework NoiGAN, to learn noise-aware knowledge graph embedding. Under the framework, the KGE model and error detection model could benefit from each other: the error detection model prepares reliable data for KGE model to improve the quality of embedding it learns, while the KGE model provides a promising reasoning model for the error detection model to better distinguish noisy triples from the correct one. • Our proposed framework can be easily generalized to various KGE models to enhance their ability in dealing with noisy knowledge graph. • We experimentally demonstrate that our new algorithm is superior to existing state-of-the-art algorithms. The KGE model and GAN can alternately and iteratively boost performance in terms of both knowledge graph completion and noise detection.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel machine learning model for graph data called graph neural networks (GNNs). We analyze the neighbor sampling technique to show that a constant number of samples are needed to guarantee the approximation error. We demonstrate that the time complexity is optimal when L = 1 with respect to the error tolerance ε. Through experiments, we show that the approximation error between the exact computation and its approximation rapidly converges to zero. This is the first constant time approximation algorithm for GNNs with a theoretical guarantee in terms of approximation error.",
        "Abstract": "The recent advancements in graph neural networks (GNNs) have led to state-of-the-art performances in various applications, including chemo-informatics, question-answering systems, and recommender systems. However, scaling up these methods to huge graphs such as social network graphs and web graphs still remains a challenge. In particular, the existing methods for accelerating GNNs are either not theoretically guaranteed in terms of approximation error, or they require at least a linear time computation cost. \nIn this study, we analyze the neighbor sampling technique to obtain a constant time approximation algorithm for GraphSAGE, the graph attention networks (GAT), and the graph convolutional networks (GCN). The proposed approximation algorithm can theoretically guarantee the precision of approximation. The key advantage of the proposed approximation algorithm is that the complexity is completely independent of the numbers of the nodes, edges, and neighbors of the input and depends only on the error tolerance and confidence probability. To the best of our knowledge, this is the first constant time approximation algorithm for GNNs with a theoretical guarantee. Through experiments using synthetic and real-world datasets, we demonstrate the speed and precision of the proposed approximation algorithm and validate our theoretical results.",
        "Introduction": "  INTRODUCTION Machine learning on graph structures has various applications such as chemo-informatics ( Gilmer et al., 2017 ), question answering systems ( Schlichtkrull et al., 2018 ), and recommender systems ( Fan et al., 2019 ). Recently, a novel machine learning model for graph data called graph neural networks (GNNs) ( Gori et al., 2005 ;  Scarselli et al., 2009 ;  Kipf & Welling, 2017 ;  Hamilton et al., 2017 ) demonstrated state-of-the-art performances in various graph learning tasks. However, large scale graphs such as social network graphs and web graphs contain billions of nodes, and even a linear time computation cost per iteration is prohibited. Therefore, applying GNNs to huge graphs is challenging. Although  Ying et al. (2018)  succeeded in applying GNNs to a web-scale network using MapReduce, it still requires massive computational resources. There are several node sampling techniques to reduce GNN computation. For example, an empirical neighbor sampling scheme is used to speed up GraphSAGE ( Hamilton et al., 2017 ). FastGCN employs a random layer-wise node sampling ( Chen et al., 2018b ). Huang et al. (2018) further improved FastGCN by using an adaptive sampling technique to reduce the variance of estimators.  Chen et al. (2018a)  proposed a variant of neighbor sampling, which used historical activations to reduce the estimator variance. Overall, the existing sampling techniques for GNNs work well in practice. However, these techniques are either not theoretically guaranteed in terms of approximation error, or thy require at least a linear time computation cost. In this study, we consider the problem of approximating the embedding of one node using GNNs in constant time with maximum precision 1 . We analyze the neighbor sampling technique ( Hamilton et al., 2017 ) to show that a constant number of samples are needed to guarantee the approximation error. It should be noted that the neighbor sampling was introduced as a heuristic method originally, and they did not provide any theoretical guarantees. Specifically, given an error tolerance ε and Under review as a conference paper at ICLR 2020 confidence probability 1 − δ, our analysis shows that the estimateẑ v of the exact embedding z v of a node v such that Pr[ ẑ v − z v 2 ≥ ε] ≤ δ and the estimate ∂zv ∂θ of the exact gradient ∂zv ∂θ of the embedding z v with respect to the network parameters θ, such that Pr[ ∂zv ∂θ − ∂zv ∂θ F ≥ ε] ≤ δ can be computed in a constant time. Especially, the uniform node sampling can approximate the exact embedding and its gradients within O( 1 ε 2L (log 1 ε + log 1 δ ) L−1 log 1 δ ) time, where L denotes the number of layers. This complexity is completely independent of the number of nodes, edges, and neighbors of the input, which enables us to deal with graphs irrespective of their size. Moreover, the complexity is a polynomial with respect to 1 ε and log 1 δ . We demonstrate that the time complexity is optimal when L = 1 with respect to the error tolerance ε. Through experiments, we show that the approximation error between the exact computation and its approximation rapidly converges to zero. To the best of our knowledge, this is the first constant time approximation algorithm for GNNs with a theoretical guarantee in terms of approximation error.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the correlation between flows' likelihoods and image semantics, and questions the rationality and applicability of using predictive likelihoods of flows for out-of-distribution (OoD) detection. We introduce a concept of semantic-invariant transformation (SIT) and evaluate two typical flow-based models on image datasets MNIST and FashionMNIST under three trivial SITs. We demonstrate that the predictive likelihoods of the target models show weak correlation to the image semantics, and suggest that it may be problematic to use flows for downstream tasks which require metrics that can reflect image semantics, such as OoD detection.",
        "Abstract": " Among deep generative models, flow-based models, simply referred as \\emph{flow}s in this paper, differ from other models in that they provide tractable likelihood. Besides being an evaluation metric of synthesized data, flows are supposed to be robust against out-of-distribution~(OoD) inputs since they do not discard any information of the inputs. However, it has been observed that flows trained on FashionMNIST assign higher likelihoods to OoD samples from MNIST. This counter-intuitive observation raises the concern about the robustness of flows' likelihood. In this paper, we explore the correlation between flows' likelihood and image semantics. We choose two typical flows as the target models: Glow, based on coupling transformations, and pixelCNN, based on autoregressive transformations. Our experiments reveal surprisingly weak correlation between flows' likelihoods and image semantics: the predictive likelihoods of flows can be heavily affected by trivial transformations that keep the image semantics unchanged, which we call semantic-invariant transformations~(SITs). We explore three SITs~(all small pixel-level modifications): image pixel translation, random noise perturbation, latent factors zeroing~(limited to flows using multi-scale architecture, e.g. Glow). These findings, though counter-intuitive, resonate with the fact that the predictive likelihood of a flow is the joint probability of all the image pixels. So flows' likelihoods, modeling on pixel-level intensities, is not able to indicate the existence likelihood of the high-level image semantics. We call for attention that it may be \\emph{abuse} if we use the predictive likelihoods of flows for OoD samples detection.",
        "Introduction": "  INTRODUCTION Deep generative models have been very successful in image generation ( Brock et al., 2018 ;  Kingma & Dhariwal, 2018 ;  Miyato et al., 2018 ), natural language generation ( Bowman et al., 2015 ;  Yu et al., 2017 ), audio synthesis( Van Den Oord et al., 2016 ) and so on. Among them, generative adversarial networks (GANs) are implicit generative models( Goodfellow et al., 2014 ) that explicit likelihood function is not required, and are trained by playing a minimax game between the discriminator and the generator; Variational auto-encoders ( VAEs,Kingma & Welling (2013) ;  Rezende et al. (2014) ) are latent variable generative models optimized by maximizing a lower bound, called evidence lower bound, of the data log-likelihood. Flow-based models ( Dinh et al., 2016 ; 2014;  van den Oord et al., 2016 ) differ from them in that they provide exact log-likelihood evaluation with change of variables theorem ( Rezende & Mohamed, 2015 ). A flow usually starts with a simple base probability dis- tribution, e.g. diagonal Gaussian, then follows a chain of transformations in order to approximate complex distributions. Each transformation is parameterized by specially designed neural networks so that the log-determinant of its Jacobian can be efficiently computed. Most of the previous works focus on how to design more flexible transformations to achieve tighter log-likelihoods, and generate more realistic samples. It is also believed that flows can be used to detect out-of-distribution(OoD) samples by assigning low likelihoods on them. However, it has been observed that flows fail to do so. For example, flows trained on FashionMNIST surprisingly assign higher likelihoods on MNIST samples ( Nalisnick et al., 2018 ;  Choi & Jang, 2018 ). Though analyses on pixel-level statistics are performed on this phenomenon ( Nalisnick et al., 2018 ), and Under review as a conference paper at ICLR 2020 density evaluation combined with uncertainty estimation is used to detect OoD samples ( Choi & Jang, 2018 ), the reasons behind flows' counter-intuitive behaviours are still not clear. Humans easily discriminate MNIST images from FashionMNIST images, since their high-level im- age semantics are perceptually different. Accordingly, it takes some metrics that can reflect the high-level image semantics for OoD detection. In this paper, we empirically explore the correlation between flows' likelihoods and image semantics, and question the rationality and applicability of using predictive likelihoods of flows for OoD detection. We first introduce a concept of semantic- invariant transformation (SIT). An SIT transforms an input without changing its high-level seman- tics, e.g. a dog image through an SIT is still supposed to be recognized as a dog. We choose two typical flow-based models as target models: Glow ( Kingma & Dhariwal, 2018 ), based on coupling transformations, and pixelCNN ( van den Oord et al., 2016 ), based on autoregressive transforma- tions. We evaluate on image datasets MNIST and FashionMNIST under three trivial SITs: image translation, random noise perturbation, and latent factors zeroing (specific to invertible flows using multi-scale architectures, e.g. Glow). We demonstrate that the predictive likelihoods of the target models show weak correlation to the image semantics in the following ways: • Small pixel translations of test images could result in obvious likelihood decreases of Glow. • Perturbing small random noises, unnoticeable to humans, to test images could lead to catas- trophic likelihood decreases of target models. This also applies even if we keep the seman- tic object of a test image intact, and only add noises to the background. • For an invertible flow using multi-scale architecture, e.g. Glow, the inferred latent variables of an image is a list of gaussianized and standardized factors. We find that the contributions of a flow's blocks to the log-likelihood are constant and independent of inputs. Thus, simply zeroing the preceding latent factors of a sample image, and feed them to flow's reverse function. We could obtain new samples with surprisingly higher likelihoods, yet with perceptually unnoticeable changes from the original image. We emphasize that all these SITs are small pixel-level modifications on test images, and undoubt- edly have no influences on humans' recognition of the semantic objects in the images. However, they lead to obvious inconsistency of flows' likelihoods on test samples. Considering that the pre- dictive likelihood of a flow is the joint probability of all the image pixels, it may not convincingly indicate the existence of a semantic object in an image. Thus it could be problematic to use flows for downstream tasks which require metrics that can reflect image semantics, e.g. OoD detection.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a Random Distance Prediction (RDP) model for unsupervised representation learning of high-dimensional tabular data. RDP leverages the distances in a randomly projected space to learn expressive features without any manually labelled data. The model is flexible to incorporate task-dependent auxiliary losses to further enhance the learned features. Experiments on 19 real-world datasets show that RDP outperforms state-of-the-art competing methods in two key unsupervised tasks, anomaly detection and clustering.",
        "Abstract": "Deep neural networks have gained tremendous success in a broad range of machine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable downstream unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a highly efficient yet theoretical proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn class structures that are implicitly embedded in the randomly projected space. Experimental results on 19 real-world datasets show our learned representations substantially outperform state-of-the-art competing methods in both anomaly detection and clustering tasks.",
        "Introduction": "  INTRODUCTION Unsupervised representation learning aims at automatically extracting expressive feature represen- tations from data without any manually labelled data. Due to the remarkable capability to learn semantic-rich features, deep neural networks have been becoming one widely-used technique to em- power a broad range of machine learning tasks. One main issue with these deep learning techniques is that a massive amount of labelled data is typically required to successfully learn these expressive features. As a result, their transformation power is largely reduced for tasks that are unsupervised in nature, such as anomaly detection and clustering. This is also true to critical domains, such as healthcare and fintech, where collecting massive labelled data is prohibitively expensive and/or is impossible to scale. To bridge this gap, in this work we explore fully unsupervised representation learning techniques to enable downstream unsupervised learning methods on those critical domains. In recent years, many unsupervised representation learning methods ( Mikolov et al., 2013a ;  Le & Mikolov, 2014 ;  Misra et al., 2016 ;  Lee et al., 2017 ;  Gidaris et al., 2018 ) have been introduced, of which most are self-supervised approaches that formulate the problem as an annotation free pretext task. These methods explore easily accessible information, such as temporal or spatial neighbour- hood, to design a surrogate supervisory signal to empower the feature learning. These methods have achieved significantly improved feature representations of text/image/video data, but they are often inapplicable to tabular data since it does not contain the required temporal or spatial supervisory in- formation. We therefore focus on unsupervised representation learning of high-dimensional tabular data. Although many traditional approaches, such as random projection ( Li et al., 2006 ), principal component analysis (PCA) ( Rahmani & Atia, 2017 ), manifold learning ( Donoho & Grimes, 2003 ;  Hinton & Roweis, 2003 ) and autoencoder ( Vincent et al., 2010 ), are readily available for handling those data, many of them ( Donoho & Grimes, 2003 ;  Hinton & Roweis, 2003 ;  Rahmani & Atia, 2017 ) are often too computationally costly to scale up to large or high-dimensional data. Approaches like random projection and autoencoder are very efficient but they often fail to capture complex class structures due to its underlying data assumption or weak supervisory signal. In this paper, we introduce a Random Distance Prediction (RDP) model which trains neural networks to predict data distances in a randomly projected space. When the distance information captures in- Under review as a conference paper at ICLR 2020 trinsic class structure in the data, the representation learner is optimised to learn the class structure to minimise the prediction error. Since distances are concentrated and become meaningless in high dimensional spaces ( Beyer et al., 1999 ), we seek to obtain distances preserved in a projected space to be the supervisory signal. Random mapping is a highly efficient yet theoretical proven approach to obtain such approximately preserved distances. Therefore, we leverage the distances in the ran- domly projected space to learn the desired features. Intuitively, random mapping preserves rich local proximity information but may also keep misleading proximity when its underlying data distribu- tion assumption is inexact; by minimising the random distance prediction error, RDP essentially leverages the preserved data proximity and the power of neural networks to learn globally consis- tent proximity and rectify the inconsistent proximity information, resulting in a substantially better representation space than the original space. We show this simple random distance prediction en- ables us to achieve expressive representations with no manually labelled data. In addition, some task-dependent auxiliary losses can be optionally added as a complementary supervisory source to the random distance prediction, so as to learn the feature representations that are more tailored for a specific downstream task. In summary, this paper makes the following three main contributions. • We propose a random distance prediction formulation, which is very simple yet offers a highly effective supervisory signal for learning expressive feature representations that optimise the distance preserving in random projection. The learned features are sufficiently generic and work well in enabling different downstream learning tasks. • Our formulation is flexible to incorporate task-dependent auxiliary losses that are comple- mentary to random distance prediction to further enhance the learned features, i.e., features that are specifically optimised for a downstream task while at the same time preserving the generic proximity as much as possible. • As a result, we show that our instantiated model termed RDP enables substantially bet- ter performance than state-of-the-art competing methods in two key unsupervised tasks, anomaly detection and clustering, on 19 real-world high-dimensional tabular datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes Supervised Deep Infomax (SDIM), an end-to-end framework that is equivalent to a generative classifier. SDIM is trained by optimizing two objectives: maximizing the mutual information between the inputs and the high-level data representations from the encoder, and ensuring that the representations satisfy supervised statistical constraints. Experiments show that SDIM with a rejection policy can effectively reject illegal inputs, including out-of-distribution samples and adversarial examples.",
        "Abstract": "Deep Infomax~(DIM) is an unsupervised representation learning framework by maximizing the mutual information between the inputs and the outputs of an encoder, while probabilistic constraints are imposed on the outputs. In this paper, we propose Supervised Deep InfoMax~(SDIM), which introduces supervised probabilistic constraints to the encoder outputs. The supervised probabilistic constraints are equivalent to a generative classifier on high-level data representations, where class conditional log-likelihoods of samples can be evaluated. Unlike other works building generative classifiers with conditional generative models, SDIMs scale on complex datasets, and can achieve comparable performance with discriminative counterparts.  With SDIM, we could perform \\emph{classification with rejection}.\nInstead of always reporting a class label, SDIM only makes predictions when test samples' largest logits surpass some pre-chosen thresholds, otherwise they will be deemed as out of the data distributions, and be rejected.  Our experiments show that SDIM with rejection policy can effectively reject illegal inputs including out-of-distribution samples and adversarial examples.",
        "Introduction": "  INTRODUCTION Non-robustness of neural network models emerges as a pressing concern since they are observed to be vulnerable to adversarial examples ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ). Many attack methods have been developed to find imperceptible perturbations to fool the target classi- fiers ( Moosavi-Dezfooli et al., 2016 ;  Carlini & Wagner, 2017 ;  Brendel et al., 2017 ). Meanwhile, many defense schemes have also been proposed to improve the robustnesses of the target mod- els ( Goodfellow et al., 2014 ;  Tramèr et al., 2017 ;  Madry et al., 2017 ;  Samangouei et al., 2018 ). An important fact about these works is that they focus on discriminative classifiers, which directly model the conditional probabilities of labels given samples. Another promising direction, which is almost neglected so far, is to explore robustness of generative classifiers ( Ng & Jordan, 2002 ). A generative classifier explicitly model conditional distributions of inputs given the class labels. During inference, it evaluates all the class conditional likelihoods of the test input, and outputs the class label corresponding to the maximum. Conditional generative models are powerful and natu- ral choices to model the class conditional distributions, but they suffer from two big problems: (1) it is hard to scale generative classifiers on high-dimensional tasks, like natural images classifica- tion, with comparable performance to the discriminative counterparts. Though generative classifiers have shown promising results of adversarial robustness, they hardly achieve acceptable classifica- tion performance even on CIFAR10 ( Li et al., 2018 ;  Schott et al., 2018 ;  Fetaya et al., 2019 ). (2) The behaviors of likelihood-based generative models can be counter-intuitive and brittle. They may assign surprisingly higher likelihoods to out-of-distribution (OoD) samples ( Nalisnick et al., 2018 ;  Choi & Jang, 2018 ).  Fetaya et al. (2019)  discuss the issues of likelihood as a metric for density modeling, which may be the reason of non-robust classification, e.g. OoD samples detection. In this paper, we propose supervised deep infomax (SDIM) by introducing supervised statistical constraints into deep infomax ( DIM, Hjelm et al. (2018) ), an unsupervised learning framework by maximizing the mutual information between representations and data. SDIM is trained by opti- mizing two objectives: (1) maximizing the mutual information (MI) between the inputs and the high-level data representations from encoder; (2) ensuring that the representations satisfy the super- vised statistical constraints. The supervised statistical constraints can be interpreted as a generative Under review as a conference paper at ICLR 2020 classifier on high-level data representations giving up the full generative process. Unlike full gener- ative models making implicit manifold assumptions, the supervised statistical constraints of SDIM serve as explicit enforcement of manifold assumption: data representations (low-dimensional) are trained to form clusters corresponding to their class labels. With SDIM, we could perform classifica- tion with rejection ( Nalisnick et al., 2019 ;  Geifman & El-Yaniv, 2017 ). SDIMs reject illegal inputs based on off-manifold conjecture ( Samangouei et al., 2018 ;  Gu & Rigazio, 2014 ), where illegal inputs, e.g. adversarial examples, lie far away from the data manifold. Samples whose class con- ditionals are smaller than the pre-chosen thresholds will be deemed as off-manifold, and prediction requests on them will be rejected. The contributions of this paper are : • We propose Supervised Deep Infomax (SDIM), an end-to-end framework whose proba- bilistic constraints are equivalent to a generative classifier. SDIMs can achieve compara- ble classification performance with similar discrinimative counterparts at the cost of small over-parameterization. • We propose a simple but novel rejection policy based on off-manifold conjecture: SDIM outputs a class label only if the test sample's largest class conditional surpasses the pre- chosen class threshold, otherwise outputs rejection. The choice of thresholds relies only on training set, and takes no additional computations. • Experiments show that SDIM with rejection policy can effectively reject illegal inputs, including OoD samples and adversarial examples generated by a comprehensive group of adversarial attacks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a hierarchical disentangle network (HDN) for deep representation learning, which aims to learn disentangled representations in a more natural and efficient manner. HDN draws lessons from hierarchical classification and generative adversarial nets to exploit the natural hierarchical characteristics among categories to divide the representation learning in a coarse-to-fine manner. Experiments are conducted on four popular object datasets to validate the effectiveness of the proposed method.",
        "Abstract": "An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long objective of representation learning. It is observed that categories have the natural multi-granularity or hierarchical characteristics, i.e. any two objects can share some common primitives in a particular category granularity while they may possess their unique ones in another granularity. However, previous works usually operate in a flat manner (i.e. in a particular granularity) to disentangle the representations of objects. Though they may obtain the primitives to constitute objects as the categories in that granularity, their results are obviously not efficient and complete. In this paper, we propose the hierarchical disentangle network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner, such that each level only focuses on learning the specific representations in its granularity and finally the common and unique representations in all granularities jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the disentanglement and interpretability of the encoded representations, a novel hierarchical generative adversarial network (GAN) is elaborately designed. Quantitative and qualitative evaluations on four object datasets validate the effectiveness of our method.",
        "Introduction": "  INTRODUCTION Representation learning, as one basic and hot topic in machine learning and computer vision com- munity, has achieved significant progress in recent years on different tasks such as recognition ( Rus- sakovsky et al., 2015 ), detection ( Ren et al., 2015 ;  Redmon et al., 2016 ;  Liu et al., 2016b ) and gen- eration ( Goodfellow et al., 2014 ), benefiting from the rapid development of representation learned by deep neural networks. Considering the strong capacity of deep representation, in this paper, we mainly focus on the deep representation learning framework. Despite great success the deep representations have achieved as mentioned above, two important problems are still unresolved or less considered, i.e. the interpretability and the disentanglement of the learned representations. In the past decades, various works have been developed to reveal the black box of deep learning ( Zeiler & Fergus, 2014 ;  Dosovitskiy & Brox, 2016b ;  Bau et al., 2017 ;  Simonyan et al., 2013 ;  Stock & Cissé, 2017 ;  Zhang et al., 2017 ) and move us closer to the goal of disentangling the variations within data ( Reed et al., 2014 ;  Mathieu et al., 2016 ;  Rifai et al., 2012 ;  Tran et al., 2017 ;  Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ;  Chen et al., 2016 ). Even though they have brought great insights to us, they still have some limitations. For instance, ( Chen et al., 2016 ;  Xie et al., 2017 ;  Zhao et al., 2017 ) learn to disentangle variation factors within each category using generative models, instead of investigating the similarities and differences among categories, leading to poor discriminability. Therefore, the learned representations would not well conform to human perception. Though ( Gonzalez-Garcia et al., 2018 ;  Huang et al., 2018 ) try to obtain the domain-invariant and domain-specific knowledge, they can only handle two categories one time, which is not that efficient. In this paper, we attempt to learn disentangled representations in a more natural and efficient manner. Let us first discuss how humans understand an object. Generally speaking, an object can be regarded as the combination of many semantic attributes. Hundreds of thousands of objects in the world can be clustered and recognized by humans just because we can figure out the common and unique Under review as a conference paper at ICLR 2020 attributes of an object compared to others. Besides, a man who never play the billiards can only recognize a table in an image, while a sports fan may regard it as a billiard table. Both of them are right since categories have natural hierarchical structure. As shown in Fig. 1(a), given six leaf-level categories, they can be organized in a three-level hierarchy considering the common and different features they have. Each child category in the hierarchy is a special case of its parent category since it inherits all features from its parent category and has extra features that are not present in its parent category. From another perspective, each parent category is the abstraction of all its child categories considering it contains the attributes that are present in all its child categories. Then we come back to the task of disentangling representation learning. It aims to learn the representation encoding useful information that can be applied in other tasks (e.g. building classifiers and predictors) ( Bengio et al., 2013 ). Taking the hierarchical nature of categories into account, if we only learn the representations of an object in a flat manner for a specific category level as previous works do, it will not be scalable and comprehensive for the machine to be qualified for various tasks in the real world. Our work aims to exploit the natural hierarchical characteristics among categories to divide the rep- resentation learning in a coarse-to-fine manner, such that each level only focuses on learning the specific representations. For instance, given a billiard table image in Fig. 1(b), it tangles the in- formation of being a furniture, a table and a billiard table. We first extract the features that only contain the information of furniture from the image. By tracing from the root to leaf level, more and more information is extracted until we can recognize its belonging categories in all hierarchi- cal levels. By doing so, the disentangled representations are expected to find wide and promising applications. For example, one can transfer the semantics in a specific category level from one ob- ject to another while keep information of other levels unchanged. Besides, it would help for the hierarchical image compression task using different levels of the disentangled representations. To achieve the objective of hierarchical disentangling and simultaneously interpreting the results so that humans can understand, we propose the hierarchical disentangle network (HDN), which draws lessons from hierarchical classification and the recent proposed generative adversarial nets ( Good- fellow et al., 2014 ). Extensive experiments are conducted on four popular object datasets to validate the effectiveness of our method.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a data-driven approach to improve the accuracy of numerical methods used to solve linear and nonlinear problems, with a focus on fluid flow. Two variants of the approach are presented: a supervised version with an optimization algorithm and an unsupervised version with a differentiable PDE solver. Experiments show that the simulation accuracy of the given solver can be significantly improved, with improved dynamics and the ability to reproduce the behavior of the reference data more closely. Advantages and disadvantages of both approaches are discussed.",
        "Abstract": "Improving the accuracy of numerical methods remains a central challenge in many disciplines and is especially important for nonlinear simulation problems. A representative example of such problems is fluid flow, which has been thoroughly studied to arrive at efficient simulations of complex flow phenomena. This paper presents a data-driven approach that learns to improve the accuracy of numerical solvers. The proposed method utilizes an advanced numerical scheme with a fine simulation resolution to acquire reference data. We, then, employ a neural network that infers a correction to move a coarse thus quickly obtainable result closer to the reference data. We provide insights into the targeted learning problem with different learning approaches: fully supervised learning methods with a naive and an optimized data acquisition as well as an unsupervised learning method with a differentiable Navier-Stokes solver. While our approach is very general and applicable to arbitrary partial differential equation models, we specifically highlight gains in accuracy for fluid flow simulations.",
        "Introduction": "  INTRODUCTION Numerical methods are a central component of many disciplines and widely used for solving a variety of linear and nonlinear problems. One of the long-standing targets is fluid flow, which is renowned for its great diversity and complexity in terms of dynamics. Studies in computational fluid dynamics have focused on numerical simulations for such problems and invested huge efforts in solving spatio-temporal partial differential equations (PDEs) such as the Navier-Stokes equations, which represent the well-established physical model for fluids. Traditional methods typically improve accuracy with fine discretizations both in space and time. While the methods and computing power for numerical simulation have seen advances in recent years, there is still a pressing need for better efficiency and accuracy. For most practical applications of computer simulations, we are still far away from fully resolving all necessary scales of nature around us (Verma et al., 2018;  Cummins et al., 2018 ). To tackle this problem, we propose a data-driven approach that \"assists\" a given numerical method to improve its accuracy. To this end, we introduce a first learning-based approach that puts special emphasis on the time dimension. We demonstrate two variants to achieve this goal in the context of fluids: a supervised version with an optimization algorithm for acquisition of temporally con- strained correction data and an unsupervised version with a differentiable PDE solver that allows us to autonomously takes into account temporal information when training. We compare advantages and disadvantages of both approaches, and our experiments show that, using our trained models, the simulation accuracy of the given solver can be significantly improved. In all cases, our trained mod- els yield improved dynamics, and the learned assistance function lets a coarse simulation reproduce the behavior of the reference data more closely. In particular, we demonstrate the improvements of our approach over ad-hoc learning approaches.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a Hierarchical Graph Matching Network (HGMN) for computing the graph similarity between any pair of graph-structured objects. The HGMN model jointly learns graph representations and a graph matching metric function for computing graph similarity in an end-to-end fashion. It consists of a multi-perspective node-graph matching network for effectively learning cross-level interactions between parts of a graph and a whole graph, and a siamese graph neural network for learning global-level interactions between two graphs. Experiments demonstrate that the proposed HGMN consistently outperforms state-of-the-art graph matching network baselines for both classification and regression tasks, and is more robust when the sizes of the two input graphs increase.",
        "Abstract": "While the celebrated graph neural networks yields effective representations for individual nodes of a graph, there has been relatively less success in extending to deep graph similarity learning. \nRecent work has considered either global-level graph-graph interactions or low-level node-node interactions, ignoring the rich cross-level interactions between parts of a graph and a whole graph.\nIn this paper, we propose a Hierarchical Graph Matching Network (HGMN) for computing the graph similarity between any pair of graph-structured objects. Our model jointly learns graph representations and a graph matching metric function for computing graph similarity in an end-to-end fashion. The proposed HGMN model consists of a multi-perspective node-graph matching network for effectively learning cross-level interactions between parts of a graph and a whole graph, and a siamese graph neural network for learning global-level interactions between two graphs. Our comprehensive experiments demonstrate that our proposed HGMN consistently outperforms state-of-the-art graph matching networks baselines for both classification and regression tasks. ",
        "Introduction": "  INTRODUCTION Learning a general similarity metric between arbitrary pairs of graph-structured objects is one of the key challenges in machine learning. Such learning problems often arise in a variety of applica- tions, ranging from graph similar searching in graph-based database (Yan & Han, 2002), to Fewshot 3D Action Recognition (Guo et al., 2018), unknown malware detection (Wang et al., 2019), and promising selection in automatic theory proving (Wang et al., 2017), to name just a few. Conceptually, classical exact (or inexact) graph matching techniques (Ullmann, 1976; Caetano et al., 2009; Bunke & Allermann, 1983; Riesen et al., 2010) provide a strong tool for learning graph sim- ilarity. However, these methods usually either require input graphs with similar sizes or consider mainly the graph structures for finding a correspondence between the nodes of different graphs with- out taking into account the node representations or features. In contrast, in this paper, we consider the graph matching problem of learning a mapping between a pair of graph inputs pG 1 , G 2 q P GˆG and the similarity score y P Y, based on a set of training triplet of structured input pairs and scalar output score pG 1 1 , G 2 1 , y 1 q, ..., pG 1 n , G 2 n , y n q P GˆGˆY drawn from some fixed but unknown probability distribution. Recent years have seen a surge of interests in graph neural networks (GNNs), which have been demonstrated to be a powerful class of models for learning node embeddings of graph-structured data (Bronstein et al., 2017). Various GNN models have since been developed for learning effective node representations for node classification (Li et al., 2016; Kipf & Welling, 2016; Hamilton et al., 2017; Veličković et al., 2017), or pooling the learned node embeddings into a graph vector for graph classification (Ying et al., 2018; Ma et al., 2019), or combining with variational auto-encoder to learn the graph distribution for graph generation (Simonovsky & Komodakis, 2018; Li et al., 2018; Samanta et al., 2018; You et al., 2018). However, there is relatively less study on learning graph similarity using GNNs. To learn graph similarity, a simple yet straightforward way is to encode each graph as a vector and combine two vectors of each graph to make a decision. This approach is useful since graph- level embeddings contain important information of a pair of graphs. One obvious limitation of this approach lies in the fact of the ignorance of more fine-grained interactions among different level Under review as a conference paper at ICLR 2020 embeddings of two graphs. Very recently, a few of attempts have been made to take into account low- level interactions either by considering the histogram information of node-wise similarity matrix of node embeddings (Bai et al., 2019) or improving the node embeddings of one graph by incorporating implicit attentive neighbors of another graphs through a soft attention (Li et al., 2019). However, there are two significant challenges making these graph matching models potentially ineffective: i) how to learn different-level granularity (global level and local level) of interactions between a pair of graphs; ii) how to effectively learn richer cross-level interactions between parts of a graph and a whole graph. Inspired by these observations, in this paper, we propose a Hierarchical Graph Matching Network (HGMN) for computing the graph similarity between any pair of graph-structured objects. Our model jointly learns graph representations and a graph matching metric function for computing graph similarity in an end-to-end fashion. The proposed HGMN model consists of a novel multi- perspective node-graph matching network for effectively learning cross-level interactions between parts of a graph and a whole graph, and a siamese graph neural network for learning global-level interactions between two graphs. Our final small prediction networks consume these feature vectors from both cross-level and global-level interactions to perform either graph-graph classification or graph-graph regression tasks, respectively. Recently proposed works only compute graph similarity by considering either graph-graph classi- fication problem (with labels Y \" t´1, 1u) (Li et al., 2019), or graph-graph regression problem (with similarity score Y \" r0, 1s) (Bai et al., 2019). To demonstrate the effectiveness of our model, we systematically investigate the performance of our HGMN model compared with these recently proposed graph matching models on four datasets for both graph-graph classification and regression tasks. To bridge the gap of the lack of standard graph matching datasets, we also create one new dataset from a real application together with a previously released dataset by (Xu et al., 2017) for graph-graph classification task 1 . One important aspect is previous works did not consider the impact of the size of two input graphs, which often plays an important role in determining the performance of graph matching. Motivated by this observation, we have considered three different ranges of graph sizes from [3, 200], [20,200], and [50,200] in order to evaluate the robustness of each graph matching model. We highlight our main contributions of this paper as follows: • We propose a hierarchical graph matching network (HGMN) for computing the graph simi- larity between any pair of graph-structured objects. Our HGMN model jointly learns graph representations and a graph matching metric function for computing graph similarity in an end-to-end fashion. • In particular, we propose a multi-perspective node-graph matching network for effectively capturing the cross-level interactions between a node embeddings of a graph and a corre- sponding attentive graph-level embedding of another graph. • We systematically investigate different factors on the performance of all graph matching models such as the impact of different tasks (classification and regression) and the sizes of input graphs. • Our comprehensive experiments demonstrate that our proposed HGMN consistently out- performs state-of-the-art graph matching network baselines for both classification and re- gression tasks. Compared with previous works, our proposed model HGMN is also more robust when the sizes of the two input graphs increase.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes the Temporal Difference Weighted (TDW) algorithm, an ensemble method for reinforcement learning at test time. The TDW algorithm prioritizes confident agents to participate in action selection while reducing the contributions of agents unfamiliar with the current trajectory. The TDW algorithm is demonstrated to retain performance in tabular representation Gridworld tasks with multiple possible trajectories, and to achieve better performance than baseline algorithms in Atari tasks.",
        "Abstract": "Combining multiple function approximators in machine learning models typically leads to better performance and robustness compared with a single function. In reinforcement learning, ensemble algorithms such as an averaging method and a majority voting method are not always optimal, because each function can learn fundamentally different optimal trajectories from exploration. In this paper, we propose a Temporal Difference Weighted (TDW) algorithm, an ensemble method that adjusts weights of each contribution based on accumulated temporal difference errors. The advantage of this algorithm is that it improves ensemble performance by reducing weights of Q-functions unfamiliar with current trajectories. We provide experimental results for Gridworld tasks and Atari tasks that show significant performance improvements compared with baseline algorithms.",
        "Introduction": "  INTRODUCTION Using ensemble methods that combine multiple function approximators can often achieve bet- ter performance than a single function by reducing the variance of estimation ( Dietterich (2000) ;  Kuncheva (2014) ). Ensemble methods are effective in supervised learning, and also reinforce- ment learning ( Wiering & Van Hasselt (2008) ). There are two situations where multiple function approximators are combined: combining and learning multiple functions during training ( Freund & Schapire (1997) ) and combining individually trained functions to jointly decide actions during testing ( Breiman (1996) ). In this paper, we focus on the second setting of reinforcement learning wherein each function is trained individually and then combined them to achieve better test perfor- mance. Though there is a body of research on ensemble algorithms in reinforcement learning, it is not as sizeable as the research devoted to ensemble methods for supervised learning.  Wiering & Van Has- selt (2008)  investigated many ensemble approaches combining several agents with different value- based algorithms in Gridworld settings.  Faußer & Schwenker (2011 ;  2015a ) have shown that com- bining value functions approximated by neural networks improves performance greater than using a single agent. Although previous work dealt with each agent equally contributing to the final out- put, weighting each contribution based on its accuracy is also a known and accepted approach in supervised learning ( Dietterich (2000) ). However, unlike supervised learning, reinforcement learning agents learn from trajectories result- ing from exploration, such that each agent learns from slightly different data. This characteristic is significant in tasks with high-dimensional state-space, where there are several possible optimal trajectories to maximize cumulative rewards. In such a situation, the final joint policy function re- sulting from simple averaging or majority voting is not always optimal if each agent learned different optimal trajectories. Furthermore, it is difficult to decide constant weights of each contribution as it is possible that agents with poor episode rewards have better performance in specific areas. In this paper, we propose the temporal difference weighted (TDW) algorithm, an ensemble method for reinforcement learning at test time. The most important point of this algorithm is that confident agents are prioritized to participate in action selection while contributions of agents unfamiliar with the current trajectory are reduced. To do so in the TDW algorithm, the weights of the contributions at each Q-function are calculated as softmax probabilities based on accumulated TD errors. Extend- ing an averaging method and a majority voting method, actions are determined by weighted average or voting methods according to the weights. The advantage of the TDW algorithm is that arbitrary Under review as a conference paper at ICLR 2020 training algorithms can use this algorithm without any modifications, because the TDW algorithm only cares about the joint decision problem, which could be easily adopted in competitions and de- velopment works using reinforcement learning. In our experiment, we demonstrate that the TDW retains performance in tabular representation Gridworld tasks with multiple possible trajectories, where simple ensemble methods are significantly degraded. Second, to demonstrate the effective- ness of our TDW algorithm in high-dimensional state-space, we also show that our TDW algorithm can achieve better performance than baseline algorithms in Atari tasks ( Bellemare et al. (2013) ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework to tackle the partial multi-label learning problem. The PML-GAN model comprises four component networks: a disambiguation network, a prediction network, a generation network, and a discrimination network. Experiments on multi-label datasets under partial multi-label learning setting show that the proposed PML-GAN yields the state-of-the-art PML performance.",
        "Abstract": "Partial multi-label learning (PML), which tackles the problem of learning multi-label prediction models from instances with overcomplete noisy annotations, has recently started gaining attention from the research community. In this paper, we propose a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework for partial multi-label learning. The PML-GAN model uses a disambiguation network to identify noisy labels and uses a multi-label prediction network to map the training instances to the disambiguated label vectors, while deploying a generative adversarial network as an inverse mapping from label vectors to data samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels. Extensive experiments are conducted on multiple datasets, while the proposed model demonstrates the state-of-the-art performance for partial multi-label learning.",
        "Introduction": "  INTRODUCTION In partial multi-label learning (PML), each training instance is assigned multiple candidate labels which are only partially relevant; that is, some assigned labels are irrelevant noise. As it is typically difficult and costly to precisely annotate instances for multi-label data ( Xie & Huang, 2018 ), the task of PML naturally arises in many real-world scenarios with crowdsource annotations.  Figure 1  presents an example of training images for object recognition under the PML setting, where the union of candidate labels provided by crowdsource annotators is overcomplete and contains both ground truth labels (in black color) and irrelevant noise labels (in red color). PML is much more challenging than standard multi-label learning as the true labels are hidden among irrelevant labels and the number of true labels is unknown. The goal of PML is to learn a good multi-label prediction model from such a partial label training set, and hence reduce the annotation cost. An intuitive strategy of PML is to treat all candidate labels as relevant ground truth, thus any off-the- shelf multi-label classification methods can be adapted to induce an expected multi-label predictor ( Zhang & Zhou, 2014 ). This strategy, though simple, cannot work well since taking the noisy labels as part of the true labels will mislead the multi-label training and induce inferior prediction models. The PML work in ( Xie & Huang, 2018 ) assumes that each candidate label has a confidence score of being a true label, and learns the confidence scores and classifier in an alternative manner by minimizing a confidence weighted ranking loss between the candidate and non-candidate labels. Although this work yields some reasonable results, the estimation of label confidence scores is error-prone, especially when noisy labels dominate, which can seriously impair the classifier's performance. Another recent work in ( Fang & Zhang, 2019 ) also exploits label confidence values of candidate labels for PML. It estimates the confidence values using iterative label propagation and chooses the candidate labels with high confidence values as credible labels, which are then used to induce a multi-label prediction model. This work however suffers from the cumulative errors induced in propagation, which can impact the estimation of the credible labels and consequently impair the prediction model. In this paper, we propose a novel adversarial learning model, PML-GAN, under a generalized encoder-decoder framework to tackle the partial multi-label learning problem. The PML-GAN model comprises four component networks: a disambiguation network that predicts the probability of each candidate label being an additive noise for a training instance; a prediction network that predicts the disambiguated true labels of each instance from its input features; a generation network that generates samples in the feature space given latent vectors in the label space; and a discrimination network that separates the generated samples from the real data. The prediction network and Under review as a conference paper at ICLR 2020 disambiguation network together form an encoder that maps data samples in the input feature space to the disambiguated label vectors, while the generation network and discrimination network forms a generative adversarial network (GAN) ( Goodfellow et al., 2014 ) as an inverse decoding mapping from vectors in the multi-label space to samples in the input feature space. The learning of the overall model corresponds to a minimax adversarial game, which enhances the correspondence of input features with the output labels, and consequently boosts multi-label prediction performance. To the best of our knowledge, this is the first work that exploits GANs for PML. We conduct extensive experiments on multi-label datasets under partial multi-label learning setting. The empirical results show the proposed PML-GAN yields the state-of-the-art PML performance.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a unified optimization framework, AdvCodec, to generate adversarial text against general NLP tasks. The core component of AdvCodec is a tree based autoencoder which converts discrete text tokens into continuous semantic embedding, upon which the adversarial perturbation is optimized. Experiments demonstrate that AdvCodec achieves significantly higher untargeted and targeted attack success rate than the state-of-the-art baseline methods. In addition, comprehensive ablation studies and human studies are conducted to evaluate the attack scenarios and the quality of generated adversarial text.",
        "Abstract": "Machine learning (ML) especially deep neural networks (DNNs) have been widely applied to real-world applications. However, recent studies show that DNNs are vulnerable to carefully crafted \\emph{adversarial examples} which  only deviate from the original data by a small magnitude of perturbation. \nWhile there has been great interest on generating imperceptible adversarial examples in continuous data domain (e.g. image and audio) to explore the model vulnerabilities, generating \\emph{adversarial text} in the discrete domain is still challenging. \nThe main contribution of this paper is to propose a general targeted attack framework \\advcodec for adversarial text generation which addresses the challenge of discrete input space and be easily adapted to general natural language processing (NLP) tasks. \nIn particular, we propose a tree based autoencoder to encode discrete text data into continuous vector space, upon which we optimize the adversarial perturbation. With the tree based decoder, it is possible to ensure the grammar correctness of the generated text; and the tree based encoder enables flexibility of making manipulations on different levels of text, such as sentence (\\advcodecsent) and word (\\advcodecword) levels. We consider multiple attacking scenarios, including appending an adversarial sentence or adding unnoticeable words to a given paragraph, to achieve arbitrary \\emph{targeted attack}. To demonstrate the effectiveness of the proposed method, we consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results show that \\advcodec has successfully attacked both tasks. In particular, our attack causes a BERT-based sentiment classifier accuracy to drop from $0.703$ to $0.006$, and a BERT-based QA model's F1 score to drop from $88.62$ to $33.21$ (with best targeted attack F1 score as $46.54$). Furthermore, we show that the white-box generated adversarial texts can transfer across other black-box models, shedding light on an effective way to examine the robustness of existing NLP models.",
        "Introduction": "  INTRODUCTION Recent studies have demonstrated that deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples (Goodfellow et al., 2015; Papernot et al., 2016; Eykholt et al., 2017; Moosavi-Dezfooli et al., 2016). While there are a lot of successful attacks proposed in the con- tinuous data domain including images, audios, and videos, how to effectively generate adversarial examples in the discrete text domain still remains a hard problem. There are several challenges for generating adversarial text: 1) most existing gradient-based adversarial attack approaches are not directly applicable to the discrete structured data; 2) it is less clear how to appropriately measure the naturalness of the generated text compared to the original ones; 3) the manipulation space of text is limited, and it is unclear whether generating a new appended sentence or manipulating individual words will affect human judgements. So far, existing works on adversarial text generation either leverage heuristic solutions such as ge- netic algorithms (Jin et al., 2019) to search for potential adversarial sentences, or are limited to attacking specific NLP tasks (Cheng et al., 2018; Lei et al., 2018). In addition, effective targeted attacks have not been achieved by current attacks for any task. In this paper, we aim to provide more insights towards solving these challenges by proposing a unified optimization framework AdvCodec to generate adversarial text against general NLP tasks. In particular, the core component of AdvCodec is a tree based autoencoder which converts discrete text tokens into continuous se- mantic embedding, upon which the adversarial perturbation will be optimized regarding the chosen adversarial target. Finally, a tree based decoder will decode the generated adversarial continuous embedding vector back to the sentence level based on the tree grammar rules, aiming to both pre- Under review as a conference paper at ICLR 2020 serve the original semantic meaning and linguistic coherence. An iterative process can be applied here to ensure the attack success rate. In addition to the general adversarial text generation framework AdvCodec, this paper also aims to explore several scientific questions: 1) Since AdvCodec allows the flexibility of manipulating on different hierarchies of the tree structures, which is more attack effective and which way preserves better grammatical correctness? 2) Is it possible to achieve targeted attack for general NLP tasks such as sentiment classification and QA, given the limited degree of freedom for manipulation? 3) Is it possible to perform blackbox attack in general NLP tasks? 4) Is BERT robust in practice? 5) Do these adversarial examples affect human reader performances? To address the above questions, we explore two types of tree based autoencoders on the word (AdvCodec(Word)) and sentence level (AdvCodec(Sent)). For each encoding scenario, we generate adversarial text against different sentiment classification and QA models. Compared with the state-of-the-art adversarial text generation methods, our approach achieves significantly higher untargeted and targeted attack success rate. In addition, we perform both whitebox and blackbox settings for each attack to evaluate the model vulnerabilities. Within each attack setting, we evaluate attack strategies as appending an additional adversarial sentence or adding scatter of ad- versarial words to a paragraph, to evaluate the quantitative attack effectiveness. To provide thorough adversarial text quality assessment, we also perform 7 groups of human studies to evaluate the qual- ity of generated adversarial text compared with the baselines methods, and whether human can still get the ground truth answers for these tasks based on adversarial text. We find that: 1) both word and sentence level attacks can achieve high attack success rate, while the sentence level manipulation can consider the global grammatical constraints and generate high quality adversarial sentences. 2) various targeted attacks on general NLP tasks are possible (e.g. when attacking QA, we can ensure the target to be a specific answer or a specific location within a sentence); 3) the transferability based blackbox attacks are successful in NLP tasks. Transferring adversarial text from stronger models (in terms of performances) to weaker ones is more successful; 4) Although BERT has achieved state-of- the-art performances, we observe the performance drops are also larger than other standard models when confronted with adversarial examples, which indicates BERT is not robust under the adversar- ial settings; 5) Most human readers are not sensitive to our adversarial examples and can still answer the right answers when confronted with the adversary-injected paragraphs. In summary, our main contribution lies on: (1) We propose a general adversarial text generation framework AdvCodec that addresses the challenge of discrete text input to achieve targeted attacks against general NLP tasks (e.g. sentiment classification and QA) while preserving the semantic meaning and linguistic coherence; (2) we propose a novel tree-based text autoencoder that ensures the grammar correctness of generated text; (3) we conduct extensive experiments and successfully attack different sentiment classifiers and QA models with significant higher attack success rate than the state-of-the-art baseline methods; (4) we also perform comprehensive ablation studies including evaluating the attack scenarios of appending an adversarial sentence or adding scatter of adversarial words, as well as appending the adversarial sentence at different positions within a paragraph, and draw several interesting conclusions; (5) we leverage extensive human studies to show that the adversarial text generated by AdvCodec is natural and effective to attack neural models, while barely affecting human's judgement.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the use of ReLU activations in various types of neural networks such as Multilayer Perceptron (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). It discusses the theoretical studies on the MLP and CNN with ReLU activations, particularly on its positively scale-invariant (PSI) property. To solve the mismatch between the vector space of weights and the PSI property, a new parameter space, called path space, for MLP and CNN is proposed. Path space is a vector space constituted by the values of given paths called basis paths, whose values are sufficient to calculate the output of the MLP and CNN with ReLU activations.",
        "Abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ",
        "Introduction": "  INTRODUCTION Over the past ten years, ReLU activations have become increasingly popular in various types of neural networks such as Multilayer Perceptron(MLP) ( Nair & Hinton, 2010 ;  Glorot et al., 2011 ;  Neyshabur et al., 2015a ) , Convolutional Neural Networks (CNN) (( Krizhevsky et al., 2012 ;  He et al., 2016 )) and Recurrent Neural Networks (RNN) ( Le et al., 2015 ;  Neyshabur et al., 2016 ). Theoretical studies on the MLP and CNN with ReLU activations, particularly on its positively scale-invariant (PSI) property have also been conducted ( Neyshabur et al., 2016 ;  2015a ;  Dinh et al., 2017 ;  Meng et al., 2018 ). Specifically, PSI property means if the incoming weights of a hidden node (or a feature map for CNN) are multiplied by a positive scalar c, and the outgoing weights of this hidden node (or the feature map) are divided by c, the output for arbitrary input will keep unchanged. However, conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which are not positively scale-invariant. This mismatch may lead to problems during the optimization process ( Neyshabur et al., 2015a ;  Meng et al., 2018 ), e.g., in an unbalanced network (i.e., the norm of incoming weights to different units have relatively large differences), the training process of vanilla stochastic gradient descent will be significantly slowed down. To solve the above mismatch, some recent studies have been conducted on paths of ReLU neural networks ( Neyshabur et al., 2015a ;  Meng et al., 2018 ). The value of a path is defined as the multiplication of weights along the path which starts from an input node, successively crosses a hidden node at every layer, and finally ends at an output node. Obviously, the value of a path is positively scale-invariant. Thus, a new parameter space, called path space, for MLP and CNN have been proposed. Path space is a vector space constituted by the values of given paths called basis paths, whose values are sufficient to calculate the output of the MLP and CNN with ReLU activations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a feature leveling network structure to improve the interpretability of deep neural networks (DNNs). The feature leveling network is designed to separate low level features from high level features to avoid mixture effect and leverage the weights of the GLM layer to explain the decision making process. The paper also proposes a feature leveling scale to measure the complexity of different sets of features in an unambiguous manner. Empirical results are used to demonstrate the effectiveness of the feature leveling network in improving interpretability and reducing the complexity of DNNs with little compromise on performance.",
        "Abstract": "Self-explaining models are models that reveal decision making parameters in an interpretable manner so that the model reasoning process can be directly understood by human beings. General Linear Models (GLMs) are self-explaining because the model weights directly show how each feature contributes to the output value. However, deep neural networks (DNNs) are in general not self-explaining due to the non-linearity of the activation functions, complex architectures, obscure feature extraction and transformation process. In this work, we illustrate the fact that existing deep architectures are hard to interpret because each hidden layer carries a mix of low level features and high level features. As a solution, we propose a novel feature leveling architecture that isolates low level features from high level features on a per-layer basis to better utilize the GLM layer in the proposed architecture for interpretation. Experimental results show that our modified models are able to achieve competitive results comparing to main-stream architectures on standard datasets while being more self-explainable. Our implementations and configurations are publicly available for reproductions.",
        "Introduction": "  INTRODUCTION Deep Neural Networks (DNNs) are viewed as back-box models because of their obscure decision making process. One reason that makes deep neural networks hard to interpret is that they are able to magically extract abstract concepts through multi-layer non-linear activations and end-to- end training. From a human perspective, it is hard to understand how features are extracted from different hidden layers and what features are used for final decision making. In response to the challenge of interpretability, two paths are taken to unbox neural networks' deci- sion learning process. One method is to design verifying algorithms that can be applied to existing models to back-trace their decision learning process. Another method is to design models that \"ex- plain\" the decision making process automatically. The second direction is promising in that the interpretability is built-in architecturally. Thus, the verification feedback can be directly used to improve the model. One class of the self-explaining models borrows the interpretability of General Linear Models (GLMs) such as linear regression. GLMs are naturally interpretable in that complicated interactions of non-linear activations are not involved. The contribution of each feature to the final decision out- put can simply be analyzed by examining the corresponding weight parameters. Therefore, we take a step forward to investigate ways to make DNNs as similar to GLMs as possible for interpretability purpose while maintaining competitive performance. Fortunately, a GLM model naturally exists in the last layer of most discriminative architectures of DNNs (See appendix A.3 for the reason that the last layer is a GLM layer). However, the GLM could only account for the output generated by the last layer and this output is not easy to interpret because it potentially contains mixed levels of features. In the following section, we use empirical results to demonstrate this mixture effect. Based on this observation, one way to naturally improve interpretation is to prevent features extracted by different layers from mixing together. Thus, we Under review as a conference paper at ICLR 2020 directly pass features extracted by each layer to the final GLM layer. This can further improve interpretability by leveraging the weights of the GLM layer to explain the decision making process. Motivated by this observation, we design a feature leveling network structure that can automatically separate low level features from high level features to avoid mixture effect. In other words, if the low level features extracted by the k th hidden layer can be readily used by the GLM layer, we should directly pass these features to the GLM rather than feeding them to the k + 1 th hidden layer. We also propose a feature leveling scale to measure the complexity of different sets of features' in an unambiguous manner rather than simply using vague terms such as \"low\" and \"high\" to describe these features. In the following sections, we will first lay out the proposed definition of feature leveling. We then will illustrate how different levels of features reside in the same feature space. Based on the above observations, we propose feature leveling network, an architectural modification on existing models that can isolate low level features from high level features within different layers of the neural network in an unsupervised manner. In the experiment section, we will use empirical results to show that this modification can also be applied to reduce the number of layers in an architecture and thus reduce the complexity of the network. In this paper, we focus primarily on fully connected neural networks(FCNN) with ReLU activation function in the hidden layers. Our main contributions are as follows: • We take a step forward to quantify feature complexity for DNNs. • We investigate the mixture effect between features of different complexities in the hidden layers of DNNs. • We propose a feature leveling architecture that is able to isolate low level features from high level features in each layer to improve interpretation. • We further show that the proposed architecture is able to prune redundant hidden layers to reduce DNNs' complexity with little compromise on performance. The remaining content is organized as follows: In section 2, we first introduce our definitions of feature leveling and use a toy example to show the mixture effect of features in hidden layers. In section 3, we give a detailed account of our proposed feature leveling network that could effectively isolate different levels of features. In section 4, we provide a high level introduction to some related works that motivated our architectural design. In Section 5, we test and analyze our proposed archi- tecture on various real world datasets and show that our architecture is able to achieve competitive performance while improving interpretability. In section 6, we show that our model is also able to automatically prune redundant hidden layers, thus reducing the complexity of DNNs.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel black-box method, Gaussian Light and Shadow (GLAS), for visual interpretation of deep learning models. GLAS perturbs an input image with a Gaussian mask (light) and inverse-Gaussian mask (shadow) to measure the importance of pixels and then fuses all importance from several thousand masks to construct the final heat map. GLAS provides scalability of explanation, making it possible to pinpoint clues for the salient explanation, which is not feasible with nonparameterized approaches. GLAS is fast and efficient, processing an image up to one order of magnitude faster than other black-box methods. Quantitative comparisons show that GLAS is superior to conventional methods and is applicable to various tasks, including object localization and visual captioning.",
        "Abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ",
        "Introduction": "  INTRODUCTION Over the last several years, convolutional neural networks (CNNs) ( LuCun et al., 2015 ) have achieved superior performance in various computer vision tasks, including image classification ( He et al., 2016 ;  Shi et al., 2018 ), object detection (Oh et al., 2017;  Zhou et al., 2016 ), semantic segmen- tation ( Pathak et al., 2015 ), and image captioning ( Xu et al., 2015 ). Despite these dramatic advances, the opacity of CNNs makes it difficult to understand why they reach particular decisions, limiting the ability to widen their application to various fields. In general, the visual interpretation of deep learning models is understood as estimating the impact of a particular neuron activation related to a given input instance. In white-box approach, architectural modification of the classification model ( Bach et al., 2015 ;  Dong et al., 2017 ;  Mahendran & Vedaldi, 2016 ;  Selvaraju et al., 2017 ;  Simonyan et al., 2013 ;  Springenberg et al., 2014 ;  Zhou et al., 2016 ;  Zeiler & Fergus, 2013 ) or access to specific layers ( Bach et al., 2015 ;  Selvaraju et al., 2017 ;  Zhang et al., 2016 ) is inevitable ( Petsiuk et al., 2018 ), resulting in severe limitation of application. In contrast, the black-box approach ( Seo et al., 2018 ;  Petsiuk et al., 2018 ;  Ribeiro et al., 2016 ;  Tian & Cai, 2017 ;  Zeiler & Fergus, 2013 ;  Zintgraf et al., 2017 ;  Fong & Vedaldi, 2017 ) aims to be inherently model agnostic. Its main concerns are how to perturb an input image and draw the model's response on the perturbed instance to the final heat map. For example, the Randomized Input Sampling for Explanation (RISE) method ( Petsiuk et al., 2018 ) perturbed an image with a randomised mask to measure the importance of pixels and then linearly fused all importance from several thousand masks. The conventional black-box methods employed unnatural and fragile perturbation schemes such as single colour out ( Seo et al., 2018 ;  Petsiuk et al., 2018 ;  Ribeiro et al., 2016 ;  Zeiler & Fergus, 2013 ), random noise ( Tian & Cai, 2017 ;  Zintgraf et al., 2017 ;  Fong & Vedaldi, 2017 ) and smoothing ( Fong & Vedaldi, 2017 ). These perturbation schemes have several limitations. First, they are deficient in pinpointing only the most discriminative, i.e., salient features that are essential for the fine-grained classification tasks where the between-class shape similarity is very high; for example, pinpointing only the red face of Red-faced Cormorant in the bird classification task in  Figure 1  is crucial for explaining why a deep learning model classifies the image as Red-faced Cormorant. Second, the conventional perturbation schemes highly suffer from local noise and, thus, fuse maps from a con- Under review as a conference paper at ICLR 2020 siderable number of perturbations for a reliable explanation. This is the main cause of slowness with conventional black-box methods. Inspired by the lighting and shadowing phenomena in nature, we propose a simple yet effective black-box method, called Gaussian light and shadow (GLAS), which simulates feature perturbation as the presence or the absence of light at the pixel level of an image. The primary idea of GLAS is to perturb an input image by the Gaussian mask (light) and inverse-Gaussian mask (shadow) and, then, record the responses of the perturbed images. GLAS uses a simple grid search; once completed over the entire image, the response maps are fused to construct the final heat map. The fusion mimics the Gaussian mixture. The proposed method has several advantages compared with other black-box methods ( Petsiuk et al., 2018 ;  Zintgraf et al., 2017 ;  Fong & Vedaldi, 2017 ). First, GLAS provides scalability of explanation that we can achieve by adjusting the variance parameter of the Gaussian mask. The scalability makes it possible to pinpoint clues for the salient explanation, which is not feasible with the nonparameterized approaches ( Mahendran & Vedaldi, 2016 ;  Zhou et al., 2016 ;  Zeiler & Fergus, 2013 ;  Petsiuk et al., 2018 ;  Zintgraf et al., 2017 ;  Fong & Vedaldi, 2017 ). The salient explanation is valid for explaining fine-grained classifications, such as classifying bird species in a CUB200 dataset, involving large between-class similarity and significant within-class variance.  Figure 1  shows an image of the Red-faced cormorant species that we can discriminate by identifying the face color. It illustrates that GLAS adjusts its gaze from the body to the red face as the scale parameter decreases and finally pinpoints the red area around the eye. Second, our pixel-wise multiplication operation with the Gaussian mask at a specific search point simulates the gradual dimming effect as going farther from the center. We argue that because of this characteristic, a significantly reduced number of perturbations is sufficient. GLAS can process an image much faster than conventional methods. To summarize, the contributions of this paper are as follows: We introduce a novel notion of salient explanation which is critical in explaining the fine-grained classification tasks. We propose a simple yet efficient black-box method, GLAS, which provides an easy way to perturb an input image based on Gaussian lighting and shadowing. (1) GLAS is fast because of the smoothly varying shape of the Gaussian mask, which generates a visual explanation up to one order of magnitude faster than other black-box methods. (3) We show the broad applicability of GLAS to various other tasks: object localization and visual captioning. Quantitative comparisons show that GRAS is superior to conventional methods.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes MxPool, a hierarchical graph representation learning method for graph classification tasks. MxPool comprises multiple graph convolution networks to learn node-level representations and multiple graph pooling networks to coarsen the graph. The node-level representations and the coarsened graphs are merged in a learnable way, respectively, and used to generate a new coarsened graph. Experiments on numerous graph classification benchmarks show that MxPool has marked superiority over other state-of-the-art graph representation learning methods, achieving 92.1% accuracy on the D&D dataset while the second best method DiffPool only achieves 80.64% accuracy.",
        "Abstract": "Graphs are known to have complicated structures and have myriad applications. How to utilize deep learning methods for graph classification tasks has attracted considerable research attention in the past few years. Two properties of graph data have imposed significant challenges on existing graph learning techniques. (1) Diversity: each graph has a variable size of unordered nodes and diverse node/edge types. (2) Complexity: graphs have not only node/edge features but also complex topological features. These two properties motivate us to use multiplex structure to learn graph features in a diverse way. In this paper, we propose a simple but effective approach, MxPool, which concurrently uses multiple graph convolution networks and graph pooling networks to build hierarchical learning structure for graph representation learning tasks. Our experiments on numerous graph classification benchmarks show that our MxPool has marked superiority over other state-of-the-art graph representation learning methods. For example, MxPool achieves 92.1% accuracy on the D&D dataset while the second best method DiffPool only achieves 80.64% accuracy.",
        "Introduction": "  INTRODUCTION Graphs are known to have complicated structures and have myriad of real world applications. Re- cently, great efforts have been put on utilizing deep learning methods for graph data analysis. Many newly proposed graph learning approaches are inspired by Convolutional Neural Networks (CNNs) ( LeCun & Bengio, 1998 ), which have been greatly successful in learning two-dimensional image data (grid structure). The convolution and pooling layers in CNNs have been redefined to process graph data. Multitude of different Graph Convolutional Networks (GCNs) ( Shuman et al., 2013 ) have been proposed, which can learn node level representations by aggregating feature information from neighbors (spatial-based approaches) ( Hamilton et al., 2017 ) or by introducing filters from the perspective of graph signal processing (spectral-based approaches) ( Bengio & LeCun, 2014 ). On the other hand, similar to the original pooling layer which comes with CNNs, graph pooling module ( Defferrard et al., 2016 ;  Zhang et al., 2018 ) could easily reduce the variance and computation com- plexity by down-sampling from original feature data, which is of vital importance, particularly for graph level classification tasks. Recently, hierarchical pooling methods that can learn hierarchical representations of graphs have been proposed ( Ying et al., 2018 ; Gao & Ji, 2019;  Lee et al., 2019 ) and shows state-of-the-art performance for graph classification tasks. However, two properties of graph data have imposed significant challenges on existing graph learn- ing techniques. 1) Diversity: each graph has a variable size of unordered nodes and has diverse node/edge types. 2) Complexity: graphs have not only node/edge features but also complex topo- logical features. These two properties can bring troubles in both of the graph convolution operation and the graph pooling operation. For example, when performing node-representation learning tasks (by graph convolution operation), it is enough to use small output embedding size for simple and small graphs, as shown in Figure 1(a), since large embedding size could result in overfitting problem. By contrast, it is necessary to set large output embedding sizes for complex and large graphs to learn complex graph structure properties, as shown Figure in 1(b). This creates a contradiction when processing a set of irregular graphs. For another example, when coarsening graphs (by graph pooling operation), if more attention is put on the graph structure, we may obtain a coarsened graph as shown in Figure 1(c). If more Under review as a conference paper at ICLR 2020 attention is put on the node features, we may obtain another coarsened graph as shown in Figure 1(d). This creates another contradiction when processing graphs with not only node features but also topological features. The diversity property and the complexity property of graph data motivate us to use multiplex GNN structure to learn graph features in a diverse way. On the other hand, as known, a common solu- tion for augmenting the traditional CNN convolution layers is to use multiple convolution kernels in order to learn multiple local features. The success of CNNs on image data also inspires us to concurrently use multiple graph convolution networks and multiple graph pooling networks to learn graph representations. In this paper, we propose MxPool in hierarchical graph representation learning for graph classifica- tion tasks 1 . MxPool comprises multiple graph convolution networks to learn node-level represen- tations and also comprises multiple graph pooling networks to coarsen the graph. The node-level representations resulted from multiple convolution networks and the coarsened graphs resulted from multiple pooling networks are merged in a learnable way, respectively. The merged node represen- tations and the merged coarsened graph are then used to generate a new coarsened graph, which is used in the next layer. This multiplex structure can adapt to graphs with different sizes and can extract useful information from different perspectives. We conduct extensive experiments on numerous graph classification benchmarks and show that our MxPool has marked superiority over other state-of-the-art graph representation learning methods. For example, MxPool achieves 92.1% accuracy on the D&D dataset while the second best method DiffPool only achieves 80.64% accuracy.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an automated machine learning (AutoML) approach to exploit memorization effects in deep neural networks when learning with noisy labels. The paper examines the behaviors of memorization effect from multiple perspectives and derives an expressive search space for exploiting memorization. An efficient algorithm is designed using natural gradient descent and is shown to be significantly faster than other popular search algorithms. Experiments on synthetic, benchmark, and real data sets demonstrate that the proposed method can achieve better performance than the state-of-the-art sample-selection approaches designed by humans.",
        "Abstract": "Sample-selection approaches, which attempt to pick up clean instances from the training data set, have become one promising direction to robust learning from corrupted labels. These methods all build on the memorization effect, which means deep networks learn easy patterns first and then gradually over-fit the training data set. In this paper, we show how to properly select instances so that the training process can benefit the most from the memorization effect is a hard problem. Specifically, memorization can heavily depend on many factors, e.g., data set and network architecture. Nonetheless, there still exists general patterns of how memorization can occur. These facts motivate us to exploit memorization by automated machine learning (AutoML) techniques. First, we designed an expressive but compact search space based on observed general patterns. Then, we propose to use the natural gradient-based search algorithm to efficiently search through space. Finally, extensive experiments on both synthetic data sets and benchmark data sets demonstrate that the proposed method can not only be much efficient than existing AutoML algorithms but can also achieve much better performance than the state-of-the-art approaches for learning from corrupted labels.",
        "Introduction": "  INTRODUCTION Learning with deep neural networks has enjoyed huge empirical success in recent years across a wide variety of tasks, from image processing to speech recognition, and from language modeling to recommender system (Goodfellow et al., 2016). However, their success highly counts on the availability of well-annotated and big data, which is barely available for real-world applications. Instead, what we are facing with in practice are large data sets which are collected from crowd- sourcing platforms or crawled from the Internet, thus containing many noisy labels (Li et al., 2017b; Patrini et al., 2017). Besides, due to the vast learning capacity of deep networks, they will eventually over-fit on these noisy labels, leading to poor predicting performance, which can be worse than that obtained from simple models (Zhang et al., 2016; Arpit et al., 2017). To reduce negative effects from noisy labels, many methods have been proposed (Sukhbaatar et al., 2015; Reed et al., 2015; Patrini et al., 2017; Ghosh et al., 2017; Malach & Shalev-Shwartz, 2017) . Recently, a promising direction is training networks only on selected instances that are more likely to be clean (Jiang et al., 2018; Han et al., 2018b; Ma et al., 2018; Yu et al., 2019; Wang et al., 2019). Intuitively, as the training data becomes less noisy, better performance can be obtained. Among those works, the representative methods are MentorNet (Jiang et al., 2018) and Co-teaching (Han et al., 2018b; Yu et al., 2019), they take small-loss samples in each mini-batch as clean instances. Specifically, MentorNet pre-trains an extra network, and then uses the extra network for selecting clean instances to guide the training. When the clean validation data is not available, MentorNet has to use a predefined curriculum (Bengio et al., 2009). Co-teaching is an improvement over MentorNet, it simultaneously maintains two networks which have identical architectures during the training process. And in each mini-batch of data, each network is updated using the other network's small-loss instances. To the success of these sample-selection methods, the memorization effect of deep networks (Zhang et al., 2016; Arpit et al., 2017) is the crux. Memorization happens widely in various architectures of deep network, e.g., multilayer perceptron (MLP) and convolutional neural network (CNN). Specif- ically, it means that deep networks tend to learn easy and correct patterns first and then over-fit on Under review as a conference paper at ICLR 2020 (possibly noisy) training data set (see Fig.1(a)-(b)). Thus, when learning with noisy labels, while the validation loss will first increase and then significantly decrease, the training loss will continuously get smaller with more training epochs. Due to such effect, sample-selection methods can learn correct patterns at early stage and then use the obtained discriminative ability to filter out corrupted instances in subsequent training epochs (Jiang et al., 2018; Han et al., 2018b; Chen et al., 2019). While the memorization effect is critical to the success of sample-selection methods, however, how to properly exploit it is not addressed in the literature. And trivial attempts can easily lead to even worse performance than standard deep networks (Han et al., 2018b). Some recent endeavors seek to evade from this problems by integrating with other auxiliary information, e.g., a small clean subset is used in (Ren et al., 2018), and knowledge graphs are utilized in (Li et al., 2017b). In this paper, motivated by the success of automated machine learning (AutoML) on designing data-dependent models (Hutter et al., 2018), and the fact that memorization heavily depends on many factors (Zhang et al., 2016; Arpit et al., 2017), we propose to exploit memorization effects automatically using AutoML techniques. Contributions are summarized as follows: • First, to have an in-depth understanding of why it is difficult to tune sample-selection methods with good performance. We examine behaviors of memorization effect from multiple perspec- tives. We find that, while there exist general patterns in how memorization occurs with the train- ing process (see Fig.1(a)-(b)), it is hard to quantize to which extend such effect can happen (see Fig.1(b)-(f)). Especially, memorization can be affected by many factors, e.g., data sets, network architectures, and the choice of the optimizers. It is exactly such complex dependency make the design of proper sample-selection rules a hard problem, which motivates us to solve the problem by AutoML techniques. • To make good use of AutoML techniques, we then derive an expressive search space for exploiting memorization, which is from the above observations, i.e., the curvature of how many instances need to be sampled during iterating should be similar with the inverse of the learning curve on the validation set. Such a space is not too huge since it has only a few variables, thus allows subsequent algorithms converging fast to promising candidates. • Then, to design an efficient algorithm, we show the failure of gradient-based methods and the inefficiency of derivative-free methods. These motivate us to take a probabilistic view of the search problem and adopt natural gradient descent (Amari, 1998; Pascanu & Bengio, 2013) for optimization. The designed algorithm can effectively address above problems and is significantly faster than other popular search algorithms. • Finally, we conduct extensive experiments on both synthetic, benchmark, and real data sets, under various settings using different network architectures. These experiments demonstrate that the proposed method can not only be much more efficient than existing AutoML algorithms, but also can achieve much better performance than the state-of-the-art sample-selection approaches designed by humans. Besides, we further visualize and explain the searched functions, which can also help design better rules to control memorization effects in the future.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel sequence-to-sequence (seq2seq) model, wave2wave, to translate continuous time-series signals of different lengths and high dimensions. The model consists of a sliding window-based representation function and a wave2wave iterative back-translation model. The proposed model is evaluated on motion capture and accelerometer data and is shown to outperform conventional seq2seq models.",
        "Abstract": "The understanding of sensor data has been greatly improved by advanced deep learning methods with big data. However, available sensor data in the real world are still limited, which is called the opportunistic sensor problem.  This paper proposes a new variant of neural machine translation seq2seq to deal with continuous signal waves by introducing the window-based (inverse-) representation to adaptively represent partial shapes of waves and the iterative back-translation model for high-dimensional data.  Experimental results are shown for two real-life data: earthquake and activity translation.  The performance improvements of one-dimensional data was about 46 % in test loss and that of high-dimensional data was about 1625 % in perplexity with regard to the original seq2seq.\n",
        "Introduction": "  INTRODUCTION The problem of shortage of training data but can be supplied by other sensor data is called an opportunistic sensor problem ( Roggen et al., 2013 ). For example in human activity logs, the video data can be missing in bathrooms by ethical reasons but can be supplied by environmental sensors which have less ethical problems. For this purpose we propose to extend the sequence-to-sequence (seq2seq) model ( Cho et al., 2014 ;  Sutskever et al., 2014 ;  Dzmitry Bahdanau, 2014 ;  Luong et al., 2015 ) to translate signal wave x (continuous time-series signals) into other signal wave y. The straight-forward extension does not apply by two reasons: (1) the lengths of x and y are radically different, and (2) both x and y are high dimensionals. First, while most of the conventional seq2seq models handle the input and output signals whose lengths are in the same order, we need to handle the output signals whose length are sometimes con- siderably different than the input signals. For example, the sampling rate of ground motion sensor is 100Hz and the duration of an earthquake is about 10sec. That is, the length of the output signal wave is 10000 times longer in this case. Therefore, the segmentation along temporal axis and dis- carding uninformative signal waves are required. Second, signal waves could be high dimensionals; motion capture data has 129 dimensions and acceleormeter data has 18 dimensions. While most of the conventional seq2seq does not require the high-dimensional settings, meaning that it is not usual to translate multiple languages simultaneously, we need to translate signal waves in high dimensions into other signal waves in high dimensions simultaneously. To overcome these two problems we propose 1) the window-based representation function and 2) the wave2wave iterative back-translation model in this paper. Our contributions are the following: • We propose a sliding window-based seq2seq model wave2wave (Section 4.1), • We propose the wave2wave iterative back-translation model (Section 4.2) which is the key to outperform for high-dimensional data.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper provides new insights into the relationship between flatness of the loss surface around local minima, robustness, and generalization of deep neural networks. We define a property of feature robustness that measures the change of the loss function under small perturbations of the features, and propose a novel flatness measure that is invariant under layer-wise reparameterization. We also define a suitable notion of representativeness of a dataset connecting feature robustness to the generalization error in form of an upper bound. The proposed flatness measure is empirically shown to strongly correlate with good generalization performance.",
        "Abstract": "The performance of deep neural networks is often attributed to their automated, task-related feature construction. It remains an open question, though, why this leads to solutions with good generalization, even in cases where the number of parameters is larger than the number of samples. Back in the 90s, Hochreiter and Schmidhuber observed that flatness of the loss surface around a local minimum correlates with low generalization error. For several flatness measures, this correlation has been empirically validated. However, it has recently been shown that existing measures of flatness cannot theoretically be related to generalization: if a network uses ReLU activations, the network function can be reparameterized without changing its output in such a way that flatness is changed almost arbitrarily. This paper proposes a natural modification of existing flatness measures that results in invariance to reparameterization. The proposed measures imply a robustness of the network to changes in the input and the hidden layers. Connecting this feature robustness to generalization leads to a generalized definition of the representativeness of data. With this, the generalization error of a model trained on representative data can be bounded by its feature robustness which depends on our novel flatness measure.",
        "Introduction": "  INTRODUCTION Neural networks (NNs) have become the state of the art machine learning approach in many appli- cations. An explanation for their superior performance is attributed to their ability to automatically learn suitable features from data. In supervised learning, these features are learned implicitly through minimizing the empirical error E emp (f, S) = 1 /|S| (x,y)∈S (f (x), y) for a training set S ⊂ X × Y drawn iid according to a target distribution D : X × Y → [0, 1], and a loss function : Y × Y → R + . Here, f : X → Y denotes the function represented by a neural network. It is an open question why minimizing the empirical error during deep neural network training leads to good generalization, even though in many cases the number of network parameters is higher than the number of training examples. That is, why deep neural networks have a low generalization error which is the difference between expected error on the target distribution D and the empirical error on a finite dataset S ⊂ X × Y. It has been proposed that good generalization correlates with flat minima of the non-convex loss surface ( Hochreiter & Schmidhuber, 1997 ; 1995) and this correlation has been empirically vali- dated ( Keskar et al., 2016 ;  Novak et al., 2018 ;  Wang et al., 2018 ). Thus, for deep neural networks trained with stochastic gradient descent (SGD), this could present a (partial) explanation for their generalization performance ( Zhang et al., 2016 ), since minibatch SGD tends to converge to flat local minima ( Zhang et al., 2018 ;  Jastrzębski et al., 2017 ). This idea was elaborated on by  Chaudhari et al. (2016)  who suggest a new training method that favors flat over sharp minima even at the cost of a slightly higher empirical error - indeed solutions found by this algorithm exhibit bet- ter generalization performance. Similarly,  Dziugaite & Roy (2017)  augment the loss to improve generalization and find that this promotes flat minima. However, as  Dinh et al. (2017)  remarked, current flatness measures-which are based only on the Hessian of the loss function-cannot Under review as a conference paper at ICLR 2020 theoretically be related to generalization: For deep neural networks with ReLU activation func- tions, there are layer-wise reparameterizations that leave the network function unchanged (hence, also the generalization performance), but change any measure derived only from the loss Hessian. Another, more intuitive explanation for generalization is that the function generalizes well if the extracted features encode a semantic similarity of the input that is robust to small changes-both in the input and the features. This al- lows to generalize from the training set to novel, sufficiently similar data. Starting from such a concept of robustness with respect to changes of features, we derive a measure of flatness that is invariant under the mentioned reparameteri- zations and that reduces to the well-known ridge regression penalty in the special case of a linear regression. This brings three seemingly related properties into our fo- cus: flatness, robustness, and generalization. The exact relationship, however, between flatness of the loss surface around local minima (measuring changes of the empiri- cal error for perturbations in parameter space), robustness (measuring changes of the error for perturbations in either input or feature space), and generalization (performance on unseen data from the target distribution) is not well-understood. This paper provides new insights into this relationship. The notion of feature robustness proposed in this paper measures the robustness of a function f = ψ • φ (e.g., a neural network) toward local changes in a feature space. That is, f can be split into a composition of functions f (x) = (ψ • φ)(x) for x ∈ X , φ : X → R m and ψ : R m → Y. The function φ is considered as a feature extraction, mapping the input X into a feature space R m , while the function ψ corresponds to the model (e.g., a classifier) with R m as its domain (see  Figure 1  for illustration). It is the feature space defined by φ where we measure robustness toward small perturbations. For neural networks, the activation values of any but the output layer can be viewed as a feature space. A function f is called -feature robust on a dataset S ⊂ X × Y if small changes in the feature space defined by φ do not change the empirical error by more than . This differs from the notion of robustness defined by  Xu & Mannor (2012)  using a cover of the sample space, which has been theoretically connected to generalization. Flatness of the loss surface, however, is a local property and we require a more local version of robustness to derive a connection between flatness and robustness. Then, indeed, feature-robustness is upper bounded by the proposed flatness measure. To finally connect the two local properties of robustness and flatness to generalization, we necessarily need a notion describing how representative the given samples are for the true distribution. We define a suitable notion, leading to an upper bound for the generalization error given by feature robustness together with representativeness. In summary, our contributions are as follows: (i) For models of the form f (x) = (ψ • φ)(x) (e.g. most (deep) neural networks) that split up into a feature extractor φ and a model ψ on the feature space defined by φ, we define a property of feature robustness that measures the change of the loss function under small perturbations of the features. This property is strongly related to flatness of the loss surface at local minima. (ii) We propose a novel flatness measure. For neural networks with ReLU activation functions, it is invariant under layer-wise reparameterization, addressing a shortcoming of previous measures of flatness. (iii) We define a suitable notion of representativeness of a dataset connecting feature robustness to the generalization error in form of an upper bound. (iv) The proposed flatness measure is empirically shown to strongly correlate with good generalization performance. Thereby, we recover Hessian based quantities as measures of flatness.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the fundamental connections between quantum entanglement and deep neural networks, and proposes a method to quantitatively calculate entanglement entropy. We demonstrate that a low-dimensional attention matrix can encode certain information of the high-dimensional matching matrix, and use this matrix to calculate entanglement entropy. We then use this entropy to adaptively design convolutional networks for text matching tasks, which significantly improves the performance on two typical tasks.",
        "Abstract": "The formal understanding of deep learning has made great progress based on quantum many-body physics.  For example, the entanglement entropy in quantum many-body systems can interpret the inductive bias of neural network and then guide the design of network structure and parameters for certain tasks. However, there are two unsolved problems in the current study of entanglement entropy, which limits its application potential. First, the theoretical benefits of entanglement entropy was only investigated in the representation of a single object (e.g., an image or a sentence), but has not been well studied in the matching of two objects (e.g., question-answering pairs). Second,  the entanglement entropy can not be qualitatively calculated since  the exponentially increasing dimension of the matching matrix. In this paper, we are trying to address these two problem by investigating the fundamental connections between the entanglement entropy and the attention matrix. We prove that by a mapping (via the trace operator) on the high-dimensional matching matrix,  a low-dimensional attention matrix can be derived. Based on such a attention matrix, we can provide a feasible solution to the entanglement entropy that describes the correlation between the two objects in matching tasks. Inspired by the theoretical property of the entanglement entropy, we can design the network architecture adaptively in a typical text matching task, i.e., question-answering task.",
        "Introduction": "  INTRODUCTION Fundamental connections between neural network and quantum mechanics have been built ( Car- leo & Troyer, 2017 ;  Levine et al., 2018 ;  Cai & Liu, 2018 ). Neural networks are adopted to solve the quantum many-body problem ( Carleo & Troyer, 2017 ;  Cai & Liu, 2018 ), while quantum many- body function has been involved to explain the expressive ability of the neural networks ( Levine et al., 2018 ). The formal understanding of deep neural network has made great progress based on quantum many-body physics.  Levine et al. (2019)  summarize the theoretical concepts and proper- ties of quantum entanglement in different neural network architectures. Specifically, for recurrent networks, the Start-End separation rank as a method of measuring quantum entanglement, in order to quantitatively describe the depth effectiveness for modeling long-term memory capacity ( Levine et al., 2017 ; 2019). For deep convolutional network,  Levine et al. (2017 ; 2019), shows that the entanglement entropy can help us understand the inductive bias of the neural network. Such under- standing also provide a guidance of how to control the inductive bias of the designed network, via the network parameters (e.g., channel numbers). However, there are still two limitations in the current study ( Levine et al., 2017 ; 2019;  Zhang et al., 2018b ). First, the quantum entanglement measure (i.e., the entanglement entropy) is considered as the indicator of the network's expressive ability, but such an indicator only reflects the intricate correlation structures of each single input object (e.g., an image or a text). In other words, previous theoretical analyses are conducted only for the representation of a single object. However, for the Under review as a conference paper at ICLR 2020 matching problem, the theoretical analysis should be extended, in order to model the correlation distributions between two objects (e.g., question-answering pairs). On the other hand, the quantitative calculation of the entanglement entropy is also a problem. In the- ory, the entanglement entropy has a relation with the network parameters reflecting the inductive bias of the network. However, in practice, the entanglement entropy remains infeasible to calculate. This is due to the fact that the tensor product occurs in the quantum many-body function for representing the image and text ( Levine et al., 2017 ;  Zhang et al., 2018b ), and the dimension of the matching ma- trix representing the quantum entanglement between two systems will increase exponentially. This leads to the difficulty in solving the entangled entropy by the singular value decomposition (SVD) of such a high-dimensional matching matrix. In this paper, we are trying to address these two problem by investigating the fundamental con- nections between the entanglement entropy and the attention matrix. Under certain conditions and mappings, we demonstrate that a low-dimensional attention matrix can encode certain information (e.g., the trace of the block matrix) of the high-dimensional matching matrix. Then based on the low- dimensional attention matrix, we can quantitatively calculate entanglement entropy. Specifically, we can represent the high-dimensional matching matrix by block matrices, and then compute the trace for each block. We will show that the resulting matrix is equivalent to the attention matrix based on the cosine distance calculation. By the SVD decomposition of this matrix, the singular values are obtained to solve the entanglement entropy quantitatively. In our work, inspired by ( Levine et al., 2018 ), we can adaptively design convolutional network based on the calculable entanglement entropy. Specifically, for the more complex the inputs (i.e., the long- range correlations), more kernels should be assigned in the relatively-deeper layers, while more kernels should be assigned in the relatively-shallower layers for short-range correlations. Intuitively, in text matching task like question answering system, the short-range correlations can refer to some simple question-answering pairs with many common words between question and answer sentences, which can be matched locally by some overlapping features (e.g., the statistics of a single word or nearby word combinations like N-gram). While long-range correlations refer to the question- answering pairs with less common words which their effective matching may need higher-level semantic information extracted from a global context. This strategy significantly improves the final performance on two typical text matching tasks called TREC-QA and YAHOO-QA. In particular,our model sets a new state-of-the-art performance in TREC-QA, with 2.9% absolute improvements.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a differentiable Bayesian neural network (DBNN) inference with respect to an input data, which is an approximation of Bayesian neural network inference for data streams. DBNN uses an online codevector histogram to approximate probabilities and is nonparametric, applied to trained BNN without significant modifications. The proposed DBNN is able to estimate both predictive results and uncertainties, and has comparable computational performance to deep deterministic neural networks (DNNs). The results of DBNN are evaluated with semantic segmentation using road scene video sequences, and show that DBNN has almost no degradation in performance compared to deep DNNs.",
        "Abstract": "While deep neural networks (NNs) do not provide the confidence of its prediction, Bayesian neural network (BNN) can estimate the uncertainty of the  prediction. However, BNNs have not been widely used in practice due to the computational cost of predictive inference. This prohibitive computational cost is a hindrance especially when processing stream data with low-latency. To address this problem, we propose a novel model which approximate BNNs for data streams. Instead of generating separate prediction for each data sample independently, this model estimates the increments of prediction for a new data sample from the previous predictions. The computational cost of this model is almost the same as that of non-Bayesian deep NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs, estimating uncertainty comparable to the results of BNNs.\n",
        "Introduction": "  INTRODUCTION While deterministic neural networks (DNNs) surpass human capability in some area in terms of prediction accuracy (He et al., 2015;  Silver et al., 2016 ;  Ardila et al., 2019 ), it could not estimate a trustworthy uncertainty of prediction. Since the prediction can not be perfect and the misprediction might result in fatal consequences in areas such as medical analysis and autonomous vehicles control, estimating uncertainty as well as predictions will be crucial for the safer application of machine learning based systems. Bayesian neural network (BNN), a neural network (NN) that uses probability distributions as weights, estimates not only predictive results but also uncertainties. This allows computer systems to make better decisions by combining uncertainty with prediction. Moreover, BNN can achieve high per- formance in a variety of fields, e.g. image recognition (Kendall et al., 2015;  Kendall & Gal, 2017 ), language modeling ( Fortunato et al., 2017 ), reinforcement learning (Kahn et al., 2017; Osband et al., 2018), meta-learning (Yoon et al., 2018;  Finn et al., 2018 ), and multi-task learning (Kendall et al., 2018), by exploiting uncertainty. Although BNNs have these theoretical advantages, they have not been used as a practical tool. The predictive inference speed has detained BNNs from wide applications. BNN executes NN inference for dozens of samples from weight distributions. Since sampling and multiple NN executions are difficult to be parallelized, the inference execution takes an order of magnitude more time. Particularly, this is a significant barrier for processing data streams with low-latency. Most time-varying data streams change continuously, and so do the predictions of BNNs. Thus, we estimate prediction by calculating the increments between two consecutive results, instead of calculating the separate prediction for each input data. This is equivalent to calculating the differentiation of the BNN's prediction for arbitrary data, because the difference of prediction for the new data is the line integration of the gradient of prediction over the new data. In this work, we propose a differentiable BNN (DBNN) inference with respect to an input data. The prediction of the DBNN is given by a Monte Carlo (MC) estimator for distributions of data streams and weights. We speed up the inference by approximating the distribution using histogram and calculating the gradient for this MC estimator. We show that the time complexity of this model is nearly the same as that of deep DNNs. We evaluate DBNN with semantic segmentation using road scene video sequences and the results show that DBNN has almost no degradation in computational Under review as a conference paper at ICLR 2020 performance compared to deep DNNs. The uncertainty predicted by DBNN is comparable to that of BNN in various situations. The main contributions of this work are as follows. • We propose online codevector histogram that estimates the probability of a high-dimensional data stream. Then, we show that this histogram can be used to obtain the MC gradient estimation. • We propose differentiable Bayesian neural network inference with respect to input data as an approximation of Bayesian neural network inference for data streams. DBNN uses the online codevector histogram to approximate probabilities. This model is nonparametric and applied to trained BNN without significant modifications. • We theoretically and empirically show that the computational performance of DBNN is almost the same as that of deep DNNs.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel method for generative models, Spherical Auto-Encoder (SAE), which circumvents the problem of variational inference in Variational Auto-Encoder (VAE). SAE is based on the volume concentration of high-dimensional spheres and the probability distribution of distances between two arbitrary sets of random points on the sphere in high dimensions. The paper also provides theoretical analysis and extensive experiments to validate the claims of SAE.",
        "Abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.",
        "Introduction": "  INTRODUCTION Deep generative models, such as Variational Auto-Encoder (VAE) ( Kingma & Welling, 2013 ; Rezende et al., 2014) and Generative Adversarial Network (GAN) ( Goodfellow et al., 2014 ), play more and more important role in machine learning and computer vision. However, the problem of variational inference in VAE is still challenging, especially for high-dimensional data like images. To be formal, let X = {x 1 , . . . , x n } denote the set of observable data points and Z = {z 1 , . . . , z n } the set of desired latent vectors, where x i ∈ R dx and z i ∈ R dz . Let p g (x|z) denote the likelihood of generated sample conditioned on latent variable z and p(z) the prior, where g denotes the decoder . The encoder f in VAE parameterizes the variational posterior q f (z|x) in light of the lower bound of the marginal log-likelihood The first term D KL [q f (z|x)||p(z)] constrains the encoded latent codes to the prior via the KL- divergence, and the second term E q [log p g (x|z)] serves to guarantee the reconstruction accuracy of inputs. For a Gaussian p g (x|z) of diagonal covariance matrix, log p g (x|z) reduces to the variance- weighted squared error ( Doersch, 2016 ). The lower-bound approximation of the log-likelihood provides a feasible solution for VAE. But it also causes new problems. For example, the generated sample g(z) deviates from the real distribution of X when sampling from the given prior due to that the learnt q f (z|x) is incapable of matching the prior distribution well. Besides, the reconstruction g(f (x)) is not satisfactory either. For imagery data, bluriness usually occurs. In order to manipulate real images for GAN models, we usually need to formulate an encoder via the framework of VAE. The variational inference also applies in this scenario. The problems of VAE are the obstacles of putting the GAN encoder in the right way either. There are other methods of learning Under review as a conference paper at ICLR 2020 (a) The prior z is projected on the sphere. (b) The prior z is not projected on the sphere. an encoder for GAN in the adversarial way such as ( Dumoulin et al., 2017 ;  Li et al., 2017 ;  Ulyanov et al., 2017 ;  Heljakka et al., 2018 ). However, the structural precision of reconstruction is generally inferior to the VAE framework, because the high-level semantics of objects outweigh low-level structures for such methods ( Donahue & Simonyan, 2019 ). Besides, the concise architecture of VAE is more preferred in this scenario. Therefore, learning precise latent variables z = f (x) is critical to applications of VAE and GAN. Using a different theory in this paper, we propose a simple method to circumvent the problem. Our contributions are summarized as follows. 1) We introduce the volume concentration of high- dimensional spheres. Based on the concentration property, we point out that projecting on a sphere for data that are distributed according to the spherical mass produces little difference from the viewpoint of the volume in high-dimensional spaces. Thus, it is plausible to perform inference pertaining to VAE on the sphere. 2) We further analyze the probability distribution of distances between two arbitrary sets of random points on the sphere in high dimensions and illustrate the phenomenon of distance convergence. Furthermore, we prove that the Wasserstein distance between two arbitrary datasets randomly drawn from a high-dimensional sphere are nearly identical, meaning that the data on the sphere are distribution-robust for generative models with respect to Wasserstein distance. 3) Based on our theoretical analysis, we propose a very simple algorithm for sampling generative models. The same principle is also harnessed to reformulate VAE. The spherical normalization is simply put on latent variables instead of variational inference while preserving randomness of latent variables by centerization. In contrast to VAE and variational inference, we name such an autoencoder as Spherical Auto-Encoder (SAE) and the associated inference as spherical inference. 4) We perform extensive experiments to validate our theoretical analysis and claims with sampling and inference.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel Transformer distillation method and two-stage learning framework for compressing large-scale pre-trained language models (PLMs) such as BERT. The proposed method encourages the transfer of linguistic knowledge from the teacher BERT to the student TinyBERT, and ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT. Experiments show that our TinyBERT can achieve more than 96% the performance of teacher BERT BASE on GLUE tasks, while having much fewer parameters and less inference time, and significantly outperforms other state-of-the-art baselines on BERT distillation.",
        "Abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be well transferred to a small “student” TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them.\n",
        "Introduction": "  INTRODUCTION Pre-training language models then fine-tuning on downstream tasks has become a new paradigm for natural language processing (NLP). Pre-trained language models (PLMs), such as BERT ( Devlin et al., 2018 ), XLNet ( Yang et al., 2019 ), RoBERTa ( Liu et al., 2019 ) and SpanBERT ( Joshi et al., 2019 ), have achieved great success in many NLP tasks (e.g., the GLUE benchmark ( Wang et al., 2018 ) and the challenging multi-hop reasoning task ( Ding et al., 2019 )). However, PLMs usually have an extremely large number of parameters and need long inference time, which are difficult to be deployed on edge devices such as mobile phones. Moreover, recent studies ( Kovaleva et al., 2019 ) also demonstrate that there is redundancy in PLMs. Therefore, it is crucial and possible to reduce the computational overhead and model storage of PLMs while keeping their performances. There has been many model compression techniques ( Han et al., 2015a ) proposed to accelerate deep model inference and reduce model size while maintaining accuracy. The most commonly used techniques include quantization ( Gong et al., 2014 ), weights pruning ( Han et al., 2015b ), and knowledge distillation (KD) ( Romero et al., 2014 ). In this paper we focus on knowledge distillation, an idea proposed by  Hinton et al. (2015)  in a teacher-student framework. KD aims to transfer the knowledge embedded in a large teacher network to a small student network. The student network is trained to reproduce the behaviors of the teacher network. Based on the framework, we propose a novel distillation method specifically for Transformer-based models ( Vaswani et al., 2017 ), and use BERT as an example to investigate the KD methods for large scale PLMs. KD has been extensively studied in NLP ( Kim & Rush, 2016 ;  Hu et al., 2018 ), while designing KD methods for BERT has been less explored. The pre-training-then-fine-tuning paradigm firstly pre-trains BERT on a large scale unsupervised text corpus, then fine-tunes it on task-specific dataset, which greatly increases the difficulty of BERT distillation. Thus we are required to design an ef- Under review as a conference paper at ICLR 2020 fective KD strategy for both stages. To build a competitive TinyBERT, we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT. Specifically, we design several loss functions to fit different representations from BERT layers: 1) the output of the embedding layer; 2) the hidden states and attention matrices derived from the Transformer layer; 3) the logits output by the prediction layer. The attention based fitting is inspired by the recent findings ( Clark et al., 2019 ) that the attention weights learned by BERT can capture substan- tial linguistic knowledge, which encourages that the linguistic knowledge can be well transferred from teacher BERT to student TinyBERT. However, it is ignored in existing KD methods of BERT, such as Distilled BiLSTM SOFT ( Tang et al., 2019 ), BERT-PKD ( Sun et al., 2019 ) and DistilBERT 2 . Then, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation. At the general distillation stage, the original BERT without fine-tuning acts as the teacher model. The student TinyBERT learns to mimic the teacher's behavior by ex- ecuting the proposed Transformer distillation on the large scale corpus from general domain. We obtain a general TinyBERT that can be fine-tuned for various downstream tasks. At the task-specific distillation stage, we perform the data augmentation to provide more task-specific data for teacher- student learning, and then re-execute the Transformer distillation on the augmented data. Both the two stages are essential to improve the performance and generalization capability of TinyBERT. A detailed comparison between the proposed method and other existing methods is summarized in  Table 1 . The Transformer distillation and two-stage learning framework are two key ideas of the proposed method. The main contributions of this work are as follows: 1) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be well transferred to TinyBERT. 2) We propose a novel two-stage learning framework with performing the proposed Transformer distillation at both the pre-training and fine-tuning stages, which ensures that Tiny- BERT can capture both the general-domain and task-specific knowledge of the teacher BERT. 3) We show experimentally that our TinyBERT can achieve more than 96% the performance of teacher BERT BASE on GLUE tasks, while having much fewer parameters (∼13.3%) and less inference time (∼10.6%), and significantly outperforms other state-of-the-art baselines on BERT distillation.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel stochastic optimization algorithm, PoweredSGD, which modifies the gradient terms via a nonlinear function called the Powerball function. The paper provides convergence proofs and experiments using popular deep learning models and benchmark datasets. The results demonstrate that PoweredSGD can improve the generalization performance of deep neural networks compared to existing methods such as SGD, SGDM, AdaGrad, RMSProp, and Adam.",
        "Abstract": "In this paper, we propose a novel technique for improving the stochastic gradient descent (SGD) method to train deep networks, which we term \\emph{PowerSGD}. The proposed PowerSGD method simply raises the stochastic gradient to a certain power $\\gamma\\in[0,1]$ during iterations and introduces only one additional parameter, namely, the power exponent $\\gamma$ (when $\\gamma=1$, PowerSGD reduces to SGD). We further propose PowerSGD with momentum, which we term \\emph{PowerSGDM}, and provide convergence rate analysis on both PowerSGD and PowerSGDM methods. Experiments are conducted on popular deep learning models and benchmark datasets. Empirical results show that the proposed PowerSGD and PowerSGDM obtain faster initial training speed than adaptive gradient methods,  comparable generalization ability with SGD, and improved robustness to hyper-parameter selection and vanishing gradients. PowerSGD is essentially a gradient modifier via a nonlinear transformation. As such, it is orthogonal and complementary to other techniques for accelerating gradient-based optimization. ",
        "Introduction": "  INTRODUCTION Stochastic optimization as an essential part of deep learning has received much attention from both the research and industry communities. High-dimensional parameter spaces and stochastic objective functions make the training of deep neural network (DNN) extremely challenging. Stochastic gradient descent (SGD) ( Robbins & Monro, 1951 ) is the first widely used method in this field. It iteratively updates the parameters of a model by moving them in the direction of the negative gradient of the objective evaluated on a mini-batch. Based on SGD, other stochastic optimization algorithms, e.g., SGD with Momentum (SGDM) ( Qian, 1999 ), AdaGrad ( Duchi et al., 2011 ), RMSProp ( Tieleman & Hinton, 2012 ), Adam ( Kingma & Ba, 2015 ) are proposed to train DNN more efficiently. Despite the popularity of Adam, its generalization performance as an adaptive method has been demonstrated to be worse than the non-adaptive ones. Adaptive methods (like AdaGrad, RMSProp and Adam) often obtain faster convergence rates in the initial iterations of training process. Their performance, however, quickly plateaus on the testing data ( Wilson et al., 2017 ). In  Reddi et al. (2018) , the authors provided a convex optimization example to demonstrate that the exponential moving average technique can cause non-convergence in the RMSProp and Adam, and they proposed a variant of Adam called AMSGrad, hoping to solve this problem. The authors provide a theoretical guarantee of convergence but only illustrate its better performance on training data. However, the generalization ability of AMSGrad on test data is found to be similar to that of Adam, and a considerable performance gap still exists between AMSGrad and SGD ( Keskar & Socher, 2017 ;  Chen et al., 2018 ). Indeed, the optimizer is chosen as SGD (or with Momentum) in several recent state-of-the-art works in natural language processing and computer vision ( Luo et al., 2018 ;  Wu & He, 2018 ), where in these instances SGD does perform better than adaptive methods. Despite the practical success of SGD, obtaining sharp convergence results in the non-convex setting for SGD to efficiently escape saddle points (i.e., convergence to second-order stationary points) remains a topic of active research ( Jin et al., 2019 ;  Fang et al., 2019 ). Related Works: SGD, as the first efficient stochastic optimizer for training deep networks, iteratively updates the parameters of a model by moving them in the direction of the negative gradient of the Under review as a conference paper at ICLR 2020 objective function evaluated on a mini-batch. SGDM brings a Momentum term from the physical perspective, which obtains faster convergence speed than SGD. The Momentum idea can be seen as a particular case of exponential moving average (EMA). Then the adaptive learning rate (ALR) technique is widely adopted but also disputed in deep learning, which is first introduced by AdaGrad. Contrast to the SGD, AdaGrad updates the parameters according to the square roots of the sum of squared coordinates in all the past gradients. AdaGrad can potentially lead to huge gains in terms of convergence ( Duchi et al., 2011 ) when the gradients are sparse. However, it will also lead to rapid learning rate decay when the gradients are dense. RMSProp, which first appeared in an unpublished work ( Tieleman & Hinton, 2012 ), was proposed to handle the aggressive, rapidly decreasing learning rate in AdaGrad. It computes the exponential moving average of the past squared gradients, instead of computing the sum of the squares of all the past gradients in AdaGrad. The idea of AdaGrad and RMSProp propelled another representative algorithm: Adam, which updates the weights according to the mean divided by the root mean square of recent gradients, and has achieved enormous success. Recently, research to link discrete gradient-based optimization to continuous dynamic system theory has received much attention ( Yuan et al., 2016 ;  Mazumdar & Ratliff, 2018 ). While the proposed optimizer excels at improving initial training, it is completely complementary to the use of learning rate schedules ( Smith & Topin, 2019 ;  Loshchilov & Hutter, 2016 ). We will explore how to combine learning rate schedules with the PoweredSGD optimizer in future work. While other popular techniques focus on modifying the learning rates and/or adopting momentum terms in the iterations, we propose to modify the gradient terms via a nonlinear function called the Powerball function by the authors of  Yuan et al. (2016) . In  Yuan et al. (2016) , the authors presented the basic idea of applying the Powerball function in gradient descent methods. In this paper, we 1) systematically present the methods for stochastic optimization with and without momentum; 2) provide convergence proofs; 3) include experiments using popular deep learning models and benchmark datasets. Another related work was presented in  Bernstein et al. (2018) , where the authors presented a version of stochastic gradient descent which uses only the signs of gradients. This essentially corresponds to the special case of PoweredSGD (or PoweredSGDM) when the power exponential γ is set to 0. We also point out that despite the name resemblance, the power PowerSign optimizer proposed in Bello et al. (2017) is a conditional scaling of the gradient, whereas the proposed PoweredSGD optimizer applies a component-wise trasformation to the gradient.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel framework, called sensible adversary, to overcome the trade-off between natural accuracy and robustness of deep neural networks (DNNs) against adversarial attacks. The framework restricts adversarial perturbations not to cross the Bayes decision boundary besides the -ball constraint, so that the perturbation ball is adaptively modified for every single data point. We theoretically establish the Bayes rule is most robust against the sensible adversary and propose an efficient algorithm for sensible adversarial training. We experimentally demonstrate that sensible adversarial training enables to stably learn a robust and accurate model.",
        "Abstract": "The trade-off between robustness and standard accuracy has been consistently reported in the machine learning literature. Although the problem has been widely studied to understand and explain this trade-off, no studies have shown the possibility of a no trade-off solution. In this paper, motivated by the fact that the high dimensional distribution is poorly represented by limited data samples, we introduce sensible adversarial learning and demonstrate the synergistic effect between pursuits of natural accuracy and robustness. Specifically, we define a sensible adversary which is useful for learning a defense model and keeping a high natural accuracy simultaneously. We theoretically establish that the Bayes rule is the most robust multi-class classifier with the 0-1 loss under sensible adversarial learning. We propose a novel and efficient algorithm that trains a robust model with sensible adversarial examples, without a significant drop in natural accuracy. Our model on CIFAR10 yields state-of-the-art results against various attacks with perturbations restricted to l∞ with ε = 8/255, e.g., the robust accuracy 65.17% against PGD attacks as well as the natural accuracy 91.51%.\n",
        "Introduction": "  INTRODUCTION With many impressive successes of deep learning, there are a multitude of applications of deep neural networks (DNNs) that permeate in our everyday life. As DNNs are applied in security- critical systems such as malware detection, face identification, and autonomous driving, robustness of DNNs against adversarial attacks, i.e., the intently perturbed inputs to fool the system, has become an important research topic ( Szegedy et al., 2013 ;  Papernot et al., 2016 ;  Biggio et al., 2013 ). One of the most widely studied classes of adversarial perturbations is p -norm constrained adver- sarial perturbations ( Szegedy et al., 2013 ).  Madry et al. (2017)  formalize the adversarial learning against this class of perturbations as a minimization problem of adversarial risk defined in a follow- ing way. Let (X, y) ∈ X × Y be from some unknown distribution P X,Y . Given a loss function : Y × Y → R and a constraint constant > 0, the adversarial robust risk is Many adversarial learning methods can be interpreted as empirical minimization of (1) ( Goodfellow et al., 2014 ;  Kurakin et al., 2016b ;  Ruitong Huang & Szepesvari, 2015 ;  Madry et al., 2017 ). For this optimization problem,  Madry et al. (2017)  propose to train a robust model with the augmented data generated by the projected gradient descent method (PGD). On this adversarial training, they make two important observations. First, it costs natural accuracy. A network trained with adversarial examples tends to have a lower natural accuracy than a naturally trained network. This trade-off is observed even with a small training . Second, the adversarial training requires a larger model ca- pacity than the natural training does. If the model capacity is only sufficient for the natural learning, the adversarial training can converge to a constant function. For a large , the optimization problem of (1) itself may pose the trade-off. For instance,  Tsipras et al. (2018)  show an example of an inherent tension between pursuits of accuracy and robustness when is large enough to change the true class. For a smaller , however, the formulation in (1) does not explicitly pose any conflict between the pursuit of robustness and accuracy. Note that R rob (f ) is an upper bound of the standard risk of f . A perfectly robust model f with R rob (f ) = 0 is also perfectly accurate for natural learning. If the perfect classifier exists in a given model class, the trade-off may be caused by the large sample complexity of adversarially robust generalization Under review as a conference paper at ICLR 2020 ( Schmidt et al., 2018 ;  Yin et al., 2018 ;  Stutz et al., 2019 ). Without sufficiently large amount of data, the empirical minimization of (1) may result in a large standard risk by converging to a model of a poor robust risk. On the other hand, if robust learning converges to a constant function, it cannot achieve natural accuracy. In this sense, to resolve the trade-off problem, we may need to deal with the increased requirement on the model capacity. In this paper, we propose a novel framework, called sensible adversary, in order to overcome the trade-off between natural accuracy and robustness. In particular, we restrict adversarial perturbations not to cross the Bayes decision boundary besides the -ball constraint, so that the perturbation ball is adaptively modified for every single data point. Our main contributions are: • Under the framework of sensible adversary, the pursuit of robustness and accuracy given an enough model capacity can align with each other, i.e., there is no trade-off. We theoretically establish the Bayes rule is most robust against the sensible adversary. If the Bayes decision boundary can be far from data manifolds at least by , our pursuit of sensible robustness does not cost any adversarial robust risk. • We propose an efficient algorithm for sensible adversarial training , which utilizes sensible adversaries in the absence of the true Bayes rule. This sensible adversarial training enjoys robustness without a significant drop of natural accuracy. Furthermore, the algorithm is not sensitive to the model capacity. When insufficient model capacity is given, our algorithm does not collapse to a constant function. Instead, it trains a model as robust as possible. • We experimentally demonstrate that sensible adversarial training enables to stably learn a robust and accurate model. In particular, on CIFAR10, we achieve 91.51% natural test accuracy and 57.23% robust test accuracy against ∞ PGD attacks constrained to = 8/255. To the best of our knowledge, there is no approaches known to achieve natural accuracy more than 90%, while achieving more than 55% of robust accuracy against PGD attacks of = 8/255. Moreover, no previous approaches pursuing robustness against this attack achieved the natural accuracy more than . 1.1 RELATED WORK  Madry et al. (2017)  formalize adversarial learning as a mini-max problem given perturbation restric- tion, and theoretically and empirically established the feasibility of the optimization. Our sensible adversary redefines the set of perturbation in the inner maximization problem on which their the- oretical result is directly applicable.  Tsipras et al. (2018)  investigate the possible source of robust trade-off. The key idea is that when there are features that are useful for natural classification but vulnerable to adversarial perturbations, a robust model would abandon these features because other- wise all of these features can adversarially move to promote incorrect prediction. Our work explores the possibility of learning a robust model while not allowing such collective adversarial migration. While a class change by adversarial examples typically has been prevented by using a small ,  Sug- gala et al. (2018)  explicitly ignore an adversarial perturbation that crosses the decision boundary of a Bayes rule.  Zhang et al. (2019)  also investigate the Bayes decision boundary to resolve the trade-off problem. They search for a model f having a small weighted sum of the natural risk and a probability that an adversarial example can cross the decision boundary of f .  Gilmer et al. (2018)  show that in high dimensional setting, even small test error can imply the existence of adversarial examples for most of data points. Our effort to prioritize natural accuracy to find a robust model is consistent to the view in  Gilmer et al. (2018) . More related work will be presented in Appendix A.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel Deep Learning Alternating Minimization (DLAM) algorithm for training deep neural networks. The proposed algorithm is based on a reformulation of the loss function of a deep neural network as a nested function associated with multiple linear and nonlinear transformations across multi-layers. This nested structure is then decomposed into a series of linear and nonlinear equality constraints by introducing auxiliary variables and penalty hyperparameters. The linear and nonlinear equality constraints generate multiple subproblems, which can be minimized alternately. The proposed DLAM algorithm is highly generic and sufficiently flexible to be utilized in common fully-connected deep neural network models, as well as being easily extendable to other models such as convolutional neural networks and recurrent neural networks. The proposed algorithm is guaranteed to converge to a critical point under mild conditions and is shown to perform well compared to existing methods such as SGD and ADMM.",
        "Abstract": "In recent years, stochastic gradient descent (SGD) and its variants have been the dominant optimization methods for training deep neural networks. However, SGD suffers from limitations such as the lack of theoretical guarantees, vanishing gradients, excessive sensitivity to input, and difficulties solving highly non-smooth constraints and functions. To overcome these drawbacks, alternating minimization-based methods for deep neural network optimization have attracted fast-increasing attention recently. As an emerging and open domain, however, several new challenges need to be addressed, including 1) Convergence depending on the choice of hyperparameters, and 2) Lack of unified theoretical frameworks with general conditions. We, therefore, propose a novel Deep Learning Alternating Minimization (DLAM) algorithm to deal with these two challenges. Our innovative inequality-constrained formulation infinitely approximates the original problem with non-convex equality constraints, enabling our proof of global convergence of the DLAM algorithm under mild, practical conditions, regardless of the choice of hyperparameters and wide range of various activation functions. Experiments on benchmark datasets demonstrate the effectiveness of DLAM.",
        "Introduction": "  INTRODUCTION Stochastic gradient descent (SGD) and its variants have become popular optimization methods for training deep neural networks. These methods split a dataset into multiple batches and then optimize them sequentially by gradient descent in each epoch. SGD has two main advantages: not only is it simple to implement, but it can also be applied in online settings where new coming training data are used to train models. However, while many researchers have provided solid theoretical guarantees on the convergence of SGD ( Kingma & Ba (2014) ;  Reddi et al. (2018) ;  Sutskever et al. (2013) ), the assumptions of their proofs cannot be applied to problems involving deep neural networks, which are highly nonsmooth and nonconvex. Aside from the lack of theoretical guarantees, several additional drawbacks restrict the applications of SGD. It suffers from the gradient vanishing problem, meaning that the error signal diminishes as the gradient is backpropagated, which prevents the neural networks from utilizing further training ( Taylor et al. (2016) ), and the gradient of the activation function is highly sensitive to the input (i.e. poor conditioning), so a small change in the input can lead to a dramatic change in the gradient. To tackle these intrinsic drawbacks of gradient descent optimization methods, alternating minimization methods have started to attract attention as a potential way to solve deep learning problems. Here, the loss function of a deep neural network is reformulated as a nested function associated with multiple linear and nonlinear transformations across multi-layers. This nested structure is then decomposed into a series of linear and nonlinear equality constraints by introducing auxiliary variables and penalty hyperparameters. The linear and nonlinear equality constraints generate multiple subproblems, which can be minimized alternately. Some recent alternating minimization methods have focused on applying the Alternating Direction Method of Multipliers (ADMM) ( Taylor et al. (2016) ;  Wang et al. (2019) ) and Block Coordinate Descent (BCD) ( Jinshan Zeng (2018) ), with empirical evaluations demonstrating good scalability in terms of the number of layers and high accuracy on the test sets, especially for neural networks that are very deep, thanks to parallelism ( Taylor et al. (2016) ;  Wang et al. (2019) ). For more information, please refer to Section H in the supplementary materials. These methods also avoid gradient vanishing problems and allow for non-differentiable activation functions such as binarized neural networks ( Courbariaux et al. (2015) ), as well as allowing for complex non- smooth regularization and the constraints that are increasingly important for deep neural architectures that are required to satisfy practical requirements such as interpretability, energy-efficiency, and cost awareness Carreira-Perpinan & Wang (2014). However, as an emerging domain, alternating minimization for deep model optimization suffers from a number of unsolved challenges including: 1. Convergence properties are sensitive to penalty parameters. One recent work by Wang et al. firstly proved the convergence guarantee of ADMM in the fully-connected neural network problem ( Wang et al. (2019) ). However, such convergence guarantee is dependent on the choice of penalty hyperparameters: the convergence can not be guaranteed any more when penalty hyperparameters are small. 2. Lack of unified theoretical frameworks with general conditions. The global convergence of ADMM on deep learning has rarely been explored ( Wang et al. (2019) ;  Zeng et al. (2019) ). And existing few works are tailored for and limited to few specific loss functions and activation functions: Zeng et al. proved that the ADMM is convergent for square loss function and twice differentiable activation functions (e.g. sigmoid) ( Zeng et al. (2019) ); Wang et al. proved the convergence of ADMM for the Relu activation function ( Wang et al. (2019) ). Therefore, there lacks a unified theoretical framework which covers wide range of commonly used losses and activation functions. In order to simultaneously address these technical problems, we propose a new formulation of the deep neural network problem, along with a novel Deep Learning Alternating Minimization (DLAM) algorithm. The proposed framework is highly generic and sufficiently flexible to be utilized in common fully-connected deep neural network models, as well as being easily extendable to other models such as convolutional neural networks ( Krizhevsky et al. (2012) ) and recurrent neural networks ( Mikolov et al. (2010) ). Specifically, we, for the first time, transform the original deep neural network optimization problem into an inequality-constrained problem that can be infinitely approximate to the original one. Applying this innovation to an inequality-constraint based transformation ensures the convexity of all subproblems, and hence easily ensures global minima, the inequality-constraint prevents the output of a nonlinear function from changing much and reduces sensitivity to the input. The operation of matrix inversion is avoided by the quadratic approximation technique and a backtracking algorithm. Moreover, while existing methods require typically strict and complex conditions, such as Kurdyka-ojasiewicz (KL) properties ( Lau et al. (2018) ) to prove convergence, our proposed method requires simple and mild conditions to guarantee convergence and covers most of the commonly-used loss functions and activation functions, and the choice of hyperparameters has no effect on the convergence of our DLAM algorithm theoretically. Our contributions include: • We propose a novel formulation for deep neural network optimization. The deeply nested acti- vation functions are disentangled into separate functions innovatively coordinated by inequality constraints that are inherently convex. • We present a novel and efficient DLAM algorithm. A quadratic approximation technique and a backtracking algorithm are utilized to avoid matrix inversion. Every subproblem has a closed-form solution, further boosting efficiency. • We investigate several attractive convergence properties of the DLAM algorithm under mild conditions. The model assumptions are very mild, ensuring that most deep learning problems will satisfy our assumptions. The new DLAM algorithm is guaranteed to converge to a critical point. • We conduct experiments on benchmark datasets to validate our proposed DLAM algorithm. Experiments on two benchmark datasets show that the new algorithm performs well compared with SGD or its variants and ADMM. The rest of paper is organized as follows. In Section 2, we present the problem formulation and the new DLAM algorithm. In Section 3, we introduce the main convergence results for the DLAM algorithm. Section 4 reports the results of the extensive experiments conducted to validate the convergence and effectiveness of the new DLAM. Section 5 concludes by summarizing the research.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to text generation using generative adversarial networks (GANs) without resorting to reinforcement learning (RL) strategies. The proposed method, No Neural Network as the Discriminator GAN (N3DGAN), uses a simple temperature sweeping technique at inference stage to overcome the exposure bias caused by maximum likelihood estimation (MLE) models. Results show that N3DGAN outperforms existing RL-based GANs models by a large margin over quality-diversity space under several metrics.",
        "Abstract": "Text generation is a critical and difficult natural language processing task. Maximum likelihood estimate (MLE) based models have been arguably suffered from exposure bias in the inference stage and thus varieties of language generative adversarial networks (GANs) bypassing this problem have emerged. However, recent study has demonstrated that MLE models can constantly outperform GANs models over quality-diversity space under several metrics. In this paper, we propose a quality-diversity controllable language GAN.",
        "Introduction": "  INTRODUCTION Text generation is one of the most challenging and widely used natural language processing task and it serve as a critical module on numerous practical applications. In recent years, neural autoregres- sive models have shown superiority on various natural language generation (NLG) tasks ( Zhang & Lapata, 2014 ;  Gal & Ghahramani, 2016 ;  Du et al., 2017 ;  Fiorini & Lu, 2018 ). The most common method of training these models is the maximum likelihood estimation (MLE) which maximize the log predictive likelihood of each true token in the training sequence given the previous observed to- kens. However, MLE models have been arguably suffered from exposure bias ( Bengio et al., 2015 ;  Ranzato et al., 2016 ) which means a discrepancy between training and inference stages due to the fact that at inference time, the model predicts next token based on the sequence generated by itself which may not occur in training set. In order to overcome this problem, scheduled sampling ( Ben- gio et al., 2015 ) and professor forcing ( Lamb et al., 2016 ) are proposed, but scheduled sampling is proven to be fundamentally inconsistent ( Huszár, 2015 ) and professor forcing use an adversarial method similar to GANs ( Goodfellow et al., 2014 ). Varieties of generative adversarial networks (GANs) ( Goodfellow et al., 2014 ) models for text gen- eration have emerged as alternatives to MLE models with the hope of not suffering from exposure bias. However, due to the discrete nature, GANs have difficulty in text generation since the gradients from discriminator can not be passed to generator explicitly and thus most these GANs based mod- els ( Yu et al., 2017 ;  Guo et al., 2018 ;  Lin et al., 2017 ;  Fedus et al., 2018 ;  Xu et al., 2018 ) treat text generation as a sequential decision making process ( Bachman & Precup, 2015 ) and utilize policy gradient ( Williams, 1992 ) to overcome this difficulty. Despite the impressive achievements, they inevitably have a problem of high variance due to the use of reinforcement learning (RL) strategies.  Caccia et al. (2018)  have made several surprising observations and argued that, through simple temperature sweeping at inference stage, MLE models can constantly outperform these RL-based GANs models by a large margin over quality-diversity space under several metrics. They attribute this to the fact that the negative impact coming with RL is severer than so-called exposure bias caused by MLE. A RL-free approach to train GANs for text generation without resorting to an explicit neural network as the discriminator, named No Neural Network as the Discriminator GAN (N3DGAN), is proposed by ( Li et al., 2019 ) and shows state-of-the-art results on both synthetic and real datasets under quality-only metrics.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a self-supervised learning framework, termed local prior matching (LPM), for automatic speech recognition (ASR). The proposed framework leverages a strong language model as a prior for self-supervision, allowing an ASR model to learn from unlabeled speech. Experiments on the LibriSpeech corpus demonstrate that LPM can reduce the word error rate (WER) by 2.1% and 9.5% absolute on two test sets, effectively bridging the gap to fully-supervised learning by 54% and 73%. Augmenting LPM with additional 500 hours of unlabeled speech reduces WER on the noisier test set by 14.2% in total.",
        "Abstract": "We propose local prior matching (LPM), a self-supervised objective for speech recognition. The LPM objective leverages a strong language model to provide learning signal given unlabeled speech. Since LPM uses a language model, it can take advantage of vast quantities of both unpaired text and speech. The loss is theoretically well-motivated and simple to implement. More importantly, LPM is effective. Starting from a model trained on 100 hours of labeled speech, with an additional 360 hours of unlabeled data LPM reduces the WER by 26% and 31% relative on a clean and noisy test set, respectively. This bridges the gap by 54% and 73% WER on the two test sets relative to a fully supervised model on the same 360 hours with labels. By augmenting LPM with an additional 500 hours of noisy data, we further improve the WER on the noisy test set by 15% relative. Furthermore, we perform extensive ablative studies to show the importance of various configurations of our self-supervised approach.",
        "Introduction": "  INTRODUCTION Fully supervised learning remains the mainstream paradigm for state-of-the-art automatic speech recognition (ASR). These systems require huge annotated data sets (Li et al., 2017; Chiu et al., 2018; Hannun et al., 2014; Amodei et al., 2016), which are time-consuming and expensive to collect. This hinders the development of accurate ASR systems for low resource languages (Precoda, 2013). In fact, out of over 6,000 spoken languages, fewer than 150 are supported by commercial ASR service providers. In sharp contrast to how we teach machines to recognize speech, humans do not learn by listening to thousand hours of speech and simultaneously reading the corresponding transcriptions. Instead, as noted in (Chomsky, 1986; Kuhl, 2004; Glass, 2012; Dupoux, 2018), humans possess an inherent ability to learn from vast quantities of unlabeled speech. Consider the case of conversing with someone with a strong accent. Even when the speaker pronounces several words in an unusual way, one can often correctly understand the sentence. We argue that the source of indirect supervision in processing unlabeled speech comes from prior knowledge about the world and the context of the speech. Inspired by this, we devise a self-supervised learning framework termed local prior matching (LPM). We apply this framework to speech recognition allowing an ASR model to learn from unla- beled speech by leveraging a strong language model, which serves as the prior for self-supervision. Given an unlabeled utterance, the ASR model proposes multiple hypotheses and the language model provides a learning signal by evaluating the plausibility of each one. We evaluate the LPM method on the LibriSpeech corpus (Panayotov et al., 2015), using 100 hours of labeled speech to seed the proposal model. Using 360 hours of additional labeled data reduces the word error rate (WER) by 3.8% and 13.1% absolute on an easier and a more-challenging test set, respectively. Using the same 360 hours but without labels, LPM reduces the WER by 2.1% and 9.5% absolute on the same two test sets, effectively bridging the gap to fully-supervised learning by 54% and 73%. In addition, by augmenting LPM with another 500 hours, for a total of 860 hours of unlabeled speech, LPM reduces WER on the noisier test set by 14.2% in total. Hence, LPM is able to surpass the performance of using 360 hours of labeled data by taking advantage of about twice the amount of unlabeled data. We also conduct extensive ablation studies and analyses in order to demonstrate the significance of each proposed component. For reproducibility, software will be open source and made publicly available in the camera-ready version.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Chordal-GCN, a novel clustering-based method for semi-supervised node classification on large-scale sparse networks. Chordal-GCN fully exploits the exact graph structure without any approximation or random sampling, while requiring limited memory usage. The proposed model is able to train a large-scale graph in a partially separable manner, where the connections between subgraphs are handled by a consistency loss. The memory and time complexity of Chordal-GCN are analyzed and compared with other state-of-the-art GCN models. The performance of Chordal-GCN is evaluated on benchmark datasets and demonstrates superior performance in large-scale datasets.",
        "Abstract": "Despite the impressive success of graph convolutional networks (GCNs) on numerous applications, training on large-scale sparse networks remains challenging. Current algorithms require large memory space for storing GCN outputs as well as all the intermediate embeddings. Besides, most of these algorithms involves either random sampling or an approximation of the adjacency matrix, which might unfortunately lose important structure information. In this paper, we propose Chordal-GCN for semi-supervised node classification. The proposed model utilizes the exact graph structure (i.e., without sampling or approximation), while requires limited memory resources compared with the original GCN. Moreover, it leverages the sparsity pattern as well as the clustering structure of the graph. The proposed model first decomposes a large-scale sparse network into several small dense subgraphs (called cliques), and constructs a clique tree. By traversing the tree, GCN training is performed clique by clique, and connections between cliques are exploited via the tree hierarchy. Furthermore, we implement Chordal-GCN on large-scale datasets and demonstrate superior performance.\n",
        "Introduction": "  INTRODUCTION Graph convolutional network (GCN) ( Kipf & Welling, 2017 ) is a generalization of convolutional neural networks (CNNs) ( LeCun & Bengio, 1998 ) to the graph structure. For a given node, the graph convolution operation aggregates the embeddings (features) of its neighbors, followed by a non-linear transformation. By stacking multiple graph convolutional layers, one can learn node representations by utilizing features of its distant neighborhood. The original GCN model, as well as its numerous variations, has shown great success in a variety of applications, including semi- supervised node classification ( Kipf & Welling, 2017 ), inductive node embedding ( Hamilton et al., 2017 ), link prediction ( van den Berg et al., 2017 ), and knowledge graphs ( Schlichtkrull et al., 2018 ). Despite the success of GCNs, training GCNs on large-scale graphs remains challenging due to the memory issue: we need to store all the parameters and outputs of GCN. Thus, the memory space scales linearly in the size of graph while quadratically in the feature dimension ( Chiang et al., 2019 ;  Zou et al., 2019 ). This prevents applications of GCN on many real-world networks, where the graphs usually contain millions or even billions of nodes. Methods aimed at large-scale training have been proposed and can be roughly divided into two categories: (1) sampling-based methods and (2) clustering-based methods. For sampling-based methods, only a few neighbors for every node will be sampled in every GCN layer, and thus the size of intermediate embeddings for every layer will be reduced for each mini-batch. Works in this track include  Hamilton et al. (2017) ;  Chen et al. (2018a ; b );  Zou et al. (2019) . However, ignorance of some neighbors might lose important structure information, which is the main drawback of all the sampling methods. Another direction of research notices the sparsity of real-world networks and exploits the clustering structure of the graph. For example, Cluster-GCN ( Chiang et al., 2019 ) separates the graph into several clusters, and in every iteration of training, only one or a few clusters are picked to calculate the stochastic gradient for the mini-batch. However, Cluster-GCN ignores all the inter-cluster links, which are not negligible in many real-world networks. For example,  Figure 1  shows the sparsity pattern of three citation networks. We first rearrange the vertices via an approx- imate minimum degree (AMD) ordering algorithm ( Amestoy et al., 1996 ), and then observe a nice arrow pattern in the adjacency matrices. This indicates the existence of some highly-cited papers Under review as a conference paper at ICLR 2020 (in the right bottom corner), which have impacts on multiple communities. In Cluster-GCN, these highly-cited papers are randomly put into one community and the adjacency matrix is approximated by a block-diagonal matrix; i.e., Cluster-GCN removes the fletching part of the arrow patterns in  Fig- ure 1 . This approximation ruins the beautiful arrow pattern, and thus ignores the multi-community influence of some seminal papers. The above difficulties can be easily tackled by the Chordal-GCN, a novel clustering-based method for the semi-supervised node classification task. Recall that Cluster-GCN ignores all the inter- cluster links and trains each cluster separately; comparatively, in Chordal-GCN, we keep all the links, train every cluster separately, and at the same time capture the connections between clusters by an additional loss term. This partially separable training process can be achieved with the help of chordal sparsity theory ( Vandenberghe & Andersen, 2015 ): we first decompose a large-scale sparse graph into several small dense subgraphs, and we construct a tree of which the nodes are the subgraphs. Note that two subgraphs are adjacent in the tree if they share some vertices. In the training process, we traverse the tree from leaf to root; and when training on a certain subgraph, we minimize the usual GCN loss, plus an additional term called consistency loss. With the consistency loss, messages in the children subgraphs can be passed to their parent, and thus the relationship between subgraphs is leveraged via the hierarchy of the tree. Therefore, Chordal-GCN exploits the sparsity pattern as well as the clustering structure of the graph without any approximation or random sampling, while requires similar memory space to Cluster-GCN. Our contribution is summarized as follows: • We propose Chordal-GCN for semi-supervised node classification on large-scale sparse networks. The proposed model fully exploits the exact graph structure, while requires limited memory usage on large-scale graphs (much smaller than the original GCN). • Chordal-GCN is able to train a large-scale graph in a partially separable manner; i.e., in every iteration, the training is performed on a subgraph, and the connections between sub- graphs are handled by a consistency loss. • We analyze the memory and time complexity of Chordal-GCN and compare them with other state-of-the-art GCN models. Also, we evaluate the performance of Chordal-GCN on benchmark datasets and demonstrate superior performance in large-scale datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Single Image Super-Resolution (SISR) is a challenging task in computer vision research, and various priors of natural images have been exploited to regularize the solution. Sparse Coding (SC) based methods for SR have been widely used due to their excellent performance, but lack a unified framework to improve their performance. This paper proposes a Convolutional Sparse Coding (CSC) based image SR framework which combines the merits of sparse coding and deep learning to exploit the sparse prior locally.",
        "Abstract": "Convolutional Sparse Coding (CSC) has been attracting more and more attention in recent years, for making full use of image global correlation to improve performance on various computer vision applications. However, very few studies focus on solving CSC based image Super-Resolution (SR) problem. As a consequence, there is no significant progress in this area over a period of time. In this paper, we exploit the natural connection between CSC and Convolutional Neural Networks (CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is introduced to solve CSC problem and it can be implemented using CNN architectures. Then we develop a novel CSC based SR framework analogy to the traditional SC based SR methods. Two models inspired by this framework are proposed for pre-/post-upsampling SR, respectively. Compared with recent state-of-the-art SR methods, both of our proposed models show superior performance in terms of both quantitative and qualitative measurements.",
        "Introduction": "  INTRODUCTION Single Image Super-Resolution (SISR), which aims to restore a visually pleasing High-Resolution (HR) image from its Low-Resolution (LR) version, is still a challenging task within computer vision research community ( Timofte et al., 2017 ; 2018). Since multiple solutions exist for the mapping from LR to HR space, SISR is highly ill-posed. To regularize the solution of SISR, various priors of natural images have been exploited, especially the current leading learning-based methods ( Wang et al., 2015 ;  Dong et al., 2016 ;  Mao et al., 2016 ;  Kim et al., 2016a ;b;  Tai et al., 2017a ;b;  Lim et al., 2017 ;  Ahn et al., 2018 ;  Haris et al., 2018 ;  Li et al., 2018 ; Zhang et al., 2018) are proposed to directly learn the non-linear LR-HR mapping. By modeling the sparse prior in natural images, the Sparse Coding (SC) based methods for SR ( Yang et al., 2008 ; 2010; 2014) with strong theoretical support are widely used owing to their excellent per- formance. Considering the complexity in images, these methods divide the image into overlapping patches and aim to jointly train two over-complete dictionaries for LR/HR patches. There are usu- ally three steps in these methods' framework. First, overlapping patches are extracted from input image. Then to reconstruct the HR patch, the sparse representation of LR patch can be applied to the HR dictionary with the assumption that LR/HR patch pair shares similar sparse representation. The final HR image is produced by aggregating the recovered HR patches. Recently, with the development of Deep Learning (DL), many researchers attempt to combine the advantages of DL and SC for image SR. Dong et al. ( Dong et al., 2016 ) firstly proposed the seminal CNN model for SR termed as SRCNN, which exploits a shallow convolutional neural network to learn a nonlinear LR-HR mapping in an end-to-end manner and dramatically overshadows conven- tional methods ( Yang et al., 2010 ;  Timofte et al., 2014 ). However, sparse prior is ignored to a large extent in SRCNN for it adopts a generic architecture without considering the domain expertise. To address this issue, Wang et al. ( Wang et al., 2015 ) implemented a Sparse Coding based Network (SCN) for image SR, by combining the merits of sparse coding and deep learning, which fully ex- ploits the approximation of sparse coding learned from the LISTA ( Gregor & LeCun, 2010 ) based sub-network. It's worth to note that most of SC based methods utilize the sparse prior locally ( Papyan et al., 2017b ), i.e., coping with overlapping image patches. Thus the consistency of pixels in overlapped Under review as a conference paper at ICLR Compared with SC based image SR methods ( Yang et al., 2008 ; 2010), the lack of a unified framework has hindered progress towards improving the performance of CSC based image SR.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a stochastic goal recognition design (S-GRC) problem with deceptive opponent modeling and proactive network interdiction. The opponent's deceptive policy is modeled as a soft multi-criteria decision policy with a tunable parameter to balance goal achievement and deception preference. The primary objective is to offline redesign the environment by soft action interdiction to control online goal recognition. Experiments are conducted in two different environment representations: random generated connected graph and real road network. Results show that the deceptive opponent modeling with entropy regularization is robust to multi-modal stochastic behaviors, and the soft decision policy based stochastic goal recognition design model for action removal bridges the gap between observation and decision making.",
        "Abstract": "Goal recognition based on the observations of the behaviors collected online has been used to model some potential applications. Newly formulated problem of goal recognition design aims at facilitating the online goal recognition process by performing offline redesign of the underlying environment with hard action removal.\nIn this paper, we propose the stochastic goal recognition control (S-GRC) problem with two main stages: (1) deceptive opponent modeling based on maximum entropy regularized Markov decision processes (MDPs) and (2) goal recognition control under proactively static interdiction.\nFor the purpose of evaluation, we propose to use the worst case distinctiveness (wcd) as a measure of the non-distinctive path without revealing the true goals, the task of S-GRC is to interdict a set of actions that improve or reduce the wcd.\nWe empirically demonstrate that our proposed approach control the goal recognition process based on opponent's deceptive behavior.",
        "Introduction": "  INTRODUCTION Goal recognition (GR), also called intention recognition, is the task of inferring the goals of an agent according to the observed actions or states collected online ( Sadri, 2011 ), which enables humans or agents to make proactive response plans. Goal recognition design (GRD) is to deliberately redesign the environment for improved online goal recognition ability ( Keren et al., 2014 ), which includes two key models, the first measures how efficiently and effectively the online goal recognition system performs in a given setting, the second optimizes the goal recognition setting via redesign ( Keren et al., 2019 ). Goal recognition control (GRC) aims at soft action interdiction under bounded re- source by adding cost compared to the hard action removal of GRD ( Luo et al., 2019 ), in which the interdiction can be proactively static inhibition or online dynamic block by allocating security resources to protect goals against the attacker. In fact, under complex environment (both adversarial and cooperative), the information of an op- ponent's goals are asymmetric and can not be obtained through communication ( Le Guillarme, 2016 ). Most recent advancements in GR utilize planning recognition as planning (PRAP) ( Ramírez & Geffner, 2009 ), generative game-theoretic frameworks ( Ang et al., 2017 ), and goal recognition as planning (GRAP) ( Pereira et al., 2019 ). There are three key assumptions widely used: (1) agents performs optimal plans to real the goals; (2) the environment is fully observable, that is the states and actions of the agents are observable; and (3) the agent's actions are deterministic. However, these existing frameworks seldom address the deceptive behaviors of actively misleading the goal recognition process. We will relax the first and third assumptions to handle non-optimal agents and stochastic actions. Such as one game-theoretic approach provided with a unified treatment of both threat assessment and response planning in ( Guillarme et al., 2017 ), after evaluating the threat, the defender will allocate road barrier or patrolling force to protect critical infrastructure, as illustrated in  Figure 1 . In order to identify the goals and improve security, we employ opponent modeling to deal with the non-stationary strategies stemming from deception, in which maximum entropy regularized Markov decision process (MDP) is utilized to shape the multi-modal adversarial strategies ( Shen & How, Under review as a conference paper at ICLR 2020  2019). After finding the maximal non-distinctive agent path, we are seeking some optimal modifi- cations (e.g., interdiction to block or inhibit action) to accelerate the online goal recognition. In this work, we propose the stochastic goal recognition design (S-GRC) problem with deceptive opponent modeling and proactive network interdiction: the opponent's deceptive policy is modeled as one soft multi-criteria decision policy with one tunable parameter to balance goal achievement and deception preference; and the primary objective is offline redesigning the environment by soft action interdiction to control online GR. To validate the model, we evaluate our approach in two different environment representations: ran- dom generated connected graph and real road network. Our experiments demonstrate that (1) de- ceptive opponent modeling with entropy regularization make it robust to multi-modal stochastic be- haviors; (2) soft decision policy based stochastic goal recognition design model for action removal bridge the gap between observation and decision making.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents BOGCN-NAS, a Neural Architecture Search (NAS) algorithm that utilizes Bayesian Optimization (BO) and Graph Convolutional Network (GCN) to efficiently explore the search space and capture the structural information of architectures related to the performance. BOGCN-NAS is more resource-efficient than existing methods, and outperforms current state-of-the-art searching methods on multiple search spaces for computer vision and natural language processing tasks. The proposed algorithm is also applied to multi-objective NAS, considering accuracy and number of parameters as search objectives, and is applied to open domain search with NASNet and ResNet Style search spaces. The results of the experiments demonstrate that BOGCN-NAS can find a more competitive Pareto front compared with other sample-based methods.",
        "Abstract": "Neural Architecture Search (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using Bayesian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4x more efficient than Random Search and 7.8x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NasBench-101.",
        "Introduction": "  INTRODUCTION Recently Neural Architecture Search (NAS) has aroused a surge of interest by its potentials of free- ing the researchers from tedious and time-consuming architecture tuning for each new task and dataset. Specifically, NAS has already shown some competitive results comparing with hand-crafted architectures in computer vision: classification (Real et al., 2019b), detection, segmentation (Ghiasi et al., 2019; Chen et al., 2019; Liu et al., 2019a) and super-resolution (Chu et al., 2019). Meanwhile, NAS has also achieved remarkable results in natural language processing tasks (Luong et al., 2018; So et al., 2019). A variety of search strategies have been proposed, which may be categorized into two groups: one- shot NAS algorithms (Liu et al., 2019b; Pham et al., 2018; Luo et al., 2018), and sample-based algorithms (Zoph & Le, 2017; Liu et al., 2018a; Real et al., 2019b). One-shot NAS algorithms em- bed the architecture searching process into the training stage by using weight sharing, continuous relaxation or network morphisms. However, those methods cannot guarantee the optimal perfor- mance of the final model due to those approximation tricks and is usually sensitive to the initial seeds (Sciuto et al., 2019). On the other hand, sample-based algorithms are relatively slower but reliable. They explore and exploit the search space using some general search algorithms by provid- ing potential candidates with higher accuracy. However, it requires fully training of huge amounts of candidate models. Typically, the focus of most existing NAS methods has been on the accuracy of the final searched model alone, ignoring the cost spent in the search phase. Thus, the comparison between existing search algorithms for NAS is very difficult. (Wang et al., 2019b) gives us an example of evaluating the NAS algorithms from this view. They compare the number of training architectures sampled until finding the global optimal architecture with the top accuracy in the NAS datasets. Besides accuracy, in real applications, there are many other objectives we should concern, such as speed/accuracy Under review as a conference paper at ICLR 2020 trade-off. Hence, in this paper, we aim at designing an efficient multi-objective NAS algorithm to adaptively explore the search space and capture the structural information of architectures related to the performance. The common issue faced by this problem is that optimizing objective functions is computationally expensive and the search space always contains billions of architectures. To tackle this problem, we present BOGCN-NAS, a NAS algorithm that utilizes Bayesian Optimization (BO) together with Graph Convolutional Network (GCN). BO is an efficient algorithm for finding the global optimum of costly black-box function (Mockus et al., 1978). In our method, we replace the popular Gaus- sian Processes model with a proposed GCN model as the surrogate function for BO (Jones, 2001). We have found that GCN can generalize fairly well with just a few architecture-accuracy pairs as its training set. As BO balances exploration and exploitation during searching and GCN extracts embeddings that can well represent model architectures, BOGCN-NAS is able to obtain the opti- mal model architecture with only a few samples from the search space. Thus, our method is more resource-efficient than the previous ones. Graph neural network has been proposed in previous work for predicting the parameters of the architecture using a graph hypernetwork (Zhang et al., 2019). However, it's still a one-shot NAS method and thus cannot ensure the performance of the final found model. In contrast, we use graph embedding to predict the performance directly and can guarantee performance as well. The proposed BOGCN-NAS outperforms current state-of-the-art searching methods, including Evo- lution (Real et al., 2019b), MCTS (Wang et al., 2019b), LaNAS (Wang et al., 2019a). We observe consistent gains on multiple search space for CV and NLP tasks, i.e., NASBench-101 (denoted NAS- Bench) (Ying et al., 2019) and LSTM-12K (toy dataset). In particular, our method BOGCN-NAS is 128.4× more efficient than Random Search and 7.8× more efficient than previous SOTA LaNAS on NASBench (Wang et al., 2019a). We apply our method to multi-objective NAS further, considering adding more search objectives including accuracy and number of parameters. Our method can find more superior Pareto front on NASBench. Our algorithm is applied on open domain search with NASNet search space and ResNet Style search space, which finds competitive models in both sce- narios. The results of experiment demonstrate our proposed algorithm can find a more competitive Pareto front compared with other sample-based methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper studies the fundamental problem of Differential Privacy (DP) when applied to Machine Learning (ML) tasks, known as Differentially Private Empirical Risk Minimization (DP-ERM). We analyze the utility guarantees of three different methods of introducing randomness to the algorithm: output perturbation, objective perturbation, and gradient perturbation. We introduce an expected curvature to characterize the optimization property accurately when there is perturbation noise at each gradient update, and establish the utility guarantees for DP-GD and DP-SGD for both convex and strongly convex objectives based on the expected curvature. Our results justify the empirical advantage of gradient perturbation and close the mismatch between theoretical guarantee and empirical observation.",
        "Abstract": "Gradient perturbation, widely used for differentially private optimization, injects noise at every iterative update to guarantee differential privacy. Previous work first determines the noise level that  can satisfy the privacy requirement and then analyzes the utility  of noisy gradient updates as in non-private case.  In this paper, we explore how the privacy noise affects the optimization property. We show that for differentially private convex optimization, the utility guarantee of both DP-GD and  DP-SGD is determined by an \\emph{expected curvature} rather than the minimum curvature. The \\emph{expected curvature} represents the average curvature over the optimization path, which is usually much larger than the minimum curvature and hence can help us achieve a significantly improved utility guarantee. By using the \\emph{expected curvature}, our theory justifies the advantage of gradient perturbation over other perturbation methods and closes the gap between theory and practice. Extensive experiments on real world datasets corroborate our theoretical findings.",
        "Introduction": "  Introduction Machine learning has become a powerful tool for many practical applications. The training process often needs access to some private dataset, e.g., applications in financial and medical fields. Recent work has shown that the model learned from training data may leak unintended information of individual records (Fredrikson et al., 2015;  Wu et al., 2016 ;  Shokri et al., 2017 ;  Hitaj et al., 2017 ). It is known that Differential privacy (DP) ( Dwork et al., 2006a ;b) is a golden standard for privacy preserving data analysis. It provides provable privacy guarantee by ensuring the influence of any individual record is negligible. It has been deployed into real world applications by large-scale corporations and U.S. Census Bureau ( Erlingsson et al., 2014 ;  McMillan, 2016 ;  Abowd, 2016 ; Ding et al., 2017). We study the fundamental problem when differential privacy meets machine learning: the differentially private empirical risk minimization (DP-ERM) problem ( Chaudhuri & Mon- teleoni, 2009 ;  Chaudhuri et al., 2011 ;  Kifer et al., 2012 ;  Bassily et al., 2014 ;  Talwar et al., 2015 ;  Wu et al., 2017 ;  Zhang et al., 2017 ; Wang et al., 2017;  Smith et al., 2017 ;  Jayaraman et al., 2018 ;  Feldman et al., 2018 ;  Iyengar et al., 2019 ; Wang & Gu, 2019). DP-ERM minimizes the empirical risk while guaranteeing that the output of learning algorithm is differentially private with respect to the training data. Such privacy guarantee provides strong protection against potential adversaries ( Hitaj et al., 2017 ;  Rahman et al., 2018 ). In order to guarantee privacy, it is necessary to introduce randomness to the algorithm. There are usually three ways to introduce randomness according to the time of adding noise: output perturbation, objective perturbation and gradient perturbation. Output perturbation ( Wu et al., 2017 ;  Zhang et al., 2017 ) first runs the learning algorithm the same as in the non-private case then adds noise to the output parameter. Objective perturbation ( Chaudhuri et al., 2011 ;  Kifer et al., 2012 ;  Iyengar et al., 2019 ) perturbs the objective (i.e., the empirical loss) then release the minimizer of the perturbed objective. Gradient perturbation ( Song et al., 2013 ;  Bassily et al., 2014 ;  Abadi et al., 2016 ; Wang et al., 2017;  Lee & Kifer, 2018 ;  Jayaraman et al., 2018 ) perturbs each intermediate update. If each Under review as a conference paper at ICLR 2020 update is differentially private, the composition theorem of differential privacy ensures the whole learning procedure is differentially private. Gradient perturbation comes with several advantages over output/objective perturbations. Firstly, gradient perturbation does not require strong assumption on the objective because it only needs to bound the sensitivity of gradient update rather than the whole learning process. Secondly, gradient perturbation can release the noisy gradient at each iteration without damaging the privacy guarantee as differential privacy is immune to post processing ( Dwork et al., 2014 ). Thus, it is a more favorable choice for certain applications such as distributed optimization ( Rajkumar & Agarwal, 2012 ;  Agarwal et al., 2018 ;  Jayaraman et al., 2018 ). At last, gradient perturbation often achieves better empirical utility than output/objective perturbations for DP-ERM. However, the existing theoretical utility guarantee for gradient perturbation is the same as or strictly inferior to that of other perturbation methods as shown in  Table 1 . This motivates us to ask \"What is wrong with the theory for gradient perturbation? Can we justify the empirical advantage of gradient perturbation theoretically?\" We revisit the analysis for gradient perturbation approach. Previous work ( Bassily et al., 2014 ; Wang et al., 2017;  Jayaraman et al., 2018 ) derive the utility guarantee of gradient perturbation via two steps. They first determine the noise variance at each step that meets the privacy requirement and then derive the utility guarantee by using the convergence analysis the same as in non-private case. However, the noise to guarantee privacy naturally affects the optimization procedure, but previous approach does not exploit the interaction between privacy noise and optimization of gradient perturbation. In this paper, we utilize the fact the privacy noise affects the optimization procedure and establish new and much tighter utility guarantees for gradient perturbation approaches. Our contribution can be summarized as follows. • We introduce an expected curvature that can characterize the optimization property accurately when there is perturbation noise at each gradient update. • We establish the utility guarantees for DP-GD for both convex and strongly convex objectives based on the expected curvature rather than the usual minimum curvature. • We also establish the the utility guarantees for DP-SGD for both convex and strongly convex objectives based on the expected curvature. To the best of our knowledge, this is the first work to remove the dependency on minimum curvature for DP-ERM algorithms. In DP-ERM literature, there is a gap between the utility guarantee of non-strongly convex objectives and that of strongly convex objectives. However, by using the expected curvature, we show that some of the non-strongly convex objectives can achieve the same order of utility guarantee as the strongly convex objectives, matching the empirical observation. This is because the expected curvature could be relatively large even for non-strongly convex objectives. As we mentioned earlier, prior to our work, there is a mismatch between theoretical guar- antee and empirical observation of gradient perturbation approach compared with other two perturbation approaches. Our result theoretically justifies the advantage of gradient perturbation and close the mismatch.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a white box network (WBN) with a selection layer for reverse engineering. The WBN is designed to continuously composite function blocks to create a target function, and is motivated by the question of whether knowing the functions used in certain fields can benefit the process. Experiments are conducted to imitate programmable logic controller (PLC) data and verify whether the model can obtain the ladder logic diagram, as well as to validate the selection layer. The WBN is compared to PathNet, and experiments are conducted to verify whether it functions like PathNet.",
        "Abstract": "Neural networks have significantly benefitted real-world tasks. The universality of a neural network enables the approximation of any type of continuous functions. However, a neural network is regarded as a non-interpretable black box model, and this is fatal to reverse engineering as the main goal of reverse engineering is to reveal the structure or design of a target function instead of approximating it. Therefore, we propose a new type of a function constructing network, called the white box network. This network arranges function blocks to construct a target function to reveal its design. The network uses discretized layers, thus rendering the model interpretable without disordering the function blocks. Additionally, we introduce an end-to-end PathNet structure through this discretization by considering the function blocks as neural networks",
        "Introduction": "  INTRODUCTION Reverse engineering can be defined as the deconstruction of an object to reveal its design or ar- chitecture, or to extract knowledge from the object [ Eldad, 2005 ]. Typically, in machine learning, obtaining the internal functions of a device only with its input and output values is a regression problem. Before machine learning was developed, devices were disassembled to analyze them. However, advances in deep learning have allowed us to approximate various functions in regression tasks; therefore, we can now solve this regression problem without actually looking inside the ob- ject. While this development in deep learning ( Bengio, 2009 ;  Bengio et al., 2013 ) has benefitted reverse engineering significantly, some limitations still exist. First, deep learning is a black box model; therefore, the results may not be interpreted. In this case, information regarding the object such as what it does and how it works may not be extracted. This causes field engineers to not entirely trust the results of deep learning even if the correct values were reflected. Next, deep learning ap- proximates the target function and does not reveal the actual design and architecture of the device. An artificial neural network is a mathematical simulation of a neural network composed of human neurons and synapses that simulate the manner in which the human brain recognizes patterns. How- ever, a device typically comprises an electronic circuit and block functions; therefore, it may not fit perfectly with the neural network structure, which can cause large errors when certain data are used. We herein propose a white box network (WBN) with a selection layer, specially designed for reverse engineering. This network continuously composites function blocks to create a target function. We assume that all types of function blocks that can constitute the target function are provided in ad- vance. This differs from the normal neural network because the WBN reveals the exact functions with the correct inputs and their ordering to construct the target function, instead of merely ap- proximating them. Our WBN is different from normal neural networks as the model architecture is discretized using a discretized matrix called the selection layer. Because discretization is inter- pretable ( Chen et al., 2016 ), our WBN can reconstruct the target function. The WBN is motivated by the questions, Can we benefit from knowing the functions that are used in certain fields?. For example, in a programmable logic controller (PLC), a target function can be obtained through a ladder logic diagram comprising well-known function blocks. Therefore, we can obtain a target function by ordering those function blocks with the correct inputs. As such, in this study, we create a WBN that can automate reverse engineering. For the experiment, we imitated the PLC data and verified whether our model could obtain their ladder logic diagram. Additionally, we conducted other experiments to validate the selection layer. We assumed that we Under review as a conference paper at ICLR 2020 may not know the functions used in a certain field. Therefore, we regarded the given function blocks as neural networks. Subsequently, the network was ordered and connected to the appropriate neural networks to obtain the target function that resembles PathNet ( Fernando et al., 2017 ). Using this concept, we create an end-to-end PathNet structure and perform experiments to verify whether it functions like PathNet.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel method for local explanation of deep neural networks (DNNs) that provides users with understandable rationale for a specific decision. The proposed method consists of a mask generator and a distribution controller, which takes the hidden feature maps in black-box classifiers as inputs and guides the outputs with expected distributions as relevance scores. The method is designed to improve the discrimination of supporting features and avoid non-trivial hyper-parameter tuning. Experiments demonstrate that the proposed method not only obtains higher quantitative performance for explaining the behavior of black-box classifiers, but also provides discriminative masks for intuitive explanation.",
        "Abstract": "Existing local explanation methods provide an explanation for each decision of black-box classifiers, in the form of relevance scores of features according to their contributions. To obtain satisfying explainability, many methods introduce ad hoc constraints into the classification loss to regularize these relevance scores. However, the large information gap between the classification loss and these constraints increases the difficulty of tuning hyper-parameters. To bridge this gap, in this paper we present a simple but effective mask predictor. Specifically, we model the above constraints with a distribution controller, and integrate it with a neural network to directly guide the distribution of relevance scores. The benefit of this strategy is to facilitate the setting of involved hyper-parameters, and enable discriminative scores over supporting features. The experimental results demonstrate that our method outperforms others in terms of faithfulness and explainability. Meanwhile, it also provides effective saliency maps for explaining each decision. ",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have achieved high classification accuracy in a wide range of fields, such as computer vision (He et al., 2016; Simonyan & Zisserman, 2014) and natural language pro- cessing (Greff et al., 2016; Mikolov et al., 2010). Despite the superior performance, DNN models lack meaningful explanations on how a specific decision is made, and are often regarded as black- box classifiers. To address this issue, various global and local explanation methods have been pro- posed. The former group aims to inspect the structures and the parameters of a complex model (Erhan et al., 2009; Chen et al., 2016). The latter group provides users understandable rationale for a specific decision with relevance scores 1 (Simonyan et al., 2014; Du et al., 2018a). In this paper, we focus on local explanation as it extracts the intuitive evidence behind the decision of each instance. To obtain the relevance scores for local explanation, gradient-based methods com- pute the partial derivative of the class probability with respect to an input instance. However, instead of directly pointing out why the target class is derived based on input, it is likely to answer the question (Montavon et al., 2018): What makes this instance more or less similar to the target class? To tackle this limitation, perturbation-based explanation methods are proposed. These methods per- turb the input and aim to find the smallest region, which alone allows a confident classification or prevents a confident classification once being removed (Dabkowski & Gal, 2017; Fong & Vedaldi, 2017). By applying various ad hoc constraints, these methods improve explainability and maintains faithfulness 2 . Nevertheless, most of these methods either focus on distinguishing the supporting features from background and ignore the discrimination on these features, or are only able to pre- dict discriminative scores on a small part of supporting features. To understand the behaviour of a classifier, the discriminative scores over all supporting features are preferred. The comparison of an expected salinecy map to others are shown in Fig.1(a) 3 . Moreover, the information gap between the classification loss and the constraints in turn increases the difficulty of tuning hyper-parameters. To address these issue, we propose a simple but effective mask predictor. The work is built upon the following observation: a large portion of contributions to each decision are held by only a small fraction of features (Fong & Vedaldi, 2017; Chattopadhay et al., 2018; Du et al., 2018a). The proposed predictor consists of a mask generator and a distribution controller. The former takes the hidden feature maps in black-box classifiers as inputs, and the latter guides the outputs with expected distributions as relevance scores. We show that, with an easy setting of the involved hyper- parameters in the controller, it can directly enforce the relevance scores towards right-skewed distri- butions (Clauset et al., 2009). Then, the scores of supporting features becomes more discriminative, which correspond to the small portion of the right tail, and the majority features have low scores and are regarded as unimportant. For illustration, an example of a right-skewed distribution is shown in Fig.1(b), and the masks predicted based on the distributions controllers with the right-skewed to the left-skewed are displayed in Fig.1(c). Furthermore, we introduce classification losses to opti- mize mask predictors, which removes all constraints and avoid non-trivial hyper-parameter tuning. We show that such a simplifications will not sacrifice the explainability too much with an ablation study. Finally, we introduce two metrics for comprehensively evaluating relevance scores in terms of faithfulness and explainability, respectively. The main contributions of our work are as follows. • We present the conception of distribution controllers on relevance scores, and integrate it with a trainable mask generator to improve the discrimination of supporting features. We provide two practical implementations of controllers to enforce scores towards the desired right-skewed distributions, where the involved hyper-parameters can be easily set. • We introduce the classification loss to train the proposed model. It avoids the non-trivial hyper-parameter tuning on ad hoc constraints and also improves the faithfulness of mim- icking target black-box classifiers. • We empirically demonstrate the effectiveness of the above innovations. Specifically, we change the setting of distributions and show the controllers can guide the scores towards varying preset distributions. Besides, we perform an ablation study by adding constraints back to analyze their effect on masks. Furthermore, the experiments also demonstrate that our method not only obtains higher quantitative performance for explaining the behavior of black-box classifiers, but also provides discriminative masks for intuitive explanation.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a discriminability distillation learning (DDL) method for group-based recognition tasks. DDL defines the discriminability of an element within a group from an explicable view and proposes an efficient indicator. A light-weight network is used to distill discriminability from the assessed elements, which saves computation burden compared to existing methods. Experiments on set-to-set face recognition and action recognition show the effectiveness of DDL for both efficiency and accuracy, achieving state-of-the-art results.",
        "Abstract": "Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set or sequence.\nThe computer vision community tries to tackle it by aggregating the elements in a group based on an indicator either defined by human such as the quality or saliency of an element, or generated by a black box such as the attention score or output of a RNN. \n\nThis article provides a more essential and explicable view. \nWe claim the most significant indicator to show whether the group representation can be benefited from an element is not the quality, or an inexplicable score, but the \\textit{discrimiability}. \nOur key insight is to explicitly design the \\textit{discrimiability} using embedded class centroids on a proxy set, \nand show the discrimiability distribution \\textit{w.r.t.} the element space can be distilled by a light-weight auxiliary distillation network. \nThis processing is called \\textit{discriminability distillation learning} (DDL).\nWe show the proposed DDL can be flexibly plugged into many group based recognition tasks without influencing the training procedure of the original tasks. Comprehensive experiments on set-to-set face recognition and action recognition valid the advantage of DDL on both accuracy and efficiency, and it pushes forward the state-of-the-art results on these tasks by an impressive margin.",
        "Introduction": "  INTRODUCTION With the rapid development of deep learning and the easy access to large-scale group data, recog- nition tasks using group information have drawn great attention in the computer vision community. The rich information provided by different elements can complement each other to boost the per- formance of tasks such as face recognition, action recognition, and person re-identification (Wang et al., 2017b; Zhong et al., 2018; Girdhar et al., 2017; Simonyan & Zisserman, 2014; Yang et al., 2017; Liu et al., 2019a; Rao et al., 2017b). While traditional practice for group-based recognition is to either aggregate the whole set by average (Li et al., 2014; Taigman et al., 2014) or max pool- ing (Chowdhury et al., 2016), or just sampling randomly (Wang et al., 2016), the fact that certain elements contribute negatively in recognition tasks has been ignored. Thus, an important issue is to select representatives from sets for efficient group understanding. To tackle such cases, previous methods aim at defining the \"quality\" or \"saliency\" for each element in a group (Liu et al., 2017c; Yang et al., 2017; Rao et al., 2017b; Nikitin et al., 2017). The weights for each element can be automatically learned by self-attention. For example, Liu et al. (2017c) proposes the Quality Aware Network (QAN) to learn quality score for each image inside an image set during network training. Other works adopt the same idea and extend to specific tasks such as video-based person re-identification (Li et al., 2018; Wu et al., 2018) and action recognition (Wang et al., 2018c) by learning spatial-temporal attentions. However, the whole online quality or atten- tion learning procedures are either manually designed or learned through a black box, which lacks explainability. In this work, we explore deeper into the underlying mechanism for defining effective elements in- stead of relying on self-learned attention. Assuming that a base network has already been trained for element-based recognition using class labels, we define the \"discriminability\" of one sample by how difficult it is for the network to discriminate its class. As pointed out by Liu et al. (2018) that the feature embedding of elements lies close to the centroid of their corresponding class are the representatives, while features far away or closer to other classes are the confusing ones which Under review as a conference paper at ICLR 2020 are not discriminative enough. Inspired by this observation, we identify a successful discriminabil- ity indicator by measuring one embedding's distance with class centroids and compute the ratio of between positive and hardest-negative, where the positive is its distance with its class's corre- sponding centroid and the hardest-negative is the closest counterpart. This indicator is defined as the discriminability distillation regulation (DDR). Armed with recent theories on the homogeneity between class centroids and projection weights of classifiers (Wang et al., 2017a; Liu et al., 2017b; 2018; Deng et al., 2019a), the entire distance- measuring procedure can be easily accomplished by simply encoding all elements in one group. Thus, the DDR scores can be assessed for each element after the training of the base network. This assessing procedure is highly flexible without human supervision nor re-training the base network, so it can be adapted to any existing base. With our explicitly designed discriminability indicator on the training set, the distillation of such discriminability can be successfully performed with a light- weight discriminability distillation network (DDNet), which shows the superiority of our proposed indicator. We call the whole procedure uniformly as discriminability distillation learning (DDL). The next step is towards finding a better aggregation policy. At the test phase, all elements are firstly sent to the light-weight DDNet. Then element features will be weighted aggregated by their DDR score into group representation. Moreover, in order to achieve the trade-off between accu- racy and efficiency, we can filter elements by DDR score and only extract element features of high score. Since the base model tends to be heavy, the filter can save much computation consump- tion. We evaluate the effectiveness of our proposed DDL on several classical yet challenging tasks. Comprehensive experiments show the advantage of our method on both recognition accuracy and computation efficiency. We achieve state-of-the-art results without modifying the base networks. We highlight our contributions as follows: (1) We define the discriminability of one element within a group from a more essential and explicable view, and propose an efficient indicator. (2) We verify that a light-weight network has the capacity of distilling discriminability from the assessed elements. Combining the post-processing with the network, the great computation burden can be saved com- paring with existing methods. (3) We validate the effectiveness of DDL for both efficiency and accuracy on set-to-set face recognition and action recognition through extensive studies. State-of- the-art results can be achieved.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Function Feature Learning (FFL) method to measure the similarity between different trained neural networks. FFL addresses the random permutation of weights of neural networks by using a chain alignment rule and then projects the aligned weights into a common space. The proposed FFL approach achieves consistent high accuracy for three types of neural networks, which shows the effectiveness of FFL and the soundness of the finding that there exist strong relations between different local solutions optimized by the Stochastic Gradient Descent (SGD) algorithm. Additionally, the paper investigates the chain based semantics and the results suggest that the semantics are hierarchical. The paper also analyzes several factors of neural networks and finds that 1) adding more layers or changing the ReLU activation function into leaky ReLU has little impact on the structure of local solutions; 2) changing plain networks into residual networks has some impact on local solutions; 3) SGD often converges to a stable structure of local solutions while the Adam optimizer does not. \n\nAbstract: This paper proposes a Function Feature Learning (FFL) method to measure the similarity between different trained neural networks. FFL addresses the random permutation of weights of neural networks by using a chain alignment rule and then projects the aligned weights into a common space. The proposed FFL approach achieves consistent high accuracy for three types of neural networks, which shows the effectiveness of FFL and the soundness of the finding that there exist strong relations between different local solutions optimized by the Stochastic Gradient Descent (SGD) algorithm. Additionally, the paper investigates the chain based semantics and the results suggest that the semantics are hierarchical, as well as analyzing several factors of neural networks.",
        "Abstract": "We present a Function Feature Learning (FFL) method that can measure the similarity of non-convex neural networks. The function feature representation provides crucial insights into the understanding of the relations between different local solutions of identical neural networks. Unlike existing methods that use neuron activation vectors over a given dataset as neural network representation, FFL aligns weights of neural networks and projects them into a common function feature space by introducing a chain alignment rule. We investigate the function feature representation on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), finding that identical neural networks trained with different random initializations on different learning tasks by the Stochastic Gradient Descent (SGD) algorithm can be projected into different fixed points. This finding demonstrates the strong connection between different local solutions of identical neural networks and the equivalence of projected local solutions. With FFL, we also find that the semantics are often presented in a bottom-up way. Besides, FFL provides more insights into the structure of local solutions. Experiments on CIFAR-100, NameData, and tiny ImageNet datasets validate the effectiveness of the proposed method.",
        "Introduction": "  INTRODUCTION Neural networks have achieved remarkable empirical success in a wide range of machine learning tasks ( LeCun et al., 1989 ;  Krizhevsky et al., 2012 ;  He et al., 2016 ) by finding a good local solution. How to better understand the characteristics of local solutions of neural networks remains an open problem. Recent evidence shows that identical neural networks trained with different initializations achieve nearly the same classification accuracy. Are these trained models (local solutions) equiva- lent? ( Li et al., 2016 ) claimed that neural networks converge to apparently distinct solutions in which it is difficult to find one-to-one mappings of neuron units. ( Raghu et al., 2017 ;  Morcos et al., 2018 ;  Kornblith et al., 2019 ) concentrated on comparing representations of neural networks using the in- termediate output of neural networks over a given dataset. These studies provide important insights into the understanding of similarity of neurons by probing and aligning the intermediate output (or neuron activation) representation of data points, but they do not focus on how to directly measure the similarity of function feature representations of neural networks using weights of networks. In this paper, we propose a Function Feature Learning (FFL) method to measure the similarity be- tween different trained neural networks. Instead of using intermediate activation/response values of neural networks over a bunch of data points, FFL directly learns an effective weight feature represen- tation from trained neural networks. To address the problem of random permutated weights ( Figure 1 ), a chain alignment rule is introduced to eliminate permutation variables. The aligned weights are then learned to project into a function feature representation space by classifying different classes of local solutions. The learned function features can be used to describe the characteristics of local solutions. With FFL, one can validate some assumptions about the similarity of local solutions. Function feature learning is built upon data feature learning. Given a set of data points, data feature learning is to learn a function f i that can describe the underlying representations to measure data similarity. Similarly, given a set of data representation functions {f i }, function feature learning is to learn a function F that can measure the similarity of {f i }. Specifically, an identical neural network with different weights forms a family of functions {f i } that could cover different function types (an Under review as a conference paper at ICLR 2020 identical neural network with different weights can be used as different function types for different learning tasks in practice). Function feature learning attempts to discover characteristics of functions and thus provides an effective metric for function similarity measure. In this paper, we propose to describe the function feature representation by using weights of neural networks instead of network structures because neural networks often share a common set of functional building blocks, e.g., global/local linear units, activation units, and normalization units. Overall, we make four main contributions as follows. • We propose a Function Feature Learning (FFL) method to measure the similarity of iden- tical neural networks trained from different initializations. FFL first addresses the random permutation of weights of neural networks by using a chain alignment rule and then projects the aligned weights into a common space. We find that there exist strong relations between different local solutions optimized by the Stochastic Gradient Descent (SGD) algorithm. • We investigate function feature representations of Multi-Layer Perceptron (MLP), Convo- lutional Neural Network (CNN), and Recurrent Neural Network (RNN) on the CIFAR-100, NameData, and tiny ImageNet datasets. With the chain alignment rule, the proposed FFL approach achieves consistent high accuracy for three types of neural networks, which shows the effectiveness of FFL and the soundness of the aforementioned finding. • We investigate the chain based semantics and the results suggest that the semantics are hierarchical. The projection directions of all layers are arranged in order along with the depth of neural networks. In short, the semantics are presented in a bottom-up way. • We analyze several factors of neural networks and find that 1) adding more layers or chang- ing the ReLU activation function into leaky ReLU has little impact on the structure of local solutions; 2) changing plain networks into residual networks has some impact on local so- lutions; 3) SGD often converges to a stable structure of local solutions while the Adam optimizer does not. Related Work. Neural networks are often regarded as black-boxes due to the non-convexity. To bet- ter understand these black-boxes, various approaches provide effective tools for visual interpretabil- ity of neural networks ( Simonyan et al., 2013 ;  Dosovitskiy & Brox, 2016 ;  Zeiler & Fergus, 2014 ;  Zhou et al., 2015 ;  Selvaraju et al., 2016 ). These approaches utilized gradient of the class scores with respect to input or de-convolution operations to visualize the attention activations at high-level semantics. Instead of building visual interpretability foundations between input and output, recent research ( Raghu et al., 2017 ;  Morcos et al., 2018 ;  Kornblith et al., 2019 ) focused on representations of neural networks by exploiting intermediate activations/features to describe the similarity of neural net- works. For example, SVCCA ( Raghu et al., 2017 ) used singular value decomposition and canonical correlation analysis tools for network representations and similarity comparison of neural networks. After that, a projection weighted CCA approach was developed for better understanding similarity of neural networks. In ( Kornblith et al., 2019 ), a centered kernel alignment method was proposed to measure the relation between data representational similarity matrices. Our approach concentrates on the function/weight feature representation but not intermediate representations of data points, which is greatly different from these works.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a study on the transferability of adversarial examples in black-box scenarios. It examines the effectiveness of existing attack algorithms in white-box scenarios and their transferability in non-targeted and targeted black-box scenarios. It also proposes a new approach to improve the transferability of targeted black-box attacks by using ensemble neural networks in the optimization based method. The results of the study demonstrate that the proposed approach can significantly improve the transferability of targeted black-box attacks.",
        "Abstract": "Neural networks show great vulnerability under the threat of adversarial examples.\n   By adding small perturbation to a clean image, neural networks with high classification accuracy can be completely fooled.\n   One intriguing property of the adversarial examples is transferability.  This property allows adversarial examples to transfer to networks of unknown structure, which is harmful even to the physical world.\n   The current way of generating adversarial examples is mainly divided into optimization based and gradient based methods.\n   Liu et al. (2017) conjecture that gradient based methods can hardly produce transferable targeted adversarial examples in black-box-attack.\n   However, in this paper, we use a simple technique to improve the transferability and success rate of targeted attacks with gradient based methods.\n   We prove that gradient based methods can also generate transferable adversarial examples in targeted attacks.\n   Specifically, we use knowledge distillation for gradient based methods, and show that the transferability can be improved by effectively utilizing different classes of information.\n   Unlike the usual applications of knowledge distillation, we did not train a student network to generate adversarial examples.\n   We take advantage of the fact that knowledge distillation can soften the target and obtain higher information, and combine the soft target and hard target of the same network as the loss function.\n   Our method is generally applicable to most gradient based attack methods.",
        "Introduction": "  INTRODUCTION Neural networks have been shown to be susceptible to adversarial examples. They can be misled when we add small perturbation to a clean image, even though the perturbation may be invisible to the human eye. Adversarial examples are widely used in different physical attack scenarios, includ- ing face recognition, voice recognition, and autonomous driving. In the physical world, however, it is difficult to obtain the structure of neural networks. This creates a need for attack algorithms in black-box scenarios. As attack algorithms are created for defense purposes, early discovery of adversarial examples that harm the physical world can help us defend against unknown threats. Many researchers have observed that adversarial examples can be transferred between different net- works.  Papernot et al. (2016)  create adversarial examples which can be transferred to black-box scenarios by attacking a constructed substitute model. Existing attack algorithms have high attack success rate in white-box scenarios and good transferability in non-targeted attacks. However, there is a very low transferability in targeted black-box scenes which is more harmful. Due to the tricky degree of targeted attack transferability problem, almost all existing researches to improve the trans- ferability focus on the untargeted attack part. In fact, it is easy to understand that an untargeted attack only needs to move the original image away from its category, while a targeted attack needs to reach the target category while penetrating the classification boundary.  Liu et al. (2017)  consider gradient based method only searching attacks in a 1-D subspace. In this approach, the subspace contains just a small subset of all target labels. They improve transferabil- ity by using ensemble neural networks in the optimization based method. They also prove that the approach of ensemble models is not effective for the gradient based method. For their experimental results and hypotheses, there is no difference between attacking ensemble neural networks and at- tacking a neural network for the transferability of gradient based method. We will show this is not correct by our study.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel problem setting, Predicate Zero-Shot Learning (PZSL), which focuses on recognizing unseen predicates between pairs of localized entities. A basic model is introduced to perform compatibility learning, leveraging linguistic priors from the corpus and knowledge base. The visual feature and semantic embedding spaces are aligned with the seen predicates as anchors. An unbalanced sampled-softmax is developed to adjust the gradient penalty of the infrequent predicates. The solution of the PZSL problem will greatly promote many downstream tasks, such as image captioning and visual question answering.",
        "Abstract": "Visual relationship recognition models are limited in the ability to generalize from finite seen predicates to unseen ones. We propose a new problem setting named predicate zero-shot learning (PZSL): learning to recognize the predicates without training data. It is unlike the previous zero-shot learning problem on visual relationship recognition which learns to recognize the unseen relationship triplets (<subject, predicate, object>) but requires all components (subject, predicate, and object) to be seen in the training set. For the PZSL problem, however, the models are expected to recognize the diverse even unseen predicates, which is meaningful for many downstream high-level tasks, like visual question answering, to handle complex scenes and open questions. The PZSL is a very challenging task since the predicates are very abstract and follow an extreme long-tail distribution. To address the PZSL problem, we present a model that performs compatibility learning leveraging the linguistic priors from the corpus and knowledge base. An unbalanced sampled-softmax is further developed to tackle the extreme long-tail distribution of predicates. Finally, the experiments are conducted to analyze the problem and verify the effectiveness of our methods. The dataset and source code will be released for further study. ",
        "Introduction": "  INTRODUCTION Visual relationship recognition ( Johnson et al., 2015 ;  Lu et al., 2016 ;  Xu et al., 2017 ) aims to estimate the relationships between pairs of localized entities, i.e., performing the recognition of triplets <subject, predicate, object>. It structurally describes images, which provides rich semantic information of an image to many applications including visual question answering (VQA) ( Li et al., 2018 ), image captioning ( Yang et al., 2019 ) and image retrieval ( Johnson et al., 2015 ). The relationship recognition methods are mainly supervised to recognize the entities and then combine various entities in pairs to identify predicates between them. There is an increasing interest in relationship zero-shot learning (ZSL) that learns to recognize the unseen relationship triplets, where the studies ( Lu et al., 2016 ;  Yu et al., 2017 ) on this ZSL problem setting assume the components (subject, predicate, and object) of the relationship triplet are seen. However, almost all of them only focus on dozens of frequent predicates and do not study the generalization of the seen predicates to the unseen ones. In this work, we propose the predicate zero-shot learning (PZSL) problem setting focusing on rec- ognizing the unseen predicates (no manual annotations or image samples). For example, given no instance of chew in the training data, the model is expected to recognize it during testing. Recog- nizing diverse even unseen predicates is significant for providing very rich relationship information, describing the complex scenes, and analogizing the known abstract concepts to the novel ones. The solution of the PZSL problem will greatly promote many downstream tasks, such as generating image caption with vivid predicates which are even unseen in the description corpus (image captioning) and answering the open questions (with novel predicates) on the complex scene (VQA). objects to recognize the unseen object. However, it is difficult to define the attributes of predicates. b) Predicates of existing datasets follow an extreme long-tail distribution (92.26% predicates with the number of instances lower than 10 in Visual Genome ( Krishna et al., 2017 )). The statistics of the dataset (VG-Zero) in this work are shown in Fig. 4. Under this distribution, the model tends to collapse to output few frequent predicates. Note that if the infrequent predicates are not recognized, the unseen predicates are more unlikely to be recognized. To address the PZSL problem, we introduce a basic model to perform compatibility learning ( Frome et al., 2013 ;  Akata et al., 2016 ; 2015) ( Fig. 1 ), leveraging the linguistic priors from the corpus and knowledge base ( Wang et al., 2018 ; Kampffmeyer et al., 2018). To represent the abstract predicates, we adopt the pre-trained word (sentence) vectors to initialize the predicates, connect them with linguistic relations defined in knowledge bases, and map them into a semantic embedding space (middle of Fig.1). A visual module is then applied to map paired image regions (left of  Fig. 1 ) into a visual feature space. The visual feature and semantic embedding spaces fall in the common space (top of  Fig. 1 ). During training, the visual feature and semantic embedding space are aligned with the seen predicates as anchors, i.e., a visual feature and semantic embedding labeled with the same predicate fall onto the same point/area in the common space. During testing, the samples in the test set are mapped into the visual feature space and matched with the nearest neighbor semantic embeddings of predicates (like chew). Furthermore, to tackle the long-tail distribution, an unbalanced sampled-softmax is developed to adjust the gradient penalty of the infrequent predicates.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a meta-learning framework, MetaVRF, which integrates variational inference and kernels for few-shot learning. The framework uses a long short-term memory (LSTM) based inference network to capture the shared knowledge across tasks and leverage task-specific knowledge to achieve an adaptive kernel to the current task. Experiments on a variety of few-shot learning problems demonstrate that MetaVRF achieves competitive or even better performance when compared to state-of-the-art algorithms, and can be applied to test settings with different ways and shots from those of training setting.",
        "Abstract": "Meta-learning for few-shot learning involves a meta-learner that acquires shared knowledge from a set of prior tasks to improve the performance of a base-learner on new tasks with a small amount of data. Kernels are commonly used in machine learning due to their strong nonlinear learning capacity, which have not yet been fully investigated in the meta-learning scenario for few-shot learning. In this work, we explore kernel approximation with random Fourier features in the meta-learning framework for few-shot learning. We propose learning adaptive kernels by meta variational random features (MetaVRF), which is formulated as a variational inference problem. To explore shared knowledge across diverse tasks, our MetaVRF deploys an LSTM inference network to generate informative features, which can establish kernels of highly representational power with low spectral sampling rates, while also being able to quickly adapt to specific tasks for improved performance. We evaluate MetaVRF on a variety of few-shot learning tasks for both regression and classification. Experimental results demonstrate that our MetaVRF can deliver much better or competitive performance than recent meta-learning algorithms.",
        "Introduction": "  INTRODUCTION Humans have the instinct to effortlessly learn new concepts from a few examples and show great generalization ability to new samples. However, existing machine learning models, e.g., deep neural networks (DNNs) ( Krizhevsky et al., 2012 ;  He et al., 2016a ), rely highly on large-scale annotated training data ( Deng et al., 2009 ) to achieve satisfactory performance. The huge gap between human intelligence and DNNs motivates us to try and progress the task of learning from a few samples, a.k.a. few-shot learning ( Fei-Fei et al., 2006 ;  Lake et al., 2015 ;  Ravi & Larochelle, 2017 ). Learning to learn, or meta-learning ( Schmidhuber, 1992 ), has recently received great interests in the machine learning community and offers a promising tool for few-shot learning ( Andrychowicz et al., 2016 ;  Ravi & Larochelle, 2017 ;  Finn et al., 2017 ). Generally speaking, a meta-learner ( Ravi & Larochelle, 2017 ;  Bertinetto et al., 2019 ) is trained to improve the performance of a base-learner on individual tasks, which is also fast adapted to solve new tasks. The crux of meta-learning for few-shot learning is to explore the common knowledge, such as a good parameter initialization ( Finn et al., 2017 ) or efficient optimization update rule ( Andrychowicz et al., 2016 ;  Ravi & Larochelle, 2017 ), shared across different tasks. The knowledge is accumulated and distilled throughout the learning stage, making the model adaptable to new but related tasks ( Finn et al., 2017 ). Kernel approximation by random Fourier features (RFFs) ( Rahimi & Recht, 2007 ) is an effective technique for efficient kernel learning ( Gärtner et al., 2002 ), which has recently become increas- ingly popular ( Sinha & Duchi, 2016 ;  Carratino et al., 2018 ). It resorts to the Fourier transform of shift-invariant kernels and constructs explicit feature maps using the Monte Carlo approximation of the Fourier representation. The desired kernel function is approximated by the inner products between these random features. Though demonstrating great potential as a strong base learner, kernel approximation with random features has not yet been fully explored in the meta-learning scenario for few-shot learning. It has already been shown that the classification performance of the kernel with random features does not correlate well with the accurate approximation of kernels. Learning adaptive kernels with random features, for instance, by data-driven sampling strategies ( Sinha & Duchi, 2016 ), can improve the performance with a low sampling rate compared to using universal random features ( Avron et al., 2016 ;  Chang et al., 2017 ). However, since only a few samples are Under review as a conference paper at ICLR 2020 available in each task, it is challenging to learn adaptive kernels with data-driven random features while maintaining high representational capacity for few-shot learning tasks. To obtain powerful kernels for few-shot learning tasks, we need to fully explore the relationship among diverse tasks and capture their shared knowledge to generate informative random features. In this work, we propose meta variational random features (MetaVRF) to approximate kernels in a data-driven manner for few-shot learning, which integrates variational inference and kernels in the meta-learning framework. Learning kernels with random Fourier features for few-shot learning allows us to leverage the universal approximation property of kernels to capture shared knowledge in related tasks, and meanwhile it enables us to learn adaptive basis functions to quickly and efficiently adapt to new tasks. Learning adaptive kernels with data-driven random features can be naturally cast into variational inference that approximates probability density through optimization, where the posterior over the random basis function is the spectral distribution of a translation-invariant kernel. The inference of the posterior is conducted in the context of tasks to exploring their dependency for capturing shared knowledge. We adopt a long short-term memory (LSTM) based inference network ( Hochreiter & Schmidhuber, 1997 ), which establishes task context inference to capture the task dependency. Specifically, during the inference, the cell state in the LSTM carries and accumulates the shared knowledge which is updated for each task throughout the course of learning. The remember and forget operations in the LSTM use new information to episodically refine the cell state by gaining experience from a batch of tasks, which can eventually produce random features of highly representational capability for all tasks. For an individual task, the task specific information is first extracted from the support set, and then combined with the shared knowledge in the shared cell state together as the joint condition, to infer the adaptive spectral distribution of the kernels. As a result, the task context inference can not only learn to extract and maintain the shared knowledge across tasks, but also leverage the task-specific knowledge to achieve an adaptive kernel to the current task. The inference framework of our MetaVRF is illustrated in  Figure 1 . Extensive experiments on a variety of few-shot learning problems such as regression and classification demonstrate that, our MetaVRF method achieves competitive or even better performance when compared to state-of-the-art algorithms. Due to the advantages of kernels, our MetaVRF can be applied to test settings with different ways and shots from those of training setting, in which the promising results again validate the effectiveness of our MetaVRF for few-shot learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel metric, Cross Category Kullback-Leibler divergence (CCKL), to characterize the standard performance of deep classification models. Through a Taylor expansion of CCKL, the relation between standard generalization and adversarial behavior of deep neural networks (DNNs) is revealed in a general way. Experiments demonstrate that current DNNs rely heavily on optimizing a lower order non-robust component to generalize, which is a major underlying reason for the adversarial behavior. The paper also suggests a possible direction for simultaneously achieving decent standard accuracy and adversarial robustness.",
        "Abstract": "The vulnerability to slight input perturbations is a worrying yet intriguing property of deep neural networks (DNNs). Though some efforts have been devoted to investigating the reason behind such adversarial behavior, the relation between standard accuracy and adversarial behavior of DNNs is still little understood. In this work, we reveal such relation by first introducing a metric characterizing the standard performance of DNNs. Then we theoretically show this metric can be disentangled into an information-theoretic non-robust component that is related to adversarial behavior, and a robust component. Then, we show by experiments that DNNs under standard training rely heavily on optimizing the non-robust component in achieving decent performance. We also demonstrate current state-of-the-art adversarial training algorithms indeed try to robustify DNNs by preventing them from using the non-robust component to distinguish samples from different categories. Based on our findings, we take a step forward and point out the possible direction of simultaneously achieving decent standard generalization and adversarial robustness. It is hoped that our theory can further inspire the community to make more interesting discoveries about the relation between standard accuracy and adversarial robustness of DNNs.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have achieved wide success over the last decade. In literature, the majority of deep learning models pursue boosted performance from different aspects (Krizhevsky et al., 2012;  Simonyan & Zisserman, 2014 ;  Szegedy et al., 2015 ;  He et al., 2016 ; 2015;  Ioffe & Szegedy, 2015 ). However, it is found these powerful models are susceptible to perturbations, even those imperceptible to humans ( Szegedy et al., 2013 ). Therefore beyond the main research stream, there are also works devoted to investigating effective attacks ( Goodfellow et al., 2014 ;  Moosavi- Dezfooli et al., 2016 ;  Kurakin et al., 2016 ;  Zhao et al., 2018 ) and designing adversarial robust models ( Goodfellow et al., 2014 ;  Miyato et al., 2015 ;  Madry et al., 2017 ;  Zhang et al., 2019 ). It is observed that the adversarial robustness is usually achieved at the cost of a non-trivial degradation of standard performance. Previous efforts ( Schmidt et al., 2018 ;  Tsipras et al., 2018 ;  Nakkiran, 2019 ) trying to understand this phenomenon are usually based on simple toy models or heavy assumptions, and do not provide a general theoretic framework that explicitly shows how adversarial robustness is related to standard generalization. The work ( Ilyas et al., 2019 ) empirically shows the adversarial samples might be human imperceptible features that could help generalization, but it does not provide any general theoretic framework to properly explain this phenomenon. In another work ( Zhang et al., 2019 ), though a bound on the gap between standard accuracy and adversarial accuracy is derived so that the gap between these two quantities could be explicitly controlled, the underlying reason of such trade-off is still not fully characterized. In this work, we start from a new perspective and consider the performance of deep classification models with a new metric Cross Category Kullback-Leibler divergence (CCKL), namely the Kull- back-Leibler (KL) divergence between the model's output distributions over input data from different categories, instead of the traditional metric of accuracy. Interestingly, by applying Taylor expansion on CCKL, we show that it can be disentangled into a lower order non-robust component, which is related to causing adversarial behavior, and a higher order robust component. By applying such Under review as a conference paper at ICLR 2020 disentanglement, we are able to reveal the relation between standard generalization and adversarial robustness of the deep learning model in a relatively general setting. Furthermore, we demonstrate by experiments that current deep learning models rely heavily on optimizing the lower order non-robust component to generalize, which is a major underlying reason for the adversarial behavior. We also show the state-of-the-art adversarial training algorithms are all in fact trying to constrain the model from using the lower order non-robust component to discriminate data of different categories. Based on these findings, we claim that enabling the model to rely more on the higher order robust component instead of the adversary-prone lower order component might be the key to achieving decent standard accuracy and adversarial robustness simultaneously. Our contributions are summarized as follows: • We propose a new metric, Cross Category Kullback-Leibler divergence (CCKL), to char- acterize the standard performance. This metric can be more naturally connected with the adversarial behavior of DNNs than the traditional metric of accuracy. • By applying a simple Taylor expansion on CCKL, we theoretically reveal the relation between standard generalization and adversarial behavior of DNNs in a general way without relying on toy models. • Based on the above novelties, we take a further step and point out the possible direction for simultaneously achieving decent standard accuracy and adversarial robustness.",
        "label": 0
    },
    {
        "Summary": " \n\nAbstract: This paper proposes a new semi-supervised learning method, called Coaching, which uses a teacher model to generate pseudo labels for unlabeled data, from which a student model learns by imitation. The student's performance on labeled data is used as reward to train the teacher with policy gradient, allowing the teacher to generate better pseudo labels for the student to learn from.",
        "Abstract": "Recent semi-supervised learning (SSL) methods often have a teacher to train a student in order to propagate labels from labeled data to unlabeled data. We argue that a weakness of these methods is that the teacher does not learn from the student’s mistakes during the course of student’s learning.  To address this weakness, we introduce Coaching, a framework where a teacher generates pseudo labels for unlabeled data, from which a student will learn and the student’s performance on labeled data will be used as reward to train the teacher using policy gradient.\n\nOur experiments show that Coaching significantly improves over state-of-the-art SSL baselines. For instance, on CIFAR-10, with only 4,000 labeled examples, a WideResNet-28-2 trained by Coaching achieves 96.11% accuracy, which is better than 94.9% achieved by the same architecture trained with 45,000 labeled. On ImageNet with 10% labeled examples, Coaching trains a ResNet-50 to 72.94% top-1 accuracy, comfortably outperforming the existing state-of-the-art by more than 4%. Coaching also scales successfully to the high data regime with full ImageNet. Specifically, with additional 9 million unlabeled images from OpenImages, Coaching trains a ResNet-50 to 82.34% top-1 accuracy, setting a new state-of-the-art for the architecture on ImageNet without using extra labeled data.",
        "Introduction": "  INTRODUCTION Professional players in competitive sports such as chess, tennis, or swimming often have coaches to help improving their performance. Although coaches typically do not play as well as the players, they observe the players and provide instructions to improve the players' performance. Modern semi-supervised learning (SSL) algorithms do not follow this strategy. They instead have a teacher model that generates pseudo labels for unlabeled data, from which a student model learns by imitation (e.g.,  Lee (2013) ;  Tarvainen & Valpola (2017) ;  Laine & Aila (2017) ). A weakness of these methods is that the teacher does not adjust itself based on the student's performance and cannot adapt to make the student better over time, unlike professional sport coaches develop their players. Here, we propose a new semi-supervised learning method, called Coaching as shown in  Figure 1 , where the teacher learns throughout the course of student's training. In our method, a teacher generates pseudo labels for unlabeled data, from which the student will learn. The student's performance on labeled data will be used as reward to train the teacher with policy gradient.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a dual encoding framework for unsupervised inductive representation learning of graphs. The framework leverages feature information to generate embeddings of nodes and applies bi-attention between representations of two nodes that co-occur in a short random-walk. The paper also combines the idea of transductive and inductive approaches by associating an additive global embedding bias to each node. Experiments are conducted to demonstrate the efficiency and effectiveness of the proposed framework.",
        "Abstract": "Graph representation learning embeds nodes in large graphs as low-dimensional vectors and  benefit to many downstream applications. Most embedding frameworks, however, are inherently transductive and unable to  generalize to unseen nodes or learn representations across different graphs. Inductive approaches, such as GraphSAGE,  neglect different contexts of nodes and cannot learn node embeddings dually. In this paper, we present an unsupervised dual encoding framework, \\textbf{CADE},  to generate context-aware representation of nodes by combining real-time neighborhood structure with neighbor-attentioned representation, and preserving extra memory of known nodes. Experimently, we exhibit that our approach is effective by comparing to state-of-the-art methods.",
        "Introduction": "  INTRODUCTION The study of real world graphs, such as social network analysis (Hamilton et al. (2017a)), molecule screening (Duvenaud et al. (2015)), knowledge base reasoning (Trivedi et al. (2017)), and biological protein-protein networks (Zitnik & Leskovec (2017)), evolves with the development of computing technologies. Learning vector representations of graphs is effective for a variety of prediction and graph analysis tasks (Grover & Leskovec (2016); Tang et al. (2015)). High-dimensional informa- tion about neighbors of nodes are represented by dense vectors, which can be fed to off-the-shelf approaches to tasks, such as node classification (Wang et al. (2017); Bhagat et al. (2011)), link prediction (Perozzi et al. (2014); Wei et al. (2017)), node clustering (Nie et al. (2017); Ding et al. (2001)), recommender systems (Ying et al. (2018a)) and visualization (Maaten & Hinton (2008)). There are mainly two types of models for graph representation learning. Transductive approaches (Perozzi et al. (2014); Grover & Leskovec (2016); Tang et al. (2015)) are able to learn representations of existing nodes but unable to generalize to new nodes. However, in real-world evolving graphs such as social networks, new users will join and must be represented. Inductive approaches were proposed to address this issue. GraphSAGE (Hamilton et al. (2017b)), a hierarchical sampling and aggregating framework, successfully leverages feature information to generate embeddings of the new nodes. However, GraphSAGE has its own faults. Firstly, it samples all neighborhood nodes randomly and uniformly; secondly, it treats the output of encoder as the final representation of node. Based on the hierarchical framework of GraphSAGE, GAT (Velickovic et al. (2017)) uses given class labels to guide attention over neighborhood so as to aggregate useful feature information. However, without knowledge of ground-truth class labels, it is difficult for unsupervised approaches to apply attention. To address this issue, we introduce a dual encoding framework for unsupervised inductive representation learning of graphs. Instead of learning self-attention over neighborhoods of nodes, we exploit the bi-attention between representations of two nodes that co-occur in a short random-walk (which we call a positive pair). In  Figure 1 , we illustrate how nodes are embedded into low-dimensional vectors, where each node v has an optimal embeddings o v . Yet the direct output of encoder z v of GraphSAGE could be located anywhere. Specifically, given feature input from both sides of a positive pair (v, v p ), a neural network is trained to encode the pair into K different embeddings z k v and z k vp through different sampled neighborhoods or different encoding functions. Then, a bi-attention layer is applied to generate the most adjacent matches z v|vp and z vp|v , which will be referred as dual-representations. By putting most attention on the pair of embeddings with smallest difference, dual representation of nodes with less deviation will be generated, which can be visualized as z v|· in  Figure 1 . GraphSAGE naively assumes that unseen graph structure should be (easily) represented by known graphs data. We combine the ground truth structure and the learned dual-encoder to generate final representation. Unseen nodes can be represented based on their neighborhood structure. Current inductive approaches have no direct memory of the training nodes. We combine the idea of both transductive and inductive approaches via associating an additive global embedding bias to each node, which can be seen as a memorable global identification of each node in training sets. Our contributions can be summarized as follows: • we introduce a dual encoding framework to produce context-aware representation for nodes, and conduct experiments to demonstrate its efficiency and effectiveness; • we apply bi-attention mechanism for graph representation dual learning, managing to learn dual representation of nodes more precisely; • we combine the training of transductive global bias with inductive encoding process, as memory of nodes that are already used for training.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a new algorithm, NeuralUCB, for the stochastic contextual bandit problem which uses a deep neural network to learn the underlying reward function. The algorithm is proven to achieve a O(d√T) regret, where d is the effective dimension of a neural tangent kernel matrix and T is the number of rounds. The paper also provides empirical evidence in several proof-of-concept experiments to demonstrate the potential applications of the algorithm to real-world problems.",
        "Abstract": "We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses the neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\\tilde O(\\sqrt{T})$ regret bound, where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee.  Preliminary experiment results on synthetic data corroborate our theory, and shed light on potential applications of our algorithm to real-world problems.",
        "Introduction": "  INTRODUCTION The stochastic contextual bandit problem has been extensively studied in machine learning ( Bubeck and Cesa-Bianchi, 2012 ;  Lattimore and Szepesvári, 2019 ): at round t ∈ {1, 2, . . . , T }, an agent is presented with a set of K actions, each of which is associated with a d-dimensional feature vector. After choosing an action, the agent will receive a stochastic reward generated from some unknown distribution conditioned on the chosen action's feature vector. The goal of the agent is to maximize the expected cumulative rewards over T rounds. Contextual bandit algorithms have been applied to many real-world applications, such as personalized recommendation, advertising and Web search (e.g.,  Agarwal et al., 2009 ;  Li et al., 2010 ). The most studied model in the literature is linear contextual bandits ( Auer, 2002 ;  Abe et al., 2003 ;  Dani et al., 2008 ;  Rusmevichientong and Tsitsiklis, 2010 ;  Chu et al., 2011 ;  Abbasi-Yadkori et al., 2011 ), which assumes that the expected reward at each round is a linear function of the feature vector. Linear bandit algorithms have achieved great success in both theory and practice, such as news article recommendation ( Li et al., 2010 ). However, the linear-reward assumption often fails to hold exactly in practice, which motivates the study of nonlinear contextual bandits (e.g.,  Filippi et al., 2010 ;  Srinivas et al., 2010 ;  Bubeck et al., 2011 ;  Valko et al., 2013 ). However, they still require fairly strong assumptions on the reward function. For instance,  Filippi et al. (2010)  makes a generalized linear model assumption on the reward,  Bubeck et al. (2011)  require it to have a Lipschitz continuous property in a proper metric space, and  Valko et al. (2013)  assume the reward function belongs to some Reproducing Kernel Hilbert Space (RKHS). In order to overcome the above shortcomings, deep neural networks (DNNs) ( Goodfellow et al., 2016 ) have been introduced to learn the underlying reward function in contextual bandit problem, thanks to their strong representation power. Given the fact that DNNs enable the agent to make use of nonlinear models with less domain knowledge, existing work ( Riquelme et al., 2018 ;  Zahavy and Mannor, 2019 ) focuses on the idea called neural-linear bandit. More precisely, they use the first L − 1 layers of a DNN as a feature map, which transforms contexts from the raw input space to a low-dimensional space, usually with better representation and less frequent update. Then they learn a linear exploration policy on top of the last hidden layer of the DNN with a more frequent update. These attempts have achieved great empirical success. However, none of these work provides a theoretical guarantee on the regret of the algorithms. In this paper, we take the first step towards provable efficient contextual bandit algorithms based on deep neural networks. Specifically, we propose a new algorithm, NeuralUCB, which uses a deep neural network to learn the underlying reward function. At the core of the algorithm is an upper Under review as a conference paper at ICLR 2020 confidence bound constructed by deep neural network-based random feature mappings. Our regret analysis of NeuralUCB is built on recent results on optimization and generalization of deep neural networks ( Jacot et al., 2018 ;  Arora et al., 2019 ; Cao and Gu, 2019a). While the main focus of our paper is mostly theoretical, we also carry out proof-of-concept experiments on synthetic data to validate the effectiveness of our proposed algorithm. Our contributions are summarized as follows: • We prove that, under mild assumptions, our algorithm is able to achieve a O( d √ T ) regret, where d is the effective dimension of a neural tangent kernel matrix and T is the number of rounds. Our regret bound recovers the O(d √ T ) regret for linear contextual bandit as a special case ( Abbasi- Yadkori et al., 2011 ), where d is the dimension of context. • We propose a neural contextual bandit algorithm using neural network-based exploration. It can be regarded as an extension of existing linear bandit algorithms ( Li et al., 2010 ;  Abbasi-Yadkori et al., 2011 ), from linear reward functions to any bounded reward functions. • We provide empirical evidence in several proof-of-concept experiments to demonstrate potential applications of our algorithm to real-world problems. Notation: Scalars are denoted by lower case letters, vectors by lower case bold face letters, and matrices by upper case bold face letters. For a positive integer k, [k] denotes {1, . . . , k}. For a vector θ ∈ R d , we denote its 2 norm by θ 2 = d i=1 θ 2 i and its j-th coordinate by [θ] j . For a matrix A ∈ R d×d , we denote its spectral norm, Frobenius norm, and (i, j)-th entry by A 2 , A F , and [A] i,j , respectively. We denote a sequence of vectors by {θ j } t j=1 , and similarly for matrices. For two sequences {a n } and {b n }, we use a n = O(b n ) to denote that there exists some constant C > 0 such that a n ≤ Cb n , a n = Ω(b n ) to denote that there exists some constant C > 0 such that a n ≥ C b n . In addition, we use O(·) to hide logarithmic factors. We say a random variable X is ν-sub-Gaussian if E exp(λ(X − EX)) ≤ exp(λ 2 ν 2 /2) for any λ > 0.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents AnoDM, a novel anomaly detection approach based on unsupervised disentangled representation learning and manifold learning. AnoDM combines a β-VAE's reconstruction error and distances between latent representations of test points and training points in a t-SNE map to create an anomaly score function. AnoDM is a general framework, and any disentangled representation learning and manifold learning techniques can be applied. The paper also introduces an improved version of β-VAE for time series data, which replaces the convolutional network in the encoder with a temporal convolutional network.",
        "Abstract": "Identifying anomalous samples from highly complex and unstructured data is a crucial but challenging task in a variety of intelligent systems. In this paper, we present a novel deep anomaly detection framework named AnoDM (standing for Anomaly detection based on unsupervised Disentangled representation learning and Manifold learning). The disentanglement learning is currently implemented by beta-VAE for automatically discovering interpretable factorized latent representations in a completely unsupervised manner. The manifold learning is realized by t-SNE for projecting the latent representations to a 2D map.  We define a new anomaly score function by combining beta-VAE's reconstruction error in the raw feature space and local density estimation in the t-SNE space. AnoDM was evaluated on both image and time-series data and achieved better results than models that use just one of the two measures and other deep learning methods.",
        "Introduction": "  INTRODUCTION Detecting anomalies in data flow of modern intelligent systems is an important but challenging problem. Formally speaking, anomaly detection problems can be statistically viewed as identi- fying outliers having low probabilities from the modelling of data distribution p(x). Practically, since statistical modelling of the data is often difficult, it degenerates to domain description (Tax & Duin, 1999) or supervised prediction (Gornitz et al., 2013) problems in some cases. The exact explanation of an anomalous data point depends on the specific domain of focus. In data centers, it probably indicates an attempt of cyber intrusion. In recognition systems, it could be an adversarial attack. In biomedical information systems, it means possible onset of certain diseases. In Internet of Things (IoT) systems, it may represent a hardware failure or alarming event captured by sensors. An anomalous sample is not always associated with negativity. Sometimes, it leads to novel discoveries in scientific explorations. However, from the data analytics perspective, anomaly detection is a difficult task due to the follow- ing reasons. (1) Many forms of data, e.g., images, text, and other types of sequences, are often highly unstructured and complex. How can these data be well represented and high-level information be extracted by an algorithm? (2) The sample sizes of modern data sets are often extremely large and most of them are unlabelled. Unfortunately, traditional methods do not scale and perform well on these data. (3) When data of multiple modalities are naturally available for same events in a system, a robust and precise algorithm needs to be designed to integrate these information for system diag- nosis or decision making. (4) Many intelligent systems, such as IoTs, require real-time detection and reaction of abnormal events to avoid costly and irrevocable damages. Thus, anomaly monitor- ing algorithms to be designed in these platforms must be highly efficient. In summary, anomaly detection raises challenges in representability, scalability, multimodality, and time complexity. Deep learning (LeCun et al., 2015) offers great potentials to overcome these challenges. (1) Rep- resentation learning mechanisms (such as convolution for images, embedding for discrete symbols, and recurrence for time-series) have been developed in supervised and unsupervised deep models to consider the nature of specific types of input samples and encode them into vectors of contin- uous values as corresponding latent representations. (2) Most deep learning models are trained using stochastic gradient descent that splits a giant training set into mini-batches. Thus, learning be- Under review as a conference paper at ICLR 2020 comes unrestricted and blessed by a large sample size. Particularly, stochastic variational inference (Hoffman et al., 2013; Zhang et al., 2018) has successfully enabled scalable learning and inference for deep generative models (DGMs) on a vast amount of unlablled data. (3) The development of deep learning programming packages, such as PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2016), greatly eases the assembly of multiple network components (corresponding to different modalities) together for multimodal representation learning (Li et al., 2018b). (4) Once a deep model is learned, the inference or encoding step is very efficient, thanks to the highly parallel computing architectures and techniques. In some applications, if the domain of anomalous and normal samples is well defined, anomaly detection can be reduced to binary classification problems. However, in many situations, either the domain of anomalous samples cannot be fully understood or modelled, or the domain of the normal samples is too complicated to be modelled in one class. DGMs are more suitable than supervised methods in such cases. DGMs are concerned with the joint distribution of visible and latent variables with a hierarchy of stochastic (and deterministic) layers. With proper emphasis on disentanglement of latent representations, DGMs have the potential of dissecting hidden factors that are key to sample generation. Unsupervised disentangled representation learning (Bengio et al., 2013) renders several benefits. (1) It helps better understand our data, providing a path towards explainable AI. (2) It gives a better control on the generation process of novel samples. (3) The disentanglement of latent factors may provide an opportunity to distinguish anomalies based on the landscape of latent space, which is our interest in this paper. It has been shown that the likelihood of a data point p(x) estimated in DGM is not a reliable measure for detecting abnormal samples (Nalisnick et al., 2019). Instead, reconstruction error is widely used as an anomaly score function (An & Cho, 2015). As a variant of variational autoencoder (VAE) (Kingma & Welling, 2014), β-VAE (Higgins et al., 2017) is designed for unsupervised discovery of interpretable factorized latent representations from raw image data. An adjustable hyperparameter β is introduced to balance the extent of learning constraints (a limit on the capacity of the latent information channel and an emphasis on learning statistically independent latent factors) and reconstruction accuracy. It was demonstrated that β- VAE with appropriately tuned value of β (when β > 1) qualitatively outperforms VAE (when β = 1, β-VAE is exactly VAE). Burgess et al. (2018) proposed a modification to the training regime of β- VAE by progressively increasing the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in β-VAE, without the previous trade-off in the reconstruction accuracy. Hoffman et al. (2017) introduced a reformulation of β-VAE for 0 < β < 1. They argued that, within in this range, training β-VAE is equivalent to optimizing an approximate log-marginal likelihood bound of VAE under an implicit prior. Manifold learning is a family of nonlinear dimensionality reduction techniques. The t-distributed stochastic neighbor embedding (t-SNE) (van der Maaten & Hinton, 2008) is an unsupervised man- ifold learning method primarily used for data exploration and visualization by approximating high- dimensional data distribution using a two or three-dimensional map that could preserve local and certain global structures of the data. The use of t-SNE for anomaly detection has been sceptical (van der Maaten & Hinton, 2008). However, no comprehensive investigation has been made in this topic. Taking advantages of both disentangled representation learning (using β-VAE as an imple- mentation) and low-dimensional manifold learning (using t-SNE as an implementation), we propose a novel anomaly detection approach named AnoDM, standing for Anomaly detection based on unsu- pervised Disentangled representation learning and Manifold learning. We introduce a new anomaly score function by combining: (1) β-VAE's reconstruction error, and (2) distances between latent representations of test points and training points in t-SNE map. AnoDM is a general framework, thus any disentangled representation learning and manifold learning techniques can be applied. The choice of a lower-level encoding scheme in β-VAE depends on data type of interest. For image data, deterministic convolutional network (CNN) is used in the encoder. In case of time series (sequence) data, we design an improved version of β-VAE by replacing CNN with temporal convolutional net- work (TCN) (Bai et al., 2018), a generic architecture for convolutional sequence prediction, in the encoder. We incorporate TCN as part of the encoder, because Bai et al. (2018) have shown that TCN outperforms canonical recurrent networks such as LSTMs (Hochreiter & Schmidhuber, 1997) across a range of supervised learning tasks and recommended that CNN should be regarded as the first method to try for sequence modeling tasks. Regarding the decoding architecture, we simply choose CNN, because by choosing a simpler CNN architecture as a part of the decoder, the model can achieve a comparable even better performance but take much less running time.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper argues that filter ambiguity is a major obstacle to the interpretability of Convolutional Neural Networks (CNNs). To address this issue, a novel training strategy is proposed to enforce the one-class for one-filter relationship during training in a supervised manner. Experiments prove that this method yields better representation for images, leading to improved interpretability of the network. Advantages of this method are demonstrated by sparser correlation between filters, sparser filter-class correlation, better explanation for misclassification and more precise localization of labeled objects.",
        "Abstract": "Convolutional neural networks (CNNs) have often been treated as “black-box” and successfully used in a range of tasks. However, CNNs still suffer from the problem of filter ambiguity – an intricate many-to-many mapping relationship between filters and features, which undermines the models’ interpretability. To interpret CNNs, most existing works attempt to interpret a pre-trained model, while neglecting to reduce the filter ambiguity hidden behind. To this end, we propose a simple but effective strategy for training interpretable CNNs. Specifically, we propose a novel Label Sensitive Gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner, in which redundant channels experience a periodical shutdown as flowing through a learnable gate varying with input labels. To reduce redundant filters during training, LSG is constrained with a sparsity regularization. In this way, such training strategy imposes each filter’s attention to just one or few classes, namely class-specific. Extensive experiments demonstrate the fabulous performance of our method in generating sparse and highly label- related representation of the input. Moreover, comparing to the standard training strategy, our model displays less redundancy and stronger interpretability.\n",
        "Introduction": "  INTRODUCTION Convolutional Neural Networks (CNNs) demonstrate extraordinary performance in various visual tasks (Krizhevsky et al., 2012; He et al., 2016; Girshick, 2015; He et al., 2017a). However, the strong expressive power of CNNs is still far from interpretable, which significantly limits its applications that require humans' trust or interaction, e.g. self-driving and medical image analysis (Caruana et al., 2015; Bojarski et al., 2017). In this paper, we argue that filter ambiguity is one of the most critical reasons that hampers the interpretability of CNNs. As a matter of fact, previous studies has shown that 1) filters in CNNs generally extract features of a mixture of various semantic concepts, including objects, parts, scenes, textures, materials and colors (Zhang et al., 2018b; Bau et al., 2017); and that 2) there is also redundant overlap between features extracted by different filters (Prakash et al., 2019). The intricate many-to-many correspondence relationship between filters and features is so-called filter ambiguity as shown on the left of  Figure 1 . Obviously, in high convolutional layers which might capture class-related feature, filter ambiguity contradicts our intention of an interpretable CNN, because it hinders humans from interpreting the concepts of a filter (Zhang et al., 2018b), which has been shown as an essential role in the visualiza- tion and analysis of networks (Olah et al., 2018) in human-machine collaborative systems (Zhang et al., 2017a;c). Moreover, the unnecessary overlap between features extracted by different filters leads to under-utilization of a model's expressiveness (Prakash et al., 2019) Therefore, reducing filter ambiguity is critical to obtain better feature with better interpretability and less redundancy. However, it is non-trivial to achieve such a goal barricaded by substantial challenges. First, most interpretability-related research simply focuses on post-hoc interpretation of filters (Szegedy et al., 2013; Bau et al., 2017), which manages to interpret the main semantic concepts captured by a filter but fails to alleviate the filter ambiguity prevalent in pretrained models. Second, many existing works such as VAEs' variants (Higgins et al., 2017; Burgess et al., 2018; Kim & Mnih, 2018; Chen et al., 2018; Kumar et al., 2017) and InfoGAN (Chen et al., 2016) try to disentangle data representation and obtain better interpretability in an unsupervised way. However, it is proved that unsupervised Under review as a conference paper at ICLR 2020 learning on disentangled features without inductive bias is impossible (Locatello et al., 2018), which challenges the works above. Considering the aforementioned challenges, we shed light on enforcing the one-class for one-filter relationship during training (instead of post-hoc) in a supervised manner only with classification la- bels. To this end, we propose a novel training strategy that coerces each filter into extracting features from only one or few classes for classification tasks, namely disentangling filters towards class- specific. Specifically, we design a Label Sensitive Gate (LSG) structure on the top of convolutional filters, which limits each filter's activation only to its specific input label(s). In our training pro- cess, we periodically insert LSG into the CNN and jointly minimize the classification cross-entropy and the sparsity of LSG, so as to keep the model's performance on classification and meanwhile encourage class-specific filters. Experiments proved the LGS filter yield better representation for images hence leading to promising applications. Our training method makes data representation sparse and highly correlated with the labeled class, which not only illustrates the alleviation of filter ambiguity but also enhances the interpretability of the network. The advantages of our method are concretely substantiated by sparser correlation between filters, sparser filter-class correlation, better explanation for misclassification and more precise localization of labeled objects.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel clustering algorithm, Extreme Value k-means (EV k-means), which utilizes Generalized Extreme Value (GEV) and Generalized Pareto Distribution (GPD) to transform the Euclidean space into an extreme value space and measure the similarity between samples and centroids. An acceleration method for Euclidean distance computation is also proposed to solve the bottleneck of k-means. An online version of EV k-means is proposed for clustering streaming data, which can learn the parameters of GEV and GPD online. Experiments on synthetic and real datasets show that EV k-means and online EV k-means significantly outperform compared algorithms.",
        "Abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.",
        "Introduction": "  INTRODUCTION Clustering is a fundamental and important task in the unsupervised learning (Jain, 2010; Rui Xu & Wunsch, 2005). It aims at clustering data samples of high similarity into the same cluster. The most well-known clustering algorithm is the k-means, whose objective is to minimize the sum of squared distances to their closest centroids. k-means has been extensively studied in the literature, and some heuristics have been proposed to approximate it (Jain, 2010; Dubes & Jain, 1988). The most famous one is Lloyd's algorithm (Lloyd, 1982). The k-means algorithm is widely used due to its simplicity, ease of use, geometric intuition (Bottesch et al., 2016). Unfortunately, its bottleneck is that computational complexity reaches O(nkd) (Rui Xu & Wunsch, 2005), since it requires computing the Euclidean distances between all samples and all centroids. The data is embedded in the Euclidean space (Stemmer & Kaplan, 2018), which causes the failure on clustering non-convex clusters (Beliakov & King, 2006). Even worse, k-means is highly sensitive to the initial centroids, which usually are randomly initialized. Thus, it is quite possible that the objective of k-means converges to a local minimum, which causes the instability of k-means, and is less desirable in practice. Despite a stable version - k-means++ (Arthur & Vassilvitskii, 2007) gives a more stable initialization, fundamentally it is still non-trivial to extend k-means in clustering data samples of non-convex shape. To solve these problems, this paper improves the clustering ability of k-means by measuring the similarity between samples and centroids by EVT (Coles et al., 2001). In particular, we consider the generalized extreme value (GEV) (Jenkinson, 1955) distribution or generalized Pareto distribution (GPD) (Pickands III et al., 1975; DuMouchel, 1975) to transform the Euclidean space into a proba- bility space defined as, extreme value space. GEV and GPD are employed to model the maximum distance and output the probability that a distance is an extreme value, which indicates the similarity of a sample to a centroid. Further, we adopt the Block Maxima Method (BMM) (Gumbel, 2012) to choose the maximal distance for helping GEV fit the data. The Peaks-Over-Thresh (POT) method (Leadbetter, 1991) is utilized to model the excess of distance exceeding the threshold, and thus very useful in fitting the data for GPD. Formally, since both GEV and GPD can measure the similarity of samples and centroids, they can be directly utilized in k-means, i.e., GEV k-means and GPD k-means, which are uniformly Under review as a conference paper at ICLR 2020 called Extreme Value k-means (EV k-means) algorithm. In contrast to k-means, EV k-means is a probability-based clustering algorithm that clusters samples according to the probability output from GEV or GPD. Furthermore, to accelerate the computation of Euclidean distance, We expand the samples and the centroids into two tensors of the same shape, and then accelerate with the high performance parallel computing of GPU. For clustering steaming data, we propose online Extreme Value k-means based on Mini Batch k- means (Sculley, 2010). When fit the GEV distribution, we use mini batch data as a block. For the fitting of GPD, we dynamically update the threshold. The parameters of GEV or GPD are learned by stochastic gradient descent (SGD) (LeCun et al., 1998). The main contributions are described as follows. (1) This paper utilizes EVT to improve k-means in addressing the problem of clustering data of non-convex shape. We thus propose the novel Extreme Value k-means, including GEV k-means and GPD k-means. A method for accelerating Euclidean distance computation has also been proposed to solve the bottleneck of k-means. (2) Under the strong theoretical support provided by EVT, we use GEV and GPD to transform Euclidean space into extreme value space, and measure the similarity between samples and centroids. (3) Based on Mini Batch k-means, We propose online Extreme value k-means for clustering streaming data, which can learn the parameters of GEV and GPD online. We corroborate the effectiveness of EV k-means and online EV k-means by conducting experiments on synthetic datasets and real datasets. Experimental results show that EV k-means and online EV k-means significantly outperform compared algorithms consistently across all experimented datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a hierarchical structure for parameter regularization on multiple groupwise hyperspheres, and explores a discrete angular metric to find an appropriate metric in this space. Experiments are conducted on various datasets and deep network models to evaluate the proposed method.",
        "Abstract": "Regularization is known to be an inexpensive and reasonable solution to alleviate over-fitting problems of inference models, including deep neural networks. In this paper, we propose a hierarchical regularization which preserves the semantic structure of a sample distribution. At the same time, this regularization promotes diversity by imposing distance between parameter vectors enlarged within semantic structures. To generate evenly distributed parameters, we constrain them to lie on \\emph{hierarchical hyperspheres}. Evenly distributed parameters are considered to be less redundant. To define hierarchical parameter space, we propose to reformulate the topology space with multiple hypersphere space. On each hypersphere space, the projection parameter is defined by two individual parameters. Since maximizing groupwise pairwise distance between points on hypersphere is nontrivial (generalized Thomson problem), we propose a new discrete metric integrated with continuous angle metric. Extensive experiments on publicly available datasets (CIFAR-10, CIFAR-100, CUB200-2011, and Stanford Cars), our proposed method shows improved generalization performance,  especially when the number of super-classes is larger.",
        "Introduction": "  INTRODUCTION Diversity promoting learning has been widely adopted via enlarging pairwise distances ( Xie et al., 2018 ;  2017a ;  Liu et al., 2018 ), increasing orthogonality ( Xie et al., 2018 ), reducing covariance be- tween parameters ( Xie et al., 2017b ), or reducing correlation on feature ( Cogswell et al., 2016 ) to improve generalization performance. Among them, diversity promoting regularization ( Xie et al., 2017a ; b ;  Liu et al., 2018 ) by enforcing large diversity between projection parameters achieves a reasonable performance without modifying the model structure. While optimizing the objective function with a covariance matrix in ( Xie et al., 2017b ; 2018) is nontrivial, the diversity promoting regularization via minimizing energy of parameters of deep neural networks has been proposed ( Liu et al., 2018 ) in a simpler way. By minimizing a pairwise distance between parameters on a hyper- sphere with the known metric, they achieved the improved generalization performance. Following an efficient regularization on the hypersphere, we explore further this direction with three main concepts (hierarchical and hyperspherical learning with discrete metrics). 1) Why hierarchical learning? Hierarchical inference explains human intelligence. In ( Kurzweil, 2013 ), it states that \"the neocortex contains about 300 million very general pattern recognizers, arranged in a hierarchy\". Applying the hierarchy of multiple classes based on semantic taxonomy is a natural choice to devise machine intelligence. Effectiveness of the hierarchical learning can be found in ( Verma et al., 2012 ). 2) Why hyperspherical learning? Hypersphere is the set of points at the equidistance (radius) from a given point (centroid) in a certain dimensional space. Due to the denominator in the unit-length nor- malization ( w w , w ∈ R d+1 ), the angular distance defined on the hypersphere converges when the magnitude of w goes infinity while Euclidean distance goes infinity. Due to this bounded property, a hierarchical structure with multiple separated hyperspheres can be defined. 3) Why discrete metric learning? If vector points form discontinuous series with discrete represen- tation (e.g. multi-dimensional binary or ternary), they are isolated from each other with a certain margin. This property may fit with a disconnected/groupwise manifold space problem. We note that making points to be equidistributed where a pairwise distance is maximized is a nontrivial task. As Under review as a conference paper at ICLR 2020 In this paper, we propose to apply a hierarchical structure to parameter regularization on the multiple groupwise hyperspheres. In order to find an appropriate metric in this space, we explore a discrete angular metric. We examine the proposed method on extensive experimental setups in terms of datasets and deep network models.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a lossless Super Resolution (SR) model to obtain images with satisfying quality from low-quality inputs (C-JPG). The deterioration in C-JPG makes the SR processing a huge challenge. To address this, the authors introduce a two-stage model that first recovers LR images from their C-JPG counterparts and then learns the mapping between the recovered LR images and their HR counterparts. The proposed model is empirically proved to be more difficult than SR with non-JPG inputs and surpasses traditional SR models.",
        "Abstract": "Super Resolution (SR) is a fundamental and important low-level computer vision (CV) task. Different from traditional SR models, this study concentrates on a specific but realistic SR issue: How can we obtain satisfied SR results from compressed JPG (C-JPG) image, which widely exists on the Internet. In general, C-JPG can release storage space while keeping considerable quality in visual. However, further image processing operations, e.g., SR, will suffer from enlarging inner artificial details and result in unacceptable outputs. To address this problem, we propose a novel SR structure with two specifically designed components, as well as a cycle loss. In short, there are mainly three contributions to this paper. First, our research can generate high-qualified SR images for prevalent C-JPG images. Second, we propose a functional sub-model to recover information for C-JPG images, instead of the perspective of noise elimination in traditional SR approaches. Third, we further integrate cycle loss into SR solver to build a hybrid loss function for better SR generation. Experiments show that our approach achieves outstanding performance among state-of-the-art methods.",
        "Introduction": "  INTRODUCTION With the marvelous achievement of deep learning (DL) in computer vision (CV), Super Resolution (SR) attracts much attention for its crucial value as the basis of many high-level CV tasks ( Chen et al., 2017 ;  He et al., 2016 ). Deep learning Super Resolution (DL-SR) algorithms ( Kim et al., 2016 ;  Lim et al., 2017 ;  Haris et al., 2018 ;  Zhang et al., 2018c ;b) strive for finding the complex nonlinear mapping between low resolution (LR) images and their high resolution (HR) counterparts. However, the learned model only reflects the inverse of down-scaled mapping, which is used to obtain LR images from their HR fathers. In other words, if there are some spots/stains in LR inputs, the SR model will treat them as inherent elements, and the corresponding SR outputs will enlarge these undesirable details. In reality, on the Internet, JPG compression is probably the most commonly used pattern for storage space reduction. That is to say, the LR image will be further processed into a compressed JPG (C-JPG) image. The quality of C-JPG will greatly drop, and the compression may yield unpleasant artifacts, for example, the presence of obvious partition lines, which vastly deteriorates the overall visual feeling. Hence, directly solving high-level CV tasks with these C-JPG images will lead to poor performance. In this paper, we propose a lossless SR model to obtain images with satisfying quality from the low-quality inputs (C-JPG). The deterioration in C-JPG makes the SR processing a huge challenge. In this paper, we focus on the more realistic C-JPG SR problem. Many SR methods regarding to the real-world condition images have been already developed, such as  Zhang et al. (2018a) ;  Yuan et al. (2018) . Among them, some models regard the noise as a kernel estimating problem which can be solved by addictive Gaussian noises. However, the distribution of most real images are inconsistent with the hypothetical Gaussian distribution. Taking C-JPG images as a example, the image compression operation is related to decreasing information from original image instead of adding specific noises. Other models learn the related information from irrelevant LR-HR images to obtain similar representations by unsupervised strategy. All of them cannot solve the problem well. In general, most LR images are produced through performing traditional interpolation method (mostly bicubic) on their HR fathers. The SR training process should recover this down-scaled map- ping in a reverse manner. Referring to our C-JPG SR issue, when searching images from Google, a Under review as a conference paper at ICLR 2020 Bicubic RCAN Matlab+RCAN Ours lot of unpleasant details are displayed, especially in the edges of objects. However, the low quality of image makes former SR methods fail to generate applicable images. As shown in  Fig. 1 , it is shown that the SR generations of traditional bicubic interpolation, leading SR algorithm RCAN, and RCAN with pre-denoising input all demonstrate poor quality with the low quality C-JPG inputs. Damaged grids are apparently enlarged by the approaches designed for traditional non-JPG datasets. More specialized analysis can be found in the research of  Köhler et al. (2017) . Note that the image pairs with fixed down-scaled kernel have been successfully learnt by SR models, such as SRGAN ( Ledig et al., 2017 ), EDSR ( Lim et al., 2017 ), and RDN ( Zhang et al., 2018c ). In this study, we deliberately build a more complicated dataset by adding JPG format LR images to the training data. To be specific, we have three kinds of training inputs: C-JPG LR, LR, and HR images. The whole training process includes two separate functional components: missing detail recuperative part (JPG recovering stage) and SR mapping learning part (SR generating stage). In order to remove ring, checkerboard effects, as well as other noise, the former half sub-model is trained with pre-processed C-JPG LR images as inputs, and original LR ones as the supervised information. The function of this stage is to recover LR image from its compression counterpart. Hence, the outputs (LR(C − JP G)) of the first part are greatly improved and free of partition lines phenomenon. Based on these improved LR images, the latter sub-model continues to learn the mapping between (LR(C −JP G)) and HR. Therefore, an integrated pipeline for SR representation between C-JPG and HR images is achieved through the jointly two sub-models. In short, there are mainly three contributions in this study: • Our research can be regarded as an universal SR method that generates SR images from C- JPG inputs, which is empirically proved to be more difficult than SR with non-JPG inputs. • We regard this specific SR task as a recovering information process for the inputs, compared with the former denoising assumption including down-sampling and degradation parts. In this viewpoint, a recovering model is firstly introduced to generate satisfied intermediates from C-JPG inputs. • We further propose an integrated SR model training pipeline with two-level data, i.e., C- JPG LR and LR images, as well as a new integrated loss function. The experimental results demonstrate our method can surpass traditional SR models.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel attack method, SemanticAdv, which generates unrestricted adversarial examples with semantically meaningful patterns by leveraging an attribute-conditional image editing model to interpolate between source and target images in the feature-map space. The proposed method is evaluated on two tasks, face verification and landmark detection, and extended to generate adversarial street-view images. Results show that the proposed attack exhibits high transferability and leads to a 65% query-free black-box attack success rate on a real-world face verification platform, outperforming pixel-wise perturbations in attacking existing defense methods.",
        "Abstract": "Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation\" by limiting the Lp norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate “unrestricted adversarial examples\". Such semantic based perturbation is more practical compared with the Lp bounded perturbation. In particular, we propose an algorithm SemanticAdv which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various “adversarial\" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against real-world black-box services such as Azure face verification service based on transferability. To further demonstrate the applicability of SemanticAdv beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have demonstrated great successes in advancing the state-of-the-art performance of discriminative tasks ( Krizhevsky et al., 2012 ;  Goodfellow et al., 2016 ;  He et al., 2016 ;  Collobert & Weston, 2008 ;  Deng et al., 2013 ;  Silver et al., 2016 ). However, recent research found that DNNs are vulnerable to adversarial examples which are carefully crafted instances aiming to induce arbitrary prediction errors for learning systems. Such adversarial examples containing small magnitude of perturbation have shed light on understanding and discovering potential vulnerabilities of DNNs ( Szegedy et al., 2013 ;  Goodfellow et al., 2014b ;  Moosavi-Dezfooli et al., 2016 ;  Papernot et al., 2016 ;  Carlini & Wagner, 2017 ;  Xiao et al., 2018b ; c;a; 2019 ). Most existing work focused on constructing adversarial examples by adding L p bounded pixel-wise perturbations ( Goodfellow et al., 2014b ) or spatially transforming the image ( Xiao et al., 2018c ;  Engstrom et al., 2017 ) (e.g., in-plane rotation or out-of-plane rotation). Generating unrestricted perturbations with semantically meaningful patterns is an important yet under-explored field. At the same time, deep generative models have demonstrated impressive performance in learning disentangled semantic factors through data generation in an unsupervised ( Radford et al., 2015 ;  Karras et al., 2018 ;  Brock et al., 2019 ) or weakly-supervised manner based on semantic attributes ( Yan et al., 2016 ;  Choi et al., 2018 ). Empirical findings in ( Yan et al., 2016 ;  Zhu et al., 2016a ;  Radford et al., 2015 ) demonstrated that a simple linear interpolation on the learned image manifold can produce smooth visual transitions between a pair of input images. In this paper, we introduce a novel attack SemanticAdv which generates unrestricted perturbations with semantically meaningful patterns. Motivated by the findings mentioned above, we leverage an attribute-conditional image editing model ( Choi et al., 2018 ) to synthesize adversarial examples by interpolating between source and target images in the feature-map space. Here, we focus on changing a single attribute dimension to achieve adversarial goals while keeping the generated adversarial image reasonably-looking (e.g., see  Figure 1 ). To validate the effectiveness of the proposed attack method, we consider two tasks, namely, face verification and landmark detection, as face recognition field has been extensively explored and the commercially used face models are relatively robust Under review as a conference paper at ICLR 2020 since they require a low false positive rate. We conduct both qualitative and quantitative evaluations on CelebA dataset ( Liu et al., 2015 ). To demonstrate the applicability of SemanticAdv beyond face domain, we further extend SemanticAdv to generate adversarial street-view images. We treat semantic layouts as input attributes and use the image editing model ( Hong et al., 2018 ) pre-trained on Cityscape dataset ( Cordts et al., 2016 ). Please find more visualization results on the anonymous website: https://sites.google.com/view/generate-semantic-adv-example. The contributions of the proposed SemanticAdv are three-folds. First, we propose a novel semantic- based attack method to generate unrestricted adversarial examples by feature-space interpolation. Second, the proposed method is able to generate semantically-controllable perturbations due to the attribute-conditioned modeling. This allows us to analyze the robustness of a recognition system against different types of semantic attacks. Third, as a side benefit, the proposed attack exhibits high transferability and leads to a 65% query-free black-box attack success rate on a real-world face verification platform, which outperforms the pixel-wise perturbations in attacking existing defense methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel model-based reinforcement learning algorithm, Soft Stochastic Value Gradient (S2VG), which combines the maximum entropy reinforcement learning framework with a dynamics model and reward model. S2VG leverages the learned dynamics and reward model to improve the sample-efficiency of the algorithm, while mitigating the model-bias issue. It also avoids the importance sampling in the off-policy setting and reduces the variance of the gradient estimation. Experiments on several benchmarks demonstrate the effectiveness of S2VG.",
        "Abstract": "Model-based reinforcement learning (MBRL) has shown its advantages in sample-efficiency over model-free reinforcement learning (MFRL). Despite the impressive results it achieves, it still faces a trade-off between the ease of data generation and model bias. In this paper, we propose a simple and elegant model-based reinforcement learning algorithm called soft stochastic value gradient method (S2VG). S2VG combines the merits of the maximum-entropy reinforcement learning and MBRL, and exploits both real and imaginary data. In particular, we embed the model in the policy training and learn $Q$ and $V$ functions from the real (or imaginary) data set. Such embedding enables us to compute an analytic policy gradient through the back-propagation rather than the likelihood-ratio estimation, which can reduce the variance of the gradient estimation. We name our algorithm Soft Stochastic Value Gradient method to indicate its connection with the well-known stochastic value gradient method in \\citep{heess2015Learning}.",
        "Introduction": "  INTRODUCTION Reinforcement learning can be generally classified into two categories: model-free reinforcement learning (MFRL) and model-based reinforcement learning (MBRL). The last several years have witnessed the great success of MFRL especially in playing video games, robotic control and motion animation ( Mnih et al., 2015 ; Lillicrap et al., 2015;  Schulman et al., 2017 ;  Peng et al., 2018 ). How- ever, even for some simple tasks, hundreds of millions of samples are required for an agent to learn a good control policy. In many industry scenarios, such as health care and financial services, the algorithm requiring tremendous interactions with the environment is not applicable or too expensive to deploy. To this end, several recent works have advocated the model-based approach, where the higher sample-efficiency is achieved by leveraging the learned dynamics and reward model ( Buck- man et al., 2018 ;  Feinberg et al., 2018 ). It generally augments the real data with the data from dynamics models, uses rollout to improve target for temporal difference learning, or directly incor- porates the model into the Bellman equation ( Luo et al., 2018 ; Heess et al., 2015). These works have demonstrated promising results on several benchmarks with a small number of interactions with the environment. Despite its recent success, MBRL still faces a challenging problem, i.e., the model-bias, where the imperfect dynamics model would degrade the performance of the algorithm ( Kurutach et al., 2018 ). Unfortunately, such things always happen when the environment is sufficiently complex. There are a few efforts to mitigate such issue by combining model-based and model-free approaches. Heess et al. (2015) compute the value gradient along real system trajectories instead of planned ones to avoid the compounded error.  Kalweit & Boedecker (2017)  mix the real data and imaginary data from the model and then train Q function. An ensemble of neural networks can be applied to model the environment dynamics, which effectively reduces the error of the model ( Kurutach et al., 2018 ;  Clavera et al., 2018 ;  Chua et al., 2018 ). We observe that most recent algorithms with promising results apply Dyna-style update ( Sutton, 1990 ;  Kurutach et al., 2018 ;  Luo et al., 2018 ). They collect real data using current policy to train the dynamics model. Then the policy is improved using state-of-the-art model-free reinforcement learning algorithms with imagined data generated by the learned model. Our insight is that why not directly embed the model into the policy improvement? To this end, we derive a model-based reinforcement learning algorithm in the framework of the maximum entropy reinforcement learning Under review as a conference paper at ICLR 2020 ( Ziebart et al., 2008 ). Dynamics model and reward model are trained with the real data set collected from the environment. Then we simply train Q and V function using the real data set with the update rule derived from the maximum entropy principle (several other advanced ways to include the imaginary data can also be applied, see details in section 3). In the policy improvement step, the stochastic actor samples an action with real state as the input, and then the state switches from s to s according to the learned dynamics model. We link the learned dynamics model, reward model, and policy to compute an analytic policy gradient by the back-propagation. Comparing with likelihood- ratio estimator usually used in MFRL method, such value gradient method would reduce the variance of the policy gradient (Heess et al., 2015). The other merit of S2VG is its computational efficiency. Several state-of-the-art MBRL algorithms generate hundreds of thousands imaginary data from the model and a few real samples. Then the huge imaginary data set feeds into MFRL algorithms, which may be sample-efficient in terms of real samples but not computational-friendly. On the contrary, our algorithm embeds the model in the policy update. Thus we can implement it efficiently by computing policy gradient several times in each iteration (see our algorithm 1) and do not need to do calculation on the huge imaginary data set. We name our algorithm soft stochastic value gradient to indicate its connection with SVG (Heess et al., 2015). Notice there are several differences between S2VG and SVG. Firstly, to alleviate the issue of the compounded error, SVG proposes a relatively conservative algorithm where just real data is used to evaluate policy gradients. Thus imaginary data is wasted, even the data from the short rollouts from the model can be trusted to some extent. In our work, the policy is trained with the model and imaginary dataset m times in each iteration of the algorithm. Secondly, we derive our algorithm in the framework of the maximum entropy reinforcement learning. The maximum entropy updates could improve the robustness under the model estimation error ( Ziebart et al., 2010 ). In addition, it encourages the exploration, prevents the early convergence to the sub-optimal policies, and shows state-of-the-art performance in MFRL ( Haarnoja et al., 2018 ). Thirdly, S2VG avoids the importance sampling in the off-policy setting by sampling the action from π and transition from f (s, a), which further reduces the variance of the gradient estimation.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the effectiveness of Stochastic Gradient Descent (SGD) and its variants, such as AdaGrad, RMSProp, AdaDelta, and Adam, in training deep neural networks. It is shown that AdaGrad can converge faster than vanilla SGD in sparse settings, but its performance is limited in non-sparse settings. Recent theories and empirical studies have suggested that Adam's unstable design of adaptive learning rate may impair the optimization process, leading to non-convergence and weak generalization ability.",
        "Abstract": "Adaptive optimization algorithms such as RMSProp and Adam have fast convergence and smooth learning process. Despite their successes, they are proven to have non-convergence issue even in convex optimization problems as well as weak performance compared with the first order gradient methods such as stochastic gradient descent (SGD). Several other algorithms, for example AMSGrad and AdaShift, have been proposed to alleviate these issues but only minor effect has been observed. This paper further analyzes the performance of such algorithms in a non-convex setting by extending their non-convergence issue into a simple non-convex case and show that Adam's design of update steps would possibly lead the algorithm to local minimums. To address the above problems, we propose a novel adaptive gradient descent algorithm, named AdaX, which accumulates the long-term past gradient information exponentially. We prove the convergence of AdaX in both convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with SGD.\n",
        "Introduction": "  INTRODUCTION In the era of deep learning, Stochastic Gradient Descent (SGD), though proposed in the last century, remains the most effective algorithm in training deep neural networks ( Robbins & Monro, 1951 ). Many methods have been created to accelerate the training process and boost the performance of SGD, such as momentum ( Polyak, 1964 ) and Nesterov's acceleration ( Nesterov, 1983 ). Recently, adaptive optimization methods have become popular as they adjust parameters' learning rates in different scales, instead of directly controlling the overall step size. These algorithms schedule the learning rates using a weighted average of the past gradients. For example, AdaGrad ( Duchi et al., 2011 ) chooses the square root of the global average of the past gradients as the denominator of the adaptive learning rates. It is shown that when the gradients are sparse or small, AdaGrad can converge faster than vanilla SGD. However, its performance is quite limited in non-sparse settings. Other adaptive algorithms have been proposed to replace the global average in AdaGrad by us- ing the exponential moving average of past gradients, such as RMSProp ( Tieleman & Hinton, 2012 ), AdaDelta ( Zeiler, 2012 ), and Adam ( Kingma & Ba, 2015 ). Among all these variants, Adam ( Kingma & Ba, 2015 ) is the most popular yet controversial optimization algorithm since it has faster convergence rate than the others. However, Adam has worse performance (i.e. generalization ability in testing stage) compared with SGD. Recent theories ( Wilson et al., 2017 ;  Reddi et al., 2018 ) have also shown that Adam suffers from non-convergence issue and weak generalization ability. For ex- ample,  Reddi et al. (2018)  proposed that Adam's non-convergence problem originate from a mistake in their proof of convergence. They constructed a counterexample for Adam and thoroughly proved that Adam did not guarantee convergence even in a simple convex setting. In the meantime,  Zhou et al. (2019)  theoretically proved that Adam could make a large update when gradients are small, and a small update when gradients are large, which probably lead the optimization process to wrong directions.  Shazeer & Stern (2018)  also empirically showed that Adam's parameter updates are not stable and its second moment could be out of date.  Luo et al. (2019)  examined the effective learning rate of Adam in training and found that Adam would produce too large or too small extreme learning rates. All the above analyses have suggested that Adam's unstable design of adaptive learning rate may impair the optimization process.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes an untargeted-attack algorithm called LabelFool, which perturbs an image to be mis-classified as a label similar to its ground truth, while also guaranteeing imperceptibility in the image space and maintaining a high attack rate in fooling classifiers. Experiments on ImageNet demonstrate that adversarial samples generated by LabelFool are much less recognizable in the label space by human observers than other attacks, while still guaranteeing imperceptibility in the image space and maintaining a high attack rate in fooling classifiers.",
        "Abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.",
        "Introduction": "  INTRODUCTION Deep neural networks are powerful learning models that achieve state-of-the-art pattern recogni- tion performance in classification tasks ( Krizhevsky et al., 2012b ;  LeCun et al., 2010 ;  He et al., 2016 ). Nevertheless, it is found that adding well-designed perturbations to original samples can make classifiers of deep neural networks fail ( Szegedy et al., 2013 ). These kinds of samples are called adversarial samples. Techniques for generating adversarial samples are called attackers. We think the ideal attacker should satisfy three levels of requirements. The first requirement is fooling networks which means making classifiers fail to classify an image successfully. For example, a dog image can be classified as a cat after added some well-designed perturbations. There are a number of methods for achieving a high attack rate ( Goodfellow et al., 2015 ;  Carlini & Wagner, 2017 ;  Dong et al., 2018 ). The second requirement for the ideal attacker is the imperceptibility in the image space. This means the magnitude of perturbations in the pixel level needs to be as tiny as possible so that it is imper- ceptible to human eyes. For example, additive perturbations are minimized with l p norm to generate Under review as a conference paper at ICLR 2020 imperceptible adversarial samples ( Seyed-Mohsen et al., 2016 ). Extreme cases also exist where only changing one or a few pixels ( Su et al., 2019 ;  Modas et al., 2019 ) can make classifiers fail.  Moosavi-Dezfooli et al. (2017)  even show the existence of universal (image-agnostic) perturbations. The third requirement for the ideal attacker, which is newly proposed in this paper, is the imper- ceptibility of the error made by the classifier in the label space. It means making the classifier to mis-classify an image as the label which is similar to its ground truth, so that people won't notice the misclassification. For example, in  Figure 1 , a human user will probably ignore the mis-classification if an attacker caused a \"church\" to be mis-classified as a \"monastery\" as the third attacker does. However, a human user will easily notice the mistake if an attacker caused a \"church\" to be mis- classified as a \"dome\" as the second attacker does or caused an apparent perturbation in the image space as the first attacker does. In real applications, a human user will take defensive measures as soon as he notices the attack. Therefore making the whole attack process imperceptible is crucial for letting observers' guard down. Tiny perturbations in the image space but large perturbations in the label space can muddle through on the input terminal. But as soon as observers check on the output terminal and see the obviously-incorrect label for an input, they will realize that the classifier fail due to some attacks and take defensive measures immediately, just as  Figure 1  shows. This justifies the power of attacks which also confuse people in the label space. So the imperceptibility in the label space is quite important. However, to our best knowledge, few attackers have realized this point. In this paper, we propose an untargeted-attack algorithm called LabelFool, to perturb an image to be mis-classified as the label which is similar to its ground truth, so that people won't notice the mis- classification. In the meantime, LabelFool also guarantees the imperceptibility in the image space as well as maintaining a high attack rate in fooling classifiers. There are two steps by which we accom- plish our goal. The first step is to choose a target label which is similar to the input image's ground truth. The second step is to perturb the input to be classified as this target label. The way is finding the classification boundary between the current label and the target label, and then moving the input towards this boundary until it is classified as the target label. We conduct a subjective experiment on ImageNet ( Deng et al., 2009 ) which shows that adversarial samples generated by our method are indeed much less recognizable in the label space by human observers than other attacks. We also perform objective experiments on ImageNet to demonstrate that adversarial samples generated by LabelFool still guarantee the imperceptibility in the image space as well as maintaining a high attack rate in fooling classifiers.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Neural Architecture Search (NAS), an approach to automate architecture engineering by solving the architecture design problem. Recent approaches have adopted a weight sharing strategy to reduce the computation cost, but have two issues: deeply coupled weights in the supernet and joint optimization introducing bias during optimization. The one-shot paradigm alleviates the second issue, but still has coupled weights in the supernet. This paper proposes a new approach to address these issues and improve the efficiency and flexibility of the architecture search.",
        "Abstract": "We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method (Benderet al., 2018), however, is hard to train and not yet effective on large scale datasets like ImageNet.  This work propose a Single Path One-Shot model to address the challenge in the training.  Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally.\nComprehensive experiments verify that our approach is flexible and effective.  It is easy to train and fast to search.  It effortlessly supports complex search spaces(e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency).  It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.",
        "Introduction": "  INTRODUCTION Deep learning automates feature engineering and solves the weight optimization problem. Neural Architecture Search (NAS) aims to automate architecture engineering by solving one more problem, architecture design. Early NAS approaches ( Zoph et al., 2018 ;  Zhong et al., 2018a ;b;  Liu et al., 2018a ;  Real et al., 2018 ;  Tan et al., 2018 ) solves the two problems in a nested manner. A large number of architectures are sampled and trained from scratch. The computation cost is unaffordable on large datasets. Recent approaches (Wu et al., 2018a;  Cai et al., 2018 ;  Liu et al., 2018b ;  Xie et al., 2018 ;  Pham et al., 2018 ;  Zhang et al., 2018c ;  Brock et al., 2017 ;  Bender et al., 2018 ) adopt a weight sharing strategy to reduce the computation. A supernet subsuming all architectures is trained only once. Each architecture inherits its weights from the supernet. Only fine-tuning is performed. The computation cost is greatly reduced. Most weight sharing approaches use a continuous relaxation to parameterize the search space (Wu et al., 2018a;  Cai et al., 2018 ;  Liu et al., 2018b ;  Xie et al., 2018 ;  Zhang et al., 2018c ). The archi- tecture distribution parameters are jointly optimized during the supernet training via gradient based methods. The best architecture is sampled from the distribution after optimization. There are two issues in this formulation. First, the weights in the supernet are deeply coupled. It is unclear why inherited weights for a specific architecture are still effective. Second, joint optimization introduces further coupling between the architecture parameters and supernet weights. The greedy nature of the gradient based methods inevitably introduces bias during optimization and could easily mislead the architecture search. They adopted complex optimization techniques to alleviate the problem. The one-shot paradigm ( Brock et al., 2017 ;  Bender et al., 2018 ) alleviates the second issue. It defines the supernet and performs weight inheritance in a similar way. However, there is no architecture relaxation. The architecture search problem is decoupled from the supernet training and addressed in a separate step. Thus, it is sequential. It combines the merits of both nested and joint optimization approaches above. The architecture search is both efficient and flexible. The first issue is still problematic. Existing one-shot approaches ( Brock et al., 2017 ;  Bender et al., 2018 ) still have coupled weights in the supernet. Their optimization is complicated and involves sensitive hyper parameters. They have not shown competitive results on large datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel Localized Meta-Learning (LML) framework for learning-to-learn, which enables deep neural networks to quickly adapt to unseen tasks. The framework formulates meta-knowledge as a conditional hyperposterior given task data distribution, which allows a meta learner to adaptively generate an appropriate prior for a new task. An LCC-based prior predictor is proposed to parameterize the dependence of the conditional hyperposterior on the task data distribution. The proposed LML framework is evaluated on various tasks and demonstrates improved performance over existing meta-learning methods.",
        "Abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. However, such meta-knowledge is often represented as a fixed distribution, which is too restrictive to capture various specific task information. In this work, we present a localized meta-learning framework based on PAC-Bayes theory. In particular, we propose a LCC-based prior predictor that allows the meta learner adaptively generate local meta-knowledge for specific task. We further develop a pratical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method. ",
        "Introduction": "  INTRODUCTION Recent years have seen a resurgence of interest in the field of meta-learning, or learning-to-learn ( Thrun & Pratt, 2012 ), especially for empowering deep neural networks the capability of fast adapt- ing to unseen tasks just as humans ( Finn et al., 2017 ;  Ravi & Larochelle, 2017 ). More concretely, the neural networks are trained from a sequence of datasets, associated with different learning tasks sampled from a meta-distribution (also called task environment (Baxter, 2000;  Maurer, 2005 )). The principal aim of meta learner is to extract transferable meta-knowledge from observed tasks and facilitate the learning of new tasks sampled from the same meta-distribution. The performance is measured by the generalization ability from a finite set of observed tasks, which is evaluated by learning related unseen tasks. For this reason, there has been considerable interest in theoretical bounds on the generalization in terms of the meta-learning algorithm ( Denevi et al., 2018b ; a ). One typical line of work ( Pentina & Lampert, 2014 ;  Amit & Meir, 2018 ) use PAC-Bayes bound to analyze the generalization behavior of the meta learner and quantify the relation between the ex- pected loss on new tasks and the average loss on the observed tasks. In this setup, we formulate meta-learning as hierarchical Bayes. Accordingly, meta-knowledge is instantiated as a global dis- tribution over all possible priors, which we call hyperprior and is chosen before observing training tasks. Each prior is a distribution over a family of classifiers w.r.t. a particular task. To learn versa- tile meta-knowledge across tasks, the meta learner observes a sequence of training tasks and adjusts its hyperprior into a hyperposterior distribution over the set of priors. To solve a new task, the base learner produces a posterior distribution over a family of classifiers based on the associated sample set and the prior generated by the hyperposterior. However, such meta-knowledge is shared across tasks. The global hyperposterior is rather generic, typically not well-tailored to various specific tasks. Consequently, it leads to sub-optimal perfor- mance for any individual prediction task. As a motivational example, suppose we have two dif- ferent tasks: distinguishing motorcycle versus bicycle and distinguishing motorcycle versus car. Intuitively, each task uses distinct discriminative patterns and thus the desired meta-knowledge is required to extract these patterns simultaneously. This could be a challenging problem to represent it with a global hyperposterior since the most significant patterns in the first task could be irrelevant or even detrimental to the second task. Hence, we are motivated to pursue a meta-learning framework to effectively define the hyperpos- terior. The inspiration comes from the PAC-Bayes literature on data distribution dependent priors ( Catoni, 2007 ;  Parrado-Hernández et al., 2012 ;  Dziugaite & Roy, 2018 ). The choice of posterior in each task is constrained by the need to minimize the relative entropy between prior and pos- terior since this divergence forms part of the bound and is typically large in standard PAC-Bayes Under review as a conference paper at ICLR 2020 approaches ( Lever et al., 2013 ). Thus, choosing an appropriate prior for each task which is close to the related posterior could yield improved generalization bounds. Inspired by this, we propose a Localized Meta-Learning (LML) framework. Instead of formulating meta-knowledge as a global hyperposterior, we learn a conditional hyperposterior given task data distribution that allows a meta learner to adaptively generate an appropriate prior for a new task. However, the task data distribution is unknown, and our only perception for it is via the associated sample set. Nevertheless, if the conditional hyperposterior is relatively stable to perturbations of the sample set, then the generated prior could still reflect the underlying task data distribution, resulting in a generalization bound that still holds with smaller probability. Following this intuition, the dependence of a conditional hyperposterior on the task data distribution is parameterized by a prior predictor using Local Coordinate Coding (LCC)( Yu et al., 2009 ). In particular, if the classifier in each task is specialized to a parametric model, including deep neural network, the proposed LCC- based prior predictor predicts the model parameters using the sample set by exploiting the local information on the latent manifold. LCC-based prior predictor is invariant under permutations of its inputs and could be further used for unseen tasks. The main contributions of this work include: (i) We present a localized meta-learning framework which provides a means to tighten the original PAC-Bayes meta-learning bound ( Pentina & Lam- pert, 2014 ;  Amit & Meir, 2018 ) by minimizing the task-complexity term by choosing data-dependent prior; (ii) We propose an LCC-based prior predictor, an implementation of conditional hyperposte- rior, to generate local meta-knowledge for specific task; (iii) We derive a practical localized meta- learning algorithm for deep neural networks by minimizing the bound; (iv) Experimental results demonstrate improved performance over meta-learning method in this field.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new probabilistic framework for generation tasks, such as learning the distribution of given data and sampling from the learned distributions, for two high-dimensional random vectors. The framework is based on the channel synthesis problem in network information theory and Wyner's common information, which is the optimal value of the optimization problem that minimizes the mutual information between the two random vectors and a common representation. The proposed model, called Wyner VAE, is a variant of variational autoencoders and allows for the decomposition of a data vector into the common representation and the local representation, which can be used for sampling with style manipulation. Experiments show the utility of the model in various sampling tasks and its superiority over existing models.",
        "Abstract": "A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems---distributed simulation and channel synthesis---in which Wyner's common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and  local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method.",
        "Introduction": "  INTRODUCTION This paper aims to develop a new probabilistic framework for generation tasks (i.e., learning the distribution of given data and sampling from the learned distributions) for two high-dimensional random vectors. To motivate the main idea, consider the following cooperative game between Alice and Bob. Suppose that given an image of a child's photo, Alice sends its description Z to Bob who draws a portrait of how the child will grow up based on it. The objective of this game is to draw a nice portrait, and thus Alice needs to help Bob in the process by providing a good description of the child's photo - any redundant information in the description may confuse Bob in his guessing process. What description does Alice need to generate and send from the child's photo? P. Cuff (2013) formulated this game of conditional generation as the channel synthesis problem in network information theory depicted in  Fig. 1 . Given a joint distribution q(x, y) = q(x)q(y|x), Alice and Bob want to generate Y according to q(y|x) based on a sample from q(x). In this problem, Alice wishes to find the most succinct description Z of X (a child's photo) such that Y (her adulthood portrait) can be simulated by Bob according to the desired distribution using this description and local randomness V (new features to draw a portrait of adults that are not contained in photos of children). The minimum description rate for such conditional generation is characterized by Wyner's common information (Wyner, 1975; El Gamal and Kim, 2011) denoted by J(X; Y) and defined as the optimal value of the optimization problem minimize I q (X, Y; Z) subject to X → Z → Y variables q(z|x, y), (1) where X → Z → Y denotes a Markov chain from X to Z to Y and I q (X, Y; Z) denotes the mutual information between (X, Y) and Z, where (X, Y, Z) ∼ q(x, y)q(z|x, y). The same quantity J(X; Y) arises as the fundamental limit of the distributed simulation of correlated sources studied originally by A. Wyner (1975) in which two distributed agents wish to simulate a target distribution q(x, y) (i.e., joint generation of (X, Y)) based on the least possible amount of Under review as a conference paper at ICLR 2020 shared common randomness. (See  Fig. 1 (c,d) .) In this sense, the joint distribution q(x, y) and the conditional distributions q(y|x), q(x|y) have the same common information structure characterized by the optimization problem (1), which involves learning the joint distribution in its nature. We call the joint encoder q(z|x, y) (or equivalently, the corresponding random variable Z) as the common representation of (X, Y), and the mutual information I q (X, Y; Z) then can be viewed as a measure of the complexity of Z. The goal of this paper is to propose a new probabilistic model based on these information theoretic observations, to achieve a good performance in joint and conditional generation tasks. We apply the idea of learning succinct common representation to design a new generative model for a pair of correlated variables: seeking a succinct representation Z in learning the underlying distribution based on its sample may also help reduce the burden on the decoder's side and thereby achieve a better generative performance. The rest of the paper gradually develops our framework as follows. We first define a probabilistic model based on the motivating problems, which we aim to train and use for generation tasks (Section 2.1), and then establish a general principle for learning the model based on the optimization problem (1) (Section 2.2). We propose one instantiation of the principle with a standard variational technique by introducing additional encoder distributions (Section 2.3). The proposed model with its training method can be viewed as a variant of variational autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014), and is thus called Wyner VAE. (See Appendix A for a brief introduction on VAEs.) The new encoder components introduced in Wyner VAE allow us to decompose a data vector into the common representation and the local representation, which can be used for sampling with style manipulation (Section 2.4). We carefully compare our model and show its advantages over the existing VAE variants (Vedantam et al., 2018; Suzuki et al., 2016; Sohn et al., 2015; Wang et al., 2016) and the information bottleneck (IB) principle (Tishby et al., 1999) (Section 3), which is a well-known information theoretic principle in representation learning. In the experiments, we empirically show the utility of our model in various sampling tasks and its superiority over existing models and that learning a succinct common representation achieves better generative performance in generation tasks (Section 4).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to improve the performance of Mask R-CNN for instance segmentation. The proposed method introduces a feature pyramid network (FPN) to address the scale variation problem, and a unified branch to explore the interrelations between object detection and instance segmentation. The experimental results demonstrate that the proposed method achieves state-of-the-art performance on the challenging COCO dataset.",
        "Abstract": "As a concise and classic framework for object detection and instance segmentation, Mask R-CNN achieves promising performance in both two tasks. However, considering stronger feature representation for Mask R-CNN fashion framework, there is room for improvement from two aspects. On the one hand, performing multi-task prediction needs more credible feature extraction and multi-scale features integration to handle objects with varied scales. In this paper, we address this problem by using a novel neck module called SA-FPN (Scale Aware Feature Pyramid Networks). With the enhanced feature representations, our model can accurately detect and segment the objects of multiple scales. On the other hand, in Mask R-CNN framework, isolation between parallel detection branch and instance segmentation branch exists, causing the gap between training and testing processes. To narrow this gap, we propose a unified head module named EJ-Head (Effective Joint Head) to combine two branches into one head, not only realizing the interaction between two tasks, but also enhancing the effectiveness of multi-task learning. Comprehensive experiments show that our proposed methods bring noticeable gains for object detection and instance segmentation. In particular, our model outperforms the original Mask R-CNN by 1~2 percent AP in both object detection and instance segmentation task on MS-COCO benchmark. Code will be available soon.",
        "Introduction": "  INTRODUCTION In the past few years, object detection and instance segmentation results were rapidly improved by the powerful baseline system Mask R-CNN( He et al. (2017) ), which extends Faster R-CNN( Girshick (2015) ) by adding a branch for predicting an object mask in parallel with the existing part for bound- ing box recognition. This method is conceptually natural and offers extensibility and robustness, shows a surprisingly smooth, flexible, and fast system for instance segmentation results. However, this remarkable multi-task learning method suffers from a common problem of mod- ern detection methods. That is scale variation, since Convolutional Neural Network is sensitive to scales. And what's more, performing multi-tasks needs more credible feature extraction execution and multi-scale complementary features integration. Therefore, it is urgent to tackle this problem. Feature pyramid is a common practice. FPN( Lin et al. (2017a) ) augmented a top-down path with lateral connections for object detection. It exploits the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. Rethinking the extracted multi-scale features of general FPN, the top-down pathway FPN only introduces high-level semantic information to low-level feature, while ignore the role of low-level feature for localization. FPN still has room for improvement. Another aspect for improvement of Mask R-CNN in multi-task learning is about the parallel isolated branches of Mask R-CNN. The segmentation branch of Mask R-CNN is based on the output of Region Proposal Network (RPN) in training stage, which ignores the inherently tie in those two tasks and is inconsistent with testing processes. In common sense, instance segmentation is connected detection based on the bounding box strictly, which is more meticulous than the bounding box. However, the bounding box is easy to obtain than the masking label. It is worthy of trying to explore and enhance the interrelations between object detection and instance segmentation.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Graph Filter Discriminant (GFD) Score metric to measure the power of a graph convolutional filter in discriminating node representations of different classes on a specific graph. We analyze existing GNNs' filters with this assessment method to answer three fundamental questions of GNNs: (1) Is there a best filter that works for all graphs? (2) If not, what are the properties of graph structure that will influence the performance of graph convolutional filters? (3) Can we design an algorithm to adaptively find the appropriate filter for a given graph? We propose the Adaptive Filter Graph Neural Network (AF-GNN) which can adaptively learn a proper model for the given graph. We show that the proposed Adaptive Filter can better capture graph topology and separate features on both real-world datasets and synthetic datasets.",
        "Abstract": "Graph Neural Networks (GNNs) have received tremendous attention recently due to their power in handling graph data for different downstream tasks across different application domains. The key of GNN is its graph convolutional filters, and recently various kinds of filters are designed. However, there still lacks in-depth analysis on (1) Whether there exists a best filter that can perform best on all graph data; (2) Which graph properties will influence the optimal choice of graph filter; (3) How to design appropriate filter adaptive to the graph data. In this paper, we focus on addressing the above three questions. We first propose a novel assessment tool to evaluate the effectiveness of graph convolutional filters for a given graph. Using the assessment tool, we find out that there is no single filter as a `silver bullet' that perform the best on all possible graphs. In addition, different graph structure properties will influence the optimal graph convolutional filter's design choice. Based on these findings, we develop Adaptive Filter Graph Neural Network (AFGNN), a simple but powerful model that can adaptively learn task-specific filter. For a given graph, it leverages graph filter assessment as regularization and learns to combine from a set of base filters. Experiments on both synthetic and real-world benchmark datasets demonstrate that our proposed model can indeed learn an appropriate filter and perform well on graph tasks.",
        "Introduction": "  INTRODUCTION Graph Neural Networks (GNNs) are a family of powerful tools for representation learning on graph data, which has been drawing more and more attention over the past several years. GNNs can obtain informative node representations for a graph of arbitrary size and attributes, and has shown great ef- fectiveness in graph-related down-stream applications, such as node classification ( Kipf & Welling, 2017 ), graph classification ( Wu et al., 2019b ), graph matching ( Bai et al., 2019 ), recommendation systems ( Ying et al., 2018 ), and knowledge graphs ( Schlichtkrull et al., 2018 ). As GNNs have superior performance in graph-related tasks, the question as to what makes GNNs so powerful is naturally raised. Note that GNNs adopt the concept of the convolution operation into graph domain. To obtain a representation of a specific node in a GNN, the node aggregates representations of its neighbors with a convolutional filter. For a task related to graph topology, the convolutional filter can help GNN nodes to get better task-specific representations ( Xu et al., 2019 ). Therefore, it is the filter that makes GNNs powerful, and thus the key to designing robust and accurate GNNs is to design proper graph convolutional filters. Recently, many GNN architectures are proposed ( Zhou et al., 2018 ) with their own graph filter designs. However, none of them have properly answered the following fundamental questions of GNNs: (1) Is there a best filter that works for all graphs? (2) If not, what are the properties of graph structure that will influence the performance of graph convolutional filters? (3) Can we design an algorithm to adaptively find the appropriate filter for a given graph? In this paper, we focus on addressing the above three questions for semi-supervised node classifi- cation task. Inspired by studies in Linear Discriminant Analysis (LDA), we propose a Graph Filter Under review as a conference paper at ICLR 2020 Discriminant (GFD) Score metric to measure the power of a graph convolutional filter in discrimi- nating node representations of different classes on a specific graph. We have analyzed all the existing GNNs' filters with this assessment method to answer the three aforementioned questions. We found that no single filter design can achieve optimal results on all possible graphs. In other words, for different graph data, we should adopt different graph convolutional filters to achieve optimal perfor- mance. We then experimentally and theoretically analyze how graph structure properties influence the optimal choice of graph convolutional filters. Based on all of our findings, we propose the Adaptive Filter Graph Neural Network (AF-GNN), which can adaptively learn a proper model for the given graph. We use the Graph Filter Discriminant Score (GFD) as a an extra loss term to guide the network to learn a good data-specific filter, which is a linear combination of a set of base filters. We show that the proposed Adaptive Filter can better capture graph topology and separate features on both real-world datasets and synthetic datasets. We highlight our main contributions as follows: • We propose an assessment tool: Graph Filter Discriminant Score, to analyze the effective- ness of graph convolutional filters. Using this tool, we find that no best filter can work for all graphs, the optimal choice of a graph convolutional filter depends on the graph data. • We propose Adaptive Filter Graph Neural Network that can adaptively learn a proper filter for a specific graph using the GFD Score as guidance. • We show that the proposed model can find better filters and achieve better performance compared to existing GNNs, on both real-word and newly created benchmark datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel domain adaptation method, DMPN, which models the feature distributions of the source and target data as Gaussian mixture distributions to measure the discrepancy between the two domains. The proposed method minimizes the distances between the corresponding Gaussian component means between the source and target data, as well as the negative log likelihood of generating the target features from the source feature distribution. Experiments on Digits Image transfer tasks and synthetic-to-real image transfer task demonstrate that DMPN can provide superior results than state-of-the-art approaches.",
        "Abstract": "State-of-the-art Unsupervised Domain Adaptation (UDA) methods learn transferable features by minimizing the feature distribution discrepancy between the source and target domains. Different from these methods which do not model the feature distributions explicitly, in this paper, we explore explicit feature distribution modeling for UDA. In particular, we propose Distribution Matching Prototypical Network (DMPN) to model the deep features from each domain as Gaussian mixture distributions. With explicit feature distribution modeling, we can easily measure the discrepancy between the two domains. In DMPN, we propose two new domain discrepancy losses with probabilistic interpretations. The first one minimizes the distances between the corresponding Gaussian component means of the source and target data. The second one minimizes the pseudo negative log likelihood of generating the target features from source feature distribution. To learn both discriminative and domain invariant features, DMPN is trained by minimizing the classification loss on the labeled source data and the domain discrepancy losses together. Extensive experiments are conducted over two UDA tasks. Our approach yields a large margin in the Digits Image transfer task over state-of-the-art approaches. More remarkably, DMPN obtains a mean accuracy of 81.4% on VisDA 2017 dataset. The hyper-parameter sensitivity analysis shows that our approach is robust w.r.t hyper-parameter changes.",
        "Introduction": "  INTRODUCTION Recent advances in deep learning have significantly improved state-of-the-art performance for a wide range of applications. However, the improvement comes with the requirement of a massive amount of labeled data for each task domain to supervise the deep model. Since manual labeling is expensive and time-consuming, it is therefore desirable to leverage or reuse rich labeled data from a related domain. This process is called domain adaptation, which transfers knowledge from a label rich source domain to a label scarce target domain ( Pan & Yang, 2009 ). Domain adaptation is an important research problem with diverse applications in machine learning, computer vision ( Gong et al., 2012 ;  Gopalan et al., 2011 ;  Hoffman et al., 2014 ;  Saenko et al., 2010 ) and natural language processing ( Collobert et al., 2011 ; Glorot et al., 2011). Traditional methods try to solve this problem via learning domain invariant features by minimizing certain distance met- ric measuring the domain discrepancy, for example Maximum Mean Discrepancy (MMD) ( Gretton et al., 2009 ;  Pan et al., 2008 ; 2010) and correlation distance ( Sun & Saenko, 2016 ). Then labeled source data is used to learn a model for the target domain. Recent studies have shown that deep neu- ral networks can learn more transferable features for domain adaptation (Glorot et al., 2011;  Yosinski et al., 2014 ). Consequently, adaptation layers have been embedded in the pipeline of deep feature learning to learn concurrently from the source domain supervision and some specially designed do- main discrepancy losses ( Tzeng et al., 2014 ;  Long et al., 2015 ;  Sun & Saenko, 2016 ;  Zellinger et al., 2017 ). However, none of these methods explicitly model the feature distributions of the source and target data to measure the discrepancy. Inspired from the recent works by  Wan et al. (2018)  and  Yang et al. (2018) , which have shown that modeling feature distribution of a training set improves classification performance, we explore explicit distribution modeling for UDA. We model the feature distributions Under review as a conference paper at ICLR 2020 as Gaussin mixture distributions, which facilitates us to measure the discrepancy between the source and target domains. Our proposed method, i.e., DMPN, works as follows. We train a deep network over the source domain data to generate features following a Gaussian mixture distribution. The network is then used to assign pseudo labels to the unlabeled target data. To learn both discriminative and domain invariant features, we fine-tune the network to minimize the cross-entropy loss on the labeled source data and domain discrepancy losses. Specifically, we propose two new domain discrepancy losses by exploiting the explicit Gaussian mixture distributions of the deep features. The first one minimizes the distances between the corresponding Gaussian component means between the source and target data. We call it Gaussian Component Mean Matching (GCMM). The second one minimizes the negative log likelihood of generating the target features from the source feature distribution. We call it Pseudo Distribution Matching (PDM). Extensive experiments on Digits Image transfer tasks and synthetic-to-real image transfer task demonstrate our approach can provide superior results than state-of-the-art approaches. We present our proposed method in Section 3, extensive experiment results and analysis in Section 4 and conclusion in Section 5.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel generative adversarial network (GAN) model, Manifold Learning and Aligning GAN (MLA-GAN), which is capable of learning the multi-manifold structure of real-world datasets and the correlations between the manifolds. MLA-GAN is similar to existing GAN models with multiple generators, but the generators are restricted such that their inverse maps can be represented by a single deep encoder. This restriction guides the inverse maps to learn semantically similar mappings, and the model achieves a semantically plausible manifold alignment. The proposed model is evaluated on real-world datasets and shows state-of-the-art FID scores in data generation and plausible alignments with disentangled features. Additionally, due to the abstracting nature of an encoder, the smooth features can be inferred even for data lying on an untrained manifold.",
        "Abstract": "We present a generative adversarial network (GAN) that conducts manifold learning and alignment (MLA): A task to learn the multi-manifold structure underlying data and to align those manifolds without any correspondence information. Our main idea is to exploit the powerful abstraction ability of encoder architecture. Specifically, we define multiple generators to model multiple manifolds, but in a particular way that their inverse maps can be commonly represented by a single smooth encoder. Then, the abstraction ability of the encoder enforces semantic similarities between the generators and gives a plausibly aligned embedding in the latent space. In experiments with MNIST, 3D-Chair, and UT-Zap50k datasets, we demonstrate the superiority of our model in learning the manifolds by FID scores and in aligning the manifolds by disentanglement scores. Furthermore, by virtue of the abstractive modeling, we show that our model can generate data from an untrained manifold, which is unique to our model.",
        "Introduction": "  INTRODUCTION Real-world datasets generally have multi-manifold structure. Smoothly varying features such as the size or the pose of an animal introduce smooth manifold structure, while the existence of dis- crete features such as species makes the structure not a single manifold but multiple disconnected manifolds (e.g., one manifold per species) ( Khayatkhoei et al., 2018 ). Importantly, these manifolds are correlated to each other in their structure since the transformation rules according to the size (stretching the foreground patch) or the pose (rotating the patches of body parts) are similar even for different species. Hence, our desire is to build a model that learns not only the multi-manifold structure but also the correlations between the manifolds. As for the learning of manifold structure, generative adversarial networks (GANs) ( Goodfellow et al., 2014 ) have been known for their remarkable performance. In particular, recently proposed GAN models have successfully demonstrated multi-manifold learning by using multiple generator networks ( Khayatkhoei et al., 2018 ;  Ghosh et al., 2017 ;  Hoang et al., 2018 ) or giving a mixture density on the latent space ( Xiao et al., 2018 ;  Gurumurthy et al., 2017 ). However, none of these models have taken the correlations between the manifolds into account. On the other hand, a field of research named manifold alignment ( Ham et al., 2003 ) has its primary concern in learning the correlations between the manifolds. In particular, unsupervised manifold alignment ( Cui et al., 2014 ;  Wang & Mahadevan, 2009 ) aims to learn plausible correspondences between the data points of different manifolds, without any ground-truth correspondence informa- tion is given. However, most existing methods focus only on finding the correspondences, not on learning the structure of the manifolds itself. In this work, we propose a model that performs both of the tasks, which we call as manifold learn- ing and aligning GAN (MLA-GAN). MLA-GAN is similar to the GANs with multiple generators ( Khayatkhoei et al., 2018 ;  Ghosh et al., 2017 ;  Hoang et al., 2018 ), but the generators are restricted such that their inverse maps can be represented by a single deep encoder. Due to the abstracting property of deep encoders, this restriction guides the inverse maps to learn semantically similar mappings, and the model achieves a semantically plausible manifold alignment. The main contribu- tions of this work can be summarized as follows. • The MLA problem: Up to our knowledge, MLA-GAN is the first model that performs both multi-manifold learning and alignment with real-world datasets. It shows state-of- the-art FID scores in data generation and plausible alignments with disentangled features. • Generalization to an untrained manifold: Due to the abstracting nature of an encoder, the smooth features can be inferred even for data lying on an untrained manifold. Exploiting this, one can generate other data points lying on this manifold. • Easily applicable: Due to the new regularizer we propose, the inverse maps used for re- stricting the generators do not need to be actually computed. This makes MLA-GAN easily applicable to any existing GAN architectures.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a model-agnostic meta-learning (MAML) algorithm and proposes a guideline for selecting the learning rates. We investigate the MAML algorithm and derive a sufficient condition of the inner learning rate α and meta-learning rate β for a simplified MAML to locally converge to local minima from any point in the neighborhood of the local minima. We find that the upper bound β c of meta-learning rate depends on inner learning rate α, and that β c of α ≈ α c is larger than that of α = 0. This is verified by experiments, which implies a guideline for selecting the learning rates: first, search for the largest possible α; next, tune β.",
        "Abstract": "Model-agnostic meta-learning (MAML) is known as a powerful meta-learning method. However, MAML is notorious for being hard to train because of the existence of two learning rates. Therefore, in this paper, we derive the conditions that inner learning rate $\\alpha$ and meta-learning rate $\\beta$ must satisfy for MAML to converge to minima with some simplifications. We find that the upper bound of $\\beta$ depends on $ \\alpha$, in contrast to the case of using the normal gradient descent method. Moreover, we show that the threshold of $\\beta$ increases as $\\alpha$ approaches its own upper bound. This result is verified by experiments on various few-shot tasks and architectures; specifically, we perform sinusoid regression and classification of Omniglot and MiniImagenet datasets with a multilayer perceptron and a convolutional neural network. Based on this outcome, we present a guideline for determining the learning rates: first, search for the largest possible $\\alpha$; next, tune $\\beta$ based on the chosen value of $\\alpha$.",
        "Introduction": "  INTRODUCTION A pillar of human intelligence is the ability to learn and adapt to unseen tasks quickly and based on only a limited quantity of data. Although machine learning has achieved remarkable results, many recent models require massive quantities of data and are designed for solving particular tasks. Meta-learning, one of the ways of tackling this problem, tries to develop a model that can adapt to new tasks quickly by learning to learn new concepts from few data points ( Schmidhuber, 1987 ;  Thrun & Pratt, 1998 ). Among meta-learning algorithms, model-agnostic meta-learning (MAML), a gradient-based meta- learning method proposed by  Finn et al. (2017) , has recently been extensively studied. For example, MAML is used for continual learning ( Finn et al., 2019 ;  Jerfel et al., 2019 ;  Spigler, 2019 ;  Al-Shedivat et al., 2018 ), reinforcement learning ( Finn et al., 2017 ;  Al-Shedivat et al., 2018 ;  Gupta et al., 2018 ;  Deleu & Bengio, 2018 ;  Liu & Theodorou, 2019 ) and probablistic inference ( Finn et al., 2018 ;  Yoon et al., 2018 ;  Grant et al., 2018 ). The reason why MAML is widely used is because MAML is simple but efficient and applicable to a wide range of tasks independent of model architecture and the loss function. However, MAML is notorious for being hard to train ( Antoniou et al., 2019 ). One of the reasons why training MAML is hard is the existence of two learning rates in MAML: the inner learning rate α and meta-learning rate β. A learning rate is known to be one of the most important parameters, and tuning this parameter may be challenging even if the simple gradient descent (GD) method is used. Nevertheless, we do not yet know the relationship between these two learning rates and have little guidance on how to tune them. Hence, guidelines for choosing these parameters are urgently needed. In this paper, we investigate the MAML algorithm and propose a guideline for selecting the learning rates. First, in Section 2 we briefly explain by using an approximation how MAML can be regarded as optimization with the negative gradient penalty. Because the gradient norm is related to the shape of the loss surface, a bias towards a larger gradient norm can make training unstable. Next, based on the approximation explained in Section 2, in Section 3, we derive a sufficinent condition of α and β for a simplified MAML to locally converge to local minima from any point in the neighborhood of the local minima. Furthermore, by removing a constraint, we derive a sufficient condition for local convergence with fewer simplifications as well. We find that the upper bound β c of meta-learning rate depends on inner learning rate α. In particular, β c of α ≈ α c is larger than that of α = 0, Under review as a conference paper at ICLR 2020 where α c is the upper bound of α. This is verified by experiments in Section 5. These results imply a guideline for selecting the learning rates: first, search for the largest possible α; next, tune β.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a framework called DO-AutoEncoder (DO-AE) for learning causally related concepts in images. DO-AE discovers the causal relationship of concepts in the images by comparing complexities of generative models in either hypothetic directions and the generative models are realized by AutoEncoder structures. The latent codes are then used to reconstruct the original input images and the whole AutoEncoder structure is trained by minimizing the reconstruction error. The framework is evaluated on two datasets, PHY2D and PHY3D, which are collections of images of abstract graphic description of physics laws. Results show that DO-AE is able to discover causally related concepts in images and offers better explainability and generalization performance for downstream tasks.",
        "Abstract": "Some fundamental limitations of deep learning have been exposed such as lacking generalizability and being vunerable to adversarial attack. Instead, researchers realize that causation is much more stable than association relationship in data. In this paper, we propose a new framework called do-calculus AutoEncoder(DO-AE) for deep representation learning that fully capture bivariate causal relationship in the images which allows us to intervene in images generation process. DO-AE consists of two key ingredients: causal relationship mining in images and intervention-enabling deep causal structured representation learning. The goal here is to learn deep representations that correspond to the concepts in the physical world as well as their causal structure. To verify the proposed method, we create a dataset named PHY2D, which contains abstract graphic description in accordance with the laws of physics. Our experiments demonstrate our method is able to correctly identify the bivariate causal relationship between concepts in images and the representation learned enables a do-calculus manipulation to images, which generates artificial images that might possibly break the physical law depending on where we intervene the causal system.",
        "Introduction": "  INTRODUCTION Recent breakthrough of deep learning (LeCun et al., 2015;  Goodfellow et al., 2016 ) has significantly advanced the development of Artificial Intelligence (AI). However, statistical prediction and infer- ence models, no matter traditional machine learning models such as random forest, support vector machines or deep models have their limitations such as lacking generalizability, being vulnerable to adversarial attacks as well as lacking interpretability ( Zhang et al., 2016 ;  Papernot et al., 2016 ). All these limitations prevent AI from wide applications to high stake scenarios including healthcare, au- tonomous driving ( Chen et al., 2015 ) etc. Tremendous amount of efforts from the AI community has been devoted to understanding and improving the robustness of deep models and researchers come to consensus that after training with huge amount of data, deep models exploit \"superficial\" associ- ation in data to make prediction. However, such kind of subtle association relationship is unstable across domains. For example, the association relationship between pixels related to \"grassland\" and the label \"elephant\" is real but it is rather weak. Deep models, trained with hundreds of thousands of natural images where elephants reside on grassland, exploit too much pixel level association of the image to the label. Unfortunately, such models fail to detect an elephant if the background of the im- age is changed to a living room. More and more evidences show that deep models exploit association instead of causation when making the prediction, rendering it weak in term of domain generalization and robustness against adversarial attacks. As a consequence, researchers are advocating bringing causality into the life-cycle of deep neural networks. In Judea Pearl book \"The book of why\" ( Pearl & Mackenzie, 2018 ), he gives ladder of causation: The lowest level is concerned with patterns of association in observed data. The next level focuses on intervention and the highest one is counterfactual inference. In order to realize real Machine Intelligence, we must replace reasoning by association with reasoning by causality. However, how to incorporate causality into current machine learning framework is very challenging and little work has been found yet until the most recently, researchers start to look at disentangled representation learning. In the deep learning community, researchers realized that a good representation enables better generalizability and might possibly enjoy better interpretability. Disentangled representation is believed to be a good representation and become a research focus in a past few years ( Hsu et al., Under review as a conference paper at ICLR 2020 2017 ;  Narayanaswamy et al., 2017 ;  Locatello et al., 2018 ). The idea behind disentangled repre- sentation learning is that we prefer the learned representations as independent as possible such that they correspond to the independent generative factors of the image, e.g. the illumination, azimuth, elevation etc for 3D graphics. The advantage of disentangled representation is that it offers bet- ter explainablity and it is claimed that deep models with disentangled representations enjoy better generalization performance for downstream tasks. Although significant progress has been achieved in the area of deep representation learning in the past few years, most of existing work assume that the representations or the latent codes are sta- tistically independent and they are trained by adding a regularization term of mutual information between latent codes to the loss function ( Chen et al., 2016 ). It is of great interest and has shown advantage over entangled representation, but forcing the latent codes to be independent might limit itself both in term of theoretical understanding and practical applications. In various scenarios, we are rather interested in learning latent representations that correspond to the causally related con- cepts. For example, given an image consisting concepts like tree, sunshine and shadow, an idea representation learning algorithm is expected to yield latent codes that could each corresponds to the above three concepts. Obviously, these three concepts are by no means independent, i.e. the concept of tree and that of sunshine together causes the concept of shadow. If we are able to extract latent codes with their causal structure, we are able to intervene the whole system, e.g. intervene the latent code of shadow such that we can generate images with the shadow removed. This could be potentially of great value in many applications such as autonomous driving. In this paper, we propose a framework called DO-AutoEncoder (DO-AE) to learn causally related concepts in images, which is achieved by firstly discovering the causal relationship of concepts in the images by comparing complexities of generative models (G(cause), G(effect|cause)) in either hypothetic directions and the generative models are realized by AutoEncoder structures. The latent codes are then used to reconstruct the original input images and the whole AutoEncoder structure is trained by minimizing the reconstruction error. The framework mimics the whole data generating mechanism, i.e. the joint distribution P (cause, effect) is factorized as P (cause)P (effect|cause) ac- cording to the causal direction. As Figure 10 shows, the light is projected from the vertical direction upside down, yielding the time-varying shadow of a swinging pendulum. So the causal direction is from angle (of pendulum) to length (of shadow). We aim at developing deep representation learning model and algorithm to discover such causally related concepts from a series of images describing the law of physics. In addition to the pendulum data, in this paper we create a series of dataset named PHY2D, the collection of images of abstract graphic description of physics laws.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a framework for Knowledge Graph (KG) embedding from a group-theoretic perspective. It investigates the graph completion task and proves that the intrinsic structure of this task automatically produces the complete definition of groups. The framework is used to reconstruct existing models, including TransE, TransR, TorusE, RotatE, ComplEx, and DisMult, using group theory language. This is the first proof that rigorously legitimates the application of group theory in KG embedding.",
        "Abstract": "We have rigorously proved the existence of a group algebraic structure hidden in relational knowledge embedding problems, which suggests that a group-based embedding framework is essential for model design. Our theoretical analysis explores merely the intrinsic property of the embedding problem itself without introducing extra designs. Using the proposed framework, one could construct embedding models that naturally accommodate all possible local graph patterns, which are necessary for reproducing a complete graph from atomic knowledge triplets. We reconstruct many state-of-the-art models from the framework and re-interpret them as embeddings with different groups. Moreover, we also propose new instantiation models using simple continuous non-abelian groups.",
        "Introduction": "  INTRODUCTION Knowledge graphs (KGs) are prominent structured knowledge bases for many downstream semantic tasks ( Hao et al., 2017 ). A KG contains an entity set E = {e i }, which correspond to vertices in the graph, and a relation set R = {r k }, which forms edges. The entity and relation sets form a collection of factual triplets, each of which has the form (e i , r k , e j ) where r k is the relation between the head entity e i and the tail entity e j . Since large scale KGs are usually incomplete due to missing links (relations) amongst entities, an increasing amount of recent works ( Bordes et al., 2013 ;  Yang et al., 2014 ;  Trouillon et al., 2016 ;  Lin et al., 2015 ) have devoted to the graph completion (i.e., link prediction) problem by exploring a low-dimensional representation of entities and relations. More formally, each relation r acts as a mapping O r [·] from its head entity e 1 to its tail entity e 2 : The original KG dataset represents these mappings in a tabular form, and the task of KG embed- ding is to find a better representation for these abstract mappings. For example, in the TransE model ( Bordes et al., 2013 ), relations and entities are embedded in the same vector space, and the operation O r [·] is simply a vector summation: O r [e] = e + r. In general, the operation could be either linear or nonlinear, either pre-defined or learned. Since the entity and relation sets in KGs play conceptually different roles, it is reasonable to rep- resent them differently, which demands a more proper operation O r [·] for bridging the entity and relation sets. On the other hand, the graph completion task relies on the fact that relations are not independent. For example, the hypernym and hyponym are inverse to each other; while kinship relations usually support mutual inferences. Previous studies ( Sun et al., 2019 ;  Xu & Li, 2019 ) have concerned some specific cases of inter-relation dependencies, including (anti-)symmetry and compositional relations. However, one may ask a more general question: could we establish certain fundamental principles to guide the operation design, such that any inter-relation dependency could be naturally captured? In this work, we find the answer is positive, and we term these principles as hyper-relations. Group theory emerges as a natural description tool that satisfies the need for finding better operations and hyper-relations. In the following sections, we demonstrate the emergence of group definition, namely closure, identity, inverse, and associativity, from a study of knowledge graph itself, and explain in detail an implementation of group theory to tackle general relational embedding problems. One main contribution of this work is it provides a framework for addressing the KG embedding problem from a novel and more rigorous perspective: the group-theoretic perspective. This frame- Under review as a conference paper at ICLR 2020 work arises from our investigation of the graph reconstruction problem. We prove that the intrinsic structure of this task automatically produces the complete definition of groups. To our best knowl- edge, this is the first proof that rigorously legitimates the application of group theory in KG embed- ding. With this framework, we reconstructed several existing models using group theory language (see Sec 3.3), including: TransE ( Bordes et al., 2013 ), TransR ( Lin et al., 2015 ), TorusE ( Ebisu & Ichise, 2018 ), RotatE ( Sun et al., 2019 ), ComplEx ( Trouillon et al., 2016 ), DisMult ( Yang et al., 2014 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel neural network architecture - Graph-Translation-Generative-Adversarial-Nets (GT-GAN) for graph synthesis. GT-GAN consists of an encoder-decoder translator and a conditional graph discriminator to learn the one-to-more mapping (a conditional distribution) for graph translation. The graph encoder includes both the edge and the node convolution layers to jointly embed local and global information. The graph decoder consists of the \"edge deconvolution\" and \"node deconvolution\" layers, which can decode the node representations first into the latent relations of the target graph and then generate the final target graph. The graph skip-connection is also utilized to map the learned latent relations between the input and target graphs. Extensive experiments have been conducted on both synthetic and real-world datasets to demonstrate the effectiveness and efficiency of the proposed model.",
        "Abstract": "Deep graph generation models have achieved great successes recently, among which, however, are typically unconditioned generative models that have no control over the target graphs are given an input graph. In this paper, we propose a novel Graph-Translation-Generative-Adversarial-Networks (GT-GAN) that transforms the input graphs into their target output graphs. GT-GAN consists of a graph translator equipped with innovative graph convolution and deconvolution layers to learn the translation mapping considering both global and local features, and a new conditional graph discriminator to classify target graphs by conditioning on input graphs. Extensive experiments on multiple synthetic and real-world datasets demonstrate that our proposed GT-GAN significantly outperforms other baseline methods in terms of both effectiveness and scalability. For instance, GT-GAN achieves at least 10X and 15X faster runtimes than GraphRNN and RandomVAE, respectively, when the size of the graph is around 50.",
        "Introduction": "  INTRODUCTION In recent years, deep learning on graphs has seen a surge of interests, especially for graph representa- tion and recognition tasks such as node-level classification (Li et al., 2016; Kipf & Welling, 2017; Veličković et al., 2017; Gilmer et al., 2017; Hamilton et al., 2017) and graph-level classification (Niepert et al., 2016; Atwood et al., 2016;  Wu et al., 2017 ). Because of the successes in graph neural networks, researchers have recently started to explore the use of deep generative models for graph synthesis on practical applications such as designing new chemical molecular structures (Simonovsky & Komodakis, 2018; You et al., 2018). This has led to many of the recent advances in deep graph generative models where some of these approaches are domain dependent models (Kusner et al., 2017; Dai et al., 2018) for generating graphs with physical constrains, while some others consider the generation of generic graphs (Li et al., 2018; Samanta et al., 2018; Jin et al., 2018a). However, there are two main drawbacks of existing deep graph generative models. First, one significant limitation of the previous approaches is that most of these models are only suitable for small graphs with 40 or fewer nodes, which is mainly due to their one-node-per-step generation manner. More importantly, most of the existing graph generation models are unconditioned and thus ignore rich input graph information for generating a new graph. In many applications, it is crucial to guide the graph generation process by conditioning on an input graph, which can be cast as a graph translation learning problem - translating the input graph to the output graph. One straightforward way is to build a translation system by using a graph encoder-decoder architecture. However, there are several challenges for this type of approaches: 1) how to learn one-to-more mapping between the input graph and the target graphs. Different from the plain graph generation problem, a conditional graph synthesis task is to learn a distribution of target graphs conditioning on the input graph, which aims to capture the underlying implicit properties of the graphs, such as their scale-free characteristic. 2) how to jointly learn both local and global information for translation. One needs to not only learn the translation mapping in the local information (i.e. neighborhood pattern of each node), but also in the global property of the whole graph (e.g.,node degree distribution or graph density). To address the aforementioned challenges, we present a novel neural network architecture - Graph- Translation-Generative-Adversarial-Nets (GT-GAN). We first propose a conditional graph GAN architecture that consists of an encoder-decoder translator and a conditional graph discriminator to learn the one-to-more mapping (a conditional distribution) for graph translation. To jointly embed Under review as a conference paper at ICLR 2020 the local and global information, we present a novel graph encoder including both the edge and the node convolution layers. In addition, we further propose a novel graph U-net with graph skips and dedicated graph deconvolution layers including both the edge and the node deconvolution layers. Finally, GT-GAN is scalable with at most quadratic computation and memory consumption in terms of the number of nodes in a graph, making it suitable for at least modest-scale graphs (with hundreds of nodes, compared to the tens of nodes in most of existing graph generative models). We highlight our main contributions as follows: • We develop a generic framework GT-GAN consisting of a novel graph translator and conditional graph discriminator for learning a conditional distribution of target graphs given the input graphs. • We propose a novel graph encoder consisting of \"edge convolution\" layers that extract various relations among nodes containing both local and global information, and \"node convolution\" layers that embed the node representations based on the extracted relations. • We propose a novel graph decoder consisting of the \"edge deconvolution\" and \"node deconvolution\" layers, which can decode the node representations first into the latent relations of the target graph and then generate the final target graph. The graph skip-connection is also utilized to map the learned latent relations between the input and target graphs. • Extensive experiments have been conducted on both synthetic and real-world datasets on eight performance metrics to demonstrate the effectiveness and efficiency of the proposed model.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the hierarchical generative representations of Generative Adversarial Networks (GANs) and reveals the layer-wise variation factors that control the synthesis of photo-realistic images from coarse to fine. We quantify the causality between the layer-wise activations and the semantics occurring in the output image, and identify the most relevant variation factors across different layers of a GAN model. We further show that identifying such a set of manipulatable latent variation factors facilitates the semantic image manipulation with large diversity.",
        "Abstract": "Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what networks have learned inside the deep generative representations and how photo-realistic images are able to be composed from random noises. In this work, we show that highly-structured semantic hierarchy emerges from the generative representations as the variation factors for synthesizing scenes. By probing the layer-wise representations with a broad set of visual concepts at different abstraction levels, we are able to quantify the causality between the activations and the semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results suggest that the generative representations learned by GAN are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent semantics facilitates semantic scene manipulation.",
        "Introduction": "  INTRODUCTION Success of deep neural networks stems from the representation learning, which identifies the explana- tory factors underlying the high-dimensional observed data ( Bengio et al. (2013) ). Prior work has shown that many concept detectors spontaneously emerge inside the deep representations trained for classification task. For example,  Gonzalez-Garcia et al. (2018)  shows that networks for object recog- nition are able to detect semantic object parts, and  Bau et al. (2017)  confirms that deep representations from classifying images learn to detect different categorical concepts at different layers. Analyzing the deep representations and their emergent structures gives insight into the generalization ability of deep features ( Morcos et al. (2018) ) as well as the feature transferability across different tasks ( Yosinski et al. (2014) ). But current efforts on interpreting deep representations mainly focus on discriminative models ( Zhou et al. (2015) ;  Gonzalez-Garcia et al. (2018) ;  Zeiler and Fergus (2014) ;  Agrawal et al. (2014) ;  Bau et al. (2017) ). Recent advance of Generative Adversarial Networks (GANs) ( Goodfellow et al. (2014) ;  Karras et al. (2018a ; b );  Brock et al. (2019) ) is capable of transforming random noises into high-quality images, however, the nature of the learned generative representations and how a photo-realistic image is being composed over different layers of the generator in GAN remain much less explored. It is known that the internal units of Convolutional Neural Networks (CNNs) emerge as object detectors when trained to categorize scenes ( Zhou et al. (2015) ). Representing and detecting infor- mative categorical objects provides an ideal solution for classifying scenes, such as sofa and TV are representative of living room while bed and lamp are of bedroom. However, synthesizing a scene demands far more knowledge for the generative models to learn. Specifically, in order to produce highly-diverse scene images, the deep representations might be required to not only generate every individual object relevant to a specific scene category, but also decide the underlying room layout as well as render various scene attributes, e.g., the lighting condition and color scheme. Very recent work Under review as a conference paper at ICLR 2020 on interpreting GANs  Bau et al. (2019)  visualized that the internal filters at intermediate layers are specialized for generating some certain objects, but studying scene synthesis from object aspect only is far from fully understanding how GAN is able to compose a photo-realistic image, which contains multiple variation factors from layout level, category level, to attribute level. The original StyleGAN work ( Karras et al. (2018b) ) pointed out that the layer-wise latent codes actually control the synthesis from coarse to fine, but how these variation factors are composed together and how to quantify such semantic information are still uncertain. Differently, this work gives a much deeper interpretation on the hierarchical generative representations in the sense that we match these layer-wise variation factors with human-understandable scene variations at multiple abstraction levels, including layout, category (object), attribute, and color scheme. Starting with the state-of-the-art StyleGAN models ( Karras et al. (2018b) ) as the example, we reveal that highly-structured semantic hierarchy emerges from the deep generative representations with layer-wise stochasticity trained for synthesizing scenes, even without any external supervision. Layer- wise representations are first probed with a broad set of visual concepts at different abstraction levels. By quantifying the causality between the layer-wise activations and the semantics occurring in the output image, we are able to identify the most relevant variation factors across different layers of a GAN model with layer-wise latent codes: the early layers specify the spatial layout, the middle layers compose the category-guided objects, and the later layers render the attributes and color scheme of the entire scene. We further show that identifying such a set of manipulatable latent variation factors from layouts, objects, to scene attributes and color schemes facilitates the semantic image manipulation with large diversity. The proposed manipulation technique is further generalized to other GANs such as BigGAN ( Brock et al. (2019) ) and ProgressiveGAN ( Karras et al. (2018a) ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Subjective Bayesian framework for predictive uncertainty estimation in Graph Neural Networks (GNNs). The proposed framework allows simultaneous estimation of different uncertainty types associated with the predicted class probabilities of the test nodes generated by GNNs, including vacuity, dissonance, aleatoric uncertainty, and epistemic uncertainty. An efficient approximate inference algorithm, Graph-based Kernel Dirichlet Distribution Estimation (GKDE), is proposed to reduce error in predicting Dirichlet distributions. The performance of the proposed framework is validated through comprehensive experiments on six real graph datasets.",
        "Abstract": "Thanks to graph neural networks (GNNs), semi-supervised node classification has shown the state-of-the-art performance in graph data.  However, GNNs do not consider any types of uncertainties associated with the class probabilities to minimize risk due to misclassification under uncertainty in real life. In this work, we propose a Bayesian deep learning framework reflecting various types of uncertainties for classification predictions by leveraging the powerful modeling and learning capabilities of GNNs. We considered multiple uncertainty types in both deep learning (DL) and belief/evidence theory domains. We treat the predictions of a Bayesian GNN (BGNN) as nodes' multinomial subjective opinions in a graph based on Dirichlet distributions where each belief mass is a belief probability of each class. By collecting evidence from the given labels of training nodes, the BGNN model is designed for accurately predicting probabilities of each class and detecting out-of-distribution.  We validated the outperformance of the proposed BGNN, compared to the state-of-the-art counterparts in terms of the accuracy of node classification prediction and out-of-distribution detection based on six real network datasets.",
        "Introduction": "  INTRODUCTION Inherent uncertainties introduced by different root causes have emerged as serious hurdles to find effective solutions for real world problems. Critical safety concerns have been brought due to lack of considering diverse causes of uncertainties, resulting in high risk due to misinterpretation of uncertainties (e.g., misdetection or misclassification of an object by an autonomous vehicle). Graph neural networks (GNNs) ( Kipf & Welling, 2016 ;  Veličković et al., 2018 ) have gained tremendous attention in the data science community. Despite their superior performance in semi-supervised node classification and/or regression, they didn't allow to deal with various types of uncertainties. Predictive uncertainty estimation ( Malinin & Gales, 2018 ) using Bayesian NNs (BNNs) has been explored for classification prediction or regression in the computer vision applications, with well- known uncertainties, aleatoric and epistemic uncertainties. Aleatoric uncertainty only considers data uncertainty derived from statistical randomness (e.g., inherent noises in observations) while epistemic uncertainty indicates model uncertainty due to limited knowledge or ignorance in collected data. On the other hand, in the belief or evidence theory, Subjective Logic (SL) ( Josang et al., 2018 ) considered vacuity (or lack of evidence) as uncertainty in an subjective opinion. Recently other uncertainties such as dissonance, consonance, vagueness, and monosonance ( Josang et al., 2018 ) are also introduced. This work is the first that considers multidimensional uncertainty types in both DL and belief theory domains to predict node classification and out-of-distribution (OOD) detection. To this end, we incorporate the multidimensional uncertainty, including vacuity, dissonance, aleatoric uncertainty, and epistemic uncertainty in selecting test nodes for Bayesian DL in GNNs. We perform semi-supervised node classification and OOD detection based on GNNs. By leveraging the modeling and learning capability of GNNs and considering multidimensional uncertainties in SL, we propose a Bayesian DL framework that allows simultaneous estimation of different uncertainty types associated with the predicted class probabilities of the test nodes generated by GNNs. We treat the predictions of a Subjective Bayesian GNN (S-BGNN) as nodes' subjective opinions in a graph modeled as Dirichlet distributions on the class probabilities, and learn the S-BGNN model by collecting the evidence from the given labels of the training nodes (see  Figure 1 ). This work has the following key contributions: • A Subjective Bayesian framework to predictive uncertainty estimation for GNNs. Our pro- posed framework directly predicts subjective multinomial opinions of the test nodes in a graph, Under review as a conference paper at ICLR 2020 with the opinions following Dirichlet distributions with each belief probability as a class probability. Our proposed framework is a generative model, so it cal be highly applicable across all GNNs and allows simultaneously estimating different types of associated uncertainties with the class probabilities. • Efficient approximate inference algorithms: We propose a Graph-based Kernel Dirichlet distri- bution Estimation (GKDE) method to reduce error in predicting Dirichlet distribution. We designed an iterative knowledge distillation algorithm that treats a deterministic GNN as a teacher network while considering our proposed Subjective Bayesian GNN model (a realization of our proposed framework for a specific GNN) as a distilled network. This allows the expected class probabilities based on the predicted Dirichlet distributions (i.e., outputs of our trained Bayesian model) to match the predicted class probabilities of the deterministic GNN model, along with uncertainty estimated in the predictions. • Comprehensive experiments for the validation of the performance of our proposed frame- work. Based on six real graph datasets, we compared the performance of our propose framework with that of other competitive DL algorithms. For a fair comparison, we tweaked the DL algorithms to consider various uncertainty types in predicted decisions.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a semi-supervised learning approach suitable for semantic segmentation. The proposed approach is based on the idea of distilling the structural relationship between pixels and is designed to improve the performance of semantic segmentation with a relatively small number of labeled data. Experiments are conducted to evaluate the performance of the proposed approach and the results show that the proposed approach can improve the performance of semantic segmentation with a small number of labeled data.",
        "Abstract": "The consistency loss has played a key role in solving problems in recent studies on semi-supervised learning. Yet extant studies with the consistency loss are limited to its application to classification tasks; extant studies on semi-supervised semantic segmentation rely on pixel-wise classification, which does not reflect the structured nature of characteristics in prediction. We propose a structured consistency loss to address this limitation of extant studies. Structured consistency loss promotes consistency in inter-pixel similarity between teacher and student networks. Specifically, collaboration with CutMix optimizes the efficient performance of semi-supervised semantic segmentation with structured consistency loss by reducing computational burden dramatically. The superiority of proposed method is verified with the Cityscapes; The Cityscapes benchmark results with validation and with test data are 81.9 mIoU and 83.84 mIoU respectively. This ranks the first place on the pixel-level semantic labeling task of Cityscapes benchmark suite. To the best of our knowledge, we are the first to present the superiority of state-of-the-art semi-supervised learning in semantic segmentation.",
        "Introduction": "  INTRODUCTION With the deep learning approach of  Krizhevsky et al. (2012)  introduced in ILSVRC-2012 (Rus- sakovsky et al., 2015), deep learning based image processing has quickly spread among the field. Deep learning has evolved into supervised learning, where the performance of the network is greatly affected by the quantity and quality of labeled data provided. However, preparing labeled data is time-consuming and expensive as it is manually prepared. When it comes to semantic segmenta- tion, it requires pixel-by-pixel annotation making it particularly costly to prepare labeled data for processing. Currently, various techniques such as active learning ( Mackowiak et al., 2018 ), interac- tive segmentation ( Maninis et al., 2018 ), weakly-supervised leaning ( Lee et al., 2019 ) and more are developed and presented to solve the labeling cost problem in semantic segmentation. In this study, we are aimed at bypassing the labeling cost problem with semi-supervised learning technique. Semi-supervised learning is a technique to improve network performance with relatively a smaller number of labeled data when a large number of unlabeled data is also available. In reality, a simple build up of unlabeled data can be easily collected from various data sources such as web crawling, vehicle logging and more, and what makes data preparation costly is the manual labelling process to make the collected data useful in learning process. That is, semi-supervised learning technique best suits the data needs of semantic segmentation in real-world research environment. For this reason, our research designed the semi-supervised learning approach suitable for semantic segmentation. Prior studies apply various versions of semi-supervised learning technique to image classification task and report significant improvement in the performance. Unlike classification, semantic segmen- tation is a task that performs structured prediction. Specifically, prediction of classification results in a class vector, while semantic segmentation makes predictions per regional location and predicts structural characteristics of regions. We can also learn from prior studies that structural relationship between pixels is important in semantic segmentation; Liu et al. (2019) discussed the distillation of between-pixel relationships and  Xie et al. (2018)  discussed the neighbor-pixel distillation of fea- tures. Together, we pinpoint that it is important to adequately modify the semi-supervised learning to be applied in semantic segmentation.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a Two-Step Uncertainty Network (TUN) framework for sensor placement to maximize the information gain. TUN consists of two steps, namely the imagination step and the inspection step, which are deployed with pre-trained neural networks. The agent adapts a greedy algorithm to select the optimal next location to inquire based on the task-specific information gain at all the un-observed locations. This procedure emulates how humans think in such tasks, by firstly imagining the possible outcomes at un-observed locations, then inspecting the information pertaining to the task based on those possible outcomes.",
        "Abstract": "Optimal sensor placement achieves the minimal cost of sensors while obtaining the prespecified objectives. In this work, we propose a framework for sensor placement to maximize the information gain called Two-step Uncertainty Network(TUN). TUN encodes an arbitrary number of measurements, models the conditional distribution of high dimensional data, and estimates the task-specific information gain at un-observed locations. Experiments on the synthetic data show that TUN outperforms the random sampling strategy and Gaussian Process-based strategy consistently.",
        "Introduction": "  INTRODUCTION Sensor placement is widely studied in the areas of environment monitoring ( Hu et al., 2018 ;  Nguyen et al., 2015 ), structural health monitoring( Ostachowicz et al., 2019 ) , security screening( Masoudi et al., 2016 ), and adaptive computed tomography( Ouadah et al., 2017 ). The optimal sensor place- ment maximizes the objectives with minimal cost of sensors. Given the model that maps each possible set of sensor locations to the objectives, the optimal sensor placement can be formulated as an optimization problem. However, the optimization is shown to be NP-hard( Garey & Johnson, 2002 ). Thus, approximate greedy algorithms of sequential sensor placement are proposed and then proved to be near optimal under the assumptions that the objectives are monotone and submodu- lar( Nemhauser et al., 1978 ). The diagram of a sequential sensor placement is shown in Figure 1a with the black arrows. The agent inquires at a feasible location to the physical model in each step and obtains the corresponding measurement. The obtained observations are used to make inference for specific tasks. For instance, in security screening tasks the observations are used to predict the distribution of the object's label. To make an accurate inference, the agent often optimizes the information gain in each step with re- spect to the feasible location. The corresponding objective is mutual information which is approved to give near optimal approximations in sequential sensing( Krause et al., 2008 ;  Nemhauser et al., 1978 ). To optimize the objective, a model that estimates the potential information gain at each possible location is necessary. The most generic method to model the unknown spatial phenomenon is Gaus- sian Process(GP) which incorporates the knowledge of observations and predicts the uncertainty at the un-observed locations. However, the Gaussian model assumption in GP does not perform well on high dimensional data such as images as generative models. In addition, GP inherently adapts the assumptions that the uncertainty at the un-observed locations is independent of the obtained mea- surements, making the GP based sequential sensing an open-loop control( Rasmussen, 2003 ). An alternative approach to this problem arises recently is Reinforcement Learning(RL)( Ha & Schmid- huber, 2018 ). However, the performance in RL is found to be of large variance and difficult to reproduce( Henderson et al., 2018 ). In this work, we propose a framework for sensor placement to maximize the information gain called Two-step Uncertainty network (TUN). The pipeline of TUN is shown in  Figure 1  with red arrows. TUN consists of two steps, namely the imagination step and the inspection step. TUN firstly \"imag- ines\" the possible measurements at the un-observed locations. Then it estimates the task-specific information gain with the imagined measurements along with the previous observations in the in- spection step. Both steps are deployed with the pre-trained neural networks. Given the task-specific information gain at all the un-observed locations, the agent adapts a greedy algorithm to select the Under review as a conference paper at ICLR 2020 optimal next location to inquire. This procedure emulates how we human think in such tasks: given the observations, we firstly imagine the possible outcomes at un-observed locations, then inspect the information pertaining to the task based on those possible outcomes. We will derive the proposed framework in the next section.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new perspective on understanding the non-convergence behavior of Adam-Type algorithms, a class of first-order optimization algorithms with adaptive learning rate. Through analysis of the Adam algorithm, the authors identify potential faults in Adam-Type algorithms and propose a new Adam variant algorithm, AdamAL. Experiments on different machine learning tasks and models demonstrate the effectiveness of AdamAL, which outperforms the standard Adam algorithm.",
        "Abstract": "First-order adaptive optimization algorithms such as Adam play an important role in modern deep learning due to their super fast convergence speed in solving large scale optimization problems. However, Adam's non-convergence behavior and regrettable generalization ability make it fall into a love-hate relationship to deep learning community. Previous studies on Adam and its variants (refer as Adam-Type algorithms) mainly rely on theoretical regret bound analysis, which overlook the natural characteristic reside in such algorithms and limit our thinking. In this paper, we aim at seeking a different interpretation of Adam-Type algorithms so that we can intuitively comprehend and improve them. The way we chose is based on a traditional online convex optimization algorithm scheme known as mirror descent method. By bridging Adam and mirror descent, we receive a clear map of the functionality of each part in Adam. In addition, this new angle brings us a new insight on identifying the non-convergence issue of Adam. Moreover, we provide new variant of Adam-Type algorithm, namely AdamAL which can naturally mitigate the non-convergence issue of Adam and improve its performance. We further conduct experiments on various popular deep learning tasks and models, and the results are quite promising.",
        "Introduction": "  INTRODUCTION In recent years, first-order optimization algorithms with adaptive learning rate have become the dominant method to train deep neural networks because these methods show extraordinary power in solving large-scale machine learning optimization problems. By cooperating with first-order in- formation, adaptive methods iteratively update parameters by moving them to the direction of the negative gradient of the cost function with non-fixed learning rate. The first algorithm in this line of research can be dated back to McMahan & Streeter (2010), where they adaptively choose regular- ization functions for bounding objective function parameters based on the loss functions observed at each iteration. Then, in Streeter & McMahan (2010), they demonstrate that the convergence rates can often be dramatically improved through the use of preconditioning. The insight of these two methods is parallel but effective, that is, they try to modify the gradients' magnitude with adaptive pre-coordinate learning rates. Later, AdaGrad (Duchi et al., 2011) carefully chooses the precon- ditioner and provides the first practical adaptive algorithm with a tight theoretical guarantee based on Zinkevich (2003) regret analysis. Although AdaGrad achieves a great success in the sparse settings, the rapid decay of the adaptive learning rate limits its usage. This is due to AdaGrad ac- cumulating all the past gradients as its learning rate. And this unbounded learning rate becomes extremely small when one has a large number of training iterations. To address this, several variants of AdaGrad, such as AdaDelta (Zeiler, 2012) and RMSProp (Hinton et al., 2012) have been pro- posed to mitigate the rapid decay of the learning rate. Based on these, Adma (Kingma & Ba, 2014) incorporating with first momentum correction accelerates the convergence speed of first-order op- timization algorithms to a new height. The use of exponential moving average (EMA) in Adam becomes a key to Adam's success. Up to this point, we can summarize the adaptive or non-adaptive first-order optimization algorithms as follows. Denote g t ∈ R d as the gradient of generic optimization problem f with respect to its parameters x ∈ R d at iteration t, then the generic updating rule of adaptive methods can be expressed as (Reddi Under review as a conference paper at ICLR 2020 et al., 2019): x t+1 = x t − η t √ v t m t (1) where denotes the entry-wise or Hadamard product and the α t is the base learning rate. In the equation above, m t = ϑ(g 1 , · · · , g t ) is a function related to the historical gradients up to t; and v t = υ(g 1 , · · · , g t ) is a function that produces a d-dimensional vector with non-negative entry; The design of v t is simple of non-adaptive methods, such as vanilla stochastic gradient descent (Vanilla SGD), where we have v t = I, however, it is crucial for Adam-Type algorithms. For Adam, in particular, the m t and v t are computed by EMA of gradient, with coefficient β 1 and β 2 where Adam, the most popular adaptive method, has been widely adopted by the deep learning community. The root cause of the fast convergence of Adam and its variants in convex or non-convex optimiza- tion problems remains an open question (Chen et al., 2018). In addition, the generalization ability and out-of-sample behavior of Adam-Type methods are even worse than traditional non-adaptive counterparts such as Vanilla SGD (Wilson et al., 2017). In order to understand the insight behind those adaptive algorithms and close the generalization gap, several Adam-Type algorithms have been proposed including (Reddi et al., 2019; Luo et al., 2019; Zhou et al., 2018b; Balles & Hennig, 2017; Liu et al., 2019). Although they propose many different kinds of viewpoints in understanding the performance of Adam and demonstrate a series of correc- tion methods to improve Adam, we think the mechanism of Adam-Type algorithms is still unclear. For example, one common thinking about about m t and v t in Adam is first and second moments of unbiased estimator g t , however, why this second moment can be used as adaptive learning rate? Does the v t really behave as the second moment of the gradient g t ? Also, another commonly asked question is where the Adam adopts such a faster convergence speed than other first-order methods? These mysterious questions on adaptive methods finally leads us to rethink the Adam-Type algo- rithms in a neutral way. In this paper, we provide a new insight into Adam-Type algorithms, which brings a new perspective of comprehension on Adam-Type algorithms and it allows us to easily identify the misbehavior of Adam such as non-convergence issues. In the aforementioned works, the analysis of Adam-Type methods is based on Kingma & Ba (2014) framework, which limits our understanding. In fact, when we look back to the origin of adaptive learning rate methods mentioned in Streeter & McMahan (2010), we notice that the design of Adam is highly related to adaptive regu- larization of follow-the-proximally-regularized-leader (FTPRL) method and it can also be regarded as a variant of traditional mirror descent method (Xiao, 2010). The more detail and our motivation can be found in the next section. For simplicity, we use standard Adam as our touchstone through the paper to convey our main idea, and their variants follow the same thoughts. Now, we summarize our contribution in three folds: 1. We provide a new perspective in understanding the non-convergence behavior of Adam- Type algorithms based on mirror descent approach. Our analysis agrees well with the previous works but much more intuitive and effective. 2. Based on our observation, we identify potential faults in Adam-Type algorithms and we provide a new Adam variant algorithm, named AdamAL. 3. We conduct a series of experiments on different machine learning tasks and models by using our AdamAL algorithm, and the results are promising and the performance of AdamAL is never worse than Adam.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new method for generating intrinsic reward in sparse reward reinforcement learning (RL) based on an ensemble of estimation models for the environment dynamics. Numerical results show that the proposed model-ensemble-based intrinsic reward generation method yields improved performance as compared to existing reward generation methods for continuous control with sparse reward setting.",
        "Abstract": "In this paper, a new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models. In the proposed method, the mixture of multiple dynamics models is used to approximate the true unknown transition probability, and the intrinsic reward is designed as the minimum of the surprise seen from each dynamics model to the mixture of the dynamics models. In order to show the effectiveness of the proposed intrinsic reward generation method, a working algorithm is constructed by combining the proposed intrinsic reward generation method with the proximal policy optimization (PPO) algorithm. Numerical results show that for representative locomotion tasks, the proposed model-ensemble-based intrinsic reward generation method outperforms the previous methods based on a single dynamics model.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) with sparse reward is an active research area ( Andrychowicz et al., 2017 ;  de Abril & Kanai, 2018 ;  Kim et al., 2018 ;  Oh et al., 2018 ;  Tang et al., 2017 ). In typical model-free RL, an agent learns a policy to maximize the expected cumulative reward under the circumstance that the agent receives a non-zero reward from the environment for each action of the agent. On the contrary, in sparse reward RL, the environment does not return a non-zero reward for every action of the agent but returns a non-zero reward only when certain conditions are met. Such situations are encountered in many action control problems ( Andrychowicz et al., 2017 ;  Houthooft et al., 2016 ;  Oh et al., 2018 ). As in conventional RL, exploration is important at the early stage of learning in sparse reward RL, whereas the balance between exploration and exploitation is required on the later stage. Methods such as the -greedy strategy ( Mnih et al., 2015 ;  Van Hasselt et al., 2016 ) and the control of policy gradient with Gaussian random noise ( Duan et al., 2016 ;  Schulman et al., 2015a ) have been applied to various tasks for exploration. However, these methods have been revealed to be insufficient for successful learning when reward is sparse ( Achiam & Sastry, 2017 ). In order to overcome such difficulty, intrinsically motivated RL has been studied to stimulate better exploration by generating intrinsic reward for each action by the agent itself, even when reward is sparse. Recently, many intrinsically-motivated RL algorithms have been devised to deal with the sparsity of reward, e.g., based on the notion of curiosity ( Houthooft et al., 2016 ;  Pathak et al., 2017 ) and surprise ( Achiam & Sastry, 2017 ). It is shown that these algorithms are successful and outperform the previous approaches. In essence, these algorithms use a single estimation model for the next state or the environment dynamics to generate intrinsic reward. In this paper, in order to further improve the performance of sparse reward model-free RL, we propose a new method to generate intrinsic reward based on an ensemble of estimation models for the environment dynamics. The rationale behind our approach is that by using a mixture of several distributions, we can increase degrees of freedom for modeling the unknown underlying model dynamics and designing a better reward from the ensemble of estimation models. Numerical results show that the proposed model-ensemble-based intrinsic reward generation method yields improved performance as compared to existing reward generation methods for continuous control with sparse reward setting.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new robust training method called SPROUT, which is based on the general framework of vicinity risk minimization (VRM). SPROUT features self-adjusted label distribution during training instead of attack generation, and adopts Gaussian augmentation and Mixup to further enhance robustness. Compared to adversarial training, SPROUT spares the need for attack generation and thus makes its training scalable, while attaining better or comparable robustness performance on a variety of experiments. SPROUT can find robust models from either randomly initialized models or pretrained models, and its robustness performance is less sensitive to network width.",
        "Abstract": "Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy and reliable machine learning systems. Current robust training methods such as adversarial training explicitly specify an ``attack'' (e.g., $\\ell_{\\infty}$-norm bounded perturbation) to generate adversarial examples during model training in order to improve adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases. Compared with state-of-the-art adversarial training methods (PGD-$\\ell_\\infty$ and TRADES) under $\\ell_{\\infty}$-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.",
        "Introduction": "  INTRODUCTION While deep neural networks (DNNs) have achieved unprecedented performance on a variety of datasets and across domains, developing better training algorithms that are capable of strengthening model robustness is the next crucial milestone toward trustworthy and reliable machine learning systems. In recent years, DNNs trained by standard algorithms (i.e., the natural models) are shown to be vulnerable to adversarial input perturbations (Biggio et al., 2013; Szegedy et al., 2014). Adversarial examples crafted by designed input perturbations can easily cause erroneous decision making of natural models (Goodfellow et al., 2015) and thus intensify the demand for robust training methods. State-of-the-art robust training algorithms are primarily based on the methodology of adversarial training (Goodfellow et al., 2015; Madry et al., 2018), which calls specific attacking algorithms to generate adversarial examples during model training in order to learn robust models. Albeit effective, adversarial training based methods have the following limitations: (i) poor scalability - the process of generating adversarial examples incurs considerable computation overhead. For instance, our own experiments show that, with the same computation resources, standard adversarial training (with 7 attack iterations per sample in every minibatch) of Wide ResNet on CIFAR-10 consumes 10 times more clock time per training epoch when compared with standard training; (ii) attack specificity - adversarially trained models are usually most effective against the same attack they trained on, and the robustness may not generalize well to other types of attacks (Tramèr & Boneh, 2019; Kang et al., 2019); (iii) preference toward wider network - adversarial training are more effective when the networks have sufficient capacity (e.g., having more neurons in network layers) (Madry et al., 2018). To address the aforementioned limitations, in this paper we propose a new robust training method named SPROUT, which is short for self-progressing robust training. We motivate SPROUT by intro- ducing a general framework that formulates robust training objectives via vicinity risk minimization (VRM), which includes many robust training methods as special cases. It is worth noting that the robust training methodology of SPROUT is fundamentally different from adversarial training, as SPROUT features self-adjusted label distribution during training instead of attack generation. In addition to our proposed parametrized label smoothing technique for progressive adjustification of training label distribution, SPROUT also adopts Gaussian augmentation and Mixup (Zhang et al., 2018) to further enhance robustness. In contrast to adversarial training, SPROUT spares the need for attack generation and thus makes its training scalable by a significant factor, while attaining better or Under review as a conference paper at ICLR 2020 comparable robustness performance on a variety of experiments. We also show that SPROUT can find robust models from either randomly initialized models or pretrained models, and its robustness performance is less sensitive to network width.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a new convolutional neural network (CNN) pooling method, frequency pooling (F-pooling), which is both shift-equivalent and anti-aliasing in theory. F-pooling is based on the Discrete Fourier Transform (DFT) and Nyquist sampling theorem, and is proven to be the optimal anti-aliasing down sampling operation from the perspective of reconstruction. Experiments on CIFAR-100 and a subset of ImageNet demonstrate that F-pooling increases accuracy and robustness w.r.t shifts of commonly-used network architectures.",
        "Abstract": "Convolutional layer utilizes the shift-equivalent prior of images which makes it a great success for image processing. However, commonly used down sampling methods in convolutional neural networks (CNNs), such as max-pooling, average-pooling, and strided-convolution, are not shift-equivalent. This destroys the shift-equivalent property of CNNs and degrades their performance. In this paper, we propose a novel pooling method which is \\emph{strict shift equivalent and anti-aliasing} in theory. This is achieved by (inverse) Discrete Fourier Transform and we call our method frequency pooling. Experiments on image classifications show that frequency pooling improves accuracy and robustness w.r.t shifts of CNNs. ",
        "Introduction": "  INTRODUCTION Convolutional neural networks (CNNs) have achieved great success on image processing  Goodfel- low et al. (2016) , natural language processing  Yin et al. (2017) , game playing  Mnih et al. (2013)  and so on. One of the reasons is that convolutions utilize the shift-equivalent prior of signals. Modern CNNs include not only convolutional layers but also down sampling or pooling layers. As an im- portant part of CNNs, pooling layers are used to reduce spatial resolution of feature maps, aggregate spatial information, and reduce computational and memory cost. Based on classical Nyquist sampling theorem  Nyquist (1928) , the sampling rate must be at least as twice as the highest frequency of a signal. Otherwise, frequency aliasing will appear, i.e. high- frequencies of the signal alias into low-frequencies. This leads sub-optimal when reconstruction the signals and misleads the following processing because orthogonal components are mixed again. To anti-alias, a traditional solution is that applying low-pass filter to the signal before down sampling it. Following the spirit of blurred down sampling, early CNNs use average pooling to achieve down sampling  Lecun et al. (1989) . Later, empirical evidence suggests max-pooling  Scherer et al. (2010)  and stride convolutions  Long et al. (2015)  provide better performance. They are widely used in CNNs but they don't consider about anti-aliasing. Shift-equivalent is another expected property of pooling. Otherwise it will destroy the shift- equivalent of CNNs and thus the shift-equivalent prior of signals is not fully utilized. Unfortunately, most commonly used poolings are not shift-equivalent. Worse, small shifts in the input can drasti- cally change the output when stacking multiple max-pooling or stride convolutions  Engstrom et al. (2017) ;  Azulay & Weiss (2018) ;  Zhang (2019) . Shift-equivalent is believed to be a fundamental property of CNNs. However, the fact that CNNs with poolings are not shift-equivalent has been ignored by the community. Until recently,  Zhang (2019)  propose anti-aliasing pooling (AA-pooling) by low-pass filtering before down sampling. They observe increased accuracy and better generalization on image classification when low-pass filtering is integrated correctly. Specifically, they decompose a pooling operator with down sampling factor s into two parts: a pooling operator with factor 1 and a blur filter with factor s. Although AA- pooling reduces the aliasing effects and makes the outputs more stable w.r.t input shifts, it is neither strict shift-equivalent nor anti-aliasing in theory. In this paper, we propose a strict shift equivalent and anti-aliasing pooling in theory. We first transform a signal or image into frequency domain via Discrete Fourier Transform (DFT). Then we only retain its low frequencies, i.e. the frequencies which are smaller than Nyquist sampling Under review as a conference paper at ICLR 2020 rate. Finally, we transform the low frequencies back into time domain via inverse DFT. We call our method frequency pooling (F-pooling). Note that a similar method is proposed in  Ryu et al. (2018) . However, they only focus on classification accuracy. Shift equivalent of their method is not evaluated in both theory and practice. Compared with previous works, the novelties and contributions of F- pooling are summarized as follows: • We formally define shift-equivalent of functions which contain down sampling operations. A suitable up sampling operation U must be involved in the definition. Without it, the definition for discrete signal is ill-posed. We believe a formal mathematical treatment has great value for further research. • We prove that F-pooling is the optimal anti-aliasing down sampling operation from the perspective of reconstruction. We also prove that F-pooling is shift equivalent. The up sampling operation U plays an important rule in our proofs. To best of our knowledge, F-pooling is the first pooling method which has those properties. • Experiments on CIFAR-100 and a subset of ImageNet demonstrate that F-pooling remark- ably increases accuracy and robustness w.r.t shifts of commonly-used network architec- tures. Moreover, the shift consistency of F-pooling is improved more when we replace zero padding of convolutions with circular padding.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a generalization bound for meta-learning that elucidates the tradeoff between task-specific and task-wise overfitting. It also proposes Discrete InfoMax Codes (DIMCO), a deep neural network model that outputs a discrete representation of each datapoint, which is designed to have a low value of a specific term in the generalization bound. Experiments demonstrate that DIMCO generalizes better than previous meta-learning methods when trained with small datasets, and is more memory- and time-efficient compared to previous image retrieval methods.",
        "Abstract": "This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.",
        "Introduction": "  INTRODUCTION Generalizing to unseen data is a problem of vital importance in machine learning. Many deep meta- learning methods optimize for generalization by directly minimizing the loss of held-out validation data. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class ( Snell et al., 2017 ;  Finn et al., 2017 ), learning unsupervised update rules that generalize to different domains ( Metz et al., 2018 ), and accelerating training procedures that are millions of steps long ( Flennerhag et al., 2018 ). However, the meta-learning setup introduces a new overfitting problem: the model may overfit to the distribution of tasks seen during training. In other words, the meta-learning framework decreases task-specific overfitting at the cost of introducing task-wise overfitting. The primary aim of this work is to elucidate this tradeoff between task-specific and task-wise overfitting in meta-learning. Specifically, we use tools from information theory to bound the overall generalization gap of a meta-learner. Analogously to how generalization bounds for standard learning algorithms reveal the role of dataset size in generalizing to new datapoints, our bound reveals the roles of both dataset size (m) and number of datasets (n) in generalizing to new tasks. The specific form of our generalization gap suggests that meta-learning models can be implicitly regularized by constructing them to express each datapoint with a small number of bits. To this end, we propose Discrete InfoMax COdes (DIMCO), a deep neural network model that outputs a discrete representation of each datapoint. Note that a continuous representationx ∈ R n (e.g.  Snell et al. (2017) ) uses 32n bits per datapoint. This is wildly inefficient in terms of bit-efficiency: our experiments in Section 5 show that DIMCO's discrete representation requires roughly 10× less bits per datapoint to achieve similar performance compared to continuous methods. DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Our specific contributions are: 1. Derive a generalization bound for meta-learning that makes the tradeoff between task- specific and task-wise overfitting concrete. 2. Propose DIMCO, a neural network model that is designed to have a low value of a specific term in our generalization bound. 3. Empirically demonstrate that DIMCO generalizes better than previous meta-learning meth- ods when trained with small datasets, and that it is more memory- and time-efficient compared to previous image retrieval methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel framework, Asynchronous Multi-Agent Generative Adversarial Imitation Learning (AMAGAIL), which allows agents to imitate expert demonstrations to make optimal decisions without direct interactions with the environment in asynchronous Markov Games. AMAGAIL inversely learns each expert's decision-making policy and is proven to guarantee subgame perfect equilibrium. The experiment results demonstrate that compared to GAIL and MAGAIL, AMAGAIL can better infer the policy of each expert agent using their demonstration data collected from asynchronous decision-making scenarios.",
        "Abstract": "Imitation learning aims to inversely learn a policy from expert demonstrations, which has been extensively studied in the literature for both single-agent setting with Markov decision process (MDP) model, and multi-agent setting with Markov game (MG) model. However, existing approaches for general multi-agent Markov games are not applicable to multi-agent extensive Markov games, where agents make asynchronous decisions following a certain order, rather than simultaneous decisions. We propose a novel framework for asynchronous multi-agent generative adversarial imitation learning (AMAGAIL) under general extensive Markov game settings, and the learned expert policies are proven to guarantee subgame perfect equilibrium (SPE), a more general and stronger equilibrium than Nash equilibrium (NE). The experiment results demonstrate that compared to state-of-the-art baselines, our AMAGAIL model can better infer the policy of each expert agent using their demonstration data collected from asynchronous decision-making scenarios (i.e., extensive Markov games).",
        "Introduction": "  INTRODUCTION Imitation learning (IL) also known as learning from demonstrations allows agents to imitate expert demonstrations to make optimal decisions without direct interactions with the environment. Espe- cially, inverse reinforcement learning (IRL) ( Ng et al. (2000) ) recovers a reward function of an expert from collected demonstrations, where it assumes that the demonstrator follows an (near-)optimal policy that maximizes the underlying reward. However, IRL is an ill-posed problem, because a number of reward functions match the demonstrated data ( Ziebart et al. (2008 ; 2010);  Ho & Ermon (2016) ; Boularias et al. (2011)), where various principles, including maximum entropy, maximum causal entropy, and relative entropy principles, are employed to solve this ambiguity ( Ziebart et al. (2008 ; 2010); Boularias et al. (2011);  Ho & Ermon (2016) ;  Zhang et al. (2019) ). Going beyond imitation learning with single agents discussed above, recent works including  Song et al. (2018) , Yu et al. (2019) , have investigated a more general and challenging scenario with demon- stration data from multiple interacting agents. Such interactions are modeled by extending Markov decision processes on individual agents to multi-agent Markov games (MGs) ( Littman & Szepesvári (1996) ). However, these works only work for synchronous MGs, with all agents making simultane- ous decisions in each turn, and do not work for general MGs, allowing agents to make asynchronous decisions in different turns, which is common in many real world scenarios. For example, in multi- player games ( Knutsson et al. (2004) ), such as Go game, and many card games, players take turns to play, thus influence each other's decision. The order in which agents make decisions has a significant impact on the game equilibrium. In this paper, we propose a novel framework, asynchronous multi-agent generative adversarial imita- tion learning (AMAGAIL): A group of experts provide demonstration data when playing a Markov game (MG) with an asynchronous decision-making process, and AMAGAIL inversely learns each expert's decision-making policy. We introduce a player function governed by the environment to capture the participation order and dependency of agents when making decisions. The participa- tion order could be deterministic (i.e., agents take turns to act) or stochastic (i.e., agents need to take actions by chance). A player function of an agent is a probability function: given the perfectly known agent participation history, i.e., at each previous round in the history, we know which agent(s) participated, it provides the probability of the agent participating in the next round. With the gen- eral MG model, our framework generalizes MAGAIL ( Song et al. (2018) ) from the synchronous Under review as a conference paper at ICLR 2020 Markov games to (asynchronous) Markov games, and the learned expert policies are proven to guar- antee subgame perfect equilibrium (SPE) ( Fudenberg & Levine (1983) ), a stronger equilibrium than the Nash equilibrium (NE) (guaranteed in MAGAIL  Song et al. (2018) ). The experiment results demonstrate that compared to GAIL ( Ho & Ermon (2016) ) and MAGAIL ( Song et al. (2018) ), our AMAGAIL model can better infer the policy of each expert agent using their demonstration data collected from asynchronous decision-making scenarios.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a benchmark of controlled real-world noisy labels, building on two existing datasets for coarse and fine-grained image classification. Through a large-scale study comparing synthetic and real-world noise, the authors find that DNNs generalize much better on real-world noise than synthetic noise, and that ImageNet architectures generalize well on noisy data when networks are fine-tuned. The benchmark and findings of this paper will facilitate future deep learning research on real-world noisy data.",
        "Abstract": "Performing controlled experiments on noisy data is essential in thoroughly understanding deep learning across a spectrum of noise levels. Due to the lack of suitable datasets, previous research have only examined deep learning on controlled synthetic noise, and real-world noise has never been systematically studied in a controlled setting. To this end, this paper establishes a benchmark of real-world noisy labels at 10 controlled noise levels. As real-world noise possesses unique properties, to understand the difference, we conduct a large-scale study across a variety of noise levels and types, architectures, methods, and training settings. Our study shows that: (1) Deep Neural Networks (DNNs) generalize much better on real-world noise. (2) DNNs may not learn patterns first on real-world noisy data. (3) When networks are fine-tuned, ImageNet architectures generalize well on noisy data. (4) Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. (5) Robust learning methods that work well on synthetic noise may not work as well on real-world noise, and vice versa. We hope our benchmark, as well as our findings, will facilitate deep learning research on noisy data.\n",
        "Introduction": "  INTRODUCTION Deep Neural Networks (DNNs) trained on noisy data demonstrate intriguing properties. For exam- ple, DNNs are capable of memorizing completely random training labels but generalize poorly on clean test data (Zhang et al., 2017). When trained with stochastic gradient descent, DNNs learn patterns first before memorizing the label noise (Arpit et al., 2017). These findings inspired recent research on noisy data. As training data are usually noisy, the fact that DNNs are able to memorize the noisy labels highlights the importance of deep learning research on noisy data. To study DNNs on noisy data, previous work often performs controlled experiments by injecting a series of synthetic noises into a well-annotated dataset. The noise level p may vary in the range of 0%-100%, where p = 0% is the clean dataset whereas p = 100% represents the dataset of zero correct labels. The most commonly used noise in the literature is uniform (or symmetric) label- flipping noise, in which the label of each example is independently and uniformly changed to a random (incorrect) class with probability p. Controlled experiments on noise levels are essential in thoroughly understanding a DNN's properties across a spectrum of noise levels and faithfully com- paring the strengths and weaknesses of different methods. The synthetic noise enables researchers to experiment on controlled noise levels, and drives the development of theory and methodology in this field. On the other hand, some studies were also verified on real-world noisy datasets, e.g. on WebVision (Li et al., 2017a), Clothing-1M (Xiao et al., 2015), Fine-grained Images (Krause et al., 2016), and Instagram hashtags (Mahajan et al., 2018), where the images are automatically tagged with noisy labels according to their surrounding texts. However, these datasets do not provide true labels for the training images. Their underlying noise levels are not only fixed but also unknown, rendering them infeasible for controlled studies on noise levels. In this paper, we refer image-search noise in these datasets as \"real-world noise\" to distinguish it from synthetic label-flipping noise. To study real-world noise in a controlled setting, we establish a benchmark of controlled real-world noisy labels, building on two existing datasets for coarse and fine-grained image classification: Mini- ImageNet (Vinyals et al., 2016) and Stanford Cars (Krause et al., 2013). We collect noisy labels using text-to-image and image-to-image search via Google Image Search. Every training image is Under review as a conference paper at ICLR 2020 independently annotated by 3-5 workers, resulting in a total of 527,489 annotations over 147,108 images. We create ten different noise levels from 0% to 80% by gradually replacing the original images with our annotated noisy images. Our new benchmark will enable future research on the real-world noisy data with a controllable noise level. We find that real-world noise possesses unique properties in its visual/semantic relevance and un- derlying class distribution. To understand the differences, we conduct a large-scale study comparing synthetic noise, namely blue-pilled noise (or Blue noise), and real-world noise (or Red noise 1 ). Specifically, we train DNNs across 10 noise levels, 7 network architectures, 6 existing robust learn- ing methods, and 2 training settings (fine-tuning and training from random initialization). Our study reveals several interesting findings. First, we find that DNNs generalize much better on real-world noise than synthetic noise. Our results verify Zhang et al. (2017)'s finding of deep learning generalization on synthetic noise. However, we observe a considerably smaller general- ization gap on real-world noise. This does not mean that real-world noise is easier to tackle. On the contrary, we find that real-world noise is more difficult for robust DNNs to improve. Second, our results substantiate Arpit et al. (2017)'s finding that DNNs learn patterns first on noisy data. But we find this behavior becomes insignificant on real-world noise and completely disappears on the fine-grained classification dataset. This finding lets us rethink the role of \"early stopping\" (Yao et al., 2007; Arpit et al., 2017) on real-world noisy data. Third, we find that when networks are fine-tuned, ImageNet architectures generalize well on noisy data, with a correlation of r = 0.87 and 0.89 for synthetic and real-world noise, respectively. This finding generalizes Kornblith et al. (2019)'s finding, i.e. ImageNet architectures generalize well across clean datasets, to the noisy data. Our contribution is twofold. First, we establish a large benchmark of controlled real image search noise. Second, we conduct perhaps the largest study in the literature to understand DNN training across a wide variety of noise levels and types, architectures, methods, and training settings. We hope our benchmark along with our findings, resulted from a considerable amount of manual label- ing effort (∼520K annotations) and computing resources (∼3K experiments), will facilitate future deep learning research on real-world noisy data. Our main findings are summarized as follows: 1. DNNs generalize much better on real-world noise than synthetic noise. Real-world noise appears to be less harmful, yet it is more difficult for robust DNN methods to improve. 2. DNNs may not learn patterns first on the real-world noisy data. 3. When networks are fine-tuned, ImageNet architectures generalize well on noisy data. 4. Adding noisy examples to a clean dataset may improve performance as long as the noise level is below a certain threshold (30% in our experiments).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Boosting Network (BoN) paradigm for efficiently and simultaneously learning both the structures and parameters of a neural network from arbitrary classes on mobile devices or even Internet of Things (IoT) devices. The proposed method is based on the Split Linearized Bregman Iteration (SplitLBI) and consists of two parts of growing both filters and layers, i.e., Growing and Training filters algorithm (GT-filters Alg), and Growing and Training layers algorithm (GT-layers Alg). Experiments on MNIST, Cifar-10, and Cifar-100 datasets show that the proposed GT-Net Alg can achieve comparable or even better performance than the competitors, with much less computational cost, and smaller size of found network.",
        "Abstract": "Network structures are important to learning good representations of many tasks in computer vision and machine learning communities. These structures are either manually designed, or searched by Neural Architecture Search (NAS) in previous works, which however requires either expert-level efforts, or prohibitive computational cost. In practice, it is desirable to efficiently and simultaneously learn both the structures and parameters of a network from arbitrary classes with budgeted computational cost. We identify it as a new learning paradigm -- Boosting Network, where one starts from simple models, delving into complex trained models progressively.\nIn this paper, by virtue of an iterative sparse regularization path -- Split Linearized Bregman Iteration (SplitLBI), we propose a simple yet effective boosting network method that can simultaneously grow and train a network by progressively adding both convolutional filters and layers. Extensive experiments with VGG and ResNets validate the effectiveness of our proposed algorithms.",
        "Introduction": "  INTRODUCTION In recent years, deep convolution neural networks have made remarkable achievements in com- pute vision and machine learning communities in addressing many important tasks, such as image classification, and segmentation. Researchers had designed many successful Deep Neural Network (DNN) architectures, from just have a few convolution layers like LeNet ( LeCun et al., 1998 ) and AlexNet ( Krizhevsky et al., 2012 ), to have more than 10 layers, e.g., VGG ( Simonyan & Zisserman, 2014 ) and GoogleLeNet ( Szegedy et al., 2015 ), and even have hundreds and thousands of layers like ResNet ( He et al., 2016a ). Designing a neural network architecture requires expert-level efforts to specify the key network hyper-parameters, such as type of layers, number of filters and layers (i.e., network width and depth) and so on. Since the capacity of over-parameterized networks largely depends on the number of total parameters, the number of filters and layers of networks are the key hyper-parameters that shape the expressive power of neural networks. In machine learning communities, most researchers resort to AutoML methods, e.g., Neural Ar- chitecture Search (NAS), in automating architecture engineering. Critically, NAS methods indeed surpass the manually designed architectures on many tasks, such as image classification and object detection ( Zoph & Le, 2016 ;  Zoph et al., 2018 ). To search a good architecture, various search strate- gies have been employed, such as random search, Bayesian optimization, reinforcement learning, and so on. Most of them require significant amount of computational cost, which is normally orders of magnitude higher than training a network. Furthermore, some of the found architectures by NAS have much more parameters than manually designed ones on the same dataset. As the field of representation learning moves closer towards artificial intelligence, it becomes im- portant to efficiently and simultaneously learn both the structures and parameters of a network from arbitrary classes on mobile devices or even Internet of Things (IoT) devices. This requires more flexible strategies in dynamically handling the network width and depth, according to the scale of dataset. To this end, this paper studies a new paradigm - Boosting network (BoN), where one starts from simple models, delving into complex trained models progressively. Specifically, BoN could simultaneously grow the structures and train the parameters from a simple initialized network on the data gradually to complex ones. Formally, we demand the following properties of an algorithm qualified as BoN: Under review as a conference paper at ICLR 2020 • It should incorporate both architecture growth (including filters and layers) and parameter learning simultaneously, in which the width and depth of network can be gradually updated, and the parameters of network should be updated at the same time; • It should provide a comparable classifier for prediction tasks, as the state-of-the-art hand- crafted architectures on the same dataset; • Its computational requirements, the total parameters of final boosted network, and memory footprint should remain bounded, ideally in the same order of magnitude as training a manually engineered architecture on the same dataset. The first two criteria express the essence of boosting network; the third criterion identifies the key difference from NAS and other trivial or brute-force solutions, such as randomly searching. This paper proposes a method for the BoN task based on the Split Linearized Bregman Iteration (SplitLBI) ( Huang et al., 2016 ;  Fu et al., 2019 ), originally proposed by  Huang et al. (2016)  to learn high dimensional sparse linear models and found applications in medical image classification ( Sun et al., 2017 ), computer vision ( Zhao et al., 2018 ), and training neural networks ( Fu et al., 2019 ). Particularly, based on differential inclusions of inverse scale spaces ( Huang et al., 2018 ), SplitLBI has the merit of learning both an over-parameterized model weight set (Over-Par set) as the Stochas- tic Gradient Descent (SGD), and structural sparsity model weight set (Stru-Spa set) in a coupled inverse scale space. Essentially, SplitLBI optimizes the Stru-Spa set as sparse approximation of the Over-Par set, by gradually selecting the important filters and weights from Over-Par set, along the training epochs. Equipped with SplitLBI, our key idea of BoN comes from progressively growing networks by check- ing the parameters within Over-Par and Stru-Spa set. Essentially along the training epochs, if enough parameters in Over-Par set have been selected in Stru-Spa set, it would be more advisable to increase the capacity of Over-Par Set by adding new parameters. Formally, to boosting a network, we introduce a Growing and Training Network Algorithm (GT-Net Alg), consisting of two parts of growing both filters and layers, i.e., Growing and Training filters algorithm (GT-filters Alg), and Growing and Training layers algorithm (GT-layers Alg). Given an initial network, the GT-filters Alg can effectively grow the filters of each layer, and train the network parameters at the same time. Furthermore, the GT-layers Alg firstly employs GT-filters Alg to com- pute the filter configuration for the layers of each block, and then periodically check whether to add new layer to the block along the training procedure. We conduct extensive experiments on several benchmark datasets, including MNIST, Cifar-10, and Cifar-100. It shows that our GT-Net Alg can achieve comparable or even better performance than the competitors, with much less computational cost, and smaller size of found network. This indicates the effectiveness of our proposed algorithms. Up to our knowledge, this is the first time that a BoN type algorithm of all the three aspects above is addressed in literature.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes two metric functions to measure the distributional difference between real text and generated text, and a method to estimate them. Through experiments, it is shown that there is an obvious discrepancy between the real text and the generated text even when it is generated by a well-adjusted language model. Additionally, it is demonstrated that the feedback signal from the discriminator cannot improve the generator using existing methods. Finally, experiments on two existing language GANs, SeqGAN and RelGAN, show that the distributional discrepancy between real text and generated text increases with more adversarial learning rounds, demonstrating that both of these language GANs fail.",
        "Abstract": "The text generated by neural language models is not as good as the real text. This means that their distributions are different. Generative Adversarial Nets (GAN) are used to alleviate it. However, some researchers argue that GAN variants do not work at all. When both sample quality (such as Bleu) and sample diversity (such as self-Bleu) are taken into account, the GAN variants even are worse than a well-adjusted language model. But, Bleu and self-Bleu can not precisely measure this distributional discrepancy. In fact, how to measure the distributional discrepancy between real text and generated text is still an open problem. In this paper, we theoretically propose two metric functions to measure the distributional difference between real text and generated text. Besides that, a method is put forward to estimate them. First, we evaluate language model with these two functions and find the difference is huge. Then, we try several methods to use the detected discrepancy signal to improve the generator. However the difference becomes even bigger than before. Experimenting on two existing language GANs, the distributional discrepancy between real text and generated text increases with more adversarial learning rounds. It demonstrates both of these language GANs fail. ",
        "Introduction": "  INTRODUCTION Text generation by neural language models (LM), such as LSTM ( Hochreiter & Schmidhuber, 1997 ) have given rise to much progress and are now used to dialogue generation ( Li et al., 2017 ), machine translation ( Wu et al., 2016 ) and image caption ( Xu et al., 2015 ). However, the generated sentences are still poor in semantics or global coherence, even not perfect grammatically speaking ( Caccia et al., 2019 ). It means that the discrepancy between generated text and real text is large. One reason is the ar- chitecture and parameters' number of LM itself ( Radford et al., 2019 ;  Santoro et al., 2018 ). Many researchers attribute it to the exposure bias ( Bengio et al., 2015 ) because the LM is trained with a maximum likelihood estimate (MLE) and predicts the next word conditioned on words from the ground-truth during training. But it only conditions on the words generated by itself during refer- ence. Statistically, this discrepancy means the two distributional functions of real texts and generated texts is different. Reducing this distributional difference may be a practicable way to improve text generation. Some researchers try to reduce this difference with GAN ( Goodfellow et al., 2014 ). They use a discriminator to detect the discrepancy between real samples and generated samples, and feed the signal back to upgrade the generator (a LM). In order to solve the non-differential issue that arises by the need to handle discrete tokens, reinforcement learning (RL) ( Williams, 1992 ) is adapted by SeqGAN ( Yu et al., 2017 ), RankGAN ( Lin et al., 2017 ), and LeakGAN ( Guo et al., 2018 ). The Gumble-Softmax is also introduced by GSGAN ( Jang et al., 2017 ) and RelGAN ( Nie et al., 2019 ) to solve this issue. These language GANs pre-train both the generator (G) and the discriminator (D) before adversarial learning 1 . During adversarial learning, for each round, the G is trained several epochs and then, the D is trained tens of epochs. Learning stops when the model converges. Furthermore, considering the generated texts' quality and diversity simultaneously ( Shi et al., 2018 ), Under review as a conference paper at ICLR 2020 MaskGAN ( Fedus et al., 2018 ), DpGAN ( Xu et al., 2018 ), FMGAN ( Chen et al., 2018 ) and RelGAN ( Nie et al., 2019 ) are proposed. They evaluate the generated text with Bleu and self-Bleu ( Zhu et al., 2018 ) or LM socre and reverse LM score ( Cífka et al., 2018 ), and claim these GANs improve the performance of generator. However recently questions have been rasied over these claims.  Semeniuta et al. (2018)  and  Cac- cia et al. (2019)  showed that via more precise experiments and evaluation, these considered GAN variants are defeated by a well-adjusted language model . d' Autume et al. (2019)  trained language GANs from scratch, nevertheless, they only achieve the \"comparable\" performance against LM.  He et al. (2019)  quantifies the exposure bias and concludes it is either 3 percent lower in performance or indistinguishable. All the aforementioned methods treat GAN as a black box for evaluation. For those language GANs, there are several critical issues such as whether the D detects the discrepancy or not; the detected discrepancy is severe or not, the signals from D could improve the generator or not are still un- clear. In this paper, we try to solve these problems via investigating GAN in both pre-training and the adversarial learning process. Theoretically analysing the signal from D, we obtain two met- ric functions to measure the distributional difference. With these two functions, we first measure the difference between the real text and the generated text by a MLE-trained language model (pre- train). Second, we try some methods to update generator with feedback signal from D, then, we use these metric functions to evaluate the updated generator. Finally, we analysis the existing language GANs during the adversarial learning with these two functions. All the code and data could be find https://github.com/. Our contributions are as follows: • We propose two metric functions to measure the distributional difference between real text and generated text. Besides that, a method is put forward to estimate them. • Evaluated using these two functions, a number of experiment show there is an obvious discrepancy between the real text and the generated text even when it is generated by a well-adjusted language model. • Although this discrepancy could be detected by D, the feedback signal from D can not improve G using existing methods. • Experimenting on two existing language GANs, SeqGAN and RelGAN, the distributional discrepancy between real text and generated text increases with more adversarial learning rounds. It demonstrates both of these language GANs fail.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a refined adversarial image captioning (AIRL) method to generate natural and diverse descriptions of images. The AIRL method disentangles reward for each action (i.e., word in a sentence) from different image-caption pairs and learns a compact reward function. A conditional term is introduced in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. The discriminator evaluates captions using a learned compact reward function, and the generator produces qualitative image descriptions. Experiments demonstrate the effectiveness of the proposed method.",
        "Abstract": "Adversarial learning has shown its advances in generating natural and diverse descriptions in image captioning. However, the learned reward of existing adversarial methods is vague and ill-defined due to the reward ambiguity problem. In this paper, we propose a refined Adversarial Inverse Reinforcement Learning (rAIRL) method to handle the reward ambiguity problem by disentangling reward for each word in a sentence, as well as achieve stable adversarial training by refining the loss function to shift the stationary point towards Nash equilibrium. In addition, we introduce a conditional term in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. Our experiments on MS COCO show that our method can learn compact reward for image captioning.",
        "Introduction": "  INTRODUCTION Image captioning is a task of generating descriptions of a given image in natural language. In a general encoder-decoder structure ( Vinyals et al., 2015 ), image features are encoded in a CNN and decoded into a caption in a word by word manner. Based on the loss function, standard approaches to the problem could be divided into three categories: MLE (Maximum Likelihood Estimation), RL (Reinforcement Learning) and GAN (Generative Adversarial Network). Early proposed methods were based on MLE function and made improvements by designing specific model structure ( Xu et al., 2015 ). MLE adopts the cross-entropy loss and learns a one-hot distribution for each word in the sentence. By maximizing the probability of the ground truth word whilst suppressing other reasonable vocabularies, the probability distribution learned by MLE tends to be sparse and the generated captions have limited diversity ( Dai et al., 2017 ). On the other hand, RL has advantages in boosting the model performance by optimizing the handcrafted metrics ( Rennie et al., 2017 ;  Liu et al., 2017 ;  Chen et al., 2019 ). However, due to the reward hacking problem, RL maximizes the reward in an unintended way and fails to produce human-like descriptions ( Li et al., 2019a ). Considering naturalness and diversity of the generated captions, GAN has raised attention in image captioning for its capability of producing descriptions that are indistinguishable from human-written ones ( Dai et al., 2017 ;  Shetty et al., 2017 ;  Chen et al., 2019 ;  Dognin et al., 2019 ). In image captioning, the generator of GAN learns true data distribution by maximizing the reward function learned from a discriminator, and the discriminator distinguishes the generated sample from the true data. The adversarial training converges to an equilibrium point (i.e., Nash equilibrium) at which both the generator and discriminator cannot improve ( Goodfellow et al., 2014 ). As shown in  Figure 1 , the learned distribution of GAN is closer to the ground truth distribution than that of other methods (i.e., MLE and RL) on different splits. However, previous work of adversarial networks in image captioning gives one reward function D for a complete sentence consisting of n words. This strategy causes the reward ambiguity problem ( Ng et al., 1999 ) since which word(s) causes the reward to increase or decrease is not accounted for, and thus there are many optimal policies that determine the sentence can explain one reward. An example is that each image in MS COCO has five ground truth captions. Although these captions may vary in formats, each caption has the same reward value in GAN. From the perspective on the system level, learning sentence-level reward from different image-caption pairs is analogous to learning reward of a trajectory from different system dynamics, which makes the discriminator unable to distinguish the true reward functions from those shaped by the environment dynamics (Fu et al., 2018). Facing the challenge, we adopt AIRL (Fu et al., 2018) to solve the reward ambiguity problem by disentangling reward for each action (i.e., word in a sentence) from different image-caption pairs and learning a compact reward function. compact means words with similar semantics, such as children and kids, correspond to close reward values. Driven by the compact reward function of the discriminator, the generator learns the optimal policy and thus produces qualitative descriptions. However, there are still two major problems to address: 1) AIRL is difficult to converge to Nash equilibrium using policy gradient (See Section 4.2 for details); 2) AIRL is designed without mode control, and thus the outputs have limited diversity, which is a commonly encountered issue called mode collapse ( Mirza & Osindero, 2014 ). In this paper, we propose a refined AIRL method to learn a compact reward function for each word, as well as achieve stable adversarial training by refining the loss function to shift the stationary point towards Nash equilibrium. The refined method makes it possible to reach the equilibrium point for a non-concave model function of the generator. In addition, a conditional term is introduced in the loss function to mitigate mode collapse and to increase the diversity of the generated descriptions. Both the caption evaluator (i.e., discriminator) ( Cui et al., 2018 ;  Sharif et al., 2018 ) and the generator are cast into this unified framework, where the discriminator evaluates captions using a learned compact reward function, and the generator produces qualitative image descriptions. We demonstrate the effectiveness of our method in the experiments.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel technique for protecting the privacy of sensitive training data while maintaining the accuracy of the student model in the teacher-student learning framework. The proposed method, immutable noisy ArgMax, adds a large constant to the current largest count of the count vector, which enables the student model to achieve improved privacy and utility over previous teacher-student based methods. The paper demonstrates that the proposed method improves the performance of the student model on all measures.",
        "Abstract": "Ensuring the privacy of sensitive data used to train modern machine learning models is of paramount importance in many areas of practice. One approach to study these concerns is through the lens of differential privacy. In this framework, privacy guarantees are generally obtained by perturbing models in such a way that specifics of data used to train the model are made ambiguous. A particular instance of this approach is through a ``teacher-student'' model, wherein the teacher, who owns the sensitive data, provides the student with useful, but noisy, information, hopefully allowing the student model to perform well on a given task without access to particular features of the sensitive data. Because stronger privacy guarantees generally involve more significant noising on the part of the teacher, deploying existing frameworks fundamentally involves a trade-off between utility and privacy guarantee. One of the most important techniques used in previous work involves an ensemble of teacher models, which return information to a student based on a noisy voting procedure.  In this work, we propose a novel voting mechanism, which we call an Immutable Noisy ArgMax, that, under certain conditions, can bear very large random noising from the teacher without affecting the useful information transferred to the student. Our mechanisms improve over the state-of-the-art methods on all measures, and scale to larger tasks with both higher utility and stronger privacy ($\\epsilon \\approx 0$).",
        "Introduction": "  INTRODUCTION Recent years have witnessed impressive breakthroughs of deep learning in a wide variety of do- mains, such as image classification ( He et al., 2016 ), natural language processing ( Devlin et al., 2018 ), reinforcement learning ( Silver et al., 2017 ), and many more. Many attractive applications involve training models using highly sensitive data, to name a few, diagnosis of diseases with med- ical records or genetic sequences ( Alipanahi et al., 2015 ), mobile commerce behavior prediction ( Yan, 2017 ), and location-based social network activity recognition ( Gong et al., 2018 ). However, recent studies exploiting privacy leakage from deep learning models have demonstrated that private, sensitive training data can be recovered from released models ( Nicolas Papernot, 2017 ). Therefore, privacy protection is a critical issue in this context, and thus developing methods that protect sensi- tive data from being disclosed and exploited is of paramount importance. In order to protect the privacy of the training data and mitigate the effects of adversarial attacks, various privacy protection works have been proposed in the literature ( Michie et al., 1994 ;  Nissim et al., 2007 ;  Samangouei et al., 2018 ;  Ma et al., 2018 ). The \"teacher-student\" learning framework with privacy constraints is of particular interest here, since it can provide a private student model without touching any sensitive data directly ( Hamm et al., 2016 ;  Pathak et al., 2010 ; Papernot et al., 2017). The original purpose of a teacher-student framework is to transfer the knowledge from the teacher model to help train a student to achieve similar performance with the teacher. To satisfy the privacy-preserving need, knowledge from the teacher model is carefully perturbed with random noise, before being passed to the student model. In this way, one hopes that an adversary cannot ascertain the contributions of specific individuals in the original dataset even they have full access to the student model. Using the techniques of differential privacy, such protection can be guaranteed in certain settings. However, the current teacher-student frameworks (e.g.  Nicolas Papernot (2017)  and ( Papernot et al., 2018 )) involve a trade-off between student's performance and privacy. This Under review as a conference paper at ICLR 2020 is because the amount of noise perturbation required is substantial to ensure privacy at the desired level, which leads to degraded information passed to student and results in inaccurate models. In this paper, we develop a technique to address the aforementioned problem, which facilitates the deployment of accurate models with near zero privacy cost. Instead of using traditional noisy ArgMax, we propose a new approach named immutable noisy ArgMax as describe in Section 2. We redesign the aggregation approach via adding a large constant into the current largest count of the count vector, which enables immutable noisy ArgMax into teacher-student model. As a result, this method improves both privacy and utility over previous teacher-student based methods. The primary technical contributions of this paper is a novel mechanism for aggregating teachers' an- swers that are more immutable against larger noise without changing the consensus of the teachers. We show that our proposed method improves the performance of student model on all measures.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel reinforcement learning framework based on the information bottleneck (IB) framework to improve sample efficiency. The framework is derived from the \"information extraction-compression process\" observed in deep learning, and is optimized using the Stein variational gradient method. Experiments show that combining actor-critic algorithms with the IB framework is more sample-efficient than their original versions. The relationship between the IB framework and MINE is also analyzed, and an algorithm to optimize the IB framework without constructing the lower bound is derived.",
        "Abstract": "The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms.We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. \nWe incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information-bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE).\nWe experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.",
        "Introduction": "  Introduction In training a reinforcement learning algorithm, an agent interacts with the environment, ex- plores the (possibly unknown) state space, and learns a policy from the exploration sample data. In many cases, such samples are quite expensive to obtain (e.g., requires interactions with the physical environment). Hence, improving the sample efficiency of the learning al- gorithm is a key problem in RL and has been studied extensively in the literature. Popular techniques include experience reuse/replay, which leads to powerful off-policy algorithms (e.g., ( Mnih et al., 2013 ;  Silver et al., 2014 ;  Van Hasselt et al., 2015 ;  Nachum et al., 2018a ;  Espeholt et al., 2018 )), and model-based algorithms (e.g., (Hafner et al., 2018;  Kaiser et al., 2019 )). Moreover, it is known that effective representations can greatly reduce the sample complexity in RL. This can be seen from the following motivating example: In the envi- ronment of a classical Atari game: Seaquest, it may take dozens of millions samples to converge to an optimal policy when the input states are raw images (more than 28,000 di- mensions), while it takes less samples when the inputs are 128-dimension pre-defined RAM data( Sygnowski & Michalewski, 2016 ). Clearly, the RAM data contain much less redundant information irrelevant to the learning process than the raw images. Thus, we argue that an efficient representation is extremely crucial to the sample efficiency. In this paper, we try to improve the sample efficiency in RL from the perspective of representation learning using the celebrated information bottleneck framework ( Tishby et al., 2000 ). In standard deep learning, the experiments in ( Shwartz-Ziv & Tishby, 2017 ) show that during the training process, the neural network first \"remembers\" the inputs by increasing the mutual information between the inputs and the repre- sentation variables, then compresses the inputs to efficient representation related to Under review as a conference paper at ICLR 2020 the learning task by discarding redundant information from inputs (decreasing the mu- tual information between inputs and representation variables). We call this phenomena \"information extraction-compression process\" \"information extraction-compression process\" \"information extraction-compression process\"(information E-C process). Our experiments shows that, similar to the results shown in ( Shwartz-Ziv & Tishby, 2017 ), we first (to the best of our knowledge) observe the information extraction-compression phenomena in the context of deep RL (we need to use MINE(Belghazi et al., 2018) for estimating the mu- tual information). This observation motivates us to adopt the information bottleneck (IB) framework in reinforcement learning, in order to accelerate the extraction-compression pro- cess. The IB framework is intended to explicitly enforce RL agents to learn an efficient representation, hence improving the sample efficiency, by discarding irrelevant information from raw input data. Our technical contributions can be summarized as follows: 1. We observe that the \"information extraction-compression process\" also exists in the context of deep RL (using MINE(Belghazi et al., 2018) to estimate the mutual information). 2. We derive the optimization problem of our information bottleneck framework in RL. In order to solve the optimization problem, we construct a lower bound and use the Stein variational gradient method developed in ( Liu et al., 2017 ) to optimize the lower bound. 3. We show that our framework can accelerate the information extraction-compression process. Our experimental results also show that combining actor-critic algorithms (such as A2C, PPO) with our framework is more sample-efficient than their original versions. 4. We analyze the relationship between our framework and MINE, through this rela- tionship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound. Finally, we note that our IB method is orthogonal to other methods for improving the sample efficiency, and it is an interesting future work to incorporate it in other off-policy and model-based algorithms.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes FURL, a simple, scalable, resource-efficient, and privacy preserving method that enables existing collaborative personalization techniques to work in the Federated Learning (FL) setting with minimal changes. FURL splits the model into federated and private parameters, and provides formal constraints under which the parameter splitting does not affect model performance. Empirical results show that FURL significantly improves the performance of models in the FL setting, with improvements of 8% and 51% on two real-world datasets. Additionally, user embeddings learned in FL have a similar structure to those learned in centralized training.",
        "Abstract": "Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.",
        "Introduction": "  INTRODUCTION Collaborative personalization, like learning user embeddings jointly with the task, is a powerful way to improve accuracy of neural-network-based models by adapting the model to each user's behavior ( Grbovic & Cheng, 2018 ;  Ni et al., 2018 ;  Lee et al., 2017 ;  Jaech & Ostendorf, 2018 ;  McGraw et al., 2016 ;  Vosecky et al., 2014 ). However, model personalization usually assumes the availability of user data on a centralized server. To protect user privacy, it is desirable to train personalized models in a privacy-preserving way, for example, using Federated Learning ( McMahan et al., 2016 ; Konen et al., 2016b). Personalization in FL poses many challenges due to its distributed nature, high communication costs, and privacy constraints ( Li et al., 2019a ;  Bonawitz et al., 2019 ;  Caldas et al., 2018 ;  Li et al., 2019b ; 2018;  Liu et al., 2018 ;  Yang et al., 2019 ;  Konen et al., 2016a ). To overcome these difficulties, we propose a simple, communication-efficient, scalable, privacy- preserving scheme, called FURL, to extend existing neural-network personalization to FL. FURL can personalize models in FL by learning task-specific user representations (i.e., embeddings) ( Lerer et al., 2019 ;  Grbovic & Cheng, 2018 ;  Ni et al., 2018 ;  Lee et al., 2017 ;  Jaech & Ostendorf, 2018 ) or by personalizing model weights ( Tang & Wang, 2018 ). Research on collaborative personalization in FL ( Smith et al., 2017 ;  Sebastian Caldas, 2019 ;  Chen et al., 2018 ;  Yao et al., 2019 ) has generally focused on the development of new techniques tailored to the FL setting. We show that most existing neural-network personalization techniques, which satisfy the split-personalization constraint (1,2,3), can be used directly in FL, with only a small change to Federated Averaging ( McMahan et al., 2016 ), the most common FL training algorithm. Existing techniques do not efficiently train user embeddings in FL since the standard Federated Av- eraging algorithm ( McMahan et al., 2016 ) transfers and averages all parameters on a central server. Conventional training assumes that all user embeddings are part of the same model. Transferring all user embeddings to devices during FL training is prohibitively resource-expensive (in terms of communication and storage on user devices) and does not preserve user privacy. FURL defines the concepts of federated and private parameters: the latter remain on the user device instead of being transferred to the server. Specifically, we use a private user embedding vector on Under review as a conference paper at ICLR 2020 each device and train it jointly with the global model. These embeddings are never transferred back to the server. We show theoretically and empirically that splitting model parameters as in FURL affects neither model performance nor the inherent structure in learned user embeddings. While global model ag- gregation time in FURL increases linearly in the number of users, this is a significant reduction compared with other approaches ( Smith et al., 2017 ;  Sebastian Caldas, 2019 ) whose global aggre- gation time increases quadratically in the number of users. FURL has advantages over conventional on-server training since it exploits the fact that models are already distributed across users. There is little resource overhead in distributing the embedding table across users as well. Using a distributed embeddings table improves the memory locality of both training embeddings and using them for inference, compared to on-server training with a centralized and potentially very large user embedding table. Our evaluation of document classification tasks on two real-world datasets shows that FURL has similar performance to the server-only approach while preserving user privacy. Learning user em- beddings improves the performance significantly in both server training and FL. Moreover, user representations learned in FL have a similar structure to those learned in a central server, indicating that embeddings are learned independently yet collaboratively in FL. In this paper, we make the following contributions: • We propose FURL, a simple, scalable, resource-efficient, and privacy preserving method that enables existing collaborative personalization techniques to work in the FL setting with only min- imal changes by splitting the model into federated and private parameters. • We provide formal constraints under which the parameter splitting does not affect model per- formance. Most model personalization approaches satisfy these constraints when trained using Federated Averaging ( McMahan et al., 2016 ), the most popular FL algorithm. • We show empirically that FURL significantly improves the performance of models in the FL setting. The improvements are 8% and 51% on two real-world datasets. We also show that performance in the FL setting closely matches the centralized training with small reductions of only 0% and 4% on the datasets. • Finally, we analyze user embeddings learned in FL and compare with the user representations learned in centralized training, showing that both user representations have similar structures.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new sequence model, Meta Gated Recursive Controller Units (METAGROSS), which recursively parameterizes the recurrent unit and imbues the model with the ability to reason deeply and recursively about certain inputs. This formulation brings about benefits pertaining to modeling data that is intrinsically hierarchical (recursive) in nature, such as natural language, music and logic. This work is a propulsion on a different frontier, i.e., learning recursively parameterized models, and is distinct from syntax-guided composition and unsupervised grammar induction.",
        "Abstract": "This paper proposes Metagross (Meta Gated Recursive Controller), a new neural sequence modeling unit. Our proposed unit is characterized by recursive parameterization of its gating functions, i.e., gating mechanisms of Metagross are controlled by instances of itself, which are repeatedly called in a recursive fashion. This can be interpreted as a form of meta-gating and recursively parameterizing a recurrent model. We postulate that our proposed inductive bias provides modeling benefits pertaining to learning with inherently hierarchically-structured sequence data (e.g., language, logical or music tasks). To this end, we conduct extensive experiments on recursive logic tasks (sorting, tree traversal, logical inference), sequential pixel-by-pixel classification, semantic parsing, code generation, machine translation and polyphonic music modeling, demonstrating the widespread utility of the proposed approach, i.e., achieving state-of-the-art (or close) performance on all tasks.",
        "Introduction": "  INTRODUCTION Sequences are fundamentally native to the world we live in, i.e., language, logic, music and time are all well expressed in sequential form. To this end, the design of effective and powerful sequential inductive biases has far-reaching benefits across many applications. Across many of these domains, e.g., natural language processing or speech, the sequence encoder lives at the heart of many powerful state-of-the-art model architectures. Models based on the notion of recurrence have enjoyed pervasive impact across many applications. In particular, the best recurrent models operate with gating functions that not only ameliorate van- ishing gradient issues but also enjoy fine-grain control over temporal compositionality ( Hochreiter & Schmidhuber, 1997 ;  Cho et al., 2014 ). Specifically, these gating functions are typically static and trained via an alternate transformation over the original input. In this paper, we propose a new sequence model that recursively parameterizes the recurrent unit. More concretely, the gating functions of our model are now parameterized repeatedly by instances of itself which imbues our model with the ability to reason deeply 1 and recursively about certain inputs. To achieve the latter, we propose a soft dynamic recursion mechanism, which softly learns the depth of recursive parameterization at a per-token basis. Our formulation can be interpreted as a form of meta-gating since temporal compositionality is now being meta-controlled at various levels of abstractions. Our proposed method, Meta Gated Recursive Controller Units (METAGROSS), marries the benefits of recursive reasoning with recurrent models. Notably, we postulate that this formulation brings about benefits pertaining to modeling data that is instrinsically hierarchical (recursive) in nature, e.g., natural language, music and logic, an increasingly prosperous and emerging area of research ( Shen et al., 2018 ; Wang et al., 2019;  Choi et al., 2018 ). While the notion of recursive neural networks is not new, our work is neither concerned with syntax-guided composition ( Tai et al., 2015 ;  Socher et al., 2013 ;  Dyer et al., 2016 ) nor unsupervised grammar induction ( Shen et al., 2017 ;  Choi et al., 2018 ;  Havrylov et al., 2019 ;  Yogatama et al., 2016 ). Instead, our work is a propulsion on a different frontier, i.e., learning recursively parameterized models which bears a totally different meaning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new optimization-based approach for step size adaptation, called Step Size Optimization (SSO). SSO is formulated as a sub-optimization problem of the gradient methods, and is motivated by the goal of minimizing a linearized loss function for the current model parameter values and gradient. We present an efficient algorithm to solve this step size optimization problem based on the Alternating Direction Method of Multipliers (ADMM). We analytically and empirically show that the additional time complexity of SSO in the gradient methods is negligible in the training of the model. We compare the training performance of SSO-SGD and SSO-Adam with two state-of-the-art step size adaptation methods (L4 and AdaBound) as well as the most commonly used gradient methods (RMSProp and Adam) on extensive benchmark datasets.",
        "Abstract": "This paper proposes a new approach for step size adaptation in gradient methods. The proposed method called step size optimization (SSO) formulates the step size adaptation as an optimization problem which minimizes the loss function with respect to the step size for the given model parameters and gradients. Then, the step size is optimized based on alternating direction method of multipliers (ADMM). SSO does not require the second-order information or any probabilistic models for adapting the step size, so it is efficient and easy to implement. Furthermore, we also introduce stochastic SSO for stochastic learning environments. In the experiments, we integrated SSO to vanilla SGD and Adam, and they outperformed state-of-the-art adaptive gradient methods including RMSProp, Adam, L4-Adam, and AdaBound on extensive benchmark datasets.",
        "Introduction": "  INTRODUCTION First-order gradient methods (simply gradient methods) have been widely used to fit model param- eters in machine learning and data mining, such as training deep neural networks. In the gradient methods, step size (or learning rate) is one of the most important hyperparameters that determines the overall optimization performance. For this reason, step size adaptation has been extensively studied from various perspectives such as second-order information ( Byrd et al., 2016 ;  Schaul et al., 2013 ), Bayesian approach ( Mahsereci & Henning, 2015 ), learning to learn paradigm ( Andrychow- icz et al., 2016 ), and reinforcement learning (Li & Malik, 2017). However, they are hardly used in practice due to lack of solid empirical evidence for the step size adaptation performance, hard im- plementation, or huge computation. For these reasons, some heuristically-motivated methods such as AdaGrad ( Duchi et al., 2011 ), RMSProp ( Tieleman & Hinton, 2012 ), and Adam ( Kingma & Ba, 2015 ) are mainly used in practice to solve the large-scale optimization problems such as training deep neural networks. Recently, two impressive methods, called L 4 ( Rolinek & Martius, 2018 ) and AdaBdound ( Luo et al., 2019 ), were proposed to efficiently adapt the step size in training of models, and showed some improvement over existing methods without huge computation. However, performance comparisons to them were conducted only on relatively simple datasets such as MNIST and CIFAR-10, even though L 4 has several newly-introduced hyperparameters, and AdaBound needs manually-desgined bound functions. Moreover, L 4 still requires about 30% more execution time, and AdaBound lacks the time complexity analysis or empirical results on training performance against actual execution time. This paper proposes a new optimization-based approach for the step size adaptation, called step size optimization (SSO). In SSO, the step size adaptation is formulated as a sub-optimization problem of the gradient methods. Specifically, the step size is adapted to minimize a linearized loss function for the current model parameter values and gradient. The motivation of SSO and the justification for the performance improvement by SSO is clear because it directly optimizes the step size to minimize the loss function. We also present a simple and efficient algorithm to solve this step size optimiza- tion problem based on the alternating direction method of multipliers (ADMM) ( Gabay & Mercier, 1976 ). Furthermore, we provide a practical implementation of SSO on the loss function with L 2 regularization ( Krogh & Hertz, 1992 ) and stochastic SSO for the stochastic learning environments. SSO does not require the second-order information ( Byrd et al., 2016 ;  Schaul et al., 2013 ) and any probabilistic models ( Mahsereci & Henning, 2015 ) to adapt the step size, so it is efficient and easy to implement. We analytically and empirically show that the additional time complexity of SSO in Under review as a conference paper at ICLR 2020 the gradient methods is negligible in the training of the model. To validate the practical usefulness of SSO, we made two gradient methods, SSO-SGD and SSO-Adam, by integrating SSO to vanilla SGD and Adam. In the experiments, we compared the training performance of SSO-SGD and SSO- Adam with two state-of-the-art step size adaptation methods (L 4 and AdaBdound) as well as the most commonly used gradient methods (RMSProp and Adam) on extensive benchmark datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a novel approach to knowledge graph alignment, which requires a few or even no aligned triplets. This approach first represents the entities and relations in low-dimensional spaces and then learns mapping functions to align the entities and relations from the source knowledge graph to the target one. This approach is proven to be quite effective and is applicable to a variety of applications such as question answering and semantic search.",
        "Abstract": "This paper studies aligning knowledge graphs from different sources or languages. Most existing methods train supervised methods for the alignment, which usually require a large number of aligned knowledge triplets. However, such a large number of aligned knowledge triplets may not be available or are expensive to obtain in many domains. Therefore, in this paper we propose to study aligning knowledge graphs in fully-unsupervised or weakly-supervised fashion, i.e., without or with only a few aligned triplets. We propose an unsupervised framework to align the entity and relation embddings of different knowledge graphs with an adversarial learning framework. Moreover, a regularization term which maximizes the mutual information between the embeddings of different knowledge graphs is used to mitigate the problem of mode collapse when learning the alignment functions. Such a framework can be further seamlessly integrated with existing supervised methods by utilizing a limited number of aligned triples as guidance. Experimental results on multiple datasets prove the effectiveness of our proposed approach in both the unsupervised and the weakly-supervised settings.",
        "Introduction": "  INTRODUCTION Knowledge graphs represent a collection of knowledge facts and are quite popular in the real world. Each fact is represented as a triplet (h, r, t), meaning that the head entity h has the relation r with the tail entity t. Examples of real-world knowledge graphs include instances which contain knowledge facts from general domain (e.g., Freebase 1 , WordNet 2 ) or facts from specific domains such as biomedical ontology (e.g., UMLS 3 ). Knowledge graphs are critical to a variety of applications such as question answering ( Bordes et al., 2014 ) and semantic search ( Guha et al., 2003 ). Research on knowledge graphs is attracting growing interest recently in both academia and industry communities. In practice, each knowledge graph is usually constructed from a single source or language, the coverage of which is limited. To enlarge the coverage and construct more unified knowledge graphs, a natural idea is to integrate multiple knowledge graphs from different sources or languages ( Arens et al., 1993 ). However, different knowledge graphs use distinct symbol systems to represent entities and relations, which are not compatible. Therefore, it is critical to align the entities and relations across different knowledge graphs (a.k.a., knowledge graph alignment) before integrating them. Recently, many methods have been proposed to align entities and relations from a source knowledge graph to a target knowledge graph ( Zhu et al., 2017a ;  Chen et al., 2017a ;b;  Sun et al., 2018a ). These methods first represent the entities and relations in low-dimensional spaces and then learn mapping functions to align the entities and relations from the source knowledge graph to the target one. Though these methods are proven quite effective, they rely on a large number of aligned triplets for training supervised alignment models, and such aligned triplets may not be available or can be expensive to obtain. As a result, the performance of these methods will be comprised. Therefore, it would be desirable to design an unsupervised or weakly-supervised approach for knowledge graph alignment, which requires a few or even without aligned triplets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper considers the minimization of an optimization problem with L-smooth and µ-strongly convex components. We focus on first order methods for solving this problem, which access the Proximal Incremental First-order Oracle (PIFO) for each individual component. We prove the lower bound complexity of PIFO algorithm is Ω((n+ √ κn) log(1/ε)) for smooth and strongly-convex components, which means the existing Point-SAGA has achieved optimal complexity and PIFO can not lead to a tighter upper bound than IFO. We provide a novel construction, showing the above result by decomposing the classical tridiagonal matrix into n groups. This technique is also used to study general convex and average smooth cases, and extend our result to non-convex problems.",
        "Abstract": "This paper studies the lower bound complexity for the optimization problem whose objective function is the average of $n$ individual smooth convex functions. We consider the algorithm which gets access to gradient and proximal oracle for each individual component.\nFor the strongly-convex case, we prove such an algorithm can not reach an $\\eps$-suboptimal point in fewer than $\\Omega((n+\\sqrt{\\kappa n})\\log(1/\\eps))$ iterations, where $\\kappa$ is the condition number of the objective function. This lower bound is tighter than previous results and perfectly matches the upper bound of the existing proximal incremental first-order oracle algorithm Point-SAGA.\nWe develop a novel construction to show the above result, which partitions the tridiagonal matrix of classical examples into $n$ groups to make the problem difficult enough to stochastic algorithms. \nThis construction is friendly to the analysis of proximal oracle and also could  be used in general convex and average smooth cases naturally.",
        "Introduction": "  INTRODUCTION We consider the minimization of the following optimization problem min x∈R d f (x) 1 n n i=1 f i (x), (1) where the f i (x) are L-smooth and µ-strongly convex. Accordingly, the condition number is defined as κ = L/µ, which is typically larger than n in real-world applications. Many machine learning models can be formulated as the above problem such as ridge linear regression, ridge logistic re- gression, smoothed support vector machines, graphical models, etc. This paper focuses on the first order methods for solving Problem (1), which access to the Proximal Incremental First-order Oracle (PIFO) for each individual component, that is, h f (x, i, γ) f i (x), ∇f i (x), prox γ fi (x) , (2) where i ∈ {1, . . . , n}, γ > 0, and the proximal operation is defined as We also define the Incremental First-order Oracle (IFO) PIFO provides more information than IFO and it would be potentially more powerful than IFO in first order optimization algorithms. Our goal is to find an ε-suboptimal solutionx such that There are several first-order stochastic algorithms to solve Problem (1). The key idea to lever- age the structure of f is variance reduction which is effective for ill-conditioned problems. For example, SVRG ( Zhang et al., 2013 ;  Johnson and Zhang, 2013 ;  Xiao and Zhang, 2014 ) can Under review as a conference paper at ICLR 2020 find an ε-suboptimal solution in O((n+κ) log(1/ε)) IFO calls, while the complexity of the clas- sical Nesterov's acceleration ( Nesterov, 1983 ) is O(n √ κ log(1/ε)). Similar results 1 also hold for SAG ( Schmidt et al., 2017 ) and SAGA ( Defazio et al., 2014 ). In fact, there exists an ac- celerated stochastic gradient method with √ κ dependency.  Defazio (2016)  introduced a simple and practical accelerated method called Point SAGA, which reduces the iteration complexity to O((n + √ κn) log(1/ε)). The advantage of Point SAGA is in that it has only one parameter to be tuned, but the iteration depends on PIFO rather than IFO.  Allen-Zhu (2017)  proposed the Katyusha momentum to accelerate variance reduction algorithms, which achieves the same iteration complex- ity as Point-SAGA but only requires IFO calls. The lower bound complexities of IFO algorithms for convex optimization have been well studied ( Agarwal and Bottou, 2015 ;  Arjevani and Shamir, 2015 ;  Woodworth and Srebro, 2016 ;  Carmon et al., 2017 ;  Lan and Zhou, 2017 ;  Zhou and Gu, 2019 ). Specifically,  Lan and Zhou (2017)  showed that at least Ω((n+ √ κn) log(1/ε)) IFO calls 2 are needed to obtain an ε-suboptimal solution for some complicated objective functions. This lower bound is optimal because it matches the upper bound complexity of Katyusha ( Allen-Zhu, 2017 ). It would be interesting whether we can establish a more efficient PIFO algorithm than IFO one.  Woodworth and Srebro (2016)  provided a lower bound Ω(n+ √ κn log(1/ε)) for PIFO algorithms, while the known upper bound of the PIFO algorithm Point SAGA [3] is O((n+ √ κn) log(1/ε)). The difference of dependency on n implies that the existing theory of PIFO algorithm is not perfect. This gap can not be ignored because the number of components n is typically very large in many machine learning problems. A natural question is can we design a PIFO algorithm whose upper bound complexity matches Woodworth and Srebro's lower bound, or can we improve the lower bound complexity of PIFO to match the upper bound of Point SAGA. In this paper, we prove the lower bound complexity of PIFO algorithm is Ω((n+ √ κn) log(1/ε)) for smooth and strongly-convex f i , which means the existing Point-SAGA ( Defazio, 2016 ) has achieved optimal complexity and PIFO can not lead to a tighter upper bound than IFO. We provide a novel construction, showing the above result by decomposing the classical tridiagonal matrix ( Nesterov, 2013 ) into n groups. This technique is quite different from the previous lower bound complexity analysis ( Agarwal and Bottou, 2015 ;  Woodworth and Srebro, 2016 ;  Lan and Zhou, 2017 ;  Zhou and Gu, 2019 ). Moreover, it is very friendly to the analysis of proximal operation and easy to follow. We also use this technique to study general convex and average smooth cases ( Allen-Zhu, 2018 ;  Zhou and Gu, 2019 ), and extend our result to non-convex problems (see Appendix J).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on valuing data in relative terms when the data is used for supervised learning. The Shapley value is proposed to value data and is formulated as a cooperative game between different training points. Algorithmically, the Shapley value inspects the marginal contribution of a point to every possible subset of training data and averages the marginal contributions over all subsets. However, existing methods to estimate the Shapley value for general models are based on Monte Carlo approximation and require to re-train the ML model for a large amount of times, thus limiting their applicability to simple models and small training data size. This paper seeks to answer the question of how to value data when it comes to large models, such as neural networks, and massive datasets.",
        "Abstract": "This paper focuses on valuating training data for supervised learning tasks and studies the Shapley value, a data value notion originated in cooperative game theory. The Shapley value defines a unique value distribution scheme that satisfies a set of appealing properties desired by a data value notion. However, the Shapley value requires exponential complexity to calculate exactly. Existing approximation algorithms, although achieving great improvement over the exact algorithm, relies on retraining models for multiple times, thus remaining limited when applied to larger-scale learning tasks and real-world datasets.\n\nIn this work, we develop a simple and efficient algorithm to estimate the Shapley value with complexity independent with the model size. The key idea is to approximate the model via a $K$-nearest neighbor ($K$NN) classifier, which has a locality structure that can lead to efficient Shapley value calculation. We evaluate the utility of the values produced by the $K$NN proxies in various settings, including label noise correction, watermark detection, data summarization, active data acquisition, and domain adaption. Extensive experiments demonstrate that our algorithm achieves at least comparable utility to the values produced by existing algorithms while significant efficiency improvement. Moreover, we theoretically analyze the Shapley value and justify its advantage over the leave-one-out error as a data value measure.",
        "Introduction": "  INTRODUCTION Data valuation addresses the question of how to decide the worth of data. Data analysis based on machine learning (ML) has enabled various applications, such as targeted advertisement, autonomous driving, and healthcare, and creates tremendous business values; at the same time, there lacks a principled way to attribute these values to different data sources. Thus, recently, the problem of data valuation has attracted increasing attention in the research community. In this work, we focus on valuing data in relative terms when the data is used for supervised learning. The Shapley value has been proposed to value data in recent works (Jia et al., 2019b;a; Ghorbani & Zou, 2019). The Shapley value originates from cooperative game theory and is considered a classic way of distributing total gains generated by the coalition of a set of players. One can formulate supervised learning as a cooperative game between different training points and thus apply the Shapley value to data valuation. An important reason for employing the Shapley value is that it uniquely possesses a set of appealing properties desired by a data value notion, such as fairness and additivity of values in multiple data uses. Algorithmically, the Shapley value inspects the marginal contribution of a point to every possible subset of training data and averages the marginal contributions over all subsets. Hence, computing the Shapley value is very expensive and the exact calculation has exponential complexity. The existing works on the Shapley value-based data valuation have been focusing on how to scale up the Shapley value calculation to large training data size. The-state-of-art methods to estimate the Shapley value for general models are based on Monte Carlo approximation (Jia et al., 2019b; Ghorbani & Zou, 2019). However, these methods require to re-train the ML model for a large amount of times; thus, they are only applicable to simple models and small training data size. The first question we ask in the paper is: How can we value data when it comes to large models, such as neural networks, and massive datasets?",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Generative Latent Flow (GLF), a novel generative model that combines a deterministic auto-encoder with a normalizing flow prior to produce high quality samples from complex data distributions. GLF is based on the observation that increasing the weight of the reconstruction loss in VAEs with a normalizing flow prior leads to a vanishing noise limit that corresponds to a deterministic auto-encoder. Experiments show that GLF achieves state-of-the-art sample quality among competing AE based models, and has the additional advantage of faster convergence.",
        "Abstract": "In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior.  Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality and diversity among AE based models on commonly used datasets, and is competitive with GANs' benchmarks. ",
        "Introduction": "  INTRODUCTION Generative models have attracted much attention in the literature on deep learning. These models are used to formulate the distribution of complex data as a function of random noise passed through a network, so that rendering samples from the distribution is particularly easy. The most dominant generative models are Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ), as they have exhibited impressive performance in generating high quality images ( Radford et al., 2015 ;  Brock et al., 2018 ) and in other vision tasks ( Zhu et al., 2017 ;  Ledig et al., 2017 ). Despite their success, training GANs can be challenging, partly because they are trained by solving a saddle point optimization problem formulated as an adversarial game. It is well known that training GANs is unstable and sensitive to hyper-parameter settings ( Salimans et al., 2016 ;  Arora et al., 2017 ), and sometimes training leads to mode collapse ( Goodfellow, 2016 ). Although there have been multiple efforts to overcome the difficulties in training GANs ( Arjovsky et al., 2017 ;  Metz Luke & Sohl- Dickstein, 2017 ;  Srivastava et al., 2017 ;  Miyato et al., 2018 ), researchers are also actively studying non-adversarial methods that are known to be less affected by these issues. Some models explicitly define p(x), the distribution of the data, and training is guided by maximizing the data likelihood. One approach is to express the data distribution in an auto-regressive pattern ( Papamakarios et al., 2017 ;  Oord et al., 2016 ); another is to express it as an invertible transformation of a simple distribution using the change of variable formula, where the invertible transformation is defined using a normalizing flow network ( Dinh et al., 2014 ; 2016;  Kingma & Dhariwal, 2018 ). While being mathematically clear and well defined, normalizing flows keep the dimensionality of the original data in order to maintain bijectivity. Consequently, they cannot provide low-dimensional representations of the data and training is computationally expensive. Considering the prohibitively long training time and advanced hardware requirements in training large scale flow models such as ( Kingma & Dhariwal, 2018 ), we believe that it is worth exploring the application of flows in the low dimensional representation spaces rather than for the original data. Another class of generative models employs an encoder-decoder structure and low dimensional latent variables to represent and generate the data. An encoder is used to produce estimates of the latent variables corresponding to a particular data point, and samples from a predefined prior distribution on the latent space are passed through a decoder to produce new samples from the data distribution. We call these auto-encoder (AE) based models, of which variational auto-encoders (VAEs) are perhaps Under review as a conference paper at ICLR 2020 the most influential ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ). VAEs use the encoder to produce approximations to the posterior distribution of the latent variable given the data, and the training objective is to maximize a variational lower bound of the data log likelihood. VAEs are easy to train, but their generation quality still lies far below that of GANs, as they tend to generate blurry images ( Dosovitskiy & Brox, 2016 ). Whereas the original VAE uses a standard Gaussian prior, it can be extended by introducing a learnable parameterized prior distribution. There have been a number of studies in this direction (see section 2), some of which use a normalizing flow parameterization, where the prior is modeled as a trainable continuous bijective transformation of the standard Gaussian. We carefully study this method, and make the surprising novel observation that in order to produce high quality samples, it is necessary to significantly increase the weight on the reconstruction loss. This corresponds to decreasing the variance of the observational noise of the generative model at each pixel, where we are assuming the data distribution is factorial Gaussian conditioned on the output of the decoder, which yields the MSE as the reconstruction loss. It is important to note that increasing this weight alone without access to a trainable prior does not consistently improve generation quality. We show that as this weight increases, we approach a vanishing noise limit that corresponds to a deterministic auto-encoder. This leads to a new algorithm we call Generative Latent Flow (GLF), which combines a deterministic auto-encoder that learns a mapping to and from a latent space, and a normalizing flow that matches the standard Gaussian to the distribution of latent variables of the training data produced by the encoder. Our contributions are summarized as follows: i) we carefully study the effects of equipping VAEs with a normalizing flow prior on image generation quality as the weight of the reconstruction loss is increased. ii) Based on this finding, introduce Generative Latent Flow, which uses auto-encoders instead of VAEs. iii) Through standard evaluations, we show that our proposed model achieves state-of-the-art sample quality among competing AE based models, and has the additional advantage of faster convergence.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes two novel methods for producing reasonable embeddings for any numerals. The methods represent the embedding of a numeral as a weighted average of a small set of prototype number embeddings, which are induced from the training corpus using either a self-organizing map or a Gaussian mixture model. The weights are computed based on the differences between the target numeral and the prototype numerals. The proposed methods are evaluated on four tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. The results show that the proposed methods can produce high-quality embeddings for both numerals and non-numerical words and improve the performance of downstream tasks.",
        "Abstract": "Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce.\nIn this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training.\nWe evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. ",
        "Introduction": "  INTRODUCTION Word embeddings, the distributed vector representations of words, have become the essential build- ing block for deep learning approaches to natural language processing (NLP). The quality of pre- trained word embeddings has been shown to significantly impact the performance of neural ap- proaches to a variety of NLP tasks. Over the past two decades, significant progress has been made in the development of word embedding techniques ( Lund & Burgess, 1996 ;  Bengio et al., 2003 ;  Bullinaria & Levy, 2007 ;  Mikolov et al., 2013b ;  Pennington et al., 2014 ). However, existing word embedding methods do not handle numerals adequately and cannot directly encode the numeracy and magnitude of a numeral ( Naik et al., 2019 ). Most methods have a limited vocabulary size and therefore can only represent a small subset of the infinite number of numerals. Furthermore, most numerals have very scarce appearances in training corpora and therefore are more likely to be out- of-vocabulary (OOV) compared to non-numerical words. For example, numerals account for 6.15% of all unique tokens in English Wikipedia, but in GloVe  Pennington et al. (2014)  which is partially trained on Wikipedia, only 3.79% of its vocabulary is numerals. Previous work ( Spithourakis et al., 2016 ) also shows that the numeral OOV problem is even more severe when learning word embed- dings from corpora with abundant numerals such as clinical reports. Even if a numeral is included in the vocabulary, its scarcity in the training corpus would negatively impact the learning accuracy of its embedding. The inadequate handling of numerals in existing word embedding methods can be problematic in scenarios where numerals convey critical information. Take the following sentences for example, \"Jeff is 190, so he should wear size XXL\" (190 is a reasonable height for size XXL) \"Jeff is 160, so he should wear size XXL\" (160 is an unreasonable height for size XXL) \"Jeff is 10, so he should wear size XS\" (10 is an age instead of a height) If the numerals in the example are OOV or their embeddings are not accurately learned, then it becomes impossible to judge the categories of the numerals or the reasonableness of the sentences. In this paper, we propose two novel methods that can produce reasonable embeddings for any nu- merals. The key idea is to represent the embedding of a numeral as a weighted average of a small set of prototype number embeddings. The prototype numerals are induced from the training corpus Under review as a conference paper at ICLR 2020 using either a self-organizing map (Kohonen, 1990) or a Gaussian mixture model. The weights are computed based on the differences between the target numeral and the prototype numerals, reflecting the inductive bias that numerals with similar quantities are likely to convey similar semantic infor- mation and thus should have similar embeddings. Numeral embeddings represented in this manner can then be plugged into a traditional word embedding method for training. We empirically evaluate our methods on four tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling. The results show that our methods can produce high-quality embeddings for both numerals and non-numerical words and improve the performance of downstream tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new approach to solving the Traveling Salesman Problem (TSP) by combining a deep neural network with Monte Carlo Tree Search (MCTS). A graph neural network (GNN) is trained to capture the local and global graph structure and predict the prior probability of each vertex belonging to the partial tour. Edge information is integrated into each update-layer to efficiently extract features from the problem, whose solution relies on the edge weight. This approach takes advantage of the powerful feature representation and scouting exploration of deep learning and MCTS, respectively.",
        "Abstract": "We present a graph neural network assisted Monte Carlo Tree Search approach for the classical traveling salesman problem (TSP). We adopt a greedy algorithm framework to construct the optimal solution to TSP by adding the nodes successively. A graph neural network (GNN) is trained to capture the local and global graph structure and give the prior probability of selecting each vertex every step. The prior probability provides a heuristics for MCTS, and the MCTS output is an improved probability for selecting the successive vertex, as it is the feedback information by fusing the prior with the scouting procedure. Experimental results on TSP up to 100 nodes demonstrate that the proposed method obtains shorter tours than other learning-based methods.",
        "Introduction": "  INTRODUCTION Traveling Salesman Problem (TSP) is a classical combinatorial optimization problem and has many practical applications in real life, such as planning, manufacturing, genetics (Applegate et al., 2006b). The goal of TSP is to find the shortest route that visits each city once and ends in the origin city, which is well-known as an NP-hard problem (Papadimitriou, 1977). In the literature, approx- imation algorithms were proposed to solve TSP (Lawler et al., 1986; Goodrich & Tamassia, 2015). In particular, many heuristic search algorithms were made to find a satisfactory solution within a reasonable time. However, the performance of heuristic algorithms depends on handcrafted heuris- tics to guide the search procedure to find competitive tours efficiently, and the design of heuristics usually requires substantial expertise of the problem (Johnson & McGeoch, 1997; Dorigo & Gam- bardella, 1997). Recent advances in deep learning provide a powerful way of learning effective representations from data, leading to breakthroughs in many fields such as speech recognition (Lecun et al., 2015). Ef- forts of the deep learning approach to tackling TSP has been made under the supervised learning and reinforcement learning frameworks. Vinyals et al. (Vinyals et al., 2015) introduced a pointer net- work based on the Recurrent Neural Network (RNN) to model the stochastic policy that assigns high probabilities to short tours given an input set of coordinates of vertices. Dai et al. (Dai et al., 2017) tackled the difficulty of designing heuristics by Deep Q-Network (DQN) based on structure2vec (Dai et al., 2016b), and a TSP solution was constructed incrementally by the learned greedy policy. Most recently, Kool et al. (Kool et al., 2019) used Transformer-Pointer Network (Vaswani et al., 2017) to learn heuristics efficiently and got close to the optimal TSP solution for up to 100 vertices. These efforts made it possible to solve TSP by an end-to-end heuristic algorithm without special expert skills and complicated feature design. In this paper, we present a new approach to solving TSP. Our approach combines the deep neural network with the Monte Carlo Tree Search (MCTS), so that takes advantage of the powerful feature representation and scouting exploration. A graph neural network (GNN) is trained to capture the local and global graph structure and predict the prior probability, for each vertex, of whether this vertex belongs to the partial tour. Besides node features, we integrate edge information into each update-layer in order to extract features efficiently from the problem whose solution relies on the edge weight.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a Recurrent Event Network (RE-NET) for modeling heterogeneous event data in the form of temporal knowledge graphs. RE-NET defines the joint probability distribution of all the events in a TKG in an autoregressive fashion, where it models the probability distribution of the concurrent events at the current time step conditioned on all the preceding events. Experiments on five public temporal knowledge graph datasets demonstrate that RE-NET outperforms state-of-the-art models of both static and temporal graph reasoning, showing its better capacity to model temporal, multi-relational graph data with concurrent events. RE-NET is also shown to be able to perform effective multi-step inference to predict unseen entity relationships in a distant future.",
        "Abstract": "Modeling dynamically-evolving, multi-relational graph data has received a surge of interests with the rapid growth of heterogeneous event data. However, predicting future events on such data requires global structure inference over time and the ability to integrate temporal and structural information, which are not yet well understood. We present Recurrent Event Network (RE-Net), a novel autoregressive architecture for modeling temporal sequences of multi-relational graphs (e.g., temporal knowledge graph), which can perform sequential, global structure inference over future time stamps to predict new events. RE-Net employs a recurrent event encoder to model the temporally conditioned joint probability distribution for the event sequences, and equips the event encoder with a neighborhood aggregator for modeling the concurrent events within a time window associated with each entity. We apply teacher forcing for model training over historical data, and infer graph sequences over future time stamps by sampling from the learned joint distribution in a sequential manner. We evaluate the proposed method via temporal link prediction on ﬁve public datasets. Extensive experiments demonstrate the strength of RE-Net, especially on multi-step inference over future time stamps.",
        "Introduction": "  INTRODUCTION Representation learning on dynamically-evolving, graph-structured data has emerged as an important problem in a wide range of applications, including social network analysis (Zhou et al., 2018a; Trivedi et al., 2019), knowledge graph reasoning (Trivedi et al., 2017; Nguyen et al., 2018; Kazemi et al., 2019), event forecasting (Du et al., 2016), and recommender systems (Kumar et al., 2019; You et al., 2019). Previous methods over dynamic graphs mainly focus on learning time-sensitive structure representations for node classification and link prediction in single-relational graphs. However, the rapid growth of heterogeneous event data (Mahdisoltani et al., 2014; Boschee et al., 2015) has created new challenges on modeling temporal, complex interactions between entities (i.e., viewed as a temporal knowledge graph or a TKG), and calls for approaches that can predict new events in different future time stamps based on the history-i.e., structure inference of a TKG over time. Recent attempts on learning over temporal knowledge graphs have focused on either predicting missing events (facts) for the observed time stamps (García-Durán et al., 2018; Dasgupta et al., 2018; Leblay & Chekol, 2018), or estimating the conditional probability of observing a future event using temporal point process (Trivedi et al., 2017; 2019). However, the former group of methods adopts an interpolation problem formulation over TKGs and thus cannot predict future events, as representations of unseen time stamps are unavailable. The latter group of methods, including Know-Evolve and its extension, DyRep, computes the probability of future events using ground-truths of the proceeding events during inference time, and cannot model concurrent events occurring within the same time window-which often happens when event time stamps are discrete. It is thus desirable to have a principled method that can infer graph structure sequentially over time and can incorporate local structural information (e.g., concurrent events) during temporal modeling. To this end, we propose a sequential structure inference architecture, called Recurrent Event Network (RE-NET), for modeling heterogeneous event data in the form of temporal knowledge graphs. Key ideas of RE-NET are based on the following observations: (1) predicting future events can be viewed as a sequential (multi-step) inference of multi-relational interactions between entities over time; (2) Under review as a conference paper at ICLR 2020 temporally adjacent events may carry related semantics and informative patterns, which can further help inform future events (i.e., temporal information); and (3) multiple events may co-occur within the same time window and exhibit structural dependencies as they share entities (i.e., local structural information). To incorporate these ideas, RE-NET defines the joint probability distribution of all the events in a TKG in an autoregressive fashion, where it models the probability distribution of the concurrent events at the current time step conditioned on all the preceding events (see Fig. 1b for an illustration). Specifically, a recurrent event encoder, parametrized by RNNs, is used to summarize information of the past event sequences, and a neighborhood aggregator is employed to aggregate the information of concurrent events for the related entity within each time stamp. With the summarized information of the past event sequences, our decoder defines the joint probability of a current event. Such an autoregressive model can be effectively trained by using teacher forcing. Global structure inference for predicting future events can be achieved by performing sampling in a sequential manner. We evaluate our proposed method on temporal link prediction task, by testing the performance of multi-step inference over time on five public temporal knowledge graph datasets. Experimental results demonstrate that RE-NET outperforms state-of-the-art models of both static and temporal graph reasoning, showing its better capacity to model temporal, multi-relational graph data with concurrent events. We further show that RE-NET can perform effective multi-step inference to predict unseen entity relationships in a distant future.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a deep generative model for molecular graphs based on invertible functions. The proposed graph residual flow (GRF) combines a generic graph convolutional network with residual flows and does not require partitioning of a latent vector. Theoretical conditions for invertibility of the GRF are derived and experiments with popular graph generation datasets demonstrate that a generative model based on the GRF can achieve a generation performance comparable to the GraphNVP with much fewer trainable parameters.",
        "Abstract": "Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called Graph Residual Flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieve comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model. ",
        "Introduction": "  INTRODUCTION We propose a deep generative model for molecular graphs based on invertible functions. We especially focus on introducing an invertible function that is tuned for the use in graph structured data, which allows for flexible mappings with less number of parameters than previous invertible models for graphs. Molecular graph generation is one of the hot trends in the graph analysis with a potential for important applications such as in silico new material discovery and drug candidate screening. Previous generative models for molecules deal with string representations called SMILES (e.g.  Kusner et al. (2017) ;  Gómez-Bombarelli et al. (2018) ), which does not consider graph topology. Recent models such as ( Jin et al., 2018 ;  You et al., 2018 ;  De Cao & Kipf, 2018 ;  Madhawa et al., 2019 ) are able to directly handle graphs. Several researchers are investigating this topic using sophisticated statistical models such as variational autoencoders (VAEs) ( Kingma & Welling, 2014 ), adversarial loss-based models such as generative adversarial networks (GANs) ( Goodfellow et al., 2014 ;  Radford et al., 2015 ), and invertible flows ( Kobyzev et al., 2019 ) and have achieved desirable performances. The decoders of these graph generation models generate a discrete graph-structured data from a (typically continuous) representation of a data sample, which is modeled by aforementioned statistical models. In general, it is difficult to design a decoder that balances the efficacy of the graph generation and the simplicity of the implementation and training. For example, MolGAN ( De Cao & Kipf, 2018 ) has a relatively simple decoder but suffers from generating numerous duplicated graph samples. The state-of-the-art VAE-based models such as ( Jin et al., 2018 ;  Liu et al., 2018 ) have good generation performance but their decoding scheme is highly complicated and requires careful training. On the contrary, invertible flow-based statistical models ( Dinh et al., 2015 ;  Kobyzev et al., 2019 ) do not require training for their decoders because the decoders are simply the inverse mapping of the encoders and are known for good generation performances in image generation ( Dinh et al., 2017 ;  Kingma & Dhariwal, 2018 ).  Liu et al. (2019)  proposes an invertible-flow based graph generation model. However, their generative model is not invertible because its decoder for graph structure is not built upon invertible flows. The GraphNVP by  Madhawa et al. (2019)  is the seminal fully invertible-flow approach for graph generation, which successfully combines the invertible maps with the generic graph convolutional networks ( GCNs, e.g Kipf & Welling (2017) ;  Schlichtkrull et al. (2017) ). However, the coupling flow ( Kobyzev et al., 2019 ) used in the GraphNVP has a serious drawback when applied to sparse graphs such as molecular graphs we are interested in. The coupling flow Under review as a conference paper at ICLR 2020 requires a disjoint partitioning of the latent representation of the data (graph) in each layer. We need to design this partitioning carefully so that all the attributes of a latent representation are well mixed through stacks of mapping layers. However, molecular graphs are highly sparse in general: degree of each node atom is at most four (valency), and only few kind of atoms comprise the majority of the molecules (less diversity).  Madhawa et al. (2019)  argued that only a specific form of partitioning can lead to a desirable performance owing to sparsity: for each mapping layer, the representation of only one node is subject to update and all the other nodes are kept intact. In other words, a graph with 100 nodes requires at least 100 layers. But with the 100 layers, only one affine mapping is executed for each attribute of the latent representation. Therefore, the complexity of the mappings of GraphNVP is extremely low in contrast to the number of layer stacks. We assume that this is why the generation performance of GraphNVP is less impressive than other state-of-the-art models ( Jin et al., 2018 ;  Liu et al., 2018 ) in the paper. In this paper we propose a new graph flow, called graph residual flow (GRF): a novel combination of a generic GCN and recently proposed residual flows ( Behrmann et al., 2019 ;  Song et al., 2019 ;  Chen et al., 2019 ). The GRF does not require partitioning of a latent vector and can update all the node attributes in each layer. Thus, a 100 layer-stacked flow model can apply the (non-linear) mappings 100 times for each attribute of the latent vector of the 100-node graph. We derive a theoretical guarantee of the invertibility of the GRF and introduce constraints on the GRF parameters, based on rigorous mathematical calculations. Through experiments with most popular graph generation datasets, we observe that a generative model based on the proposed GRF can achieve a generation performance comparable to the GraphNVP  Madhawa et al. (2019) , but with much fewer trainable parameters. To summarize, our contributions in this paper are as follows: • propose the graph residual flow (GRF): a novel residual flow model for graph generation that is compatible with a generic GCNs. • prove conditions such that the GRFs are invertible and present how to keep the entire network invertible throughout the training and sampling. • demonstrate the efficacy of the GRF-based models in generating molecular graphs; in other words, show that a generative model based on the GRF has much fewer trainable parameters compared to the GraphNVP, while still maintaining a comparable generation performance.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of non-robust features for robust learning. We propose a novel approach for constructing a robustified dataset by adding non-robust features to the clean images, and then performing model training using the perturbed images in place of the original ones. We demonstrate the effectiveness of our approach on image classification tasks, and show that it can improve the robustness of the model against adversarial attacks.",
        "Abstract": "We propose a simple approach for adversarial training. The proposed approach utilizes an adversarial interpolation scheme for generating adversarial images and accompanying adversarial labels, which are then used in place of the original data for model training. The proposed approach is intuitive to understand, simple to implement and achieves state-of-the-art performance. We evaluate the proposed approach on a number of datasets including CIFAR10, CIFAR100 and SVHN. Extensive empirical results compared with several state-of-the-art methods against different attacks verify the effectiveness of the proposed approach. ",
        "Introduction": "  INTRODUCTION Deep learning-based techniques have achieved outstanding performance in many tasks such as im- age classification ( Krizhevsky et al., 2012 ), speech recognition ( Hinton et al., 2012 ) and video game playing ( Mnih et al., 2015 ). Despite of these encouraging progress, it has been shown that these models could be easily attacked by adversarial examples ( Szegedy et al., 2014 ;  Biggio et al., 2013 ;  Carlini & Wagner, 2018 ;  Lin et al., 2017 ). The ubiquitous of adversarial examples across many tasks ( Szegedy et al., 2014 ;  Lin et al., 2017 ;  Eykholt et al., 2018 ;  Carlini & Wagner, 2018 ) and the fact that they are transferable between different models ( Tramèr et al., 2017 ;  Charles et al., 2019 ;  Moosavi-Dezfooli et al., 2017 ) raise great concerns on security of such models and hinder their actual deployment in real world applications. Researchers have been actively working on un- derstanding the cause of adversarial examples ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ) and approaches for improving model robustness against them ( Madry et al., 2018 ;  Tramèr et al., 2018 ;  Liao et al., 2018 ;  Yan et al., 2018 ). A number of theories have been developed for explaining the ex- istence of adversarial examples.  Szegedy et al. (2014)  explain that adversarial examples are possible because the image space is densely filled with low probability adversarial pockets.  Goodfellow et al. (2015)  argue that the adversarial examples are caused by the linear nature of deep networks.  Tanay & Griffin (2016)  provide the perspective that adversarial examples exist because the class boundary extends beyond the data sub-manifold and can be lying close to it in some cases. It has been further shown in some recent work that adversarial examples can be decomposed into categories with dif- ferent causes including off-manifold ones ( Stutz et al., 2019 ;  Jacobsen et al., 2019 ) and those due to natural test error ( Stutz et al., 2019 ;  Jacobsen et al., 2019 ;  Ford et al., 2019 ). Recently,  Ilyas et al. (2019)  provide a perspective that adversarial vulnerability is caused by non- robust features. The reasoning is that there are abundant of useful correlations that exist in natural data, thus it is natural to expect the models could learn to exploit any of them if no preference is given (c.f. Table 7). However, models relying on superficial statistics (non-robust features) can be brittle and generalize poorly, thus suffering from adversarial attacks ( Ilyas et al., 2019 ). The natural idea is therefore using only robust features for learning. However robust features are not easy to construct directly ( Madaan & Hwang, 2019 ). In contrast, non-robust features are much easier to construct, using the standard adversarial example generation procedure ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ;  Madry et al., 2018 ). Therefore, we can leverage non-robust features instead for robust learning. This is typically achieved by constructing a robustified dataset where the new (perturbed) images are constructed by adding non-robust features to the clean images, and then performing model training using the perturbed images in place of the original ones. This procedure is essentially the standard adversarial training approach ( Goodfellow et al., 2015 ;  Madry et al., 2018 ), which has been shown to be effective for defending against adversarial attacks ( Athalye et al., 2018 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an image generation model where new pixels are generated by modeling the differences between new pixels and pre-existing ones. The proposed copy and adjustment mechanism predicts new pixels by selecting an existing pixel and adjusting its sub-pixel values. This model is applied to unconditional image generation and image-to-image translation tasks, and a mixture model is proposed to combine different forms of pixel prediction. Experimental results are provided to support the claims.",
        "Abstract": "In natural images, transitions between adjacent pixels tend to be smooth and gradual, a fact that has long been exploited in image compression models based on predictive coding. In contrast, existing neural autoregressive image generation models predict the absolute pixel intensities at each position, which is a more challenging problem. In this paper, we propose to predict pixels relatively, by predicting new pixels relative to previously generated pixels (or pixels from the conditioning context, when available). We show that this form of prediction fare favorably to its absolute counterpart when used independently, but their coordination under an unified probabilistic model yields optimal performance, as the model learns to predict sharp transitions using the absolute predictor, while generating smooth transitions using the relative predictor.\nExperiments on multiple benchmarks for unconditional image generation, image colorization, and super-resolution indicate that our presented mechanism leads to improvements in terms of likelihood compared to the absolute prediction counterparts. ",
        "Introduction": "  INTRODUCTION It is has long been appreciated in the field of image compression ( Harrison, 1952 ;  Haskell & Netravali, 1997 ) that adjacent pixels in natural images tend to share similar colors and intensities. This is evidenced in  Figure 1 , where we plot the marginal distribution of absolute sub-pixel (i.e., color channel) intensities and the difference between the sub-pixel intensities to the corresponding sub-pixel immediately to its left. While the absolute pixel values tend to be distributed non-trivially across the whole range of possible sub-pixel values [0, 255] with peaks around the values of 100 and 255, the relative sub-pixel values closely resembles a Laplacian distribution with the mean at 0 as most pixels do not differ greatly from their neighbouring pixels ( Takamura, 1996 ). This suggests that image generation process could mostly be done by predicting the relative distance between existing and new pixels, and absolute pixel prediction is only needed to generate more abrupt transitions, such as boundaries between two different objects. In predictive coding compression models, this fact is leveraged to achieve increased compression rates by learning to predict pixels as a delta between pixels rather than predicting their absolute values (al  Mahmood & Al-Rubaye, 2014 ). Autoregressive models are one of the main forces driving research in image generation  van den Oord et al. (2016a) ;  Salimans et al. (2017) ;  Parmar et al. (2018) ;  Chen et al. (2018) ;  Child et al. (2019) . In contrast to other popular image models based on adversarial methods ( Goodfellow et al., 2014 ;  Radford et al., 2015 ) or that use latent variables ( van den Oord et al., 2017 ), autoregressive models have a tractable likelihood function that decomposes image generation into a sequence of conditionally dependent pixel predictions. This is a desirable property that provides the grounds for applying the same principles of predictive coding in the image generation process. In this paper, we propose an image generation model where new pixels are generated by modeling the differences between new pixels and pre-existing ones. In contrast with existing approaches that tackle the challenging problem of learning to generate each pixel in terms of their absolute values ( van den Oord et al., 2016b ), we model gradually shifting pixel intensities in relative terms. From the modeling perspective, we propose a copy and adjustment mechanism, where new pixels are predicted by selecting an existing pixel, and adjusting its sub-pixel values to generate a new pixel. We show that new generation methodology generalizes better for multiple datasets in unconditional image generation and image translation tasks, namely colorization ( Cheng et al., 2016 ;  Zhang et al., 2016 ;  Under review as a conference paper at ICLR 2020   Parmar et al., 2018 ). Additionally, we show that our mechanism can be adapted to copy and adjust pixels from the input image in image-to-image translation tasks. Finally, we show that the different generation mechanisms can be unified into a single loss function with tractable likelihood computation. The paper is organized as follows: Firstly, we describe our copy and adjustment model (§2). Then, we describe its application in image-to-image translation tasks (§3). Afterwards, we describe a mixture model that combines different forms of pixel prediction (§4). Then, we provide experimental results that support our claims (§5). Finally, we describe the positioning of our work with respect the previous research (§6) and proceed to the conclusions (§7).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a semi-supervised learning technique, consistency regularization, to the multi-class learning from label proportions (LLP) problem. The proposed method leverages unlabeled instances to capture the latent structure of data and obtains the state-of-the-art performance on three benchmark datasets. Additionally, a new bag generation algorithm, K-means bag generation, is developed to group training data by attribute similarity. Finally, the paper demonstrates that it is possible to select models with a validation set consisting of only bags and associated label proportions, potentially reducing the need of a validation set with instance-level labels.",
        "Abstract": "The problem of learning from label proportions (LLP) involves training classifiers with weak labels on bags of instances, rather than strong labels on individual instances. The weak labels only contain the label proportions of each bag. The LLP problem is important for many practical applications that only allow label proportions to be collected because of data privacy or annotation costs, and has recently received lots of research attention. Most existing works focus on extending supervised learning models to solve the LLP problem, but the weak learning nature makes it hard to further improve LLP performance with a supervised angle. In this paper, we take a different angle from semi-supervised learning.\nIn particular, we propose a novel model inspired by consistency regularization, a popular concept in semi-supervised learning that encourages the model to produce a decision boundary that better describes the data manifold. With the introduction of consistency regularization, we further extend our study to non-uniform bag-generation and validation-based parameter-selection procedures that better match practical needs. Experiments not only justify that LLP with consistency regularization achieves superior performance, but also demonstrate the practical usability of the proposed procedures.",
        "Introduction": "  INTRODUCTION In traditional supervised learning, a classifier is trained on a dataset where each instance is associated with a class label. However, label annotation can be expensive or difficult to obtain for some appli- cations. Take the embryo selection as an example ( Hernández-González et al., 2018 ). To increase the pregnancy rate, clinicians would transfer multiple embryos to a mother at the same time. How- ever, clinicians are unable to know the outcome of a particular embryo due to limitations of current medical techniques. The only thing we know is the proportion of embryos that implant successfully. To increase the success rate of embryo implantation, clinicians aim to select high-quality embryos through the aggregated results. In this case, only label proportions about groups of instances are provided to train the classifier, a problem setting known as learning from label proportions (LLP). In LLP, each group of instances is called a bag, which is associated with a proportion label of different classes. A classifier is then trained on several bags and their associated proportion labels in order to predict the class of each unseen instance. Recently, LLP has attracted much attention among researchers because its problem setting occurs in many real-life scenarios. For example, the census data and medical databases are all provided in the form of label proportion data due to privacy issues ( Patrini et al., 2014 ;  Hernández-González et al., 2018 ). Other LLP applications include fraud detection ( Rueping, 2010 ), object recognition ( Kuck & de Freitas, 2012 ), video event detection ( Lai et al., 2014 ), and ice-water classification ( Li & Taylor, 2015 ). The challenge in LLP is to train models without direct instance-level label supervision. To overcome this issue, prior work seeks to estimate either the individual label ( Yu et al., 2013 ;  Dulac-Arnold et al., 2019 ) or the mean of each class by the label proportions ( Quadrianto et al., 2009 ;  Patrini et al., 2014 ). However, the methodology behind developing these models do not portray LLP situations that occur in real life. First, these models can be improved by considering methods that can better leverage unlabeled data. Second, these models assume that bags of data are randomly generated, which is not the case for many applications. For example, the data of population census are collected on region, age, or occupation with varying group sizes. Third, training these models requires a Under review as a conference paper at ICLR 2020 validation set with labeled data. It would be more practical if the process of model selection relies only on the label proportions. This paper aims to resolve the previous problems. Our main contributions are listed as follows: • We first apply a semi-supervised learning technique, consistency regularization, to the multi-class LLP problem. Consistency regularization considers an auxiliary loss term to enforce network predictions to be consistent when its input is perturbed. By exploiting the unlabeled instances, our method captures the latent structure of data and obtains the SOTA performance on three benchmark datasets. • We develop a new bag generation algorithm - the K-means bag generation, where training data are grouped by attribute similarity. Using this setup can help train models that are more applicable to actual LLP scenarios. • We show that it is possible to select models with a validation set consisting of only bags and associated label proportions. The experiments demonstrate correlation between bag- level validation error and instance-level test error. This potentially reduces the need of a validation set with instance-level labels.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel dynamic convolution method, DyNet, which reduces computation cost and accelerates inference speed of state-of-the-art networks such as MobileNetV2, ShuffleNetV2 and ResNets. DyNet consists of a coefficient prediction module and a dynamic generation module, and is simple to implement. Experiments show that DyNet reduces 37.0% FLOPs of ShuffleNetV2 (1.0) while further improve the Top-1 accuracy on ImageNet by 1.0%. For MobileNetV2 (1.0), ResNet18 and ResNet50, DyNet reduces 54.7%, 67.2% and 71.3% FLOPs respectively, the Top-1 accuracy on ImageNet changes by −0.27%, −0.6% and −0.08%. Meanwhile, DyNet further accelerates the inference speed of MobileNetV2 (1.0), ResNet18 and ResNet50 by 1.87×,1.32×and 1.48× on CPU platform respectively.",
        "Abstract": "Convolution operator is the core of convolutional neural networks (CNNs) and occupies the most computation cost. To make CNNs more efficient, many methods have been proposed to either design lightweight networks or compress models. Although some efficient network structures have been proposed, such as MobileNet or ShuffleNet, we find that there still exists redundant information between convolution kernels. To address this issue, we propose a novel dynamic convolution method named \\textbf{DyNet} in this paper, which can adaptively generate convolution kernels based on image contents. To demonstrate the effectiveness, we apply DyNet on multiple state-of-the-art CNNs. The experiment results show that DyNet can reduce the computation cost remarkably, while maintaining the performance nearly unchanged. Specifically, for ShuffleNetV2 (1.0), MobileNetV2 (1.0), ResNet18 and ResNet50, DyNet reduces 40.0%, 56.7%, 68.2% and 72.4% FLOPs respectively while the Top-1 accuracy on ImageNet only changes by +1.0%, -0.27%, -0.6% and -0.08%. Meanwhile, DyNet further accelerates the inference speed of MobileNetV2 (1.0), ResNet18 and ResNet50 by 1.87x,1.32x and 1.48x on CPU platform respectively. To verify the scalability, we also apply DyNet on segmentation task, the results show that DyNet can reduces 69.3% FLOPs while maintaining the Mean IoU on segmentation task.",
        "Introduction": "  INTRODUCTION Convolutional neural networks (CNNs) have achieved state-of-the-art performance in many computer vision tasks (Krizhevsky et al., 2012; Szegedy et al., 2013), and the neural architectures of CNNs are evolving over the years (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015; He et al., 2016; Hu et al., 2018; Zhong et al., 2018a;b). However, modern high-performance CNNs often require a lot of computation resources to execute large amount of convolution kernel operations. Aside from the accuracy, to make CNNs applicable on mobile devices, building lightweight and efficient deep models has attracting much more attention recently (Howard et al., 2017; Sandler et al., 2018; Zhang et al., 2018; Ma et al., 2018). These methods can be roughly categorized into two types: efficient network design and model compression. Representative methods for the former category are MobileNet (Howard et al., 2017; Sandler et al., 2018) and ShuffleNet (Ma et al., 2018; Zhang et al., 2018), which use depth-wise separable convolution and channel-level shuffle techniques to reduce computation cost. On the other hand, model compression based methods tend to obtain a smaller network by compressing a larger network via pruning, factorization or mimic (Chen et al., 2015; Han et al., 2015a; Jaderberg et al., 2014; Lebedev et al., 2014; Ba & Caruana, 2014). Although some handcrafted efficient network structures have been designed, we observe that the significant correlations still exist among convolutional kernels, and introduce large amount of re- dundant calculations. Moreover, these small networks are hard to compress. For example, Liu et al. (2019) compress MobileNetV2 to 124M, but the accuracy drops by 5.4% on ImageNet. We theoretically analyze above observation, and find that this phenomenon is caused by the nature of static convolution, where correlated kernels are cooperated to extract noise-irrelevant features. Thus it is hard to compress the fixed convolution kernels without information loss. We also find that if we linearly fuse several convolution kernels to generate one dynamic kernel based on the input, we can obtain the noise-irrelevant features without the cooperation of multiple kernels, and further reduce the computation cost of convolution layer remarkably. Based on above observations and analysis, in this paper, we propose a novel dynamic convolution method named DyNet. The overall framework of DyNet is shown in  Figure 1 , which consists of a coefficient prediction module and a dynamic generation module. The coefficient prediction module is trainable and designed to predict the coefficients of fixed convolution kernels. Then the dynamic generation module further generates a dynamic kernel based on the predicted coefficients. Our proposed dynamic convolution method is simple to implement, and can be used as a drop-in plugin for any convolution layer to reduce computation cost. We evaluate the proposed DyNet on state-of-the-art networks such as MobileNetV2, ShuffleNetV2 and ResNets. Experiment results show that DyNet reduces 37.0% FLOPs of ShuffleNetV2 (1.0) while further improve the Top-1 accuracy on ImageNet by 1.0%. For MobileNetV2 (1.0), ResNet18 and ResNet50, DyNet reduces 54.7%, 67.2% and 71.3% FLOPs respectively, the Top-1 accuracy on ImageNet changes by −0.27%, −0.6% and −0.08%. Meanwhile, DyNet further accelerates the inference speed of MobileNetV2 (1.0), ResNet18 and ResNet50 by 1.87×,1.32×and 1.48× on CPU platform respectively.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel model for open-domain neural response generation that learns a shared latent space with semantic information, on which semantically related sentences tend to be close to one another. The model maximizes the canonical correlation between the features extracted from the prompt and the response, and uses an autoencoder to construct the full response sentence from the predicted vector. Experiments on two dialog datasets show that the model generates coherent and specific responses.",
        "Abstract": "Generic responses are a known issue for open-domain dialog generation. Most current approaches model this one-to-many task as a one-to-one task, hence being unable to integrate information from multiple semantically similar valid responses of a prompt. We propose a novel dialog generation model that learns a semantic latent space, on which representations of semantically related sentences are close to each other. This latent space is learned by maximizing correlation between the features extracted from prompt and responses. Learning the pair relationship between the prompts and responses as a regression task on the latent space, instead of classification on the vocabulary using MLE loss, enables our model to view semantically related responses collectively. An additional autoencoder is trained, for recovering the full sentence from the latent space. Experimental results show that our proposed model eliminates the generic response problem, while achieving comparable or better coherence compared to baselines.",
        "Introduction": "  INTRODUCTION The sequence-to-sequence ( Sutskever et al., 2014 ) framework has become a popular choice for de- signing open-domain neural response generation systems ( Vinyals & Le, 2015 ). Recently, trans- former ( Vaswani et al., 2017 ) based models have received increasing attention ( Wolf et al., 2018 ). Those models typically involves maximizing the probability of the ground truth response given the input prompt, trained using a cross entropy loss on the 1-of-n coding vocabulary. However, it is a known problem that models tend to generate bland and uninformative responses, such as \"I don't know\" ( Serban et al., 2016 ;  Sordoni et al., 2015 ), despite the presence of more specific responses in the training data than generic responses. This problem is not caused by model optimization error or decoding search error ( Holtzman et al., 2019 ); on the contrary this is a result of a fundamental deficiency of the end-to-end maximum likelihood objective, which models the one-to-many task as a one-to-one task. The specific issue is the following. A sequence-to-sequence decoder trained using maximum likeli- hood objective tends to prefer uninformative stop word with higher frequency in training data, over more informative words with lower frequency, because the model loss treat each informative word independently and ignoring their semantic relatedness. A similar effect happens on the utterance level when using beam search decoding. Beam search aims to find the most probable utterance. Each sentence is treated independently, and the model is unable to use semantic similarity between different utterances ( Qiu et al., 2019 ). When specific responses collectively have a high probability, it is diluted by the large number of variations and possibilities of specific answers. On the other hand, generic responses have much fewer variations, thus they become the most probable response sequences. An alternative decoding method to beam search is sampling ( Holtzman et al., 2019 ; Fan et al., 2018), which does not suffer from this problem. However, sampling does not consider the subsequent words during decoding, and the randomness in word choice makes it prone to generating implausible responses and grammatical errors. Aiming to take into account the semantic relatedness of those diverse specific responses, we propose a novel model that learns a shared latent space with semantic information, on which semantically related sentences tend to be close to one another. The generation process could be separated into two steps. The first step predicts a sentence vector of the response on the semantic latent space. Since predicting a vector is a regression problem in the latent space instead of classification in the Under review as a conference paper at ICLR 2020 vocabulary as in MLE loss, our model is able to learn that most of the probability mass of the response is around the cluster of possible specific responses. The second step constructs the full response sentence from the predicted vector. Specifically, our semantic latent space is learned by maximizing the canonical correlation ( Hotelling, 1936 ) between the features extracted from the prompt and the response. This objective preserves information most correlated with the other sentence in the pair. Since semantically similar responses are likely to correspond to a similar set of prompts, they will have similar representations in the latent space. Generic responses correlate with a different and much larger set of inputs than specific responses, so they will have very different representations in the latent space. This is illustrated in  Figure 1  showing our model's representations of prompts and responses on a t-SNE plot. For the second decoding step, we train an additional autoencoder. The decoder part is used for constructing the response sentence from the shared latent space. The autoencoder representation is split into two parts. One part correlates with the prompt and the other is independent of the prompt. This design models that some information of the response is related to the prompt, while other Under review as a conference paper at ICLR 2020 information might be completely irrelevant (e.g. a follow up question). The encoder of the correlated part is shared with the canonical correlated feature extractor for responses, while the uncorrelated part is learned independently of the prompt and regularized using a normal distribution. The decoder is only trained on the autoencoder task. It simply recovers the sentence from its semantic latent representation, and it is not conditioned on the prompt. Since the semantics of the response and the decoding are learned separately, we could still perform beam search for the most probable sequence during inference but without the generic response problem. We conduct experiments on two dialog datasets, and show that our model generates coherent and specific responses.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a semi-implicit scheme for training deep neural networks, which utilizes implicit updates to overcome small step sizes and vanishing gradient problems in stochastic gradient descent. The proposed method allows for large step sizes and leads to better training performance per epoch. Experiments on MNIST and CIFAR-10 datasets show that the proposed scheme achieves better training and validation performances than existing implicit methods.",
        "Abstract": "Neural network has attracted great attention for a long time and many researchers are devoted  to improve the effectiveness of  neural network training algorithms. Though stochastic gradient descent (SGD) and other explicit gradient-based methods are widely adopted, there are still many challenges such as gradient vanishing and small step sizes, which leads to slow convergence and instability of SGD algorithms. Motivated by error back propagation (BP)  and proximal methods, we propose a semi-implicit back propagation method for neural network training. Similar to BP, the difference on the neurons are propagated in a backward fashion and the parameters are updated with proximal mapping. The implicit update for both hidden neurons and parameters allows to choose large step size in the training algorithm. Finally, we also show that any fixed point of  convergent sequences produced by this algorithm is a stationary point of the objective loss function. The experiments on both MNIST and CIFAR-10 demonstrate that the proposed semi-implicit BP algorithm leads to  better performance in terms of both loss decreasing and training/validation accuracy, compared to SGD and a similar algorithm ProxBP.",
        "Introduction": "  INTRODUCTION Along with the rapid development of computer hardware, neural network methods have achieved enormous success in divers application fields, such as computer vision ( Krizhevsky et al., 2012 ), speech recognition ( Hinton et al., 2012 ;  Sainath et al., 2013 ), nature language process ( Collobert et al., 2011 ) and so on. The key ingredient of neuron network methods amounts to solve a highly non-convex optimization problem. The most basic and popular algorithm is stochastic gradient descent (SGD) ( Robbins & Monro, 1951 ), especially in the form of \"error\" back propagation (BP) ( Rumelhart et al., 1986 ) that leads to high efficiency for training deep neural networks. Since then many variants of gradient based methods have been proposed, such as Adagrad( Duchi et al., 2011 ), Nesterov momentum ( Sutskever et al., 2013 ), Adam (Kingma & Ba, 2014) and AMSGrad ( Reddi et al., 2019 ). Recently extensive research are also dedicated to develop second-order algorithms, for example Newton method ( Orr & Müller, 2003 ) and L-BFGS ( Le et al., 2011 ). It is well known that the convergence of explicit gradient descent type approaches require sufficient- ly small step size. For example, for a loss function with Lipschitz continuous gradient, the stepsize should be in the range of (0, 2/L) for L being the Lipschitz constant, which is in general extremely big for real datasets. Another difficulties in gradient descent approaches is to propagate the \"error\" deeply due to nonlinear activation functions, which is commonly known as gradient vanishing. To overcome these problems, implicit updates are more attractive. In ( Frerix et al., 2018 ), proximal back propagation, namely ProxBP, was proposed to utilize the proximal method for the weight up- dating. Alternative approach is to reformulate the training problem as a sequence of constrained optimization by introducing the constraints on weights and hidden neurons at each layer. Block co- ordinate descent methods ( Carreira-Perpinan & Wang, 2014 ;  Zhang & Brand, 2017 ) were proposed and analyzed to solve this constrained formulation with square loss functions. Along this line, the Alternating direction method of multipliers (ADMM) ( Taylor et al., 2016 ;  Zhang et al., 2016 ) were also proposed with extra dual variables updating. Motivated by proposing implicit weight updates to overcome small step sizes and vanishing gradient problems in SGD, we propose a semi-implicit scheme, which has similar form as \"error\" back propagation through neurons, while the parameters are updated through optimization at each layer. It can be shown that any fixed point of the sequence generated by the scheme is a stationary point of the Under review as a conference paper at ICLR 2020 objective loss function. In contrast to explicit gradient descent methods, the proposed method allows to choose large step sizes and leads to a better training performance per epoch. The performance is also stable with respect to the choice of stepsizes. Compared to the implicit method ProxBP, the proposed scheme only updates the neurons after the activation and the error is updated in a more implicit way, for which better training and validation performances are achieved in the experiments on both MNIST and CIFAR-10.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a mathematical and representational model for the primary visual cortex (V1) that explains aspects of V1 such as receptive fields resembling Gabor filters, quadrature phase relationship between adjacent simple cells, and the ability to perceive local motions. The model consists of two components: a vector representation of local image content and a matrix representation of local displacement. The vector representation is obtained by linear filters, and the matrix representation is obtained by training the model on image pairs where the second image is a deformed version of the first image. Experiments show that the model can learn V1-like units and infer displacement fields comparable to current optical flow estimation methods. The vectors and matrices can be interpreted as activities of groups of neurons and synaptic connections, respectively.",
        "Abstract": "This paper proposes a representational model for image pair such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components. (1) The vector representations of local contents of images. (2) The matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene. When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements. Our experiments show that our model can learn to infer local motions. Moreover, the model can learn Gabor-like filter pairs of quadrature phases.",
        "Introduction": "  INTRODUCTION Our understanding of the primary visual cortex or V1 ( Hubel & Wiesel, 1959 ) is still very limited ( Olshausen & Field, 2005 ). In particular, the mathematical and representational models for V1 are still in short supply. Two prominent examples of such models are sparse coding ( Olshausen & Field, 1997 ) and independent component analysis (ICA) ( Bell & Sejnowski, 1997 ). Although such models do not provide detailed explanations of V1 at the level of neuronal dynamics, they help us understand the computational problems being solved by V1. As is the case with existing models, we expect our model to explain only limited aspects of V1, some of which are: (1) The receptive fields of V1 simple cells resemble Gabor filters ( Daugman, 1985 ). (2) Adjacent simple cells have quadrature phase relationship ( Pollen & Ronner, 1981 ). (3) The V1 cells are capable of perceiving local motions. While existing models can all explain (1), our model can also account for (2) and (3) naturally. Compared to models such as sparse coding and ICA, our model serves a more direct purpose of perceiving local motions. Our model consists of the following two components. See  Figure 1  for an illustration, where the image is illustrated by the big rectangle. A pixel is illustrated by a dot. The local image content is illustrated by a small square around it. The displacement of the pixel is illustrated by a short arrow, which is within the small square. The vector representation of the local image content is represented by a long vector, which rotates as the image undergoes deformation due to the pixel displacements. Section 3 explains the notation. (1) Vector representation of local image content. The local content around each pixel is represented by a high dimensional vector. Each unit in the vector is obtained by a linear filter. These local filters Under review as a conference paper at ICLR 2020 or wavelets are assumed to form a normalized tight frame, i.e., the image can be reconstructed from the vectors using the linear filters as the basis functions. (2) Matrix representation of local displacement. The change of the image from the current time frame to the next time frame is caused by the displacements of the pixels. Each possible displacement is represented by a matrix that acts on the vector. When the image changes according to the displace- ments, the vector at each pixel is multiplied by the matrix that represents the local displacement, in other words, the vector at each pixel is rotated by the matrix representation of the displacement of this pixel. One motivation of our work comes from Fourier analysis. An image patch I can be expressed by the Fourier decomposition I = k c k e i ω k ,x . Assuming the image patch undergoes a smooth motion so that all the pixels are shifted by a constant displacement dx, the shifted image patch J(x) = I(x − dx) = k c k e −i ω k ,dx e i ω k ,x . The change from the complex number c k to c k e −i ω k ,dx corresponds to rotating a 2D vector by a 2 × 2 matrix. However, we emphasize that our model does not assume Fourier basis or its localized version such as Gabor filters. The model figures it out with generic vector and matrix representations. We train this representational model on image pairs where in each pair, the second image is a deformed version of the first image, and the deformation is known. We learn the encoding matrices for vector representation and the matrices that represent the pixel displacements from the training data. Our experiments show that our method can learn V1-like units that can be well approximated by Gabor filters with quadrature phase relationship. After learning the encoding matrices for vector representation and the matrix representations of the displacements, we can infer the displacement field using the learned model. Compared to current optical flow estimation methods ( Dosovitskiy et al., 2015 ;  Ilg et al., 2017 ), which use complex deep neural networks to predict the optical flow, our model is much simpler and is based on explicit vector and matrix representations. We also demonstrate comparable results to these methods, in terms of the inference of displacement field. In terms of biological interpretation, the vectors can be interpreted as activities of groups of neurons, and the matrices can be interpreted as synaptic connections. See subsections 4.3 and 4.4 for details.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an empirical investigation of the relationship between the amount of training data and the landscape of the loss function for a proxy of a difficult non-linear dataset generalized XOR data with extra irrelevant variables. Using simple neural networks with one hidden layer and ReLU activation, the results show that the cross-entropy energy landscape has many shallow local minima and some deep minima that are very hard to find. The number of shallow local minima grows prohibitively large as the number of irrelevant variables increase, and the chances of finding a deep minimum decrease quickly with the number of irrelevant variables. The difficulty of finding deep local minimum decreases as more training examples are used.",
        "Abstract": "Artificial neural networks (ANNs) are very popular nowadays and offer reliable solutions to many classification problems. However, training deep neural networks (DNN) is time-consuming due to the large number of parameters. Recent research indicates that these  DNNs might be over-parameterized and different solutions have been proposed to reduce the complexity both in the number of parameters and in the training time of the neural networks. Furthermore, some researchers argue that after reducing the neural network complexity via connection pruning, the remaining weights are irrelevant and retraining the sub-network would obtain a comparable accuracy with the original one. \nThis may hold true in most vision problems where we always enjoy a large number of training samples and research indicates that most local optima of the convolutional neural networks may be equivalent. However, in non-vision sparse datasets, especially with many irrelevant features where a standard neural network would overfit, this might not be the case and there might be many non-equivalent local optima. This paper presents empirical evidence for these statements and an empirical study of the learnability of neural networks (NNs) on some challenging non-linear real and simulated data with irrelevant variables. \nOur simulation experiments indicate that the cross-entropy loss function on XOR-like data has many local optima, and the number of local optima grows exponentially with the number of irrelevant variables. \nWe also introduce a connection pruning method to improve the capability of NNs to find a deep local minimum even when there are irrelevant variables. \nFurthermore, the performance of the discovered sparse sub-network degrades considerably either by retraining from scratch or the corresponding original initialization, due to the existence of many bad optima around. \nFinally, we will show that the performance of neural networks for real-world experiments on sparse datasets can be recovered or even improved by discovering a good sub-network architecture via connection pruning.",
        "Introduction": "  INTRODUCTION Artificial neural networks are very popular nowadays and they can offer good solutions to many vision problems ( Dahl et al., 2013 ;  Krizhevsky et al., 2012 ;  Le, 2013 ; He et al., 2016). However, the neural networks usually require massive amounts of training data for good generalization, which limit their applications to non-vision problems where the data is not so abundant. In this paper, we conduct an empirical investigation of the relationship between the amount of training data and the landscape of the loss function. We will see that when the training data is large, the landscape has fewer local optima, which makes it is easier to find a deep local optimum, corresponding to a model with good generalization. If the training data is limited, the energy landscape could have in some cases so many local minima that it is computationally prohibitive to find a deep local minimum with good generalization. For this experimental investigation, we will use it as a proxy of a difficult non-linear dataset generalized XOR data with extra irrelevant variables. By adjusting the dimensionality of the XOR relationship and the number of irrelevant variables we can control the degree of difficulty of the problem and see the relationship between the amount of training data and the loss landscape. For our experimental investigation, we will focus our attention to simple neural networks with one hidden layer and ReLU activation. On the XOR-based classification problems, we will show that the cross-entropy energy landscape has many shallow local minima and some deep minima that are very hard to find. We will also observe that the number of shallow local minima grows prohibitively large as the number of irrelevant variables increase, and the chances of finding a deep minimum decrease quickly with the number of irrelevant variables. This decrease is much faster for harder problems (e.g. 5D XOR) than for easier problems (3D XOR). We also observe that for a fixed problem, the difficulty of finding deep local minimum decreases as more training examples are used.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a unified deep learning framework for Structure from Motion (SfM) which explicitly enforces photo-consistency, geometric-consistency, and camera motion constraints. The framework includes a depth based cost volume (D-CV) and a pose based cost volume (P-CV) which optimize per-pixel depth values and camera poses respectively. Experiments on DeMoN datasets, ScanNet and ETH3D demonstrate that the proposed approach outperforms the state-of-the-art methods.",
        "Abstract": "Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running  to improve both. In each cost volume, we encode not only photo-metric consistency across multiple input images, but also geometric consistency to ensure that depths from multiple views agree with each other. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.\n",
        "Introduction": "  INTRODUCTION Structure from motion (SfM) is a fundamental human vision functionality which recovers 3D struc- tures from the projected retinal images of moving objects or scenes. It enables machines to sense and understand with the 3D world and is critical in achieving real-world artificial intelligence. Over decades of researches, there has been a lot of great success on SfM; however, the performance is far from perfect. Conventional SfM approaches ( Agarwal et al., 2011 ;  Wu et al., 2011a ;  Engel et al., 2017 ;  Delaunoy & Pollefeys, 2014 ) heavily rely on Bundle-Adjustment (BA) ( Triggs et al., 1999 ;  Agarwal et al., 2010 ), in which 3D structures and camera motions of each view are jointly optimized via Levenberg- Marquardt (LM) algorithm ( Nocedal & Wright, 2006 ) according to the cross-view correspondence. Though successful in certain scenarios, conventional SfM based approaches are fundamentally re- stricted by the coverage of the provided multiple views and the overlaps among them. They also typically fail to reconstruct textureless or non-lambertian (e.g. reflective or transparent) surfaces due to the missing of correspondence across views. As a result, selecting sufficiently good input views and the right scene requires excessive caution and is usually non-trivial to even experienced user. Recent researches resort to deep learning to deal with the typical weakness of conventional SfM. Early effort utilizes deep neural network as a powerful mapping function that directly regresses the structures and motions ( Ummenhofer et al., 2017 ;  Vijayanarasimhan et al., 2017 ;  Zhou et al., 2017 ;  Wang et al., 2017 ). Since the geometric constraints of structures and motions are not explicitly en- forced, the network does not learn the underlying physics and prone to overfitting. Consequently, they do not perform as accurate as conventional SfM approaches and suffer from extremely poor generalization capability. Most recently, the 3D cost volume ( Teed & Deng, 2018 ) has been intro- duced to explicit leveraging photo-consistency in a differentiable way, which significantly boosts the performance of deep learning based 3D reconstruction. However, the camera motion usually has to be known ( Yao et al., 2018 ;  Im et al., 2019 ) or predicted via direct regression ( Ummenhofer et al., 2017 ;  Zhou et al., 2017 ;  Teed & Deng, 2018 ), which still suffer from generalization issue. In this paper, we explicitly enforce photo-consistency, geometric-consistency, and camera motion constraints in a unified deep learning framework. In particular, our network includes a depth based cost volume (D-CV) and a pose based cost volume (P-CV). D-CV optimizes per-pixel depth values Under review as a conference paper at ICLR 2020 with the current camera poses, while P-CV optimizes camera poses with the current depth esti- mations. Conventional 3D cost volume enforces photo-consistency by unprojecting pixels into the discrete camera fronto-parallel planes and computing the photometric (i.e. image feature) difference as the cost. In addition to that, our D-CV further enforces geometric-consistency among cameras with their current depth estimations by adding the geometric (i.e. depth) difference to the cost. Note that the initial depth estimation can be obtained using the conventional 3D cost volume. For pose es- timation, rather than direct regression, our P-CV discretizes around the current camera positions, and also computes the photometric and/or geometric differences by hypothetically moving the camera into the discretized position. Note that the initial camera pose can be obtained by a rough estimation from the direct regression methods such as ( Ummenhofer et al., 2017 ). Our framework bridges the gap between the conventional and deep learning based SfM by incorporating explicit constraints of photo-consistency, geometric-consistency and camera motions all in the deep network. The closest work in the literature is the recently proposed BA-Net ( Tang & Tan, 2018 ), which also aims to explicitly incorporate multi-view geometric constraints in a deep learning framework. They achieve this goal by integrating the LM optimization into the network. However, the LM iterations are unrolled with few iterations due to the memory and computational inefficiency, and thus it may lead to non-optimal solutions. In contrast, our method does not have a restriction on the number of it- erations and achieves empirically better performance. Furthermore, LM in SfM originally optimizes point and camera positions, and thus direct integration of LM still requires good correspondences. To evade the correspondence issue in typical SfM, their models employ a direct regressor to predict depth at the front end, which heavily relies on prior in the training data. In contrast, our model is a fully physical-driven architecture that less suffers from over-fitting issue for both depth and pose estimation. To demonstrate the superiority of our method, we conduct extensive experiments on DeMoN datasets, ScanNet and ETH3D. The experiments show that our approach outperforms the state-of- the-art  Schonberger & Frahm (2016) ;  Ummenhofer et al. (2017) ;  Tang & Tan (2018) .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Automated Machine Learning (AutoML) is a promising technique for reducing labour in data preprocessing, feature engineering, model selection, hyper-parameter tuning, model training and performance evaluation. Feature crossing, which takes cross-product of sparse features, is a widely used method to enhance the performance of machine learning on tabular data. This paper proposes a novel method called CrossGO, which can directly and automatically learn useful cross feature fields from the interpretation of a Deep Neural Network (DNN). Experiments on several real-world datasets show that a simple LR model empowered by the final set of cross feature fields achieves approximate or even better performances comparing with DNN.",
        "Abstract": "Automatically feature generation is a major topic of automated machine learning. Among various feature generation approaches, feature crossing, which takes cross-product of sparse features, is a promising way to effectively capture the interactions among categorical features in tabular data. Previous works on feature crossing try to search in the set of all the possible cross feature fields. This is obviously not efficient when the size of original feature fields is large. Meanwhile, some deep learning-based methods combines deep neural networks and various interaction components. However, due to the existing of Deep Neural Networks (DNN), only a few cross features can be explicitly generated by the interaction components. Recently, piece-wise interpretation of DNN has been widely studied, and the piece-wise interpretations are usually inconsistent in different samples. Inspired by this, we give a definition of interpretation inconsistency in DNN, and propose a novel method called CrossGO, which selects useful cross features according to the interpretation inconsistency. The whole process of learning feature crossing can be done via simply training a DNN model and a logistic regression (LR) model. CrossGO can generate compact candidate set of cross feature fields, and promote the efficiency of searching. Extensive experiments have been conducted on several real-world datasets. Cross features generated by CrossGO can empower a simple LR model achieving approximate or even better performances comparing with complex DNN models.",
        "Introduction": "  INTRODUCTION Recently, automated machine learning (AutoML) has been widely studied, and is proven a practical and promising machine learning technique ( Yao et al., 2018 ). AutoML aims to provide a easy way to apply machine learning technique, and automate the procedure of machine learning partly or thoroughly. This reduces labour on data preprocessing, feature engineering, model selection, hyper- parameter tuning, model training and performance evaluation. Among different components of machine learning, the quality of features plays an extremely im- portant role (Liu & Motoda, 1998;  Domingos, 2012 ). Accordingly, to improve the performances of machine learning tasks, various automatic feature generation methods have been proposed ( Chapelle et al., 2015 ;  Katz et al., 2016 ;  Zhang et al., 2016 ;  Qu et al., 2016 ;  Juan et al., 2016 ;  Blondel et al., 2016 ;  Guo et al., 2017 ;  Lian et al., 2018 ;  Luo et al., 2019 ). Among these feature generation meth- ods, feature crossing, which takes cross-product of sparse features, is a promising way to capture the interactions among categorical features and is widely used to enhance the performance of machine learning on tabular data in real-world applications ( Chapelle et al., 2015 ;  Cheng et al., 2016 ;  Luo et al., 2019 ). Previous works on feature crossing mostly try to search in the set of all the possible cross features ( Rosales et al., 2012 ;  Chapelle et al., 2015 ;  Katz et al., 2016 ). Based on these methods, AutoCross ( Luo et al., 2019 ) employs engineering optimization tricks, and accelerates the searching process. However, search-based methods are not efficient when the size of original feature fields is large. Meanwhile, some deep learning-based methods combine deep neural networks and various interac- tion components ( Qu et al., 2016 ;  Cheng et al., 2016 ;  Wang et al., 2017 ;  Guo et al., 2017 ;  Lian et al., 2018 ). However, due to the existing of Deep Neural Networks (DNN), which can capture variety of Under review as a conference paper at ICLR 2020 interactions between original features, only a few cross features can be explicitly generated by the interaction components. As mentioned above, DNN is a powerful method for capturing various feature interactions in its hidden layers ( Guo et al., 2017 ). This means, DNN can generate features implicitly, but can not provide interpretable cross features explicitly. Recently, the interpreration of deep models has drawn great attention in academia, and mostly focus on piece-wise interpretation, which means assigning a piece of local interpretation for each sample ( Bastani et al., 2017 ;  Alvarez-Melis & Jaakkola, 2018 ;  Chu et al., 2018 ). This can be done via the gradient backpropagation from the prediction layer to the feature layer. Usually, local interpretations of a specific feature are inconsistent in different samples. And large interpretation inconsistency indicates that, the corresponding feature has interacted with other feature fields in the hidden layers of DNN. Therefore, the interpretations of DNN can help us find useful cross features. Accordingly, in this paper, we propose a novel method called CrossGO, which can directly and automatically learn useful cross feature fields from the interpretation of DNN. (1) First, we train a common sparse DNN with the whole training set. (2) Then, for each sample in the validation set, we calculate the gradients of the output predictions with respect to the input features, i.e., local interpretations. We also calculate the global interpretation of a specific feature as the average value of all the corresponding local interpretations. For a specific feature in a specific sample, if the corre- sponding local interpretation is far from the corresponding global interpretation, i.e., interpretation inconsistency is large, we regard this feature interacted with others by DNN in the sample. Thus, we obtain a set of feasible features that works for feature crossing in each sample. (3) Then, we employ a greedy algorithm to generate a global candidate set containing both second-order and high-order cross feature fields. (4) With the the candidate set, we can train a simple LR model, therefore rank and select feature fields according to their contribution measured in the validation set. So far, we can obtain the final set of cross feature fields. We have conducted experiments on several real-world datasets. The size of candidate set of cross feature fields is constrained to be 2N , where N is the number of original feature fields. To be noted, 2N is extremely small compared to the whole set of second-order and high-order cross feature fields, especially when N is large. And a simple LR model empowered by the final set of cross feature fields achieves approximate or even better performances comparing with DNN on all datasets. This whole process of learning feature crossing can be done via simply training a DNN model and a LR model. This shows that, our proposed CrossGO method can automatically generate a compact candidate set of cross feature fields, and efficiently find enough useful cross features.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a flow-based intrinsic curiosity module (FICM) for evaluating the novelty of observations in deep reinforcement learning (DRL) algorithms. FICM generates intrinsic rewards based on the prediction errors of optical flow estimation, incentivizing the agent to focus on moving parts of the environment and explore novel observations. The performance of FICM is validated in a variety of benchmark environments, including Atari 2600, Super Mario Bros., and ViZDoom. Results demonstrate that FICM is preferable to the previous prediction-based exploration methods in terms of exploration efficiency of the agent in several tasks and environments, especially for those featuring sophisticated moving patterns.",
        "Abstract": "Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. In this paper, we introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations.  We introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. We evaluate our method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Our results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (DRL) algorithms aim at developing the policy of an agent to maximize its cumulative rewards collected in an environment, and have gained considerable attention in a wide range of application domains, such as game playing ( Mnih et al., 2015 ;  Silver et al., 2016 ) and robot navigation ( Zhang et al., 2016 ). Despite their recent successes, however, one of the key constraints of them is the requirement of dense reward signals. In environments with sparse reward signals, it becomes extremely challenging for an agent to explore and learn a useful policy. Although simple heuristics such as -greedy ( Mnih et al., 2013 ), entropy regularization ( Mnih et al., 2016a ), and noisy network (Fortunato, 2018) were proposed, they are still far from satisfactory in such environments. Researchers in recent years have attempted to deal with the challenge by providing an agent with ex- ploration bonuses (also known as \"intrinsic rewards\") to encourage an agent to explore even when the reward signals from environments are sparse. These bonus rewards are associated with state novelty to incentivize an agent to explore those novel states. A number of novelty measurement strategies have been proposed in the past few years, such as the use of information gain ( Houthooft et al., 2016 ), count-based methods utilizing counting tables ( Bellemare et al., 2016 ;  Ostrovski et al., 2017 ), and prediction-based methods exploiting prediction errors of dynamics models ( Stadie et al., 2015 ;  Pathak et al., 2017 ;  Burda et al., 2019a ;b). These prediction-based methods differ in the tar- gets of prediction.  Pathak et al. (2017) ;  Burda et al. (2019a)  introduce a forward dynamics model for predicting the feature representation of the next state based on the current state and the action taken by the agent, which is known as next-frame prediction. Next-frame prediction for complex or rapid-changing observations, however, is rather difficult for forward dynamics models, especially when the prediction is made solely based on the current state and the taken action. It has been widely recognized that performing next-frame prediction typically requires complex feature repre- sentations ( Kingma & Welling, 2014 ;  Goodfellow et al., 2014 ;  Mirza & Osindero, 2014 ;  Lotter et al., 2017 ;  Xue et al., 2016 ). On the other hand,  Burda et al. (2019b)  proposed a self-frame prediction strategy by employing a predictor network to predict the feature representation of the current state from a fixed and randomly initialized target network. Nevertheless, the attempt of self-frame pre- diction to predict the encoded current state inherently neglects motion features in the observations. Rapid changes or moving patterns in two consecutive observations essentially serve as important signals to motivate an agent for exploration. It is thus inherently reasonable to take these features into account for prediction-based methods. As a result, in this paper we introduce a flow-based intrinsic curiosity module, called FICM, for evaluating the novelty of observations. FICM generates intrinsic rewards based on the prediction errors of optical flow estimation (i.e., the flow loss). Observations with low prediction errors, or low intrinsic rewards, indicate that the agent has seen the observations plenty of times. On the contrary, observations are considered seldom visited when the errors from the predicted flow are non-negligible. The latter case then prompts FICM to generate high intrinsic rewards to encourage the agent for exploration.  Fig. 1  provides an illustrative example of FICM for Super Mario Bros. It can be observed that the brighter parts of the flow loss in  Fig. 1 (c)  align with the attention areas (Greydanus et al., 2018) in  Fig. 1 (d) , implying the significance of motion features in intrinsically motivated exploration. In addition, as exploration bonuses ideally decrease over time, the agent is motivated to continuously explore novel observations during the training phase.  Fig. 2  validates that flow loss gradually declines to low values after millions of iterations, indicating the qualification of flow loss to serve as an intrinsic reward signal. In other words, FICM is capable of incentivizing the agent to focus on moving parts of the environment and explore novel observations. We validate the performance of FICM in a variety of benchmark environments, including Atari 2600 ( Bellemare et al., 2013 ), Super Mario Bros., and ViZDoom ( Wydmuch et al., 2018 ). We demonstrate that FICM is preferable to the previous prediction-based exploration methods in terms of exploration efficiency of the agent in several tasks and environments, especially for those featur- ing sophisticated moving patterns. We further provide a comprehensive set of ablation analysis for the proposed FICM. The primary contributions of this paper are thus summarized as the following: • We propose a new flow-based intrinsic curiosity module, called FICM, which leverages on existing methods of optical flow estimation from the field of computer vision (CV) to evaluate the novelty of observations characterizing by sophisticated motion features. • The proposed FICM encourages an RL agent to learn the motion features and understand the observations from an environment in a more comprehensive manner. • The proposed FICM is able to encode high dimensional inputs (e.g., RGB frames) and utilizes the information more effectively and efficiently. • Moreover, the proposed FICM requires only two consecutive frames to obtain sufficient information when estimating the novelty of observations. The remainder of this paper is organized as the following. Section 2 introduces the background ma- terial related to this work. Section 3 presents the proposed FICM framework. Section 4 demonstrates the experimental results and discusses their implications. Section 5 concludes the paper. For more details of the background knowledge, implementations, and setups, please refer to our appendices.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper explores how to design an efficient and stable algorithm with stochastic mirror descent (SMD) to improve sample efficiency in reinforcement learning (RL). Challenges include instability and divergence when updating the parameter of a non-convex objective function by SMD via a single batch sample, and the large variance of gradient estimator. The non-stationary sampling process with the environment leads to the large variance of existing methods on the estimate of policy gradient, resulting in poor sample efficiency.",
        "Abstract": "Improving sample efficiency has been a longstanding goal in reinforcement learning.\nIn this paper, we propose the $\\mathtt{VRMPO}$: a sample efficient policy gradient method with stochastic mirror descent.\nA novel variance reduced policy gradient estimator is the key of $\\mathtt{VRMPO}$ to improve sample efficiency.\nOur $\\mathtt{VRMPO}$ needs only $\\mathcal{O}(\\epsilon^{-3})$ sample trajectories to achieve an $\\epsilon$-approximate first-order stationary point, \nwhich matches the best-known sample complexity.\nWe conduct extensive experiments to show our algorithm outperforms state-of-the-art policy gradient methods in various settings.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) is one of the most wonderful fields of artificial intelligence, and it has achieved great progress recently (Mnih et al., 2015; Silver et al., 2017). To learn the optimal policy from the delayed reward decision system is the fundamental goal of RL. Policy gradient methods (Williams, 1992; Sutton et al., 2000) are powerful algorithms to learn the optimal policy. Despite the successes of policy gradient method, suffering from high sample complexity is still a critical challenge for RL. Many existing popular methods require more samples to be collected for each step to update the parameters (Silver et al., 2014; Lillicrap et al., 2016; Schulman et al., 2015; Mnih et al., 2016; Haarnoja et al., 2018), which partially reduces the effectiveness of the sample. Although all the above existing methods claim it improves sample efficiency, they are all empirical results which lack a strong theory analysis of sample complexity. To improve sample efficiency, in this paper, we explore how to design an efficient and stable algo- rithm with stochastic mirror descent (SMD). Due to its advantage of the simplicity of implemen- tation, low memory requirement, and low computational complexity (Nemirovsky & Yudin, 1983; Beck & Teboulle, 2003; Lei & Tang, 2018), SMD is one of the most widely used methods in machine learning. However, it is not sound to apply SMD to policy optimization directly, and the challenges are two-fold: (I) The objective of policy-based RL is a typical non-convex function, but Ghadimi et al. (2016) show that it may cause instability and even divergence when updating the parameter of a non-convex objective function by SMD via a single batch sample. (II) Besides, the large variance of gradient estimator is the other bottleneck of applying SMD to policy optimization for improving sample efficiency. In fact, in reinforcement learning, the non-stationary sampling process with the environment leads to the large variance of existing methods on the estimate of policy gradient, which results in poor sample efficiency (Papini et al., 2018; Liu et al., 2018).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper considers a practical scenario where a prediction vendor requests crowdsourced data for a target task, while the data owner does not want her private information to be leaked. The goal of privacy-preserving in this context is to protect private attributes of the sanitized data released by data owner from potential attribute inference attacks of a malicious adversary. Differential privacy (DP) has been proposed and extensively investigated to protect the privacy of collected data, however, it still suffers from attribute inference attacks. This paper proposes a solution to protect private attributes of data owners from attribute inference attacks while preserving the utility of the released data.",
        "Abstract": "With the prevalence of machine learning services, crowdsourced data containing sensitive information poses substantial privacy challenges. Existing work focusing on protecting against membership inference attacks under the rigorous framework of differential privacy are vulnerable to attribute inference attacks. In light of the current gap between theory and practice, we develop a novel theoretical framework for privacy-preservation under the attack of attribute inference. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its privacy guarantees against arbitrary adversaries. On the other hand, it is clear that privacy constraint may cripple utility when the protected attribute is correlated with the target variable. To this end, we also prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between utility and privacy. Empirically, we extensively conduct experiments to corroborate our privacy guarantee and validate the inherent trade-offs in different privacy preservation algorithms. Our experimental results indicate that the adversarial representation learning approaches achieve the best trade-off in terms of privacy preservation and utility maximization.",
        "Introduction": "  INTRODUCTION With the growing demand for machine learning systems provided as services, a massive amount of data containing sensitive information, such as race, income level, age, etc., are generated and collected from local users. This poses a substantial privacy challenge and it has become an imperative object of study in machine learning ( Abadi et al., 2016 ;  Gilad-Bachrach et al., 2016 ), computer vision ( Chou et al., 2018 ;  Wu et al., 2018 ), healthcare ( Beaulieu-Jones et al., 2018b ; a ), security ( Shokri et al., 2017 ), and many other domains. In this paper, we consider a practical scenario where the prediction vendor requests crowdsourced data for a target task, e.g, scientific modeling. The data owner agrees on the data usage for the target task while she does not want her other private information (e.g., age, race) to be leaked. The goal of privacy-preserving in this context is then to protect private attributes of the sanitized data released by data owner from potential attribute inference attacks of a malicious adversary. For example, in an online advertising scenario, while the user (data owner) may agree to share her historical purchasing events, she also wants to protect her age information so that no malicious adversary can infer her age range from the shared data. Note that simply removing age attribute from the shared data is insufficient for this purpose, due to the redundant encoding in data, i.e., other attributes may have a high correlation with age. Among many other techniques, differential privacy (DP) has been proposed and extensively in- vestigated to protect the privacy of collected data ( Dwork & Nissim, 2004 ;  Dwork et al., 2006 ). DP embraces formal guarantees for privacy problems such as defending against the membership query attacks ( Abadi et al., 2016 ;  Papernot et al., 2016 ), or ensures the distribution of any two data records statistically indistinguishable ( Erlingsson et al., 2014 ;  Duchi et al., 2013 ;  Bassily & Smith, 2015 ). However, DP still suffers from attribute inference attacks ( Fredrikson et al., 2015 ;  Cormode, 2011 ;  Gong & Liu, 2016 ), as it only prevents an adversary from gaining additional knowledge by inclusion/exclusion of a subject, not from gaining knowledge from the data itself ( Dwork et al., 2014 ). As a result, an adversary can still accurately infer sensitive attributes of data owners from differentially-private datasets. Such a gap between theory and practice calls for an important and appealing challenge:",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Generative Adversarial Network (GAN) based on an attention mechanism to improve the detail of positron images in industrial non-destructive testing. The GAN is trained using a medical CT image dataset and a small number of industrial positron images, and a memory module is constructed to contain industrial positron image features. The GAN is then jointly trained to generate higher quality images for industrial non-destructive testing. The results of the Turing test demonstrate the effectiveness of the proposed method.",
        "Abstract": "In the industrial field, the positron annihilation is not affected by complex environment, and the gamma-ray photon penetration is strong, so the nondestructive detection of industrial parts can be realized. Due to the poor image quality caused by gamma-ray photon scattering, attenuation and short sampling time in positron process, we propose the idea of combining deep learning to generate positron images with good quality and clear details by adversarial nets. The structure of the paper is as follows: firstly, we encode to get the hidden vectors of medical CT images based on transfer Learning, and use PCA to extract positron image features. Secondly, we construct a positron image memory based on attention mechanism as a whole input to the adversarial nets which uses medical hidden variables as a query. Finally, we train the whole model jointly and update the input parameters until convergence. Experiments have proved the possibility of generating rare positron images for industrial non-destructive testing using countermeasure networks, and good imaging results have been achieved.",
        "Introduction": "  INTRODUCTION In recent years, with the advancement of science and technology, especially the rapid developmen- t of high-end manufacturing, in the field of industrial non-destructive testing, in many cases, it is necessary to perform defect detection without damaging or affecting the performance and inter- nal structure of the device under test. Therefore, there is an increasing demand for corresponding detection devices. In complex industrial environments (such as aviation, internal combustion engines, chemical engi- neering, etc.), it is of great research significance to detect faults and defects in closed chambers. In this paper, the use of positron annihilation gamma photon imaging positron emission imaging technology for industrial nondestructive testing is studied. The occurrence of positron annihilation is not affected by factors such as high temperature, high pressure, corrosion, etc., so it can penetrate the dense metal material cavity, realize the undisturbed and non-destructive trace imaging of the detected object, and obtain the detected object after processing. Describe the image and perform a state analysis. Therefore, the quality of imaging technology directly affects the analysis of fault detection results. Positron Emission Tomography (PET) was first used in medical imaging. The principle is that when a radioactive positron nucleus decays, a proton in the nucleus is converted into a neutron, and a positron and a neutron are released. The positron will quickly combine with the electrons in the material in a very short time, causing a positron annihilation phenomenon, producing a pair of gamma photon pairs with opposite directions and energy of 511KeV. Photon pairs are collected, identified, processed, and finally reconstructed to obtain medical images. Commonly used PET reconstruction algorithms are analytic method (K, 2000) and statistical method ( Shepp & Vardi, 2007 ). The currently widely used algorithms are MLEM and OSEM. At present, PET technology has been widely used in the clinical diagnosis of human diseases. The advantages are quite obvious, the imaging quality is higher, and it shows great advantages in medical research. The principle of positron emission in industrial non-destructive fields is similar to medical imaging, but it has its own unique difficulties: the detection environment is more harsh, the sampling time is Under review as a conference paper at ICLR 2020 short, and at the same time, due to the phenomenon of scattering and attenuation of photons, indus- trial positron imaging is obtained. The image quality is even worse. Therefore, the reconstructed image needs to be further processed to obtain a higher quality image. In this paper, we propose adversarial networks of positron image memory module based on attention mechanism. Using medical images as basic data sets, introducing knowledge of migration learning, building memory module according to the contribution of detail features to images, a positron image generation network in the field of industrial non-destructive testing is obtained through joint training, thus achieving higher quality generation of industrial positron images. In summary, our main contributions in this paper are as follows: We are the first to advocate an idea of using Generative Adversarial Networks to enhance the detail of the positron image in the industrial non-destructive field, and realize the generation and processing of the scarce image data in the professional field. We use the medical CT image dataset as the basic training sample of the network framework, which is based on the idea of migration learning, and then extract the features of a small number of indus- trial non-destructively detected positron images, which can improve the details of the generated im- ages, and make the network model have better applicability in the field of industrial non-destructive testing. We combine the attention-based mechanism in the professional domain image feature extraction. By constructing a memory module containing industrial positron image features, we can generate image generation in a specific domain, and finally conduct an industrial lossless positron image generation model. We train the whole network jointly, through the discriminant network of the antagonistic genera- tion network, the front-end network was back-propagated, the input parameters were updated, and the model was optimized. Finally, the convergence was achieved and The Turing test was passed successfully.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes graphon, a generalization of random graphs, as a search space for stage-wise neural architecture. It develops an operationalization of the theory on graphon in the representation, scaling and search of neural stage-wise graphs. Experiments indicate that the graphs found outperform the baselines consistently across a range of model capacities.",
        "Abstract": "Search space is a key consideration for neural architecture search. Recently, Xie et al. (2019a) found that randomly generated networks from the same distribution perform similarly, which suggest we should search for random graph distributions instead of graphs. We propose graphon as a new search space. A graphon is the limit of Cauchy sequence of graphs and a scale-free probabilistic distribution, from which graphs of different number of vertices can be drawn. This property enables us to perform NAS using fast, low-capacity models and scale the found models up when necessary. We develop an algorithm for NAS in the space of graphons and empirically demonstrate that it can find stage-wise graphs that outperform DenseNet and other baselines on ImageNet. ",
        "Introduction": "  INTRODUCTION Neural architecture search (NAS) aims to automate the discovery of neural architectures with high performance and low cost. Of primary concern to NAS is the design of the search space [23], which needs to balance multiple considerations. For instance, too small a space would exclude many good solutions, whereas a space that is too large would be prohibitively expensive to search through. An ideal space should have a one-to-one mapping to solutions and sufficiently smooth in order to accelerate the search. A common technique [37, 17, 35, 19, 24, 34] to keep the search space manageable is to search for a small cell structure, typically containing about 10 operations with 1-2 input sources each. When needed, identical cells are stacked to form a large network. This technique allows cells found on, for instance, CIFAR-10 to work on ImageNet. Though this practice is effective, it cannot be used to optimize the overall network structure. In both manual and automatic network design, the overall network structure is commonly divided into several stages, where one stage operates on one spatial resolution and contains several near- identical layers or multi-layer structures (i.e., cells). For example, ResNet-34 [11] contains 4 stages with 6, 8, 12, and 6 convolutional layers, respectively. DenseNet-121 [12] contains 4 stages with 6, 12, 24, and 16 two-layer cells. AmoebaNet-A [24] has 3 stages, within each 6 cells are arranged sequentially. Among cells in the same stage, most connections are sequential with skip connections occasionally used. As an exception, DenseNet introduces connections between every pairs of cells within the same stage. Here we emphasize the difference between a stage and a cell. A cell typically contains about 10 op- erations, each taking input from 1-2 other operations. In comparison, a stage can contain 60 or more operations organized in repeated patterns and the connections can be arbitrary. A network usually contains only 3-4 stages but many more cells. In this paper, we focus on the network organization at the level of stage rather than cell. [32] recently showed that the stage structure can be sampled from probabilistic distributions of graphs, including Erdős-Rényi (ER) (1960), Watts-Strogatz (WS) (1998), and Barabási-Albert (BA) (1999), yielding high-performing networks with low in-group variance. This finding suggests the random graph distribution, rather than the exact graph, is the main causal factor behind network performance. Thus, searching for the graph is likely not as efficient as searching for the random Under review as a conference paper at ICLR 2020 graph distribution. The parameter space of random graph distributions may appear to be a good search space. We propose a different search space, the space of graphons [20], and argue for its superiority as an NAS search space. Formally introduced in Section 3, a graphon is a measurable function defined on [0, 1] 2 → [0, 1] and a probabilistic distribution from which graphs can be drawn. Graphons are limit objects of Cauchy sequences of finite graphs under the cut distance metric.  Figure 1  visualizes three adjacency matrices randomly generated by the Barabási-Albert (BA) model with increasing numbers of nodes. It is easy to see that, as the number of nodes increases, the sequence of random graphs converges to its limit, a graphon. The BA model starts with an initial seed graph with m 0 nodes and arbitrary interconnections. Here we choose a complete graph as the seed. It sequentially adds new nodes until there are n nodes in the graph. For every new node v new , m edges are added, with the probability of adding an edge between v new and the node v i being proportional to the degree of v i . In  Figure 1 , we let m = m 0 = 0.1n. The fact that different parameterization results in the same adjacency matrix suggests that directly searching in the parameter space will revisit the same configuration and is less efficient than searching in the graphon space. Additionally, graphon provides a unified and more expressive space than common random graph models.  Figure 2  illustrates the graphons for the WS and the ER models. We can observe that these random models only capture a small proportion of all possible graphons. The graphon space allows new possibilities such as interpolation or striped combination of different random graph models. Finally, graphon is scale-free, so we should be able to sample an arbitrary-sized stage-wise archi- tecture with identical layers (or cells) from a graphon. This allows us to perform expensive NAS Under review as a conference paper at ICLR 2020 on small datasets (e.g., CIFAR-10) using low-capacity models and obtain large stage-wise graphs to build large models. By relating graphon theory to NAS, we provide theoretically motivated tech- niques that scale up stage-wise graphs, which are shown to be effective in practice. Our experiments aim to fairly compare the stage-wise graphs found by our method against DenseNet and the WS random graph model by keeping other network structures and other hyperparameters constant. The results indicate that the graphs found outperform the baselines consistently across a range of model capacities. The contribution of this paper revolves around building a solid connection between theory and prac- tice. More specifically, • We propose graphon, a generalization of random graphs, as a search space for stage-wise neural architecture that consists of connections among mostly identical units. • We develop an operationalization of the theory on graphon in the representation, scaling and search of neural stage-wise graphs that perform well in fair comparisons.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to deep learning with structural sparsity, which simultaneously exploits over-parameterized models and structural sparsity. The approach is based on a family of neural networks, from simple to complex, which are solutions of differential inclusions of inverse scale spaces. The Split Linearized Bregman Iteration (SplitLBI) is used to discretize the dynamics, with provable global convergence guarantee. The SplitLBI reduces to the standard gradient descent method when the coupling regularization is weak, while it leads to a sparse mirror descent when the coupling is strong. The support set of structural sparsity parameter Γ learned in the early stage of this inverse scale space discloses important sparse subnet architectures, which can be fine-tuned or retrained to achieve comparable test accuracy as the dense, over-parameterized networks. The convergence analysis is based on the Kurdyka-Łojasiewicz framework.",
        "Abstract": "Over-parameterization is ubiquitous nowadays in training neural networks to benefit both optimization in seeking global optima and generalization in reducing prediction error. However, compressive networks are desired in many real world applications and direct training of small networks may be trapped in local optima. In this paper, instead of pruning or distilling over-parameterized models to compressive ones, we propose a new approach based on \\emph{differential inclusions of inverse scale spaces}, that generates a family of models from simple to complex ones by coupling gradient descent and mirror descent to explore model structural sparsity. It has a simple discretization, called the Split Linearized Bregman Iteration (SplitLBI), whose global convergence analysis in deep learning is established that from any initializations, algorithmic iterations converge to a critical point of empirical risks. Experimental evidence shows that\\ SplitLBI may achieve state-of-the-art performance in large scale training on ImageNet-2012 dataset etc., while with \\emph{early stopping} it unveils effective subnet architecture with comparable test accuracies to dense models after retraining instead of pruning well-trained ones.",
        "Introduction": "  INTRODUCTION The expressive power of deep neural networks comes from the millions of parameters, which are optimized by Stochastic Gradient Descent (SGD) (Bottou, 2010) and variants like Adam (Kingma & Ba, 2015). Remarkably, model over-parameterization helps both optimization and generalization. For optimization, over-parameterization may simplify the landscape of empirical risks toward locating global optima efficiently by gradient descent method (Mei et al., 2018; 2019; Venturi et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018). On the other hand, over-parameterization does not necessarily result in a bad generalization or overfitting (Zhang et al., 2017), especially when some weight-size dependent complexities are controlled (Bartlett, 1997; Bartlett et al., 2017; Golowich et al., 2018; Neyshabur et al., 2019). However, compressive networks are desired in many real world applications, e.g. robotics, self- driving cars, and augmented reality. Despite that 1 regularization has been applied to deep learning to enforce the sparsity on weights toward compact, memory efficient networks, it sacrifices some prediction performance (Collins & Kohli, 2014). This is because that the weights learned in neural networks are highly correlated, and 1 regularization on such weights violates the incoherence or irrepresentable conditions needed for sparse model selection (Donoho & Huo, 2001; Tropp, 2004; Zhao & Yu, 2006), leading to spurious selections with poor generalization. On the other hand, 2 regularization is often utilized for correlated weights as some low-pass filtering, sometimes in the form of weight decay (Loshchilov & Hutter, 2019) or early stopping (Yao et al., 2007; Wei et al., 2017). Furthermore, group sparsity regularization (Yuan & Lin, 2006) has also been applied to neural networks, such as finding optimal number of neuron groups (Alvarez & Salzmann, 2016) and exerting good data locality with structured sparsity (Wen et al., 2016; Yoon & Hwang, 2017). Yet, without the aid of over-parameterization, directly training a compressive model architecture may meet the obstacle of being trapped in local optima in contemporary experience. Alternatively, researchers in practice typically start from training a big model using common task datasets like ImageNet, and then prune or distill such big models to small ones without sacrificing too much of the performance (Jaderberg et al., 2014; Han et al., 2015; Zhu et al., 2017; Zhou et al., 2017; Zhang et al., Under review as a conference paper at ICLR 2020 2016; Li et al., 2017; Abbasi-Asl & Yu, 2017; Yang et al., 2018; Arora et al., 2018). In particular, a recent study (Frankle & Carbin, 2019) created the lottery ticket hypothesis based on empirical observations: \"dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations\". How to effectively reduce an over-parameterized model thus becomes the key to compressive deep learning. Yet, Liu et al. (2019) raised a question, is it necessary to fully train a dense, over-parameterized model before finding important structural sparsity? In this paper, we provide a novel answer by exploiting a dynamic approach to deep learning with structural sparsity. We are able to establish a family of neural networks, from simple to complex, by following regularization paths as solutions of differential inclusions of inverse scale spaces. Our key idea is to design some dynamics that simultaneously exploit over-parameterized models and structural sparsity. To achieve this goal, the original network parameters are lifted to a coupled pair, with one weight set W of parameters following the standard gradient descend to explore the over-parameterized model space, while the other set of parameters learning structure sparsity in an inverse scale space, i.e., structural sparsity set Γ. The large-scale important parameters are learned at a fast speed while the small unimportant ones are learned at a slow speed. The two sets of parameters are coupled in an 2 regularization. The dynamics enjoys a simple discretization, i.e. the Split Linearized Bregman Iteration (SplitLBI), with provable global convergence guarantee shown in this paper. Here, SplitLBI is a natural extension of SGD with structural sparsity exploration: SplitLBI reduces to the standard gradient descent method when the coupling regularization is weak, while it leads to a sparse mirror descent when the coupling is strong. Critically, SplitLBI enjoys a nice property that important subnet architecture can be rapidly learned via the structural sparsity parameter Γ following the iterative regularization path, without fully training a dense network first. Particularly, the support set of structural sparsity parameter Γ learned in the early stage of this inverse scale space discloses important sparse subnet architectures. Such architectures can be fine-tuned or retrained to achieve comparable test accuracy as the dense, over-parameterized networks. As a result, the structural sparsity parameter Γ may enable us to rapidly find \"winning tickets\" in early training epochs for the \"lottery\" of identifying successful subnetworks that bear comparable test accuracy to the dense ones. This point is empirically validated in our experiments. Historically, the Linearized Bregman Iteration (LBI) was firstly proposed in applied mathematics as iterative regularization paths for image reconstruction and compressed sensing (Osher et al., 2005; Yin et al., 2008), later applied to logistic regression (Shi et al., 2013). The convergence analysis was given for convex problems (Yin et al., 2008; Cai et al., 2009), yet remaining open for non-convex problems met in deep learning. Osher et al. (2016) established statistical model selection consistency for high dimensional linear regression under the same irrepresentable condition as Lasso, later extended to generalized linear models (Huang & Yao, 2018). To relax such conditions, SplitLBI was proposed by Huang et al. (2016) to learn structural sparsity in linear models under weaker conditions than generalized Lasso, that was successfully applied in medical image analysis (Sun et al., 2017) and computer vision (Zhao et al., 2018). In this paper, it is the first time that SplitLBI is exploited to train highly non-convex neural networks with structural sparsity, together with a global convergence analysis based on the Kurdyka-Łojasiewicz framework Łojasiewicz (1963).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new approach for defending deep neural networks against adversarial attacks. A generative cleaning network with quantized nonlinear transform is proposed to destroy the sophisticated noise patterns of adversarial attacks and reconstruct the original image content. The generative cleaning network and detector network are jointly trained using adversarial learning. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by large margins in both white-box and black-box attacks.",
        "Abstract": "Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under white-box attacks.\nIn this paper, we develop a new generative cleaning network  with quantized nonlinear transform  for effective defense of deep neural networks.  The generative cleaning network, equipped with a trainable quantized nonlinear  transform block, is able to destroy the sophisticated noise pattern of adversarial attacks and recover the original image content. The generative cleaning network and attack detector network are jointly trained using adversarial learning  to minimize both perceptual loss and adversarial loss. Our extensive experimental results demonstrate that our approach outperforms the state-of-art methods by large margins in both white-box and black-box attacks. For example, it improves the classification accuracy for white-box attacks upon the second best method by more than 40\\% on the SVHN dataset and more than 20\\% on the challenging CIFAR-10 dataset. ",
        "Introduction": "  INTRODUCTION Recent research has shown that deep neural networks are sensitive to adversarial attacks ( Szegedy et al., 2013 ). Very small changes of the input image can fool the state-of-art classifier with very high success probabilities. During the past few years, a number of methods have been proposed to construct adversarial samples to attack the deep neural networks, including fast gradient sign (FGS) method (Goodfellow et al., 2014b), Jacobian-based saliency map attack (J-BSMA) ( Papernot et al., 2016a ), and projected gradient descent (PGD) attack ( Kurakin et al., 2016 ;  Madry et al., 2018 ). It has also been demonstrated that different classifiers can be attacked by the same adversarial perturbation ( Szegedy et al., 2013 ). The fragility of deep neural networks and the availability of these powerful attacking methods present an urgent need for effective defense methods. During the past few years, a number of deep neural network defense methods have been developed, including adversarial training ( Kurakin et al., 2016 ;  Szegedy et al., 2013 ), defensive distillation ( Papernot et al., 2016b ;  Carlini & Wagner, 2016 ;  Papernot & McDaniel, 2016 ), Magnet ( Meng & Chen, 2017 ) and featuring squeezing ( He et al., 2017 ;  Xu et al., 2017 ). It has been recognized that these methods suffer from significant performance degradation under strong attacks, especially white-box attacks with large magnitude and iterations (Samangouei et al., 2018). In this work, we explore a new approach to defend various attacks by developing a generative clean- ing network with quantized nonlinear transform. We recognize that the attack noise is not random and has sophisticated patterns. The attackers often generate noise patterns by exploring the spe- cific network architecture or classification behavior of the target deep neural network so that the small noise at the input layer can accumulate along the network inference layers, finally exceed the decision threshold at the output layer, and result in false decision. On the other hand, we know a well-trained deep neural networks are robust to random noise ( Arjovsky et al., 2017 ), such as Gaus- sian noise. Therefore, the key issue in network defense is to randomize or destroy the sophisticated pattern of the attack noise while preserving the original image content. Motivated by this observation, we design a new generative cleaning network with quantized nonlin- ear transform to first destroy the sophisticated noise patterns of adversarial attacks and then recover the original image content damaged during this nonlinear transform. We also construct a detector Under review as a conference paper at ICLR 2020 network which serves as the dual network for the target classifier to be defended. The generative cleaning network and detector network are jointly trained using adversarial learning so that the de- tector network cannot detect the existence of attack noise pattern in the images recovered by the generative cleaning network. Our extensive experimental results demonstrate that our approach out- performs the state-of-art methods by large margins in both white-box and black-box attacks. It significantly improves the classification accuracy for white-box PGD attacks upon the second best method by more than 40% on the SVHN dataset from 46.90% to 93.80%, and more than 20% on the challenging CIFAR-10 dataset from 60.15% to 86.05%. The major contributions of this work can be summarized as follows. (1) We have proposed a new approach for deep neural network defense by developing a unique generative cleaning network with quantized nonlinear transform. (2) We have formulated the problem of destroying the noise patterns of adversarial attacks and reconstructing original image content into generative adversarial network design and training which considers both perceptual loss and adversarial loss. (3) Our new method has significantly improved the performance of the state-of-the-art methods in the literature under a wide variety of attacks. The rest of this paper is organized as follows. Section 2 reviews related work. The proposed method is presented in Section 3. Experimental results and performance comparison with existing methods are provided in Section 4. Section 5 concludes the paper.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new learning-to-learn method for semi-supervised learning that can be put in a meta-learning framework. The method involves learning an update rule to label unlabeled training samples such that training a model using these predicted labels improves its performance not only on itself but also on a meta-validation set. The method is highly generic and can easily be incorporated to the state-of-the-art methods and boost their performance, in particular, in the regime where the number of available labels is limited. Moreover, the method is not limited to classification problems, and can be extended to semi-supervised regression tasks where the output space is continuous and achieves significant performance gains.",
        "Abstract": "Recent semi-supervised learning methods have shown to achieve comparable results to their supervised counterparts while using only a small portion of labels in image classification tasks thanks to their regularization strategies. In this paper, we take a more direct approach for semi-supervised learning and propose learning to impute the labels of unlabeled samples such that a network achieves better generalization when it is trained on these labels. We pose the problem in a learning-to-learn formulation which can easily be incorporated to the state-of-the-art semi-supervised techniques and boost their performance especially when the labels are limited. We demonstrate that our method is applicable to both classification and regression problems including image classification and facial landmark detection tasks.",
        "Introduction": "  INTRODUCTION Semi-supervised learning (SSL) ( Chapelle et al., 2009 ) is one of the approaches to learn not only from labeled samples but also unlabeled ones. Under certain assumptions such as presence of smooth prediction functions that map data to labels, of low-dimensional manifolds that the high-dimensional data lies ( Chapelle et al., 2009 ), SSL methods provide a way to leverage the information at unlabeled data and lessens the dependency on labels. Recent work ( Tarvainen & Valpola, 2017 ;  Miyato et al., 2018 ;  Berthelot et al., 2019 ;  Xie et al., 2019 ) have shown that semi-supervised learning by using only a small portion of labels can achieve competitive results with the supervised counterparts in image classification tasks (i.e. CIFAR10, SVHN). They built on a variation of the well-known iterative bootstrapping method ( Yarowsky, 1995 ) where in each iteration a classifier is trained on the current set of labeled data, the learned classifier is used to generate label for unlabeled data. However, the generalization performance of this approach is known to suffer from fitting the model on wrongly labeled samples and overfitting into self-generated labels ( Tarvainen & Valpola, 2017 ). Thus, they mitigate these issues by various regularization strategies. While there exist several regularization ( Srivastava et al., 2014 ) and augmentation ( Zhang et al., 2018 ;  DeVries & Taylor, 2017 ) techniques in image recognition problems which are known to increase the generalization performance of deep networks, specific regularization strategies for semi-supervised classification are used to estimate correct labels for the unlabeled data by either encouraging the models to produce confident outputs ( Lee, 2013 ;  Grandvalet & Bengio, 2005 ) and/or consistent output distributions when its inputs are perturbed ( Tarvainen & Valpola, 2017 ;  Miyato et al., 2018 ;  Berthelot et al., 2019 ). The assumption here is that if a good regularization strategy exists, it can enable the network to recover the correct labels for the unlabeled data and then the method can obtain similar performance with the supervised counterpart when trained on them. Though this ad-hoc paradigm is shown to be effective, it raises a natural question for a more direct approach: Can we instead encourage the network to label the unlabeled data such that the network achieves better generalization performance when trained with them? In this paper, we propose a new learning-to-learn method for semi-supervised learning that can be put in a meta-learning framework to address this question. Our method involves learning an update rule to label unlabeled training samples such that training our model using these predicted labels improves its performance not only on itself but also on a meta-validation set. Crucially, our method is highly generic and can easily be incorporated to the state-of-the-art methods ( Lee, 2013 ;  Berthelot et al., 2019 ) and boost their performance, in particular, in the regime where the number of available labels is limited. Moreover, our method is not limited to classification problems, we show that it can Under review as a conference paper at ICLR 2020 be extended to semi-supervised regression tasks where the output space is continuous and achieves significant performance gains.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the vulnerability of deep neural networks (DNNs) to adversarial samples, and proposes a post-averaging method to defend against four popular adversarial attacking methods. Through Fourier analysis of neural networks, the authors explain why neural networks are inherently vulnerable to adversarial samples due to the underlying model structure. Experimental results on the ImageNet and the CIFAR-10 datasets show that the proposed post-averaging method can successfully defend over 80-96% of the adversarial samples generated by the examined attacks with little performance degradation on the original clean images.",
        "Abstract": "In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We then demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 80-96% of the adversarial samples generated by these methods without introducing significant performance degradation (less than 2%) on the original clean images.",
        "Introduction": "  INTRODUCTION Although deep neural networks (DNN) have shown to be powerful in many machine learning tasks,  Szegedy et al. (2013)  found that they are vulnerable to adversarial samples. Adversarial samples are subtly altered inputs that can fool the trained model to produce erroneous outputs. They are more commonly seen in image classification task and typically the perturbations to the original images are so small that they are imperceptible to human eye. Research in adversarial attacks and defences is highly active in recent years. In the attack side, many attacking methods have been proposed ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ;  Papernot et al., 2016a ;  Papernot et al., 2017 ;  Moosavi-Dezfooli et al., 2016 ;  Kurakin et al., 2016 ;  Madry et al., 2017 ;  Carlini and Wagner, 2017a ;  Chen et al., 2017 ;  Alzantot et al., 2018 ;  Brendel et al., 2017 ), with various ways to generate effective adversarial samples to circumvent new proposed defence methods. However, since different attacks usually are effective to different defences or datasets, there is no consensus on which attack is the strongest. Hence for the sake of simplicity, in this work, we will evaluate our proposed defence approach against four popular attacks for empirical analysis. In the defence side, various defence mechanisms have also been proposed, including adversarial training ( Rozsa et al., 2016 ;  Kurakin et al., 2016 ;  Tramèr et al., 2017 ;  Madry et al., 2017 ), network distillation ( Papernot et al., 2016b ), gradient masking ( Nguyen and Sinha, 2017 ), adversarial detection ( Feinman et al., 2017 ) and adding modifications to neural networks ( Xie et al., 2017 ). Nonetheless, many of them were quickly defeated by new types of attacks ( Carlini and Wagner, 2016 ;  2017b ; c ; a ;  Athalye et al., 2018 ;  Athalye and Carlini, 2018 ;  Alzantot et al., 2018 ).  Madry et al. (2017)  tried to provide a theoretical security guarantee for adversarial training by a min-max loss formulation, but the difficulties in non-convex optimization and in finding the ultimate adversarial samples for training may loosen this robustness guarantee. As a result, so far there is no defence that is universally robust to all adversarial attacks. Along the line of researches, there were also investigations into the properties and existence of adversarial samples.  Szegedy et al. (2013)  first observed the transferability of adversarial samples across models trained with different hyper-parameters and across different training sets. They also Under review as a conference paper at ICLR 2020 attributed the adversarial samples to the low-probability blind spots in the manifold. In ( Goodfellow et al., 2014 ), the authors explained adversarial samples as \"a result of models being too linear, rather than too nonlinear.\" In ( Papernot et al., 2016 ), the authors showed the transferability occurs across models with different structures and even different machine learning techniques in addition to neural networks. In summary, the general existence and transferability of adversarial samples are well known but the reason of adversarial vulnerability still needs further investigation. Generally speaking, when we view neural network as a multivariate function f (x) of input x, if a small imperceptible perturbation ∆x leads to a huge fluctuation ∆f (x), the large quantity ∆f (x)/∆x essentially corresponds to high frequency components in the Fourier spectrum of f (x). In this paper, we will start with the Fourier analysis of neural networks and elucidate why there always exist some decaying but nonzero high frequency response components in neural networks. Based on this analysis, we show that neural networks are inherently vulnerable to adversarial samples due to the underlying model structure. Next, we propose a simple post-averaging method to tackle this problem. Our proposed method is fairly simple since it works as a post-processing stage of any given neural network models and it does not require re-training the networks at all. Furthermore, we have evaluated the post-averaging method against four popular adversarial attacking methods and our method is shown to be universally effective in defending all examined attacks. Experimental results on the ImageNet and the CIFAR-10 datasets have shown that our simple post-averaging method can successfully defend over 80-96% of the adversarial samples generated by these attacks with little performance degradation (less than 2%) on the original clean images.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an analytical framework for predicting the learning curves of deep neural networks (DNNs) in the NTK-regime. We show that the learning curves of DNNs in this regime are equivalent to those of Gaussian Processes Regression (GPR) with a kernel known as the Neural Tangent Kernel (NTK). We then use the equivalence kernel (EK) to derive analytical predictions for the learning curves of DNNs in the NTK-regime. Our predictions are in good agreement with empirical results. \n\nAbstract: This paper presents an analytical framework for predicting the learning curves of deep neural networks (DNNs) in the NTK-regime. We show that the learning curves of DNNs in this regime are equivalent to those of Gaussian Processes Regression (GPR) with a kernel known as the Neural Tangent Kernel (NTK). We use the equivalence kernel (EK) to derive analytical predictions for the learning curves of DNNs in the NTK-regime, which are in good agreement with empirical results.",
        "Abstract": "A series of recent works established a rigorous correspondence between very wide deep neural networks (DNNs), trained in a particular manner, and noiseless Bayesian Inference with a certain Gaussian Process (GP) known as the Neural Tangent Kernel (NTK). Here we extend a known field-theory formalism for GP inference to get a detailed understanding of learning-curves in DNNs trained in the regime of this correspondence (NTK regime). In particular, a renormalization-group approach is used to show that noiseless GP inference using NTK, which lacks a good analytical handle, can be well approximated by noisy GP inference on a related kernel we call the renormalized NTK. Following this, a perturbation-theory analysis is carried in one over the dataset-size yielding analytical expressions for the (fixed-teacher/fixed-target) leading and sub-leading asymptotics of the learning curves. At least for uniform datasets, a coherent picture emerges wherein fully-connected DNNs have a strong implicit bias towards functions which are low order polynomials of the input.    ",
        "Introduction": "  INTRODUCTION Several pleasant features underlay the success of deep learning: The scarcity of bad minima encoun- tered in their optimization ( Draxler et al., 2018 ;  Choromanska et al., 2014 ), their ability to generalize well despite being heavily over-parameterized ( Neyshabur et al., 2018 ; 2014) and expressive ( Zhang et al., 2016 ), and their ability to generate internal representations which generalize across different domains and tasks ( Yosinski et al., 2014 ;  Sermanet et al., 2013 ). Due to the complexity of DNNs our current understanding of these features is still largely empirical. Notwithstanding, progress has been made recently in the highly over-parametrized regime ( Daniely et al., 2016 ;  Jacot et al., 2018 ) due to the fact that the networks' parameters, in all non-linear layers, change in a minor yet important manner during training. This facilitated the derivation of various bounds ( Allen-Zhu et al., 2018 ;  Cao & Gu, 2019b ; a ) on generalization and, more relevant for this work, the following correspondence with GPs: Considering finite-depth DNNs which are much wider than the dataset-size, trained with MSE loss, no weight decay, and at vanishing learning rate (the NTK-regime) one finds that the initialization-averaged predictions are the same as those of Gaussian Processes Regression (GPR) with a kernel known as the NTK. Several subsequent works corroborated these results empirically ( Lee et al., 2018 ;  Lee et al., 2019 ;  Arora et al., 2019 ) and extended them ( Arora et al., 2019 ). For fully-connected DNNs, the NTK-regime (and GPs associated with DNNs in general ( Lee et al., 2018 ;  Novak et al., 2018 )) seems to faithfully capture the generalization power of DNNs trained with MSE loss ( Lee et al., 2019 ). One of the most detailed objects quantifying generalization are learning-curves: graphs of how the test error diminishes with the number of datapoints (N ). There are currently no analytical predictions or bounds we are aware of for DNN learning-curves which are tight even just in terms of their scaling with N , let alone tight in an absolute sense. In contrast, the theory of GPR is rich with analytical tools which have yielded in the past high accuracy predictions for learning curves. One of the most transparent ones is the equivalence kernel (EK) ( Rasmussen & Williams, 2005 ) which will be introduced in section 3. In short, EK gives an intuitive functional prediction of the expected GPR predictor of a fixed target function when averaged over datasets of size N , and consequently of the learning curves.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a new and generic approximation method, Neural Auto-Regressive model Approximator (NARA), which can reduce the generation complexity of auto-regressive (AR) models by relaxing an inevitable AR nature and enabling AR models to employ powerful parallelization techniques in sequential data generation. NARA consists of three modules; (1) a prior-sample predictor, (2) a confidence predictor, and (3) the original AR model. Experiments show that NARA can largely reduce the sample inference complexity even with a heavy and complex model on a difficult data domain such as image pixels. The main contributions of this work are (1) the introduction of a new and generic approximation method that can accelerate any AR generation procedure, (2) the reliability of approximated samples remains reliable by the accompanied confidence prediction model that measures the sample credibility, and (3) the approximated samples from the method can eventually converge toward the true future sample.",
        "Abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}",
        "Introduction": "  INTRODUCTION The auto-regressive (AR) model, which infers and predicts the causal relationship between the previous and future samples in a sequential data, has been widely studied since the beginning of machine learning research. The recent advances of the auto-regressive model brought by the neural network have achieved impressive success in handling complex data including texts ( Sutskever et al., 2011 ), audio signals ( Vinyals et al., 2012 ;  Tamamori et al., 2017 ;  van den Oord et al., 2016a ), and images ( van den Oord et al., 2016b ;  Salimans et al., 2017 ). It is well known that AR model can learn a tractable data distribution p(x) and can be easily extended for both discrete and continuous data. Due to their nature, AR models have especially shown a good fit with a sequential data, such as voice generation ( van den Oord et al., 2016a ) and provide a stable training while they are free from the mode collapsing problem ( van den Oord et al., 2016b ). However, these models must infer each element x i of the data x = [x 1 , x 2 , · · · , x i , · · · , x N ] in a serial manner, requiring O(N ) times more than the other non-sequential estimators, which outputs x at once ( Garnelo et al., 2018 ;  Kim et al., 2019 ;  Kingma & Welling, 2014 ;  Goodfellow et al., 2014 ). Moreover, it is difficult to employ recent parallel computation because AR models always require a previous time step by definition. This mostly limits the use of the AR models in practice despite their advantages. To resolve the problem, we introduce a new and generic approximation method, Neural Auto- Regressive model Approximator (NARA), which can be easily plugged into any AR model. We show that NARA can reduce the generation complexity of AR models by relaxing an inevitable AR nature and enables AR models to employ the powerful parallelization techniques in the sequential data generation, which was difficult previously. NARA consists of three modules; (1) a prior-sample predictor, (2) a confidence predictor, and (3) the original AR model. To relax the AR nature, given a set of past samples, we first assume that each sample of the future sequence can be generated in an independent and identical manner. Thanks to the i.i.d. assumption, using the first module of NARA, we can sample a series of future priors and these future priors are post-processed by the original AR model, generating a set of raw predictions. The confidence predictor evaluates the credibility of these raw samples and decide whether the model needs re-sampling or not. The confidence predictor plays an important role in that the approximation errors can be accumulated during the sequential AR generation process if the Under review as a conference paper at ICLR 2020 erroneous samples with low confidence are left unchanged. Therefore, in our model, the sample can be drawn either by the mixture of the AR model or the proposed approximation method, and finally the selection of the generated samples are guided by the predicted confidence. We evaluate NARA with various baseline AR models and data domains including simple curves, image sequences ( Yoo et al., 2017 ), CelebA ( Liu et al., 2015a ), and ImageNet ( Deng et al., 2009 ). For the sequential data (simple curves and golf), we employed the Long Short-Term Memory models (LSTM) ( Hochreiter & Schmidhuber, 1997 ) as a baseline AR model while PixelCNN++ ( Salimans et al., 2017 ) is used for the image generation (CelebA and ImageNet). Our experiments show that NARA can largely reduce the sample inference complexity even with a heavy and complex model on a difficult data domain such as image pixels. The main contributions of our work can be summarized as follows: (1) we introduce a new and generic approximation method that can accelerate any AR generation procedure. (2) Compared to a full AR generation, the quality of approximated samples remains reliable by the accompanied confidence prediction model that measures the sample credibility. (3) Finally, we show that this is possible because, under a mild condition, the approximated samples from our method can eventually converge toward the true future sample. Thus, our method can effectively reduce the generation complexity of the AR model by partially substituting it with the simple i.i.d. model.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes BERT-AL, a new architecture that combines Transformer and LSTM to solve the problem of Transformer not being able to process very long text. BERT-AL is demonstrated to be effective on document-level tasks such as text summarization, machine reading comprehension, and long text classification. Experiments on the CNN/Daily Mail dataset show that BERT-AL can outperform BERTSUM, the BERT-based state-of-the-art, when finetuning from the pretrained BERT model with the same maximum sequence length. Additionally, the proposed method of applying multi-channel LSTM on hidden states from transformers can be used in other Transformer based pretrained models.",
        "Abstract": "Pretrained language models attract lots of attentions, and they take advantage of the two-stages training process: pretraining on huge corpus and finetuning on specific tasks. Thereinto, BERT (Devlin et al., 2019) is a Transformer (Vaswani et al., 2017) based model and has been the state-of-the-art for many kinds of Nature Language Processing (NLP) tasks. However, BERT cannot take text longer than the maximum length as input since the maximum length is predefined during pretraining. When we apply BERT to long text tasks, e.g., document-level text summarization: 1) Truncating inputs by the maximum sequence length will decrease performance, since the model cannot capture long dependency and global information ranging the whole document. 2) Extending the maximum length requires re-pretraining which will cost a mass of time and computing resources. What's even worse is that the computational complexity will increase quadratically with the length, which will result in an unacceptable training time. To resolve these problems, we propose to apply Transformer to only model local dependency and recurrently capture long dependency by inserting multi-channel LSTM into each layer of BERT. The proposed model is named as BERT-AL (BERT for Arbitrarily Long Document Understanding) and it can accept arbitrarily long input without re-pretraining from scratch. We demonstrate BERT-AL's effectiveness on text summarization by conducting experiments on the CNN/Daily Mail dataset. Furthermore, our method can be adapted to other Transformer based models, e.g., XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), for various NLP tasks with long text.",
        "Introduction": "  INTRODUCTION In recent years, neural networks are proposed to solve various NLP tasks. Especially, pretrained language models ( Peters et al., 2018 ;  Radford et al., 2018 ;  Devlin et al., 2019 ;  Yang et al., 2019 ) attract lots of attentions, which take advantage of the two-stages training process: pretraining on unlabeled corpus and then finetuning on specific tasks. The most famous model is BERT. BERT and its varieties are the state-of-the-art for many kinds of NLP tasks. The power of BERT does not only come from its architecture of networks, but also because it can be pretrained on a mass of text as a masked language model. BERT can be used to solve almost all NLP tasks, and especially it can perform best on datasets with short text, e.g., GLUE ( Wang et al., 2018 ) and Squad ( Rajpurkar et al., 2016 ). However, there are still many document-level tasks, e.g., document-level text summarization ( Hermann et al., 2015 ), long-document machine reading comprehension ( Hewlett et al., 2016 ) and long text classification ( Zhang et al., 2015 ). BERT cannot be finetuned for such tasks with long text directly or perform good on these tasks, since it is limited by the fixed-length position embedding which was determined during pretraining. We employ document-level text summarization as an example, which usually has longer text than the maximum sequence length of BERT. Intuitively, there are two alternative solutions: 1) Truncating inputs by the maximum sequence length to fit the BERTs constraint. 2) Increasing the length of position embedding and re-pretraining the BERT from scratch. The first method will decrease performance, obviously, since some useful information placing behind the maximum sequence length is discarded by truncating. E.g., for text summarization, if the key point sentence locates at the end of the document, it never be recalled Under review as a conference paper at ICLR 2020 even though the model is powerful. For the second method, re-pretraining the BERT from scratch will cost a mass of computing time and resources. Whats even worse is that the computational complexity will increase quadratically with the length, which will result in an unacceptable training time. For example, the XLNet-Large ( Yang et al., 2019 ) costs 2.5 days on 512 TPU v3 chips and the RoBERTa ( Liu et al., 2019 ) costs 1 day on 1024 V100 GPUs. To resolve these problems, we propose BERT-AL (BERT for Arbitrarily Long Document Under- standing) model that extracts local features by applying parallel multi-layer Transformers into chun- ked input and employs multi-channel LSTMs to capture global information crossing Transformers. This fusion breaks the limitation of BERT by the ability of capture unlimited sequential information from LSTM, and makes it be able to process arbitrarily long text. On the other hand, it also skillfully avoids gradient vanishing and exploding problem ( Li et al., 2018 ) of LSTM since only few steps are required by multi-channel LSTM. Therefore, BERT-AL can solve the problems of original BERT: 1) For document-level tasks, BERT-AL can directly take all text as the input without truncating. 2) When the input length is longer then maximum sequence length of BERT, BERT-AL still can load the pretrained BERT model, which avoids pretraining much longer BERT model from scratch. We demonstrate BERT-ALs effectiveness on text summarization by conducting experiments on the CNN/Daily Mail dataset ( Hermann et al., 2015 ) with various maximum sequence lengths of pre- trained BERT. The results prove that BERT-AL can consistently outperform BERTSUM ( Liu, 2019 ) which is the BERT-based state-of-the-art, when finetuning from the pretrained BERT model with the same maximum sequence length. Additionally, BERT-AL is a general NLP model which has no specific setting for text summarization, so it can be easily adapted to other tasks with long text, e.g., document-level machine reading comprehension and long text classification. Furthermore, the method, applying multi-channel LSTM on hidden states from transformers, also can be used in other Transformer based pretrained models, e.g., XLNet and RoBERTa. In summary, contributions of this paper are shown as follow. 1) We propose a new architecture that combines Transformer and LSTM, which resolve the problem Transformer cannot be used in very long text, and skillfully avoid LSTMs drop backs. 2) We propose multi-channel LSTM only applied on the corresponding position across different Transformers, which can take fully advantage of LSTM without hurting Transformers representation too much. 3) We conduct experiments to prove BERT-AL can outperform other models with the BERT pre- trained under the same maximum sequence length, and can perform very close to BERTSUM with at least twice the maximum length than ours.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the game of Contract Bridge, a trick-taking card game with two teams of two players. It discusses recent advancements in AI programs for the game, such as Libratus, DeepStack, and Bayesian Action Decoder, and how they have outperformed top level experts. It also discusses the historical success of AI programs in the game, such as GIB, Jack 2, and Wbridge5, and how they have demonstrated strong performances against top level professional humans.",
        "Abstract": "Contract bridge is a multi-player imperfect-information game where one partnership collaborate with each other to compete against the other partnership. The game consists of two phases: bidding and playing. While playing is relatively easy for modern software, bidding is challenging and requires agents to learn a communication protocol to reach the optimal contract jointly, with their own private information. The agents need to exchange information to their partners, and interfere opponents, through a sequence of actions. In this work, we train a strong agent to bid competitive bridge purely through selfplay, outperforming WBridge5, a championship-winning software. Furthermore, we show that explicitly modeling belief is not necessary in boosting the performance. To our knowledge, this is the first competitive bridge agent that is trained with no domain knowledge. It outperforms previous state-of-the-art that use human replays with 70x fewer number of parameters.",
        "Introduction": "  INTRODUCTION Games have long been recognized as a testbed for reinforcement learning. Recent technology advancements have outperformed top level experts in perfect information games like Chess ( Campbell et al., 2002 ) and Go ( Silver et al., 2016 ; 2017), through human supervision and selfplay. During recent years researchers have also steered towards imperfection information games, such as Poker ( Brown & Sandholm, 2018 ;  Moravčík et al., 2017 ), Dota 2 1 , and real-time strategy games (Arulkumaran et al., 2019;  Tian et al., 2017 ). There are multiple programs which focus specifically in card games. Libratus ( Brown & Sandholm, 2018 ) and DeepStack ( Moravčík et al., 2017 ) outperforms human experts in two-player Texas Holdem. Bayesian Action Decoder ( Foerster et al., 2018b ) is able to achieve near optimal performance in multi-player collaborative games like Hanabi. Contract Bridge, or simply Bridge, is a trick-taking card game with 2 teams, each with 2 players. There are 52 cards (4 suits, each with 13 cards). Each player is dealt with 13 cards. The game has two phases: bidding and playing. In the bidding phase, each player can only see their own card and negotiate in turns via proposing contract, which sets an explicit goal to aim at during the playing stage. High contracts override low ones. Players with stronger cards aim at high contracts for high reward; while failing to reach the contract, the opponent team receives rewards. Therefore, players utilize the bidding phase to reason about their teammate and opponents' cards for a better final contract. In the playing phase, one player reveals their cards publicly. In each round, each player plays one card in turn and the player with best card wins the round. The score is simply how many rounds each team can win. We introduce the game in more detail in Appendix A. Historically AI programs can handle the playing phase well. Back in 1999, the GIB program ( Gins- berg, 1999 ) placed 12th among 34 human experts partnership, in a competition without the bidding phase. In more recent years, Jack 2 and Wbridge5 3 , champions of computer bridge tournament, has demonstrated strong performances against top level professional humans.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a gradient-free optimization algorithm for minimizing a black-box function in which only the oracle of function value evaluations is available. The algorithm samples perturbed vectors from a mixture of distributions, one of which is the standard Gaussian distribution with an isotropic covariance. Theoretical guarantees are provided for the convergence of the algorithm when the underlying function is convex. Experiments are conducted to show the effectiveness of the proposed method, which is competitive with other algorithms that sample from a fixed component of the mixture.",
        "Abstract": "Evolution strategies or zeroth-order optimization algorithms have become popular in some areas of optimization and machine learning where only the oracle of function value evaluations is available. The central idea in the design of the algorithms is by querying function values of some perturbed points in the neighborhood of the current update and constructing a pseudo-gradient using the function values. In recent years, there is a growing interest in developing new ways of perturbation. Though the new perturbation methods are well motivating, most of them are criticized for lack of convergence guarantees even when the underlying function is convex. Perhaps the only methods that enjoy convergence guarantees are the ones that sample the perturbed points uniformly from a unit sphere or from a multivariate Gaussian distribution with an isotropic covariance. In this work, we tackle the non-convergence issue and propose sampling perturbed points from a mixture of distributions. Experiments show that our proposed method can identify the best perturbation scheme for the convergence and might also help to leverage the complementariness of different perturbation schemes.\n",
        "Introduction": "  INTRODUCTION We consider optimizing a function f (·) : R d → R in the setting that only querying function values is allowed and there is no access to gradients of the function. Alternatively, we aim at minimizing a black-box function in which only the oracle of function value evaluations is available, There are growing interest in studying \"gradient-free\" optimization due to its applications in hyper- parameter search (e.g.,  Bergstra et al. (2011) ;  Koch et al. (2018) ), reinforcement learning (e.g.,  Sehnke et al. (2010) ;  Mania et al. (2018) ;  Salimans et al. (2017) ;  Choromanski et al. (2018) ;  Vemula et al. (2019) ), or black-box adversarial attacks on deep neural nets (e.g.,  Chen et al. (2017) ;  Ilyas et al. (2018) ;  Papernot et al. (2017) ). In hyper-parameter search, the goal is to find the best values of a set of hyper-parameters for a machine learning model (e.g., a neural net). One can model the task of hyper-parameter search as optimizing an unknown/black box function as (1). Specifically, one can let w be in the space of hyper-parameters in the way that each element of w represents the value of a specific hyper-parameter and different points w in the hyper-parameter space correspond to different realizations of the hyper-parameters. The mapping (i.e., f (·)) from hyper-parameter values to a performance metric (e.g., classification error) of using those hyper-parameter values is unknown. Therefore, the gradient of the mapping/function is not available. One can only query the function value (i.e., obtaining f (w)) by training a model with the hyper-parameter values indicated by w. Obtaining the arg min of (1) in this case corresponds to finding the best values of the hyper- parameters (and consequently getting the best trained model). Because of the broad applicability of zeroth order optimization algorithms, recently there has been a spate of research in improving them from different respects. Standard zeroth order algorithms construct pseudo-gradients by sampling some perturbed points from a Gaussian distribution with an isotropic covariance (e.g.,  Nesterov & Spokoiny (2017) ;  Duchi et al. (2012)  or uniformly from a unit sphere (e.g.,  Flaxman et al. (2005) ;  Duchi et al. (2012) ;  Shamir (2017) ). Algorithm 1 describes a popular technique called \"'Gaussian smoothing\". It first samples some K perturbed vectors v k t from a Gaussian distribution, then a pseudo-gradient g t is constructed Under review as a conference paper at ICLR 2020 Algorithm 1 Gaussian smoothing. 1: Require: hyper-parameters σ, K and η. Obtain the next update wt+1 = wt − ηgt. 7: end for by taking a weighted average of these perturbed points. The pseudo gradient is subsequently used to obtain the next update w t+1 . Very recently, there has been a trend of studies in proposing sampling the perturbed vectors from some non-isotropic Gaussian distributions (e.g.  Maheswaranathan et al. (2019) ;  Choromanski et al. (2019a) ;  Ye et al. (2019) ). They consider sampling perturbed vectors by v k t ∼ N (0, Σ) such that the covariance Σ may not be a scale of the identity matrix.  Mah- eswaranathan et al. (2019)  propose letting Σ be related with the span of latest r pseudo-gradients to reflect the local geometry.  Choromanski et al. (2019a)  propose updating Σ in the sense to tracking the low-dimensional manifold of the gradient space.  Ye et al. (2019)  propose letting Σ be the in- verse of a diagonal matrix in a way that the update rule of each diagonal element is in the fashion of updating the second moment quantity in \"Adam\" optimization algorithm of Kingma & Ba (2015). Nevertheless, these sampling methods have not really been shown to lead to the convergence to a global minimum even when the underlying function f (·) is convex, albeit some motivating toy examples or some analysis (but not related to the convergence) are provided. In this paper, we tackle the issue by proposing sampling from a mixture of distributions in which one of them is the standard Gaussian distribution with an isotropic covariance. Our method leverages a bandit algorithm for adaptively selecting which distribution the perturbed vectors are sampled from. By building the theoretical guarantee of the bandit algorithm and the convergence guarantee of the algorithm which always sampling from an isotropic Gaussian distribution (i.e., Algorithm 1), we can show that our method converges with a high probability when the underlying function is convex. We conduct experiments to show that our algorithm works well in practice. Our proposed algorithm is competitive with any algorithms that always sample perturbed vectors from a fixed component (distribution) of the mixture which our algorithm considers. This shows the effectiveness of the proposed method, as one would not know which perturbation scheme is the best beforehand. Furthermore, the proposed algorithm might outperform any of them when the complementariness of different perturbation schemes are exploited.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the Frequency Principle (F-Principle), a common behavior of the gradient-based training process of Deep Neural Networks (DNNs), which states that DNNs often fit target functions from low to high frequencies during the training process. We design two methods, projection and filtering, to demonstrate the F-Principle in the training process of DNNs for high-dimensional benchmarks, such as MNIST and CIFAR10. We also characterize a stark difference between DNNs and conventional numerical schemes on various scientific computing problems, and explain with theories under an idealized setting how the smoothness/regularity of commonly used activation functions contributes to the F-Principle. Finally, we discuss the implication of the F-Principle to the generalization of DNNs. Our results demonstrate the universality of the F-Principle and provide an understanding of good and poor generalization of DNNs.",
        "Abstract": "We study the training process of Deep Neural Networks (DNNs) from the Fourier analysis perspective. We demonstrate a very universal Frequency Principle (F-Principle) --- DNNs often fit target functions from low to high frequencies --- on high-dimensional benchmark datasets, such as MNIST/CIFAR10, and deep networks, such as VGG16. This F-Principle of DNNs is opposite to the learning behavior of most conventional iterative numerical schemes (e.g., Jacobi method), which exhibits faster convergence for higher frequencies, for various scientific computing problems. With a naive theory, we illustrate that this F-Principle results from the regularity of the commonly used activation functions. The F-Principle implies an implicit bias that DNNs tend to fit training data by a low-frequency function. This understanding provides an explanation of good generalization of DNNs on most real datasets and bad generalization of DNNs on parity function or randomized dataset.",
        "Introduction": "  INTRODUCTION Understanding the training process of Deep Neural Networks (DNNs) is a fundamental problem in the area of deep learning. We find a common behavior of the gradient-based training process of DNNs, that is, a Frequency Principle (F-Principle): DNNs often fit target functions from low to high frequencies during the training process. In another word, at the early stage of training, the low-frequencies are fitted and as iteration steps of training increase, the high-frequencies are fitted. For example, when a DNN is trained to fit y = sin(x) + sin(2x), its output would be close to sin(x) at early stage and as training goes on, its output would be close to sin(x) + sin(2x). F-Principle was observed empirically in synthetic low-dimensional data with MSE loss during DNN training ( Xu et al., 2018 ;  Rahaman et al., 2018 ). However, in deep learning, empirical phenomena could vary from one network structure to another, from one dataset to another and could exhibit significant difference between synthetic data and high- dimensional real data. Therefore, the universality of the F-Principle remains an important problem for further study. Especially for high-dimensional real problems, because the computational cost of high-dimensional Fourier transform is prohibitive in practice, it is of great challenge to demonstrate the F-Principle. On the other hand, the mechanism underlying the F-Principle and its implication to the application of DNNs, e.g., design of DNN-based PDE solver, as well as their generalization ability are also important open problems to be addressed. In this work, we design two methods, i.e., projection and filtering methods, to show that the F- Principle exists in the training process of DNNs for high-dimensional benchmarks, i.e., MNIST ( LeCun, 1998 ), CIFAR10 ( Krizhevsky et al., 2010 ). The settings we have considered are i) different DNN architectures, e.g., fully-connected network, convolutional neural network (CNN), and VGG16 ( Simonyan & Zisserman, 2014 ); ii) different activation functions, e.g., tanh and rectified linear unit (ReLU); iii) different loss functions, e.g., cross entropy, mean squared error (MSE), and loss energy functional in variational problems. These results demonstrate the universality of the F-Principle. To facilitate the designs and applications of DNN-based schemes, we characterize a stark difference between DNNs and conventional numerical schemes on various scientific computing problems, where most of the conventional methods (e.g., Jacobi method) exhibit the opposite convergence behavior Under review as a conference paper at ICLR 2020 - faster convergence for higher frequencies. This difference implies that DNN can be adopted to accelerate the convergence of low frequencies for computational problems. We also intuitively explain with theories under an idealized setting how the smoothness/regularity of commonly used activation functions contributes to the F-Principle. Note that this mechanism is rigorously demonstrated for DNNs of general settings in a subsequent work ( Luo et al., 2019 ). Finally, we discuss that the F-Principle provides an understanding of good generalization of DNNs in many real datasets ( Zhang et al., 2016 ) and poor generalization in learning the parity function ( Shalev-Shwartz et al., 2017 ;  Nye & Saxe, 2018 ), that is, the F-Principle which implies that DNNs prefer low frequencies, is consistent with the property of low frequencies dominance in many real datasets, e.g., MNIST/CIFAR10, but is different from the parity function whose spectrum concentrates on high frequencies. Compared with previous studies, our main contributions are as follows: 1. By designing both the projection and filtering methods, we consistently demonstrate the F-Principle for MNIST/CIFAR10 over various architectures such as VGG16 and various loss functions. 2. For the application of solving differential equations, we show that (i) conventional numerical schemes learn higher frequencies faster whereas DNNs learn lower frequencies faster by the F- Principle, (ii) convergence of low frequencies can be greatly accelerated with DNN-based schemes. 3. We present theories under an idealized setting to illustrate how smoothness/regularity of activation function contributes to the F-Principle. 4. We discuss in detail the implication of the F-Principle to the generalization of DNNs that DNNs are implicitly biased towards a low frequency function and provide an explanation of good and poor generalization of DNNs for low and high frequency dominant target functions, respectively.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel approach to extract the hidden structure of the external environment by combining local learning approaches with deep generative models. The proposed Local VAE model is capable of learning similar structures between neighbors quickly by treating the neighborhood of each data point as a task to adopt the meta-learning. The numerical experiments show that the locality enables the model to achieve the disentangled representation for each subspace without any label information.",
        "Abstract": "Extracting the hidden structure of the external environment is an essential component of intelligent agents and human learning. The real-world datasets that we are interested in are often characterized by the locality: the change in the structural relationship between the data points depending on location in observation space. The local learning approach extracts semantic representations for these datasets by training the embedding model from scratch for each local neighborhood, respectively. However, this approach is only limited to use with a simple model, since the complex model, including deep neural networks, requires a massive amount of data and extended training time. In this study, we overcome this trade-off based on the insight that the real-world dataset often shares some structural similarity between each neighborhood. We propose to utilize the embedding model for the other local structure as a weak form of supervision. Our proposed model, the Local VAE, generalize the Variational Autoencoder to have the different model parameters for each local subset and train these local parameters by the gradient-based meta-learning. Our experimental results showed that the Local VAE succeeded in learning the semantic representations for the dataset with local structure, including the 3D Shapes Dataset, and generated high-quality images.",
        "Introduction": "  INTRODUCTION Extracting the hidden structure of the external environment is essential for achieving intelligent agents and modeling human learning ( Kemp & Tenenbaum, 2008 ; Lake et al., 2015;  Higgins et al., 2017 ;  Achille et al., 2018 ;  Saxe et al., 2019 ). Human beings and/or animals can effectively learn internal representations from a few amounts of experiences. Various methods of nonlinear feature extraction ( Maaten & Hinton, 2008 ;  McInnes et al., 2018 ) are recently proposed to model the com- plex environment. In addition, thanks to the developments of deep generative models ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ;  Goodfellow et al., 2014 ;  Rezende & Mohamed, 2015 ), we can now handle the high-dimensional dataset on many individual problems. Although recent studies succeeded in modeling the dataset for the specific problems, there are still challenging properties in real-world. The datasets that we are interested in are often characterized by the locality: the change in the structural relationship between the data points depending on lo- cation in observation space. For instance, a sequence of experiences gradually changes according to multiple aspects, including time, space, and modality; we need to identify each individual during the development of their faces consistently. Besides, the human-made objects often have multiple color options for the same shape or size. Many studies have incorporated locality for dimensionality reduction and representational learning ( Kambhatla & Leen, 1997 ;  Roweis & Saul, 2000 ;  Tenen- baum et al., 2000 ). For example, combining the local learning approach with classical unsupervised learning algorithms such as PCA significantly improves their model capacity. These studies aim to find mappings between the data and the coordinate space under the assumption that the data space is composed of multiple low-dimensional subspaces ( Brand, 2003 ;  Vincent & Bengio, 2003 ). We refer this approach to as local learning. They usually use a linear projection for embedding models and a 2 distance in the input space for a neighborhood construction. In general, we can arbitrarily choose the distance for the neighbor graph, and it affects the quality of the embeddings. Incorporating the local learning approaches into the training of the deep generative models will give us a new model that has both the capacity for high-dimensional inputs and flexibility for locally changing environments at the same time. However, the integration of these two paradigms is not trivial. The conventional local learning approaches train the different embedding model for each neighborhood from scratch, nevertheless the deep neural networks generally require a large amount of data and take a long training time ( LeCun et al., 2015 ). In other words, local learning approaches learn internal representations for each neighborhood by using only relatively simple models, whereas deep generative models learn one complex representation as a whole with deep neural networks. To overcome this trade-off, the structural similarity between each neighborhood is the key. In the case of the human face, although each face varies greatly depending on age, gender, and etc., there are common facial expressions ( Ekman & Keltner, 1997 ). It is reasonable to expect that each local subspace of the dataset shares some structure since most dataset tends to be governed by the consis- tent rules of the physical world ( Achille et al., 2018 ). Under the assumptions about the locality and structural similarity, a dataset has two scales of structures: the local structure inside some neighbor- hood and the global relationship between each neighborhood. Figure 1a visualizes these two scales inside the dataset. When a dataset has structural similarity, meta-learning is an effective approach. Meta-learning is an algorithm to learn the rules for each task quickly for a dataset consisting of multiple tasks ( Schmid- huber, 1987 ;  Bengio et al., 1992 ;  Andrychowicz et al., 2016 ;  Ravi & Larochelle, 2017 ;  Finn et al., 2017 ). In this study, by considering each local neighborhood as a task for meta-learning, we extract the transferable knowledge between each local structure. We propose to train the meta embedding model, which parameters capture the common local structure and quickly adapt to each subspace by utilizing the structural similarity. We generalize the typical deep generative model called the Variational Autoencoder (VAE) ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ) to be applicable to the dataset with local structure. We extend the graphical model of the VAE to have different model parameters for each local subset of the dataset (Figure 1b) while keeping to avoid a large amount of computation by using the gradient- based meta-learning ( Finn et al., 2017 ;  Grant et al., 2018 ). By treating the neighborhood of each data point as a task to adopt the meta-learning, we make our proposed Local VAE possible to learn similar structures between neighbors quickly. We evaluate the performance of our proposed model with the 3D Shapes Dataset ( Burgess & Kim, 2018 ) and the concatenated dataset of the Cars3D ( Reed et al., 2014 ) and SmallNORB ( LeCun et al., 2004 ). The numerical experiments shows that the locality enables the model to achieve the disentangled representation for each subspace without any label information.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to incentivizing the truthful reporting of private information from a decentralized population of agents. We propose a strictly proper scoring rule that incentivizes agents to report truthfully, even when the outcome space is large or infinite, and when the mechanism designer does not possess any ground truth samples. We discuss a line of follow-up works on eliciting properties of the distributions, and provide empirical results to demonstrate the effectiveness of our proposed approach.",
        "Abstract": "It is important to collect credible training samples $(x,y)$ for building data-intensive learning systems (e.g., a deep learning system). In the literature, there is a line of studies on eliciting distributional information from self-interested agents who hold a relevant information.  Asking people to report complex distribution $p(x)$, though theoretically viable, is challenging in practice. This is primarily due to the heavy cognitive loads required for human agents to reason and report this high dimensional information. Consider the example where we are interested in building an image classifier via first collecting a certain category of  high-dimensional image data. While classical elicitation results apply to eliciting a complex and generative (and continuous) distribution $p(x)$ for this image data, we are interested in eliciting samples $x_i \\sim p(x)$ from agents. This paper introduces a deep learning aided method to incentivize credible sample contributions from selfish and rational agents. The challenge to do so is to design an incentive-compatible score function to score each reported sample to induce truthful reports, instead of an arbitrary or even adversarial one. We show that with accurate estimation of a certain $f$-divergence function we are able to achieve approximate incentive compatibility in eliciting truthful samples. We then present an efficient estimator with theoretical guarantee via studying the variational forms of $f$-divergence function. Our work complements the literature of information elicitation via introducing the problem of \\emph{sample elicitation}.  We also show a connection between this sample elicitation problem and $f$-GAN, and how this connection can help reconstruct an estimator of the distribution based on collected samples.",
        "Introduction": "  INTRODUCTION The availability of a large quantity of credible samples is crucial for building high-fidelity machine learning models. This is particularly true for deep learning systems that are data-hungry. Arguably, the most scalable way to collect a large amount of training samples is to crowdsource from a decen- tralized population of agents who hold relevant sample information. The most popular example is the build of ImageNet ( Deng et al., 2009 ). The main challenge in eliciting private information is to properly score reported information such that the self-interested agent who holds a private information will be incentivized to report truthfully. At a first look, this problem of eliciting quality data is readily solvable with the seminal solution for eliciting distributional information, called the strictly proper scoring rule ( Brier, 1950 ;  Winkler, 1969 ;  Savage, 1971 ;  Matheson & Winkler, 1976 ;  Jose et al., 2006 ;  Gneiting & Raftery, 2007 ): sup- pose we are interested in eliciting information about a random vector X = (X 1 , ..., X d−1 , Y ) ∈ Ω ⊆ R d , whose probability density function is denoted by p with distribution P. As the mechanism designer, if we have a sample x drawn from the true distribution P, we can apply strictly proper scor- ing rules to elicit p: the agent who holds p will be scored using S(p, x). S is called strictly proper if it holds for any p and q that E x∼P [S(p, x)] > E x∼P [S(q, x)]. The above elicitation approach has two main caveats that limited its application: • When the outcome space |Ω| is large and is even possibly infinite, it is practically impossible for any human agents to report such a distribution with reasonable efforts. This partially inspired a line of follow-up works on eliciting property of the distributions, which we will discuss later. • The mechanism designer may not possess any ground truth samples.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes PUGAN, a modification and extension of WaveGAN architecture for efficiently synthesizing raw-waveform audio through progressive training. PUGAN generates low sampling rate audio using the first few layers of the original WaveGAN and replaces the latter layers with bandwidth extension modules composed of neural upsampling layers and encoders/decoders. The proposed approach is evaluated in terms of quantitative computational performance and qualitative metrics including human perceptual evaluation. The contributions of this paper include the novel neural modules for efficient generation of raw waveform audio, the application of resampling and sinc convolution layers suitable for sound generation, and the demonstration of the effectiveness of the proposed approach by generating raw waveform audio with significantly less number of parameters in real-time with equivalent output quality as WaveGAN.",
        "Abstract": "This paper proposes a novel generative model called PUGAN, which progressively synthesizes high-quality audio in a raw waveform. PUGAN leverages on the recently proposed idea of progressive generation of higher-resolution images by stacking multiple encode-decoder architectures. To effectively apply it to raw audio generation, we propose two novel modules: (1) a neural upsampling layer and (2) a sinc convolutional layer. Compared to the existing state-of-the-art model called WaveGAN, which uses a single decoder architecture, our model generates audio signals and converts them in a higher resolution in a progressive manner, while using a significantly smaller number of parameters, e.g., 20x smaller for 44.1kHz output, than an existing technique called WaveGAN. Our experiments show that the audio signals can be generated in real-time with the comparable quality to that of WaveGAN with respect to the inception scores and the human evaluation.",
        "Introduction": "  INTRODUCTION Synthesis of realistic sound is a long-studied research topic, with various real-world applications such as text-to-speech (TTS) ( Wang et al., 2017 ; Ping et al., 2018), sound effect ( Raghuvanshi et al., 2016 ), and music generation ( Briot et al., 2017 ;  Dong et al., 2018 ;  Huang et al., 2019 ). Various techniques have been developed ranging from the sample-based to more computational ones such as the additive/subtractive synthesis, frequency modulation granular synthesis, and even a full physics- based simulation ( Cook, 2002 ). Human's audible frequency range is up to 20 kHz, so the standard sampling rate for music and sound is 44.1 kHz. Thus, for interactive applications and live perfor- mances, the generation of the high temporal-resolution audio (i.e., 44.1 kHz) in real-time has to meet the standard of human perceptual sensitivity to sound. However, the aforementioned methods often fail to do so, due to their heavy computational complexity with respect to the data size. Because of this, professional sound synthesizers usually have no choice but to rely on hardware implementa- tions.( Wessel & Wright, 2002)  Generative adversarial networks (GANs) ( Goodfellow et al., 2014 ) have emerged as a promising approach to the versatile (e.g., conditional generation from a low-dimensional latent vector ( Mirza & Osindero, 2014 )) and high-quality (e.g., super-resolution GAN ( Ledig et al., 2017 )) image. One of the first GAN models for sound synthesis have been designed to first produce the spectrogram (or some other similar intermediate representations) ( Donahue et al., 2019 ;  Engel et al., 2019 ;  Marafioti et al., 2019 ). A spectrogram is a compact 2D representation of audio signals in terms of its frequency spectrum over time. The spectrogram can then be converted into the estimated time-domain wave- form using the Griffin & Lim algorithm ( Griffin & Lim, 1984 ). However, such a conversion process does not only introduces nontrivial errors but also runs slowly, preventing the approach from being applied at an interactive rate 1 . WaveGAN ( Donahue et al., 2019 ) was the first and state-of-the-art GAN model that can generate raw waveform audio from scratch. The first generations of sound-generating GANs, like the WaveGAN and its followers, have been influenced much by the enormously successful generative models for image synthesis. They can Under review as a conference paper at ICLR 2020 be divided into those that employ the single decoder architecture (e.g., DCGAN and StyleGAN ( Radford et al., 2016 ;  Karras et al., 2019 )) and those that encode and decode the intermediate rep- resentations in several and progressive stages (e.g., StackGAN and progressive GAN ( Zhang et al., 2017 ;  Karras et al., 2018 )). WaveGAN is the direct descendant of DCGAN with modification for the 1D audio data, while GANSynth applied the concept of progressive generation of audio, but using the 2D spectrogram, treating the audio as a 2D image. No previous work in GAN based audio gener- ation has attempted the direct and fast synthesis of 1D raw audio waveform employing the multiple and progressive encoder-decoder architecture. Therefore, in this paper, we propose PUGAN, modification and extension of WaveGAN architec- ture for efficiently synthesizing raw-waveform audio through progressive training. PUGAN gener- ates low sampling rate audio using the first few layers of the original WaveGAN (referred to as the lightweight WaveGAN module). The latter layers of WaveGAN are replaced with the bandwidth extension modules, each of which is composed of the neural upsampling layer and encoder/decoder. They progressively output (progressively trained too) the higher sampling rate audio. For the effec- tive progressive training and generation, instead of the usual upsampling method such as the nearest neighbor used in image generation, PUGAN uses a new upsampling methods often employed in the digital signal processing (DSP) field in an attempt to preserve the frequency information of the original data ( Oppenheim, 1999 ). This upsample process consists of the zero insertion and 1D convolution to function as an interpolation infinite impulse response (IIR) filter. On the discrimi- nator side, we add the Sinc convolution ( Ravanelli & Bengio, 2018 ) before the first layer to repli- cate the function of the parameterized low pass Sinc filter, also a popular technique in the DSP area. We have also evaluated PUGAN in terms of both quantitative computational performance and qualitative metrics including the human perceptual evaluation. (demo and code: https://pugan-iclr- demo.herokuapp.com/) Overall, our contributions include the following: • propose PUGAN, with novel neural modules (upsampling and bandwidth extension) for the efficient generation of raw waveform audio, • apply the concept of resampling (in the generator) and sinc convolution layers (in the dis- criminator) suitable for handling sound generation instead of the conventional upsampling or convolution methods, and • demonstrate the effectiveness of the proposed approach by generating raw waveform audio with significantly less number of parameters in real-time with equivalent output quality as WaveGAN.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces the task of Visual Hide and Seek, where a neural network must learn to steer an embodied agent around its environment in order to avoid capture from a predator. Experiments suggest that learning to play this game causes useful representations of multi-agent dynamics to emerge. The paper also analyzes the underlying reasons why these representations emerge, and shows they are due to imperfections in the agent's abilities. All software, data, environments, and models will be released publicly to promote further progress on this problem.",
        "Abstract": "We train embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. We place a variety of obstacles in the environment for the prey to hide behind, and we only give the agents partial observations of their environment using an egocentric perspective. Although we train the model to play this game from scratch without any prior knowledge of its visual world, experiments and visualizations show that a representation of other agents automatically emerges in the learned representation. Furthermore, we quantitatively analyze how agent weaknesses, such as slower speed, effect the learned policy. Our results suggest that, although agent weaknesses make the learning problem more challenging, they also cause useful features to emerge in the representation.",
        "Introduction": "  INTRODUCTION We introduce the task of Visual Hide and Seek where a neural network must learn to steer an em- bodied agent around its environment in order to avoid capture from a predator. Our hypothesis, which our experiments suggest, is that learning to play this game will cause useful representations of multi-agent dynamics emerge. We train the agent to navigate through its environment to maximize its survival time, which the model successfully learn to do. However, since we train the model from scratch to directly map pixels to action, the network can learn a representation of the environment and its dynamics.  Figure 1  illustrates the problem setup. We designed this game to mimic the typical dynamics between predator and prey. For example, we place a variety of obstacles inside the environment, which create occlusions that the agent can leverage to hide behind. We also only give the agents access to the first-person perspective of their three-dimensional environment. Consequently, this task is a substantial challenge for reinforcement learning because the state is both visual (pixel input) and partially observable (due to occlusions). To carefully study the emerged representations and agent dynamics, we probe different environmental factors as well as the agents' abilities, and our experiments quantitatively suggest that the agent automatically learns to recognize the perspective of the predator and its own visibility, which enables robust hiding behavior. However, what intrinsic structure in the game, if any, caused this strategy to emerge? We quantita- tively compare a spectrum of hide and seek games where we perturb the abilities of agents and the environmental complexity. Our experiments show that, although agent weaknesses, such as slower speed, make the learning problem more challenging, they also cause the model to learn more useful representations of its environmental dynamics. We show there is a \"sweet spot\" where the weakness is strong enough to cause useful strategies to emerge without derailing the learning process. This paper makes three principal contributions to embodied agents. Firstly, we introduce the prob- lem of visual hide-and-seek where an agent receives a partial observation of its visual environment and must navigate to avoid capture. Secondly, we empirically demonstrate that this task causes representations of other agents in the scene to emerge. Thirdly, we analyze the underlying reasons why these representations emerge, and show they are due to imperfections in the agent's abilities. The rest of this paper analyzes these contributions in detail. We plan to release all software, data, environments, and models publicly to promote further progress on this problem.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new approach to certifiably robust deep learning models using randomized smoothing. The existing randomized smoothing methods are shown to be limited to 2 attack, and use Gaussian noise for smoothing, which is sub-optimal in high dimensional spaces. This paper proposes a new method that is more effective against ∞ attacks, and provides tighter bounds on challenging datasets such as ImageNet.",
        "Abstract": "Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for $\\ell_2$ perturbation. We propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distributions, helping to design two new families of non-Gaussian smoothing distributions that work more efficiently for $\\ell_2$ and $\\ell_\\infty$ attacks, respectively. Our proposed methods achieve better results than previous works and provide a new perspective on randomized smoothing certification.",
        "Introduction": "  INTRODUCTION Deep neural networks have achieved state-of-the-art performance on many tasks such as image clas- sification ( He et al., 2016 ;  Lu et al., 2018 ) and language modeling ( Devlin et al., 2019 ). Nonethe- less, modern deep learning models have been shown to be highly sensitive to small and adversarially crafted perturbations on the inputs ( Goodfellow et al., 2015 ), which means a human-imperceptible changes on inputs could cause the model to make dramatically different predictions. Although many robust training algorithms have been developed to overcome adversarial attacking, most heuristically developed methods can be shown to be broken by more powerful adversaries eventually, (e.g.,  Atha- lye et al., 2018 ;  Madry et al., 2018 ;  Zhang et al., 2019 ;  Wang et al., 2019 ). This casts an urgent demand for developing robust classifiers with provable worst case guarantees. One promising approach for certifiably robustness is the recent randomized smoothing method (e.g.,  Cohen et al., 2019 ;  Salman et al., 2019 ;  Lee et al., 2019 ;  Li et al., 2019 ;  Lecuyer et al., 2018 ), which constructs smoothed classifiers with certifiable robustness by introducing noise on the inputs. Compared with the other more traditional verification approaches (e.g.  Wong & Kolter, 2017 ;  Jordan et al., 2019 ;  Dvijotham et al., 2018 ) that exploits special structures of the neural networks (such as the properties of ReLU), the randomized smoothing methods work more flexibly on general black- box classifiers and is shown to be more scalable and provide tighter bounds on challenging datasets such as ImageNet ( Deng et al., 2009 ). However, the existing randomized smoothing methods can only work against 2 attack, in which the perturbations are allowed within an 2 ball of certain radius. A stronger type of attack, such as the ∞ attacks, is much more challenging to defense and verify due to the larger set of perturbations, but is more relevant in practice. In addition, all the existing randomized smoothing methods use Gaussian noise for smoothing. Al- though appearing to be a natural choice, one of our key observations is that Gaussian distributions is in fact a rather sub-optimal choice in high dimensional spaces, even for 2 attack. This is due to a counter-intuitive phenomenon in high dimensional spaces ( Vershynin, 2018 ) that almost all of the probability mass of standard Gaussian distribution concentrates around the sphere of radius one (and hence \"soap bubble\" in the title), instead of the center point (which corresponds to the original input). As a result, the variance of the Gaussian noise needs to be sufficiently small to yield good approximation to the original classifiers (by squeezing the \"soap bubble\" towards the center point), Under review as a conference paper at ICLR 2020 which, however, makes it difficult to verify due to the small noise. Further, for the more challenging ∞ attack, Gaussian smoothing provably degenerates in high dimensions.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new algorithm for building a decoder for linear quadratic regulator (LQR) continuous control problems. The decoder takes the context of a new task and outputs a representation based on which the agent can easily infer a near-optimal policy. The algorithm is provably efficient, with an O √ T regret bound in the online setting and an -suboptimal policy for an unseen LQR environment after playing O −2 environments. The effectiveness of the algorithm is demonstrated through simulations of several physical environments.",
        "Abstract": "A fundamental challenge in artificially intelligence is to build an agent that generalizes and adapts to unseen environments. A common strategy is to build a decoder that takes a context of the unseen new environment and generates a policy. The current paper studies how to build a decoder for the fundamental continuous control environment, linear quadratic regulator (LQR), which can model a wide range of real world physical environments. We present a simple algorithm for this problem, which uses upper confidence bound (UCB) to refine the estimate of the decoder and balance the exploration-exploitation trade-off. Theoretically, our algorithm enjoys a $\\widetilde{O}\\left(\\sqrt{T}\\right)$ regret bound in the online setting where $T$ is the number of environments the agent played. This also implies after playing $\\widetilde{O}\\left(1/\\epsilon^2\\right)$ environments, the agent is able to transfer the learned knowledge to obtain an $\\epsilon$-suboptimal policy for an unseen environment. To our knowledge, this is first provably efficient algorithm to build a decoder in the continuous control setting. While our main focus is theoretical, we also present experiments that demonstrate the effectiveness of our algorithm.",
        "Introduction": "  INTRODUCTION Humans are able to solve a new task without any training based on previous experience in similar tasks. Our intelligent agent should be able do the same, learning from previous experience, adapting to the new ones and improving the performance as the agent gains more experience. This is a challenging problem as we need to design an adaptation mechanism which is fundamentally different from classical supervised learning methods. A common approach is to build a decoder so that once the agent sees a description of new task, i.e., the context of the new task, the decoder turns the context into a succinct representation of the new task, based on which the agent is able to design a policy to solve the task. Note this procedure resembles how a human solves a new task. For example, if a human wants to push an object on a table, the human first sees the object and the table (context). Then, in his/her mind, the context becomes a representation of this task, e.g., a sense of weight of the object. Based on this representation, the human can easily reason about how much force to exert on the object. This general approach has been applied in practice. For example,  Wu et al. (2018)  studied the visual navigation task and built a Bayesian model that takes the context of new environments and outputs the policy that enables the agent to navigate.  Killian et al. (2016)  used this approach to develop personalized medicine policies for HIV treatment. While this is a promising approach, currently we only have limited theoretical understanding. The approach can be formulated in Contextual Markov Decision Process (CMDP) framework ( Hallak et al., 2015 ). Recently, there is a line of work gave provable guarantees for CMDP ( Abbasi-Yadkori & Neu, 2014 ;  Hallak et al., 2015 ;  Dann et al., 2018 ;  Modi et al., 2018 ;  Modi & Tewari, 2019 ). These work all studied tabular MDPs, and use function approximation, e.g., linear functions, generalized linear models, etc, to model the mapping from the context to the probability transition matrix. A major drawback of these work is that they are restricted to the tabular setting and thus can only deal with discrete environments. Therefore, they can hardly model real-world continuous control tasks, like the task of pushsing an object as we described above. A natural question arises: Under review as a conference paper at ICLR 2020 Can we design a provably efficient decoder for continuous control problems? In this paper, we make an important step towards answering this question. We study the fundamental setting in continuous control, linear quadratic regulator (LQR). LQR is arguably the most widely used framework in continuous control, as LQR easily models real world physical phenomena, e.g., the pushing object task we described earlier. We propose a new algorithm that builds a decoder, so that for a new LQR task, the decoder takes LQR's context and outputs a representation based on which the agent can easily infer a near-optimal policy for new continuous control tasks. In the training phase, we build the decoder via a sequence of LQRs (in an online fashion) with unknown parameters. For each new task, we first use the current decoder to build the representation of this task, infer a policy based on this representation and use this policy to do control for this episode. There are two crucial components in our algorithm. First, after each episode, we will refine the estimate of the decoder based on the observations from this episode. Second, it is crucial to use a upper confidence bound (UCB) estimator of the decoder to build the representation so that the agent can perform a near-optimal trade-off between exploration and exploitation. In this way, we provably show the decoder improves the performance as it experiences more training tasks. Formally, we show our algorithm enjoys O √ T regret (the difference between the cumulative rewards of our algorithm and the unknown optimal policy on every seen environment) bound in the online setting. Moreover, the algorithm is able to obtain an -suboptimal policy for an unseen LQR environment after playing O −2 environments. To our knowledge, this is the first provably efficient algorithm that builds a decoder for continuous control environments. Empirically, we simulate several physical environments to illustrate the effectiveness of our algorithm.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Diagonal Graph Convolutional Networks (DiagGCN), a novel framework for learning with graph-structured data. DiagGCN is an adaptive neighborhood aggregation strategy that models and aggregates the dynamics of node importance. It consists of two coupled operations: node adaptive rescaling and node adaptive encoder. Experiments on several graph benchmark datasets show that DiagGCN consistently achieves performance superiority over existing state-of-art GCN based baselines.",
        "Abstract": "Graph convolutional networks (GCNs) and their variants have generalized deep learning methods into non-Euclidean graph data, bringing a substantial improvement on many graph mining tasks. In this paper, we revisit the mathematical foundation of GCNs and study how to extend their representation capacity. We discover that their performance can be improved with an adaptive neighborhood aggregation step. The core idea is to adaptively scale the output signal for each node and automatically train a suitable nonlinear encoder for the input signal. In this work, we present a new method named Diagonal Graph Convolutional Networks (DiagGCN) based on this idea. Importantly, one of the adaptive aggregation techniques—the permutations of diagonal matrices—used in DiagGCN offers a flexible framework to design GCNs and in fact, some of the most expressive GCNs, e.g., the graph attention network, can be reformulated as a particular instance of our model. Standard experiments on open graph benchmarks show that our proposed framework can consistently improve the graph classification accuracy when compared to state-of-the-art baselines.",
        "Introduction": "  INTRODUCTION The past decade has witnessed the remarkable success of deep neural networks across different domains, ranging from computer vision ( Krizhevsky et al., 2012 ;  He et al., 2016 ), to speech recog- nition ( Abdel-Hamid et al., 2014 ), to natural language processing ( Gehring et al., 2017 ). These breakthroughs have provoked the interest to extend neural operations from linear or spatial domains to arbitrary graph structures, e.g., social networks, gene-protein networks, and knowledge graphs. One challenge in learning with graph-structured data is how to represent each node or subgraph in neural networks. This is challenging, as graph data lies in the non-Euclidean space with com- plex structures and random size, making it infeasible to directly apply traditional neural network algorithms ( Hamilton et al., 2017 ;  Kipf & Welling, 2016 ). Recently, there have been attempts to generalize neural networks for handling graph-structured data. For example, graph neural networks (GNNs) are introduced as recursive neural networks in which node features are propagated itera- tively until equilibrium ( Gori et al., 2005 ;  Scarselli et al., 2009 ). More recently, several studies show that it is natural for convolutional neural networks (CNNs) to model graph data structure by defining spatial localized convolutional filters over graph Laplacian spectral space ( Bruna et al., 2013 ;  Henaff et al., 2015 ;  Defferrard et al., 2016 ) or more directly, over graph neighborhoods (Duvenaud et al., 2015;  Niepert et al., 2016 ;  Monti et al., 2017 ). These techniques, named as graph convolutional networks (GCNs) ( Kipf & Welling, 2016 ), enable deep neural networks to effectively capture graph topology and properties. Over the past three years, various GCN based models, such as GraphSAGE ( Hamilton et al., 2017 ), FastGCN ( Chen et al., 2018 ), and GAT ( Velickovic et al., 2018 ), have been rapidly developed, offering promising results for important graph mining tasks, such as, node classification and link prediction. In theory, GCNs can be considered as a simplification of the traditional graph spectral methods ( Kipf & Welling, 2016 ). The common strategy is to model a node's neighborhood as the receptive field and then define the \"graph convolution layer\" as a mean pooling aggregation followed by a nonlin- earity encoder. Existing GCNs stack multiple layers and learn node representations by recursively aggregating information from neighbors, and the analysis of their representational capacity can be found in a recent study ( Xu et al., 2019 ). Essentially, the recursive convolutional aggregation process Under review as a conference paper at ICLR 2020 in GCNs, or the Neural Message Passing framework ( Gilmer et al., 2017 ), is also closely related to the Weisfeiler-Lehman (WL) graph isomorphism test ( Weisfeiler & Lehman, 1968 ). Though GCNs have been extensively explored, how to better capture the graph structures through convolutions is still largely an open question. Particularly, the neighborhood aggregation step in GCNs can be seen as the diffusion process in real-world networks ( Gilmer et al., 2017 ), in which different nodes play different roles in the propagation. For example, for information diffusion in social networks, opinion leaders ( Granovetter, 1973 ) are more likely to expose information to asso- ciated communities and structural holes ( Burt, 2009 ) can spread information across different groups. However, the neighborhood aggregation step in most existing GCNs considers neither the different structural importance of nodes within each step nor the dynamics of node importance across differ- ent propagation steps ( Kipf & Welling, 2016 ;  Hamilton et al., 2017 ;  Chen et al., 2018 ). In light of this limitation, we propose an adaptive neighborhood aggregation strategy to model and aggregate the dynamics of node importance. The adaptive neighborhood aggregation step consists of two coupled operations: node adaptive rescaling and node adaptive encoder. The idea of node adaptive rescaling is to adaptively learn each node's importance and use it to automatically rescale the output signal of the original graph convolutional operator; In addition to this customized rescal- ing, each node can also adaptively choose its encoder function according to its importance, forming the concept of node adaptive encoder. With these two techniques in neighborhood aggregation, we present the diagonal graph convolutional networks (DiagGCN), which is able to dynamically depict and model nodes' importance in GCNs. Importantly, there are several advantages of the proposed DiagGCN model. First, efficiency wise, the adaptive rescaling is achieved by a simple diagonal matrix operation, making DiagGCN's com- plexity the same as the sparse version of GCNs, i.e., linear with the network volume. In addition, the rescaling step is also parallelizable across all nodes in the graph. Second, it naturally supports both transductive and inductive learning, meaning that the model can generalize inference to unseen nodes. Third, DiagGCN can be connected with graph attention with different mechanisms, includ- ing node attention, (multi-hop) edge attention, and path attention, providing a new perspective to understand graph attention. This flexible framework potentially offers a principled way to automat- ically choose good attention scheme, or reasonable permutation on the adjacency matrix, rescaling diagonal matrices, and adaptive encoders (i.e., AutoML for graph convolution/attention). Specifi- cally, we show the GAT model ( Velickovic et al., 2018 ) can be reformulated as a special case of the proposed DiagGCN model with edge attention. Finally, our extensive experiments on several graph benchmark datasets of different types, including Cora, Citeseer, Pubmed citation networks, and the protein-protein interaction network (PPI) show that the proposed framework consistently achieves performance superiority over existing state-of-art GCN based baselines.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a theoretically justified normalization approach for accelerating the training of deep neural networks. The approach is based on the premise that lowering the gradient Lipschitz constant and gradient variance is the key to boost the convergence rate of optimization algorithms with stochastic gradient methods. The proposed normalization approach is then used to explain the effectiveness of the vanilla BatchNorm, and to improve the performance of Sigmoid and Tanh activation functions. Extensive experiments are conducted to demonstrate the superiorly of the new normalization approach.",
        "Abstract": "Batch Normalization (BatchNorm) has been a default module in  modern deep networks  due to its effectiveness for accelerating training deep neural networks.  It is widely accepted that the great success of BatchNorm is owing to reduction of internal covariate shift (ICS), but recently it is demonstrated that the link between them is fairly weak. The intrinsic reason behind effectiveness of BatchNorm is still unrevealed that limits it to be made better use. In light of this,  we propose a new normalization approach,  referred to as Pre-Operation Normalization (POP-Norm), which is theoretically ensured to speed up the training convergence. Not surprisingly, POP-Norm and BatchNorm are largely the same. Hence the similarities  can help us to theoretically interpret the root of BatchNorm's effectiveness. There are still some significant distinctions between the two approaches. Just the distinctions make POP-Norm achieve faster convergence rate and better performance than BatchNorm, which are validated in extensive experiments on benchmark datasets: CIFAR10, CIFAR100 and ILSVRC2012.",
        "Introduction": "  INTRODUCTION The last decade has witnessed significant advances in deep neural networks (DNN), which brought substantial improvements for many real-world tasks, such as object( Szegedy et al., 2015 ;  He et al., 2015 ), scene( Khan et al., 2016 ;  Guo et al., 2017 ) and action recognition( Feichtenhofer et al., 2017 ), object detection( Ren et al., 2015 ;  He et al., 2017b ;  Redmon & Farhadi, 2018 ) and image segmenta- tion( Ronneberger et al., 2015 ;  He et al., 2017a ). BatchNorm (Ioffe & Szegedy, 2015) is a milestone technique in the development of DNN, and nowadays it has become a default component to con- struct modern deep networks. BatchNorm is widely-used in DNN due to that it is able to reduce the sensitivity to initialization, significantly raise the learning rate, substantially speed up the train- ing process and considerably improve the performance. Actually, along with this line a variety of works, such as layer normalization ( Ba et al., 2016 ), instance normalization ( Ulyanov et al., 2016 ), weight normalization( Salimans & Kingma, 2016 ) and group normalization ( Wu & He, 2018 ), batch renormalization, ( Ioffe, 2017 ) were sequently proposed and also have made great success. Nowadays, the practical success of BatchNorm is mainly attributed to reduction of internal covari- ate shift (ICS) by controlling the first two moments (mean and variance) of the distributions of layer inputs. Recently, this point of view is challenged by ( Santurkar et al., 2018 ). It points out that the link between reduction of ICS and performance improvement of BatchNorm is tenuous, at best. However, the success of BatchNorm is indisputable, therefore there may exist a profound mathe- matical mechanism behind BatchNorm. In fact, from the perspective of loss landscape, ( Santurkar et al., 2018 ) demonstrates BatchNorm will make the optimization landscape smoother, and then this smoothness induces a more predictive and stable behavior of the gradients. As stated above, the original theoretical foundation of the vanilla BatchNorm is not so solid and the exact reason for its effectiveness is poorly understood, hence BatchNorm maybe not optimal in the- ory and we might not make better use of it. In light of this, we try to propose a theoretically justified normalization approach for accelerating training deep neural networks, so that we can continuously further algorithmic progress. To achieve this goal, we firstly demonstrate that lowering gradient Lipschitz constant and gradient variance is the key to boost the convergence rate of optimization algorithms with stochastic gradient methods. And then we construct a normalization approach - removing the mean of inputs and dividing by the scaled l 2 norm of inputs before conducting the Under review as a conference paper at ICLR 2020 main operation (convolution or inner-production) of a layer, which can be theoretically ensured to reduce the gradient Lipschitz constant and gradient variance. The new normalization approach and BatchNorm are similar. Hence we utilize their similarity to interpret the effectiveness of the vanilla BatchNorm, along the line of lowering gradient Lipschitz constant and gradient variance, which is different from the view of the loss landscape ( Santurkar et al., 2018 ). According to our theory, we can easily explain why BatchNorm with ReLU is powerful with but ineffective with Sigmoid and Tanh as activation functions, which may help us to better understand the underlying complexity of neural networks. The main contributions of this paper are summarized as follows: • From scratch a theoretically justified normalization approach is deduced for accelerating the convergence speed of training deep neural networks. • We theoretically explain the root of BatchNorm's effectiveness and the weakness from the perspective of lowering the gradient variance and gradient Lipschitz constant. • With the help of the new normalization approach, in extensive experiments the performance for Sigmoid and Tanh achieve massive improvement, and they are even competitive to ReLU. Our paper is organised as follows. In Section 2, we will deduce the theoretical premises to accelerate stochastic optimization. Then, built on the premise, we will propose a new normalization approach in Section 3. In Section 4, we will analyze the exact root of BatchNorm's effectiveness. In Section 5, we will implement extensive experiments to demonstrate the superiorly of the new normalization approach. We will summarize our work and discuss further work in Section 6.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Variational AutoEncoders (VAEs) are a common assumption that high-dimensional real world observations can be generated by a lower-dimensional latent variable. Recent works suggest that decomposing the Evidence Lower Bound (ELBO) could lead to distinguishing the factor of disentanglement. However, Locatello et al. (2018) found that as regularization strength increases, the total correlation of sampled representation and mean representation are negatively correlated. This raises doubts on most methods of disentanglement, suggesting that \"the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases\".",
        "Abstract": "In the problem of unsupervised learning of disentangled representations, one of the promising methods is to penalize the total correlation of sampled latent vari-ables.  Unfortunately, this well-motivated strategy often fail to achieve disentanglement due to a problematic difference between the sampled latent representation and its corresponding mean representation.  We provide a theoretical explanation that low total correlation of sample distribution cannot guarantee low total correlation of the  mean representation. We prove that for the mean representation of arbitrarily high total correlation, there exist distributions of latent variables of abounded total correlation.  However, we still believe that total correlation could be a key to the disentanglement of unsupervised representative learning, and we propose a remedy,  RTC-VAE, which rectifies the total correlation penalty.   Experiments show that our model has a more reasonable distribution of the mean representation compared with baseline models, e.g.,β-TCVAE and FactorVAE.",
        "Introduction": "  INTRODUCTION VAEs (Variational AutoEncoders) Kingma & Welling (2013); Bengio et al. (2007) follow the com- mon assumption that the high-dimensional real world observations x can be re-generated by a lower- dimension latent variable z which is semantically meaningful. Recent works Kim & Mnih (2018); Chen et al. (2018); Kumar et al. (2017) suggest that decomposing the ELBO (Evidence Lower Bound) could lead to distinguishing the factor of disentanglement. In particular, recent works Kim & Mnih (2018); Chen et al. (2018) focused on a term called total correlation (TC). The popular be- lief Chen et al. (2018) is that by adding weights to this term in objective function, a VAE model can learn a disentangled representation. This approach appears to be promising since the total correlation of a sampled representation should describe the level of factorising since total correlation is defined to be the KL-divergence between the joint distribution z ∼ q(z) and the product of marginal dis- tributions j q(z j ). In this case, a low value suggests a less entangled joint distribution. However, Locatello et al. (2018) pointed out that the total correlation of sampled distribution T C sample being low does not necessarily give rise to a low total correlation of the corresponding mean representa- tion T C mean . Conventionally, the mean representation is used as the encoded latent variables, an unnoticed high T C mean is usually the culprit behind the undesirable entanglement. Moreover, Lo- catello et al. (2018) found that as regularization strength increases, the total correlation of sampled representation T C sample and mean representation T C mean are actually negatively correlated. Locatello et al. (2018) put doubts on most methods of disentanglement including penalizing the total correlation term Kim & Mnih (2018); Chen et al. (2018), and they concluded that \"the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases\".",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new adaptive data augmentation strategy based on deep generative models to improve the performance of machine learning and deep learning models when training data is scarce. The proposed method uses Generative Adversarial Networks (GANs) to automatically construct sample images from the original training set, and a novel K-fold parallel framework to make the most use of the existing data. Experiments over various datasets and tasks demonstrate the effectiveness of the proposed method in different contexts and tasks.",
        "Abstract": "Data augmentation(DA) is a useful technique to enlarge the size of the training set and prevent overfitting for different machine learning tasks when training data is scarce. However, current data augmentation techniques rely heavily on human design and domain knowledge, and existing automated approaches are yet to fully exploit the latent features in the training dataset. In this paper we propose an adaptive DA strategy based on generative models, where the training set adaptively enriches itself with sample images automatically constructed from deep generative models trained in parallel. We demonstrate by experiments that our data augmentation strategy, with little model-specific considerations, can be easily adapted to cross-domain deep learning/machine learning tasks such as image classification and image inpainting, while significantly improving model performance in both tasks. ",
        "Introduction": "  INTRODUCTION Deep learning and machine learning models produce highly successful results when given sufficient training data. However, when training data is scarce, overfitting will occur and the resulting model will be generalized poorly. It is also unavoidable that some data is very hard to retrieve. Data aug- mentation(DA) ameliorates such issues by enlarging the original data set and making more effective use of the information in existing data. Much prior work has centered on data augmentation strate- gies based on human design, including heuristic data augmentation strategies such as crop, mirror, rotation and distortion  Krizhevsky et al. (2012) ;  Simard et al. (2003) , interpolating through labeled data points in feature spaces  DeVries & Taylor (2017) , and adversarial data augmentation strategies based on  Teo et al. (2008) ;  Fawzi et al. (2016) . These methods have greatly aided many deep learn- ing tasks across several domains such as classification  Krizhevsky et al. (2012) , image segmentation  Yang et al. (2017)  and image reconstruction/inpainting  Alvarez-Gila et al. (2017) . Despite their success, these DA methods generally require domain-specific expert knowledge, man- ual operations and extensive amount of tuning depending on actual contexts Ciresan et al.;  Dosovit- skiy et al. (2016) . In particular, the need to directly operate on existing data with domain knowledge prevents many previous data augmentation strategies from being applicable to more general settings. To circumvent the need for specific domain knowledge in data augmentation, more recent work  An- toniou et al. (2017)  utilizes generative adversarial networks(GANs)  Goodfellow et al. (2014)  to produce images that better encode features in the latent space of training data. By alternatively optimizing the generator G and the discriminator D in the GAN, the GAN is able to produce im- ages similar to the original data and effectively complement the training set. It has been shown in experiments  Antoniou et al. (2017)  that GAN-based methods have indeed significantly boosted the performance of classifiers under limited data through automatic augmentation, but applications into other tasks are yet to be explored. Furthermore, given the computational complexity and difficulty in GAN training, a natural way to promote scalability is to consider parallelism  Intrator et al. (2018) ;  Durugkar et al. (2016) . In view of these considerations, we propose in this paper a new adaptive DA strategy based on deep generative models. Under such a framework, the original training set adaptively enriches itself with sample images automatically constructed from Generative Adversarial Networks (GANs) trained in parallel. Our contributions in this paper can be summarized as follows: Under review as a conference paper at ICLR 2020 • We propose a general adaptive black-box data augmentation strategy to diversify enhance training data, with no task-specific considerations. • We also include in our model a novel K-fold parallel framework, which helps make the most use of the existing data. • Experiments over various datasets and tasks demonstrate the effectiveness of our method in different contexts and tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new formulation of generalization error and generalization bound for generative adversarial networks (GANs). We establish a generalization error bound in a general version with a fixed generator, and a tighter bound with p,q weight normalization. We also establish a generalization error bound that uniformly holds over any choice of generator. Numerical experiments on Gaussian Mixture models verify that the theory of generalization error bound is consistent with the numerical performance.",
        "Abstract": "\tThis paper focuses on the theoretical investigation of unsupervised generalization theory of generative adversarial networks (GANs). We first formulate a more reasonable definition of general error and generalization bounds for GANs. On top of that, we establish a  bound for generalization error with a fixed generator in a general weight normalization context. Then, we obtain a width-independent bound by applying $\\ell_{p,q}$ and spectral norm weight normalization. To better understand the unsupervised model, GANs,  we establish the generalization bound, which uniformly holds with respect to the choice of generators. Hence, we can explain how the complexity of discriminators and generators contribute to generalization error. For $\\ell_{p,q}$ and spectral weight normalization, we provide explicit guidance on how to design parameters to train robust generators. Our numerical simulations also verify that our generalization bound is reasonable.",
        "Introduction": "  INTRODUCTION The generative adversarial network (GAN) ( Goodfellow et al., 2014 ) is one of most powerful gen- erative models for modeling complex high-dimensional tasks, such as image generation, dialogue generation, and image impainting. Many variants of GANs ( Ho & Ermon, 2016 ;  Abadi & Andersen, 2016 ;  Goodfellow et al., 2014 ;  Li et al., 2017 ;  Yu et al., 2018 ) have also been introduced to reinforce the stability of training processes to obtain more realistic models. A GAN consists of two neural networks: a discriminator and a generator. Literally, the generator generates simulated data, while the discriminator tries to discriminate between simulated data and real data. The training process of GANs is tantamount to a two-player game between a generator and a discriminator. The main goal is to obtain a good generator, which is able to successfully approximate the distribution of real data. We denote the distribution of real data and the generator- induced distribution by D real and D g , respectively. Our goal is to find a generator such that D real = D g . We revise the goal as d(D real , D g ) = 0, with a distribution distance d(·, ·). The Jensen-Shannon (JS) divergence is implicitly used in Vanillar GANs ( Goodfellow et al., 2014 ), and the 1-Wasserstein distance is employed in WGANs ( Arjovsky et al., 2017 ). Empirical experiments suggest that the Wasserstein distance is a more sensible measure for differentiating probability measures supported in low-dimensional manifolds. The generalization properties of GANs are less explored in the literature, and some exceptions are these works( Jiang et al., 2019 ; Arora et al., 2017;  Bartlett et al., 2017 ;  Zhang et al., 2017 ). Motivated by the supervised learning context, where we say training to be generalized if the gap between the training loss and the test loss is small, we can define the generalization for GANs in a similar way. Concretely, generalization for GANs means that, the population distance between D real and D g is closed to the empirical distance between the empirical distributions of D real and D g . Hence, we define the gap between the population and empirical distance as the generalization error. Though our ultimate goal is to minimize the former distance, the latter one is what we minimize in practice. Given that our training process provides a small distance between empirical distributions, a small generalization error indicates that the population distance is also small. In other words, a small generalization error guarantees that the generated distribution is close to the real data distribution. In fact, the training process of GANs is sample-dependent. In other words, the generator depends on the training data sets, which are random samples from D real . The training process minimizes Under review as a conference paper at ICLR 2020 d(D real , D g ), whereD real denotes the empirical distribution over samples. The deviation be- tweenD real and D real leads to the generalization error, i.e., the gap between d(D real , D g ) and d(D real , D g ). This motivates us to establish a bound for generalization error, that is, the general- ization bound. A tight generalization bound guarantees that the generalization error is small. The highlights and main contributions of this article are summarized as follows: • We formulate new definitions for both generalization error and generalization bound, which are more reasonable than the definitions in previous work (Arora et al., 2017;  Jiang et al., 2019 ). • We establish the generalization error bound in a general version, with a fixed generator. By applying p,q weight normalization, we obtain a tighter bound. • We establish the generalization error bound, which uniformly holds over any choice of gen- erator. Hence, we can explain how the complexity of the generator class and discriminator class contribute to the generalization error. • Numerical experiments on Gaussian Mixture models verify that the theory of generalization error bound is consistent with the numerical performance.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents WaveFlow, a compact flow-based model for raw audio. WaveFlow is trained with maximum likelihood without density distillation and auxiliary losses, simplifying the training pipeline and reducing the cost of development. WaveFlow squeezes the 1-D raw waveforms into a 2-D matrix and produces the whole audio within a fixed sequential steps. It also provides a unified view of flow-based models for raw audio and allows us to explicitly trade inference efficiency for model capacity. Results demonstrate that WaveFlow can obtain comparable likelihood and synthesize high-fidelity speech as WaveNet, while only requiring a few sequential steps to generate very long waveforms. The small memory footprint of WaveFlow is preferred in production TTS systems, especially for on-device deployment.",
        "Abstract": "In this work, we present WaveFlow, a small-footprint generative flow for raw audio, which is trained with maximum likelihood without complicated density distillation and auxiliary losses as used in Parallel WaveNet.  It provides a unified view of flow-based models for raw audio, including autoregressive flow (e.g., WaveNet) and bipartite flow (e.g., WaveGlow) as special cases. We systematically study these likelihood-based generative models for raw waveforms in terms of test likelihood and speech fidelity. We demonstrate that WaveFlow can synthesize high-fidelity speech and obtain comparable likelihood as WaveNet, while only requiring a few sequential steps to generate very long waveforms.  In particular, our small-footprint WaveFlow has only 5.91M parameters and can generate 22.05kHz speech 15.39 times faster than real-time on a GPU without customized inference kernels.",
        "Introduction": "  INTRODUCTION Deep generative models have obtained noticeable successes for modeling raw audio in high-fidelity speech synthesis and music generation (e.g.,  van den Oord et al., 2016 ;  Dieleman et al., 2018 ). Autoregressive models are among the best performing generative models for raw audio waveforms, providing the highest likelihood scores and generating high quality samples (e.g.,  van den Oord et al., 2016 ;  Kalchbrenner et al., 2018 ). One of the most successful examples is WaveNet ( van den Oord et al., 2016 ), an autoregressive model for waveform synthesis. It operates at the high temporal resolution of raw audio (e.g., 24kHz) and sequentially generates waveform samples at inference. As a result, WaveNet is prohibitively slow for speech synthesis and one has to develop highly engineered kernels for real-time inference ( Arık et al., 2017a ;  Pharris, 2018 ).  2  Flow-based models ( Dinh et al., 2014 ;  Rezende and Mohamed, 2015 ) are a family of generative models, in which a simple initial density is transformed into a complex one by applying a series of invertible transformations. One group of models are based on autoregressive transformation, including autoregressive flow (AF) and inverse autoregressive flow (IAF) as the \"dual\" of each other ( Kingma et al., 2016 ;  Papamakarios et al., 2017 ;  Huang et al., 2018 ). AF is analogous to autoregressive models, which performs parallel density evaluation and sequential synthesis. In contrast, IAF performs parallel synthesis but sequential density evaluation, making likelihood-based training very slow. Parallel WaveNet ( van den Oord et al., 2018 ) distills an IAF from a pretrained autoregressive WaveNet, which gets the best of both worlds. However, it requires the density distillation with Monte Carlo approximation and a set of auxiliary losses for good performance, which complicates the training pipeline and increases the cost of development. Instead, ClariNet ( Ping et al., 2019 ) simplifies the density distillation by computing a regularized KL divergence in closed-form. Another group of flow-based models are based on bipartite transformation ( Dinh et al., 2017 ;  Kingma and Dhariwal, 2018 ), which provide parallel density evaluation and parallel synthesis. Most recently, WaveGlow ( Prenger et al., 2019 ) and FloWaveNet ( Kim et al., 2019 ) successfully applies Glow ( Kingma and Dhariwal, 2018 ) and RealNVP ( Dinh et al., 2017 ) for waveform synthesis, respectively. However, the bipartite transformations are less expressive than the autoregressive transformations (see Section 2.3 for detailed discussion). In general, these bipartite flows require Under review as a conference paper at ICLR 2020 deeper layers, larger hidden size, and huge number of parameters to reach comparable capacities as autoregressive models. For example, WaveGlow and FloWaveNet have 87.88M and 182.64M parameters with 96 layers and 256 residual channels, respectively. In contrast, a 30-layer WaveNet has only 4.57M parameters with 128 residual channels. In this work, we present WaveFlow, a compact flow-based model for raw audio. Specifically, we make the following contributions: 1. WaveFlow is trained with maximum likelihood without density distillation and auxiliary losses used in Parallel WaveNet ( van den Oord et al., 2018 ) and ClariNet ( Ping et al., 2019 ), which simplifies the training pipeline and reduces the cost of development. 2. WaveFlow squeezes the 1-D raw waveforms into a 2-D matrix and produces the whole audio within a fixed sequential steps. It also provides a unified view of flow-based models for raw audio and allows us to explicitly trade inference efficiency for model capacity. We implement WaveFlow with a dilated 2-D convolutional architecture ( Yu and Koltun, 2015 ), and it includes both Gaussian WaveNet ( Ping et al., 2019 ) and WaveGlow ( Prenger et al., 2019 ) as special cases. 3. We systematically study the likelihood-based generative models for raw audios in terms of test likelihood and speech quality. We demonstrate that WaveFlow can obtain comparable likelihood and synthesize high-fidelity speech as WaveNet ( van den Oord et al., 2016 ), while only requiring a few sequential steps to generate very long waveforms. 4. Our small-footprint WaveFlow has only 5.91M parameters and synthesizes 22.05 kHz high- fidelity speech (MOS: 4.32) more than 40× faster than real-time on a Nvidia V100 GPU. In contrast, WaveGlow ( Prenger et al., 2019 ) requires 87.8M parameters for generating high-fidelity speech. The small memory footprint is preferred in production TTS systems, especially for on-device deployment. We organize the rest of the paper as follows. Section 2 reviews the flow-based models with autore- gressive and bipartite transformations. We present WaveFlow in Section 3 and discuss related work in Section 4. We report experimental results in Section 5 and conclude the paper in Section 6.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the importance of stability in control systems, both linear and nonlinear, and the advances made in the past decades in nonlinear control theory and reinforcement learning. It also examines the progress made by combining deep learning with reinforcement learning in high-dimensional continuous nonlinear control problems. Finally, it explores the stability problem in aircraft control systems and how it is related to trajectory perturbations caused by gusts.",
        "Abstract": "Reinforcement learning (RL) offers a principled way to achieve the optimal cumulative performance index in discrete-time nonlinear stochastic systems, which are modeled as Markov decision processes. Its integration with deep learning techniques has promoted the field of deep RL with an impressive performance in complicated continuous control tasks. However, from a control-theoretic perspective, the first and most important property of a system to be guaranteed is stability. Unfortunately, stability is rarely assured in RL and remains an open question. In this paper, we propose a stability guaranteed RL framework which simultaneously learns a Lyapunov function along with the controller or policy, both of which are parameterized by deep neural networks, by borrowing the concept of Lyapunov function from control theory. Our framework can not only offer comparable or superior control performance over state-of-the-art RL algorithms, but also construct a Lyapunov function to validate the closed-loop stability. In the simulated experiments, our approach is evaluated on several well-known examples including classic CartPole balancing, 3-dimensional robot control and control of synthetic biology gene regulatory networks. Compared with RL algorithms without stability guarantee, our approach can enable the system to recover to the operating point when interfered by uncertainties such as unseen disturbances and system parametric variations to a certain extent. ",
        "Introduction": "  INTRODUCTION Control of discrete-time nonlinear stochastic systems is an important topic in both control theory and reinforcement learning. In the past decades, the advancement of nonlinear control theory in the control community has been successfully applied in aircraft, automobiles, advanced robots and space systems (Slotine et al., 1991; Isidori, 1995). Concurrently, reinforcement learning was developed in the machine learning community to address similar nonlinear control problems (Sutton et al., 1992; Tesauro, 1995; Bertsekas & Tsitsiklis, 1996). Until recently, significant progress has been made by combining advances in deep learning (LeCun et al., 2015) with reinforcement learning. Impressive results are obtained in a series of high-dimensional continuous nonlinear control problems (Duan et al., 2016; Zhang et al., 2016; Zhu et al., 2017; Gu et al., 2017) in which control-theoretic approach is typically difficult to apply. Given a control system, regardless of which controller design method is used, control theory or reinforcement learning, the first and most important property of a system needs to be guaranteed is stability, because an unstable control system is typically useless and potentially dangerous (Slotine et al., 1991). Qualitatively, a system is described as stable if starting the system in the neighborhood of its desired operating point implies that it will stay around the point ever after. For aircraft control systems, a typical stability problem is intuitively related to the following question: will a trajectory perturbation caused by a gust result in a significant deviation in the later flight trajectory? Here, the desired operating point of the system is the flight trajectory in the absence of disturbance. Every control system, whether linear or nonlinear, involves a stability problem which should be carefully studied.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Variational auto-encoder (VAE) is a powerful deep generative model that uses amortized variational inference to scale to deep neural networks and large amounts of data. This paper investigates the use of learned priors to improve the variational lower-bound and achieve performance comparable or better than deep hierarchical VAEs. Comprehensive experiments on four binarized datasets with four different network architectures show that VAE with RealNVP prior consistently outperforms standard VAE and RealNVP posterior. Results demonstrate that using learned RealNVP prior with just one latent variable in VAE, it is possible to achieve test negative log-likelihoods (NLLs) comparable to very deep state-of-the-art hierarchical VAE on these four datasets. A hypothesis is proposed to explain why learning the prior can lead to such great improvement in model performance.",
        "Abstract": "Using powerful posterior distributions is a popular technique in variational inference.  However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, even with expressive posteriors, thus learning the prior becomes an alternative way to improve the variational lower-bound.  We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.  We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encourage appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results.  We demonstrate that, with learned RealNVP prior, ß-VAE can have better rate-distortion curve than using fixed Gaussian prior.",
        "Introduction": "  INTRODUCTION Variational auto-encoder (VAE) ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ) is a powerful deep generative model. The use of amortized variational inference makes VAE scalable to deep neural networks and large amount of data. Variational inference demands the intractable true posterior to be approximated by a tractable distribution. The original VAE used factorized Gaussian for both the prior and the variational posterior ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ). Since then, lots of more expressive variational posteriors have been proposed ( Tran et al., 2016 ;  Rezende & Mohamed, 2015 ;  Salimans et al., 2015 ;  Nalisnick et al., 2016 ;  Kingma et al., 2016 ;  Mescheder et al., 2017 ;  van den Berg et al., 2018 ). However, recent work suggested that even with powerful posteriors, VAE may still fail to match aggregated posterior to unit Gaussian prior ( Rosca et al., 2018 ), indicating there is still a gap between the approximated and the true posterior. To improve the variational lower-bound, one alternative way to using powerful posterior distribu- tions is to learn the prior, an idea initially suggested by  Hoffman & Johnson (2016) . Later on,  Huang et al. (2017)  applied RealNVP ( Dinh et al., 2017 ) to learn the prior.  Tomczak & Welling (2018)  proved the optimal prior is the aggregated posterior, which they approximate by assembling a mixture of the posteriors with a set of learned pseudo-inputs.  Bauer & Mnih (2019)  constructed a rich prior by multiplying a simple prior with a learned acceptance function.  Takahashi et al. (2019)  introduced the kernel density trick to estimate the KL divergence in ELBO, for their implicit prior. Despite the achievements of these previous works on posteriors and priors, the state-of-the-art VAE models with continuous latent variables all rely on deep hierarchical latent variables 1 , although some of them might have used complicated posteriors/priors as components in their architectures. Most latent variables in such deep hierarchical VAEs have no clear semantic meanings, just a technique for reaching good lower-bounds. This is in sharp contrast with GANs ( Goodfellow et al., 2014 ;  Arjovsky et al., 2017 ) and flow-based models ( Ho et al., 2018 ), where most of them can reach state-of-the-art results with only one latent variable, with clear semantic meanings. We thus raise and answer a question: with the help of learned priors, can shallow VAEs achieve performance comparable or better than deep hierarchical VAEs? This question is important because a shallow Under review as a conference paper at ICLR 2020 VAE would be much more promising to scale to more complicated datasets than deep hierarchical VAEs. To answer this question, we conduct comprehensive experiments on several datasets with learned RealNVP priors and just one latent variable, which even shows advantage over some deep hierarchical VAEs with powerful posteriors. We also propose a hypothesis on why learning the prior can lead to such great improvement in model performance, supported by our experimental results. In summary, our contributions are: • We conduct comprehensive experiments on four binarized datasets with four different net- work architectures. Our results show that VAE with RealNVP prior consistently outper- forms standard VAE and RealNVP posterior. • We are the first to show that using learned RealNVP prior with just one latent variable in VAE, it is possible to achieve test negative log-likelihoods (NLLs) comparable to very deep state-of-the-art hierarchical VAE on these four datasets, outperforming many previous works using complex hierarchical VAE equipped with rich priors/posteriors. • We hypothesize that, when coupled with Gaussian posteriors, the learned prior can encour- age appropriate posterior overlapping, which is likely to improve reconstruction loss and lower-bound, supported by our experimental results. • We demonstrate that, with learned RealNVP prior, β-VAE can have better rate-distortion curve ( Alemi et al., 2018 ) than using fixed Gaussian prior.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes the collaborative training algorithm of balanced random forest (CoBRF) to mitigate the challenging problems such as noisy labels, lack of training data, and misaligned or unknown categories (open set categorization). CoBRF combines the ideas of maximizing the 'class information gain' of labeled data in the source domain and minimizing the 'domain information gain' between source and target data for the domain alignment. An extensive evaluation of the domain adaptation is performed to show the performance of the proposed method according to various challenging evaluation protocols. Results show significant performance improvements compared to the baseline and state-of-the-art methods.",
        "Abstract": "In this paper, we introduce a collaborative training algorithm of balanced random forests for domain adaptation tasks which can avoid the overfitting problem. In real scenarios, most domain adaptation algorithms face the challenges from noisy, insufficient training data. Moreover in open set categorization, unknown or misaligned source and target categories adds difficulty. In such cases, conventional methods suffer from overfitting and fail to successfully transfer the knowledge of the source to the target domain. To address these issues, the following two techniques are proposed. First, we introduce the optimized decision tree construction method, in which the data at each node are split into equal sizes while maximizing the information gain. Compared to the conventional random forests, it generates larger and more balanced decision trees due to the even-split constraint, which contributes to enhanced discrimination power and reduced overfitting. Second, to tackle the domain misalignment problem, we propose the domain alignment loss which penalizes uneven splits of the source and target domain data. By collaboratively optimizing the information gain of the labeled source data as well as the entropy of unlabeled target data distributions, the proposed CoBRF algorithm achieves significantly better performance than the state-of-the-art methods. The proposed algorithm is extensively evaluated in various experimental setups in challenging domain adaptation tasks with noisy and small training data as well as open set domain adaptation problems, for two backbone networks of AlexNet and ResNet-50.",
        "Introduction": "  INTRODUCTION In recent years, domain adaptation has been researched as it can help to solve major difficulties in the real world. Due to the huge overhead in labeling large-scale training data, it is desirable if an existing network can be adapted to different target domains. More importantly, it is common that the training dataset for adaptation is noisy and small, or the labels in the target domain do not match with the source or even unknown. These are inherent challenges in the domain adaptation problem as in real world it is common for the data to contain such class bias, noise and unlabeled data. However, in practice, since the adapted networks are often overfitted to the provided source data or the data distribution of the target domain is frequently quite different from the source, they do not perform well to the target domain. To properly deal with these real-world conditions with insufficient information, it is critical to learn the shared data distribution that is effective both in the source and target domain. To this end, we propose the collaborative training algorithm of balanced random forest (CoBRF) to mitigate the challenging problems such as noisy labels, lack of training data, and misaligned or unknown categories (open set categorization). In random forests, multiple decision trees are learned by optimizing the information gain for the randomly selected subset features at each node split. Since random forests ensemble the internal decision trees, they are more robust to noise and overfitting problem than single decision trees. To improve the robustness of the random forests, we take one step further by balancing the decision trees, i.e., maximizing the number of leaf nodes for the same tree depth. Our method builds more balanced decision trees by enforcing the sizes of the data in the left and right child nodes to be equal. While this split strategy is not locally optimal in terms of information gain, the resulting decision trees have far more leaf nodes, and it endows more expressive power which can be helpful in dealing with noise and unseen data or classes. It also helps to avoid overfitting as it prevents a node committing too early Under review as a conference paper at ICLR 2020 for a specific pattern, or in other words, it postpones the decision as late as possible so that various discriminant information in the training data can be fully considered. To enforce even splits while maintaining the discriminability, the CoBRF uses the hyperplanes estimated by the linear support vector machine (SVM). First, it randomly assigns the classes in the nodes to binary pseudo labels and equalizes the sizes of two pseudo classes by randomly removing data in the larger class. Then a linear classifier is found by SVM, and its hyperplane is translated until the data sizes on both sides are equal. In a sense, it finds the even split of the data projected onto the normal direction of the hyperplane and places the hyperplane there. The node split by the translated hyperplane is simple yet effective. The ablation study in Sec. 4.2 confirms that the CoBRF boosts the performance compared to the baseline random forests. Since the above training process only considers maximizing the information gain of labeled data in the source domain, which is referred to 'class information gain', it does not resolve the domain misalignment problem between the source and target domain. Because the target labels are not available during training, we try to keep the overall distribution of the target data as close to that of the source data as possible. Since the source data are evenly split, we guide the algorithm to minimize the information gain between the source and target domain, which encourages even split of the target data also. The CoBRF combines the ideas, minimizing the 'domain information gain' between source and target data for the domain alignment while keeping the class information gain to be maximized. Note that the domain alignment term is the same as the negative information gain of the binary domain labels (source/target). Thus, the CoBRF can be seen as an example of adversarial learning, as it considers the domain information gain in an adversarial manner compared to the conventional objective function of the random forest. We summarize the main contributions as three-fold. • We introduce the collaborative training algorithm based balanced random forest (CoBRF) using the discriminative and even node split function. Linear SVM with binary pseudo labeling is used to find the discriminative hyperplane and the even split ensures the decision tree to be balanced. • We also adopt the adversarial learning of domain information gain to align the source and target data distribution. To align two domains, the information gain between the source and target data is minimized, which learns the common data distribution of both the (unlabeled) target domain and the source domain data. • We perform an extensive evaluation of the domain adaptation to show the performance of the proposed method according to various challenging evaluation protocols. Specifically, it is compared to the baseline and state-of-the-art methods using noisy and small training data, and with open-set domain adaptation protocols. In both cases we observe significant performance improvements.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of attention in graph neural networks (GNNs) and proposes an analytical paradigm to understand the functionality of attention. Through extensive experiments, the authors find that the attention distributions across heads and layers are near uniform for all citation networks, while they get more concentrated over layers on the protein-protein interaction networks (PPI) and molecular graphs, with significant diversity among heads. The authors also propose an attention-based sparsification approach to train a cheaper model without using attention to fit the downstream task.",
        "Abstract": "Does attention matter and, if so, when and how? Our study on both inductive and transductive learning suggests that datasets have a strong influence on the effects of attention in graph neural networks. Independent of learning setting, task and attention variant, attention mostly degenerate to simple averaging for all three citation networks, whereas they behave strikingly different in the protein-protein interaction networks and molecular graphs: nodes attend to different neighbors per head and get more focused in deeper layers. Consequently, attention distributions become telltale features of the datasets themselves. We further explore the possibility of transferring attention for graph sparsification and show that, when applicable, attention-based sparsification retains enough information to obtain good performance while reducing computational and storage costs. Finally, we point out several possible directions for further study and transfer of attention.",
        "Introduction": "  INTRODUCTION The modeling of graphs has become an active research topic in deep learning ( Bronstein et al., 2017 ). Dozens of neural network models have been developed for exploiting the structural information of graphs ( Scarselli et al., 2009 ;  Bruna et al., 2014 ;  Henaff et al., 2015 ; Duvenaud et al., 2015;  Niepert et al., 2016 ;  Defferrard et al., 2016 ), now collectively referred to as graph neural networks (GNNs). Built upon the success of attention in NLP ( Vaswani et al., 2017 ),  Veličković et al. (2018)  proposed the graph attention networks (GATs) to integrate multi-head self-attention into node feature update for adaptive weighting, with several extensions ( Thekumparampil et al., 2018 ;  Zhang et al., 2018 ;  Monti et al., 2018 ;  Svoboda et al., 2019 ;  Trivedi et al., 2019 ). While the use of attention in GNNs is an attractive direction, several works also report that attention contributes little to the performance of GNNs ( Zhang et al., 2018 ;  Shchur et al., 2019 ). Considering the high computational cost of attention, the question is then that does attention help and, if so, when and how? In this paper, we take a first step towards the question. We first identify the key questions for understanding attention and propose an analytical paradigm. With extensive experiments, our findings suggest that, although attention is motivated by inductive learning, its functionality depends highly on the characteristics of the datasets. The attention distributions across heads and layers are near uniform for all citation networks (Cora, Citeseer and Pubmed) while they get more concentrated over layers on the protein-protein interaction networks (PPI) and molecular graphs, with significant diversity among heads. That the attention distribution is a telltale sign of the nature of graph class is further verified with a meta graph classification experiment. With attention features as inputs, citation networks are indistinguishable whereas PPI and molecule graphs are. Inspired by these findings, we hypothesize that attention carry semantic meanings when they are non-uniform and can be helpful for transfer learning. This has been the case in the NLP commu- nity ( Radford et al., 2019 ), and is motivating many research efforts on understanding multi-head attention ( Jain & Wallace, 2019 ;  Clark et al., 2019 ;  Voita et al., 2019 ). We attempt the idea of attention based sparsification - sparsifying a graph by retaining edges where attention are higher, with the intuition being that the resulting graph preserves enough information. We find that not only such attention-based sparsification is transferable (meaning, it can work on unseen graphs), it also affords us to train a cheaper model without using attention to fit the downstream task. Finally, we discuss several possible fruitful directions for further exploration, including theory, interpretability, and unsupervised learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes RICH, a deep generative model for unsupervised learning of interpretable compositional hierarchies. RICH uses a probabilistic scene graph representation to capture the compositional relationships among interpretable symbolic entities like parts, objects, and scenes. It is paired with a decoder that renders the scene graph by recursively applying the specified transformations. Experiments demonstrate that RICH is able to learn the hierarchical scene graph representation from images containing multiple compositional objects, and shows decent generation quality and generalization ability to unseen number of objects.",
        "Abstract": "Hierarchical structure such as part-whole relationship in objects and scenes are the most inherent structure in natural scenes. Learning such representation via unsupervised learning can provide various benefits such as interpretability, compositionality, and transferability, which are important in many downstream tasks. In this paper, we propose the first hierarchical generative model for learning multiple latent part-whole relationships in a scene. During inference, taking top-down approach, our model infers the representation of more abstract concept (e.g., objects) and then infers that of more specific concepts (e.g., parts) by conditioning on the corresponding abstract concept. This makes the model avoid a difficult problem of routing between parts and whole. In experiments on images containing multiple objects with different shapes and part compositions, we demonstrate that our model can learn the latent hierarchical structure between parts and wholes and generate imaginary scenes.",
        "Introduction": "  INTRODUCTION Compositional hierarchies prevail in natural scenes where primitive entities are recursively com- posed into more abstract entities. Modeling such compositional generative process allows discovery of modular primitives that can be reused across a variety of scenes. Hence, it would bring inter- pretability and transferability, in which current deep learning models are not quite successful. Due to expensive labeling, such compositional relationships should ideally be learned in an unsupervised manner. Unsupervised approaches can also provide more flexibility and generalization ability since the model is allowed to choose the most appropriate compositional hierarchy for a given scenario. Despite its importance, there has not been much work on unsupervised generative modeling of the compositional hierarchy. Earlier work on hierarchical representation learning ( Lee et al., 2009 ) ob- tains a feature hierarchy that captures concepts at different levels of abstraction, with no explicit modeling of composition. Recent researches on deep latent variable models ( Maaløe et al., 2019 ;  Zhao et al., 2017 ;  Sønderby et al., 2016 ;  Bachman, 2016 ) mainly focus on architectural designs and training methods that harness the full expressive power of hierarchical generative models. Although they have shown impressive generation quality and disentanglement of learned representation, the compositional hierarchy is still not captured in a modular and interpretable way. To obtain inter- pretable scene representation, recent work ( Tieleman, 2014 ;  Eslami et al., 2016 ;  Crawford & Pineau, 2019 ;  Wu et al., 2017 ;  Yao et al., 2018 ;  Romaszko et al., 2017 ;  Deng et al., 2019 ) has introduced domain-specific decoders that take object pose and appearance information as input and render the object in a similar way to graphics engines. This forces the encoder to invert the rendering process, producing interpretable object-wise pose and appearance representation. In this paper, we extend the interpretable object-wise representation to the hierarchical setting. We propose a deep generative model, called RICH (Representation of Interpretable Compositional Hier- archies), that can use its hierarchy to represent the compositional relationships among interpretable symbolic entities like parts, objects, and scenes. To this end, taking inspiration from capsule net- works ( Sabour et al., 2017 ;  Hinton et al., 2018 ) and the rendering process of computer graphics, we propose a probabilistic scene graph representation that describes the compositional hierarchy as a latent tree. The nodes in the tree correspond to entities in the scene, while the edges indicate the compositional relationships among these entities. We associate an appearance latent with each node to summarize all lower-level composition, and a pose latent with each edge to specify the Under review as a conference paper at ICLR 2020 transformation from the current level to the upper level. To enforce interpretability, the probabilistic scene graph is then paired with a decoder that renders the scene graph by recursively applying the specified transformations. We also introduce learnable templates for the primitive entities. Once learned, RICH is able to generate all lower-level latents and render a partial scene given the latent at a specific level. To infer the scene graph is, however, challenging, since both the tree structure and the latent vari- ables need to be simultaneously inferred. Capsule networks have provided a bottom-up solution to learning the tree structure, but it faces the difficult routing problem caused by the exponentially many possible compositions. Instead, RICH takes a top-down approach that avoids the routing prob- lem. The intuition is that for a given scene, it is natural to first decompose it into high-level objects. If we devote our attention to one of the objects, we can then figure out its constituent parts. In cases where parts are close or have occlusion, we expect the appearance latent of the higher-level object to guide lower-level decomposition, since it summarizes the typical composition for that object. The contributions of this paper are as follows. We propose RICH, the first interpretable represen- tation learning model for compositional hierarchies through probabilistic latent variable modeling. We then implement a three-level prototype of RICH and demonstrate its effectiveness in extensive experiments. RICH is able to learn the hierarchical scene graph representation from images contain- ing multiple compositional objects. Further, it shows decent generation quality and generalization ability to unseen number of objects.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes an optimization principle that characterizes and quantifies the impacts of various deep neural network (DNN) training techniques on DNN training. The proposed principle is applicable to general stochastic algorithms in the nonconvex and over-parameterized regime and leads to guaranteed convergence to a global minimum. This paper also provides a summary of important DNN training techniques, such as neuron level activation functions, layer level batch normalization, and architecture level skip connections. The proposed principle explains why DNN trainings with different techniques can achieve the global minimum in practice and quantifies the impacts of the training techniques on training deep networks.",
        "Abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.",
        "Introduction": "  INTRODUCTION Deep learning has been successfully applied to various domains such as computer vision, natural language processing, etc, and has achieved state-of-art performance in solving challenging tasks. Although deep neural networks (DNNs) have been well-known for decades to have great expressive power  Cybenko (1989) , the empirical success of training DNNs postponed to recent years when sufficient computation power is accessible and effective DNN training techniques are developed. The milestone developments of DNN training techniques can be divided into two categories. First, various techniques have been developed at different levels of neural network design. At the neuron level, various functions have been applied to activate the neurons, e.g., sigmoid function, hyperbolic tangent (tanh) function and the more popular rectified linear unit (ReLU) function  Nair & Hinton (2010) . At the layer level, batch normalization (BN) has been widely applied to the hidden layers of DNNs to stabilize the training  Ioffe & Szegedy (2015) . Moreover, at the architecture level, skip connections have been introduced to enable successful training of deep networks  He et al. (2015) ;  Szegedy et al. (2015) ;  Srivastava et al. (2015) ;  Huang et al. (2016) . Second, various efficient stochastic optimization algorithms have been developed for DNN training, e.g., stochastic gradient descent (SGD)  Robbins & Monro (1951) ;  Rumelhart et al. (1986) , SGD with momentum  Qian (1999) ;  Nesterov (2014)  and  Adam Kingma & Ba (2015) , etc.  Table 1  provides a summary of these important DNN training techniques. While these training techniques have been widely applied in practical DNN training, there is limited understanding on how they help facilitate the training and achieve the global minimum of the network. In the existing literature, it is known that the sigmoid and tanh activation functions can cause the vanishing gradient problem, and the ReLU activation function is a popular replacement that avoids this issue  Nair & Hinton (2010) ;  Glorot et al. (2011) . On the other hand, the batch normalization is originally proposed to reduce the internal covariance shift  Ioffe & Szegedy (2015) , and more recent studies show that it allows to use a large learning rate  Bjorck et al. (2018)  and improves the loss landscape  Santurkar et al. (2018) . The skip connection has been shown to help eliminate singularities and degeneracies  Orhan & Pitkow (2018)  and improve the loss landscape  Hardt & Ma (2016) . Moreover, regarding the optimization algorithm, the momentum scheme has been well-known to accelerate convex optimization  Nesterov (2014)  and is also widely applied to accelerate nonconvex optimization  Ghadimi & Lan (2016) , whereas the Adam algorithm normalizes the update in each dimension to accelerate deep learning  Kingma & Ba (2015) . While these studies provide partial explanations to the effectiveness of various DNN training techniques, their reasonings are from very different perspectives and it is unclear whether a general principle can exist that guides these training techniques to facilitate the training process. In particular, these studies do not explain why DNN trainings with different techniques can achieve the global minimum in practice. Moreover, the existing explanations cannot quantify the impacts of the training techniques on training deep networks, and a principled quantification metric is still far from clear. This paper attempts to propose an optimization principle that characterizes and quantifies the impacts of various training techniques on DNN training. The proposed principle is applicable to general stochastic algorithms in the nonconvex and over-parameterized regime and leads to guaranteed convergence to a global minimum. We summarize our contributions as follows.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper provides theoretical analysis of the effect of over-fitting or data interpolation on the performance of learning algorithms. It is demonstrated that under proper smooth conditions, the multiplicative constant of interpolated-NN, as a function of interpolation level, is U-shaped. This U-shaped curve is used to explain the double descent phenomenon in random forest and neural network algorithms, where testing performance follows a (conventional) U-shaped curve, and as the level of overfitting increases, a second descent or even a second U-shaped testing performance curve occurs. The paper also shows that the gain from interpolation diminishes as the data dimension grows to infinity, and that other \"non-interpolating\" weighting schemes can achieve an even better performance.",
        "Abstract": "The over-parameterized models attract much attention in the era of data science and deep learning. It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even outperform traditional algorithms which are designed to avoid over-fitting. The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm. Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances. Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation strictly improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm. This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon. Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.",
        "Introduction": "  INTRODUCTION Classical statistical learning theory believes that over-fitting deteriorates prediction performance: when the model complexity is beyond necessity, the testing error must be huge. Therefore, various techniques have been proposed in literature to avoid over-fitting, such as early stopping, dropout and cross validation. However, recent experiments reveal that even with over-fitting, many learning algorithms still achieve small generalization error. For instances,  Wyner et al. (2017)  explored the over-fitting in AdaBoost and random forest algorithms;  Belkin et al. (2019a)  discovered a double descent phenomenon in random forest and neural network: with growing model complexity, testing performance firstly follows a (conventional) U-shaped curve, and as the level of overfitting increases, a second descent or even a second U-shaped testing performance curve occurs. To theoretically understand the effect of over-fitting or data interpolation,  Du & Lee (2018) ;  Du et al. (2019 ; 2018);  Arora et al. (2018 ; 2019);  Xie et al. (2017)  analyzed how to train neural networks un- der over-parametrization, and why over-fitting does not jeopardize the testing performance;  Belkin et al. (2019c)  constructed a Nadaraya-Watson kernel regression estimator which perfectly fits train- ing data but is still minimax rate optimal;  Belkin et al. (2018)  and  Xing et al. (2018)  studied the rate of convergence of interpolated nearest neighbor algorithm (interpolated-NN);  Belkin et al. (2019b) ; Bartlett et al. (2019) quantified the prediction MSE of the linear least squared estimator when the data dimension is larger than sample size and the training loss attains zero. Similar analysis is also conducted by  Hastie et al. (2019)  for two-layer neural network models with a fixed first layer. In this work, we aim to provide theoretical reasons on whether, when and why the interpolated-NN performs better than the optimal kNN, by some sharp analysis. The classical kNN algorithm for either regression or classification is known to be rate-minimax under mild conditions ( Chaudhuri & Dasgupta (2014) ), say k diverges propoerly. However, can such a simple and versatile algorithm still benefit from intentional over-fitting? We first demonstrate some empirical evidence below.  Belkin et al. (2018)  designed an interpolated weighting scheme as follows: where x (i) is the i-th closest neighbor to x with the corresponding label y(x (i) ). The parameter γ ≥ 0 controls the level of interpolation: with a larger γ > 0, the algorithm will put more weights on the closer neighbors. In particular when γ = 0 or γ = ∞, interpolated-NN reduces to kNN or 1- NN, respectively.  Belkin et al. (2018)  showed that such an interpolated estimator is rate minimax in the regression setup, but suboptimal in the setting of binary classification. Later,  Xing et al. (2018)  obtained the minimax rate of classification by adopting a slightly different interpolating kernel. What is indeed more interesting is the preliminary numerical analysis (see  Figure 1 ) conducted in the aforementioned paper, which demonstrates that interpolated-NN is even better than the rate minimax kNN in terms of MSE (regression) or mis-classification rate (classification). This observation asks for deeper theoretical exploration beyond the rate of convergence. A reasonable doubt is that the interpolated-NN may possess a smaller multiplicative constant for its rate of convergence, which may be used to study the generalization ability within the \"over-parametrized regime.\" In this study, we will theoretically compare the minimax optimal kNN and the interpolated-NN (under (1)) in terms of their multiplicative constants. On the one hand, we show that under proper smooth conditions, the multiplicative constant of interpolated-NN, as a function of interpolation level γ, is U-shaped. As a consequence, interpolation indeed leads to more accurate and stable performance when the interpolation level γ ∈ (0, γ d ) for some γ d > 0 only depending on the data dimension d. The amount of benefit (i.e., the \"performance ratio\" defined in Section 2) follows exactly the same asymptotic pattern for both regression and classification tasks. In addition, the gain from interpolation diminishes as the dimension d grows to infinity, i.e. high dimensional data benefit less from data interpolation. We also want to point out that there still exist other \"non-interpolating\" weighting schemes, such as OWNN, which can achieve an even better performance; see Section 3.4. More subtle results are summarized in the figure below. From  Figure 2 , we theoretically justify (predict) the existence of the U-shaped curve within the \"over-fitting regime\" of the recently discovered double descent phenomenon by Belkin et al. Under review as a conference paper at ICLR 2020 (2019a;b). As complementary to  Belkin et al. (2018) ;  Xing et al. (2018) , we further show in ap- pendix that interpolated-NN reaches optimal rate for both regression and classification under more general (α, β)-smoothness conditions in Section F. In the end, we want to emphasize that our goal here is not to promote the practical use of this interpolation method given that kNN is more user-friendly. Rather, the interpolated-NN algorithm is used to precisely describe the role of interpolation in generalization ability so that more solid theoretical arguments can be made for the very interesting double descent phenomenon, especially in the over-fitting regime.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes an unsupervised model for few-shot learning via a self-supervised training procedure (UFLST). UFLST integrates unsupervised learning and episodic training into a unified framework, which facilitates feature extraction and model training iteratively. The model applies progressive clustering to generate pseudo labels for unlabeled data, and this is done alternatively with feature optimization via few-shot learning in an iterative manner. Using benchmark datasets, the paper demonstrates that UFLST outperforms other unsupervised few-shot learning methods and approaches to the performances of fully supervised models.",
        "Abstract": "Learning from limited exemplars (few-shot learning) is a fundamental, unsolved problem that has been laboriously explored in the machine learning community. However, current few-shot learners are mostly supervised and rely heavily on a large amount of labeled examples. Unsupervised learning is a more natural procedure for cognitive mammals and has produced promising results in many machine learning tasks. In the current study, we develop a method to learn an unsupervised few-shot learner via self-supervised training (UFLST), which can effectively generalize to novel but related classes. The proposed model consists of two alternate processes, progressive clustering and episodic training. The former generates pseudo-labeled training examples for constructing  episodic tasks; and the later trains the few-shot learner using the generated episodic tasks which further optimizes the feature representations of data. The two processes facilitate with each other, and eventually produce a high quality few-shot learner. Using the benchmark dataset Omniglot, we show that our model outperforms other unsupervised few-shot learning methods to a large extend and approaches to the performances of supervised methods. Using the benchmark dataset Market1501, we further demonstrate the feasibility of our model to a real-world application on person re-identification.",
        "Introduction": "  INTRODUCTION Few-shot learning, which aims to accomplish a learning task by using very few training examples, is receiving increasing attention in the machine learning community. The challenge of few-shot learn- ing lies on that traditional techniques such as fine-tuning would normally incur overfitting ( Wang et al., 2018 ). To overcome this difficulty, a set-to-set meta-learning(episodic learning) paradigm was proposed ( Vinyals et al., 2016 ). In such a paradigm, the conventional mini-batch training is replaced by the episodic training, in term of that a batch of episodic tasks, each of which having the same setting as the testing environment, are presented to the learning model; and in each episodic task, the model learns to predict the classes of unlabeled points (the query set) using very few labeled exam- ples (the support set). By this, the learning model acquires the transferable knowledge (optimized feature representations) across tasks, and due to the consistency between the training and testing environments, the model is able to generalize to novel but related tasks. Although this set-to-set few-shot learning paradigm has made great progress, in its current supervised form, it requires a large number of labeled examples for constructing episodic tasks, which is often infeasible or too expensive in practice. So, can we build up a few-shot learner in the paradigm of episodic training using only unlabeled data? It is well-known that humans have the remarkable ability to learn a concept when given only several exposures to its instances, for example, young children can effortlessly learn and generalize the concept of \"giraffe\" after seeing a few pictures of giraffes. While the specifics of the human learning process are complex (trial-based, perpetual, multi-sourced, and simultaneous for multiple tasks) and yet to be solved, previous works agree that its nature is progressive and unsupervised in many cases ( Dupoux, 2018 ). Given a set of unlabeled items, humans are able to organize them into different clusters by comparing one with another. The comparing or associating process follows a coarse-to-fine manner. At the beginning of learning, humans tend to group items based on fuzzy- rough knowledge such as color, shape or size. Subsequently, humans build up associations between items using more fine-grained knowledge, i.e., stripes of images, functions of items or other domain Under review as a conference paper at ICLR 2020 knowledge. Furthermore, humans can extract representative representations across categories and apply this capability to learn new concepts ( Wang et al., 2014b ). In the present study, inspired by the unsupervised and progressive characteristics of human learning, we propose an unsupervised model for few-shot learning via a self-supervised training procedure (UFLST). Different from previous unsupervised learning methods, our model integrates unsuper- vised learning and episodic training into a unified framework, which facilitates feature extraction and model training iteratively. Basically, we adopt the episodic training paradigm, taking advan- tage of its capability of extracting transferable knowledge across tasks, but we use an unsupervised strategy to construct episodic tasks. Specifically, we apply progressive clustering to generate pseudo labels for unlabeled data, and this is done alternatively with feature optimization via few-shot learn- ing in an iterative manner ( Fig. 1 ). Initially, unlabeled data points are assigned into several clusters, and we sample a few training examples from each cluster together with their pseudo labels (the identities of clusters) to construct a set of episodic tasks having the same setting as the testing environment. We then train the few-shot learner using the constructed episodic tasks and obtain improved feature representations for the data. In the next round, we use the improved features to re- cluster data points, generating new pseudo labels and constructing new episodic tasks, and train the few-shot learner again. The above two steps are repeated till a stopping criterion is reached. After training, we expect that the few-shot learner has acquired the transferable knowledge (the optimized feature representations) suitable for a novel task of the same setting as in the episodic training. Us- ing benchmark datasets, we demonstrate that our model outperforms other unsupervised few-shot learning methods and approaches to the performances of fully supervised models.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Granger cAusal StructurE Reconstruction (GASER) framework for inductive Granger causality learning and common causal structure detection on heterogeneous multivariate time series data. GASER builds on the idea of quantifying the contributions of each variable series into the prediction of target variable via a novel designed prototypical Granger causal attention mechanism. Experiments demonstrate the superior causal structure reconstruction and prediction performances of GASER, as well as its ability to uncover common structures among heterogeneous multivariate time series.",
        "Abstract": "Granger causal structure reconstruction is an emerging topic that can uncover causal relationship behind multivariate time series data. In many real-world systems, it is common to encounter a large amount of multivariate time series data collected from heterogeneous individuals with sharing commonalities, however there are ongoing concerns regarding its applicability in such large scale complex scenarios, presenting both challenges and opportunities for Granger causal reconstruction. To bridge this gap,  we propose a Granger cAusal StructurE Reconstruction (GASER) framework for inductive Granger causality learning and common causal structure detection on heterogeneous multivariate time series. In particular, we address the problem through a novel attention mechanism, called prototypical Granger causal attention. Extensive experiments, as well as an online A/B test on an E-commercial advertising platform, demonstrate the superior performances of GASER.",
        "Introduction": "  INTRODUCTION Broadly, machine learning tasks are either predictive or descriptive in nature, often addressed by black-box methods ( Guo et al., 2018 ). With the power of uncovering relationship behind the data and providing explanatory analyses, causality inference has drawn increasing attention in many fields, e.g. marketing, economics, and neuroscience ( Pearl, 2000 ;  Peters et al., 2017 ). Since the cause generally precedes its effects, known as temporal precedence ( Eichler, 2013 ), recently, an increasing number of studies have focused on causal discovery from time series data. They are commonly based on the concept of Granger causality ( Granger, 1969 ;  1980 ) to investigate the causal relationship with quantification measures. In many real-world systems, it is common to encounter a large amount of multivariate time series (MTS) data collected from different individuals with shared commonalities, which we define as het- erogeneous multivariate time series. The underlying causal structures of such data often vary ( Zhang et al., 2017 ;  Huang et al., 2019 ). For example, in the financial market, the underlying causal drivers of stock prices are often heterogeneous across stocks of different plates. Similar phenomenons are also observed in the sales of different products in E-commerce. To this situation, most existing methods have to train separate models for MTS of each individual, which suffer from over-fitting especially given limited training samples. Although some works have been proposed to solve such problem ( Zhang et al., 2017 ;  Huang et al., 2019 ), they lack the inductive capability to do inferences for unseen samples and fall short of fully exploiting shared causal information among the heteroge- neous data which often exist in practice. For instance, the causal structures of the products belonging to the same categories are usually similar. Such shared information presents opportunities for causal reconstruction to alleviate overfitting and to do inductive reasoning. However, it is also challenging to detect common and specific causal structures simultaneously. In this paper, we propose a Granger cAusal StructurE Reconstruction (GASER) framework for inductive Granger causality learning and common causal structure detection on heterogeneous mul- tivariate time series data. Our approach builds on the idea of quantifying the contributions of each variable series into the prediction of target variable via a novel designed prototypical Granger causal attention mechanism. In order to ensure that the attention capturing Granger causality, we first de- sign an attention mechanism based on Granger causal attribution of the target series and then perform prototype learning that generates both shared and specific prototypes to improve the model's robust- Under review as a conference paper at ICLR 2020 ness. Extensive experiments demonstrate the superior causal structure reconstruction and prediction performances of GASER. In summary, our specific contributions are as follows: • A novel framework that inductively reconstructs Granger causal structures and uncovers common structures among heterogeneous multivariate time series. • A prototypical Granger causal attention mechanism that summarizes variable-wise contributions towards prediction and generates prototypes representing common causal structures. • Relative extensive experiments on real-world, benchmark and synthetic datasets as well as an on- line A/B test on an E-commercial advertising platform that demonstrate the superior performance on the causal discovery and the prediction performance comparable to state-of-the-art methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new approach to address the catastrophic forgetting problem in deep learning, called discriminative variational autoencoder (DiVA). DiVA is a generative replay-based approach that uses a generative model to replay previous tasks, and a domain translation trick to mitigate the distribution discrepancy between real and generated samples. The proposed approach is evaluated on a class incremental learning setting, and achieves state-of-the-art accuracy.",
        "Abstract": "Generative replay (GR) is a method to alleviate catastrophic forgetting in continual learning (CL) by generating previous task data and learning them together with the data from new tasks. In this paper, we propose discriminative variational autoencoder (DiVA) to address the GR-based CL problem. DiVA has class-wise discriminative latent embeddings by maximizing the mutual information between classes and latent variables of VAE. Thus, DiVA is directly applicable to classification and class-conditional generation which are efficient and effective properties in the GR-based CL scenario. Furthermore, we use a novel trick based on domain translation to cover natural images which is challenging to GR-based methods. As a result, DiVA achieved the competitive or higher accuracy compared to state-of-the-art algorithms in Permuted MNIST, Split MNIST, and Split CIFAR10 settings.",
        "Introduction": "  INTRODUCTION Even though deep learning models such as  Szegedy et al. (2015) ;  He et al. (2016) ;  Xie et al. (2017)  have been showing remarkable performance on many image recognition tasks exceeding human accuracy, they suffer from the catastrophic forgetting problem. The catastrophic forgetting problem occurs when we want to learn several tasks sequentially without any continual learning (CL) technique ( Ratcliff (1990) ;  French (1999) ;  Goodfellow et al. (2013) ). Theoretically, the catastrophic forgetting problem can be completely avoided if we store all of the past data, but it is sometimes difficult to store all of the previous data due to the limitation on memory capacity. To handle the forgetting problem in deep learning, some studies suggested weight regularization (WR) techniques such as EWC ( Kirkpatrick et al. (2017) ), SI ( Zenke et al. (2017) ), and IMM ( Lee et al. (2017) ). In these approaches, each of them has a regularization term to constrain the update of some parameters that are important for previous tasks. Other researches such as GEM ( Lopez-Paz et al. (2017) ), VCL ( Nguyen et al. (2018) ) introduced an additional memory called coreset which consists of the small number of real images of past tasks to further improve WR algorithm. We will call this approaches as experience replay (ER)-based methods in this paper. Specifically, ER-based methods are useful when we can access previous task data though the amount of past data is limited. Recently, generative replay (GR)-based approaches such as DGR ( Shin et al. (2017) ) and VGR ( Farquhar & Gal (2018) ) are also proposed to replay previous tasks with a generative model instead of storing the small number of images into coreset. They showed that if the generative model successfully approximates data distribution, this approach can be a strong way to alleviate the catastrophic forgetting problem. In this paper, we propose a new efficient and effective model called discriminative variational autoencoder (DiVA) ( Fig. 1 ) to address the GR-based CL problem. GR-based approaches can address a continual learning setting where a stream of real samples is seen only once, which can not be considered in ER-based CL algorithms. However, GR-based approaches have two shortcomings. First, they need an additional module, a generative model, in addition to a classifier. Second, distribution discrepancy between real and generated samples makes GR-based methods hard to address complex natural images such as CIFAR ( Krizhevsky & Hinton (2009) ) or ImageNet ( Deng et al. (2009) ). In this point of views, we try to handle the shortcomings. Firstly, distributions of latent variables of DiVA are clustered class-wise discriminatively on the latent space by introducing mutual information maximization between latent variable z and class condition c. Thus, DiVA is directly applicable to Under review as a conference paper at ICLR 2020 both classification and class conditional sample generation with one integrated model. Also, we apply a simple but effective domain translation (DT) trick to mitigate the distribution discrepancy and thus DiVA can achieve better classification accuracy on past tasks. To the best of our knowledge, we firstly introduce the DT trick to GR-based CL problem. In summary, we describe three main contributions of our research as follows: • We suggest a new type of conditional VAE in terms of mutual information maximization, which can be expressed as integrated classifier and learnable prior network. It makes discriminative class-wise distributions for each class on latent space. • We propose a simple but effective DT trick, which significantly improve CL performance of GR-based approaches on a natural image dataset such as CIFAR10. • By combining DiVA and DT, we achieved state-of-the-art accuracy on the class incremental learning settings.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a simple and effective stochastic neural network (SE-SNN) that models activation uncertainty through predicting a Gaussian mean and variance at each layer, which is then sampled during the forward pass. This leads to several appealing capabilities in pruning, adversarial defense and learning with label noise. The paper's contributions are (1) a new stochastic neural network formulation, (2) connections to VIB, Dropout and non-informative activation priors, and (3) comprehensive evaluations showing excellent performance on pruning-based model compression, adversarial defense, and label noise robust learning.",
        "Abstract": "Stochastic neural networks (SNNs) are currently topical, with several paradigms being actively investigated including dropout, Bayesian neural networks, variational information bottleneck (VIB) and noise regularized learning. These neural network variants impact several major considerations, including generalization, network compression, and robustness against adversarial attack and label noise. However, many existing networks are complicated and expensive to train, and/or only address one or two of these practical considerations. In this paper we propose a simple and effective stochastic neural network (SE-SNN) architecture for discriminative learning by directly modeling activation uncertainty and encouraging high activation variability. Compared to existing SNNs, our SE-SNN is simpler to implement and faster to train, and produces state of the art results on network compression by pruning,  adversarial defense and learning with label noise.",
        "Introduction": "  INTRODUCTION Stochastic neural networks (SNNs) have a long history. Recently various stochastic neural network instantiations have been topical in their applications to reducing overfitting ( Gal & Ghahramani, 2016 ;  Neelakantan et al., 2015 ) and training data requirements ( Garnelo et al., 2018 ), providing confidence estimates on predictions ( Gal & Ghahramani, 2016 ), enabling network compression ( Dai et al., 2018 ), improving robustness to adversarial attack ( Alemi et al., 2017 ), improving optimization ( Neelakantan et al., 2015 ), generative modeling ( Kingma & Welling, 2014 ), and inputting or producing probability distributions ( de Bie et al., 2019 ; Frogner et al., 2019). One of the most theoretically appealing stochastic neural network formulations is Bayesian neural networks, which place a prior distribution on the weights of the network ( Graves, 2011 ;  Blundell et al., 2015 ; Ritter et al., 2018). However this usually necessitates more complex learning and inference procedures that rely on variational approximations or sampling. Another group of works instead focus on modeling the uncertainty in neural network activations. Notably the variational information bottle- neck (VIB) approach ( Alemi et al., 2017 ) is motivated by information theoretic considerations (Tishby et al., 1999) to learn a hidden representation that carries maximum information about the output and minimum information about the input. Evaluating the required mutual information terms requires modeling probability distributions over activations rather than weights. Deep VIB ( Alemi et al., 2017 ) leads to improved generalization, adversarial robustness and model compression algorithms ( Dai et al., 2018 ). Furthermore, modeling stochastic activations through noise is often practically useful for improving exploration and local minima escape during optimization, generalization and adversarial robustness ( You et al., 2018 ;  Noh et al., 2017 ;  Bishop, 1995 ;  Gulcehre et al., 2016 ), and in some cases can be linked back to Bayesian models of weights ( Noh et al., 2017 ;  Gal & Ghahramani, 2016 ) when the noise added at each activation can be considered as a result of different samples from the weight posterior. In this paper we propose a simple and effective stochastic neural network (SE-SNN) that models activation uncertainty through predicting a Gaussian mean and variance at each layer, which is then sampled during the forward pass. This is similar to the strategy used to model activation distributions in VAE ( Kingma & Welling, 2014 ) and VIB ( Alemi et al., 2017 ). Differently, we then place a non-informative prior on activation distribution and derive an activation regularizer that directly encourages high activation variability via preferring high-entropy activations. In conjunction with a discriminative learning loss, this means that the network is optimized for activation patterns that have Under review as a conference paper at ICLR 2020 high uncertainty while simultaneously being predictive of the target variable. The interplay between these two objectives leads to several appealing capabilities in pruning, adversarial defense and learning with label noise. Pruning: Optimizing for high per-activation variability/uncertainty and predictive accuracy simultaneously lead to the network packing more entropy into the least significant neurons - so that the most crucial neurons are free to operate unperturbed. This leads to a simple pruning criterion based on each neuron's entropy value. Adversarial defense: By optimizing for both per- activation uncertainty and the network's predictive accuracy, a representation-level data augmentation policy is trained that perturbs the internal features during training for increased robustness ( Alemi et al., 2017 ;  You et al., 2018 ). Label noise: With SE-SNN, per-activation uncertainty can be easily aggregated to produce per-instance uncertainty. By optimizing for per-instance uncertainty and predictive accuracy, the network allocates the uncertainty to spread the representation and prediction of the hard-to-classify instances so as to downweight their influence on parameter learning. The result is a model robust to label noise as well as outlying training samples. To summarize, our contributions are: (1) A new simple yet effective stochastic neural network formulation. (2) We show that our SE-SNN has connections to VIB ( Alemi et al., 2017 ), Dropout ( Srivastava et al., 2014 ) and non-informative activation priors while being simpler to implement and faster to train, as well as impactful on a variety of practical problems. (3) Comprehensive evaluations show excellent performance on pruning-based model compression, adversarial defense, and label noise robust learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel Cross-Dimensional Self-Attention (CDSA) mechanism to impute missing data in multivariate, geo-tagged time series data. CDSA is the first to apply the self-attention mechanism to this task, replacing the conventional RNN-based models to speed up training and directly model the relationship between each two data values in the input data. We comprehensively study several choices of modeling self-attention crossing different dimensions and show that CDSA is independent with the order of processing each dimension. We evaluate our model on two standard benchmarks and a newly collected traffic dataset, and show that it outperforms the state-of-the-art models for both data imputation and forecasting tasks.",
        "Abstract": "Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across different dimensions (i.e. time, location and sensor measurements) while keep the size of attention maps reasonable, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. On three real-world datasets, including one our newly collected NYC-traffic dataset, extensive experiments demonstrate the superiority of our approach compared to state-of-the-art methods for both imputation and forecasting tasks. \n",
        "Introduction": "  INTRODUCTION Various monitoring applications, such as those for air quality ( Zheng et al. (2015) ), health-care ( Silva et al. (2012) ) and traffic ( Jagadish et al. (2014) ), widely use networked observation stations to record multivariate, geo-tagged time series data. For example, air quality monitoring systems employ a collection of observation stations at different locations; at each location, multiple sensors concurrently record different measurements such as PM2.5 and CO over time. Such time series are important for advanced investigation and also are useful for future forecasting. However, due to unexpected sensor damages or communication errors, missing data is unavoidable. It is very challenging to impute the missing data because of the diversity of the missing patterns: sometimes almost random while sometimes following various characteristics. Traditional data imputation methods usually suffer from imposing strong statistical assumptions. For example,  Scharf & Demeure (1991)  and  Friedman et al. (2001)  fit a smooth curve on observations in either time series ( Ansley & Kohn (1984) ; Shumway & Stoffer (1982)) or spatial distribution ( Fried- man et al. (2001) ;  Stein (2012) ). Deep learning methods ( Li et al. (2018) ;  Che et al. (2018) ;  Cao et al. (2018) ;  Luo et al. (2018a) ) have been proposed to capture temporal relationship based on RNN ( Cho et al. (2014b) ;  Hochreiter & Schmidhuber (1997) ;  Cho et al. (2014a) ). However, due to the constraint of sequential computation over time, the training of RNN cannot be parallelized and thus is usually time-consuming. Moreover, the relationship between each two distant time stamps cannot be directly modeled. Recently, the self-attention mechanism as shown in Fig. 1(b) has been proposed by the seminal work of Transformer ( Vaswani et al. (2017) ) to get rid of the limitation of sequential processing, accelerating the training time substantially and improving the performance significantly on seq-to-seq tasks in Natural Language Processing (NLP) because the relevance between each two time stamps is captured explicitly. In this paper, we are the first to adapt the self-attention mechanism to impute missing data in multivariate time series, which cover multiple geo-locations and contain multiple measurements as Under review as a conference paper at ICLR 2020 shown in Fig. 1(a). In order to impute a missing value in such unique multi-dimensional data, it is very useful to look into available data in different dimensions (i.e. time, location and measurement), as shown in Fig. 1(c), to capture the intra-correlation individually. To this end, we investigate several choices of modeling self-attention across different dimensions. In particular, we propose a novel Cross-Dimensional Self-Attention (CDSA) mechanism to capture the attention crossing all dimension jointly yet in a decomposed manner. In summary, we make the following contributions: (i) We are the first to apply the self-attention mechanism to the multivariate, geo-tagged time series data imputation task, replacing the conventional RNN-based models to speed up training and directly model the relationship between each two data values in the input data. (ii) For such unique time series data of multiple dimensions (i.e. time, location, measurement), we comprehensively study several choices of modeling self-attention crossing different dimensions. Our proposed CDSA mechanism models self-attention crossing all dimensions jointly yet in a dimension-wise decomposed way, preventing the size of attention maps from being too large to be tractable. We show that CDSA is independent with the order of processing each dimension. (iii) We extensively evaluate on two standard benchmarks and our newly collected traffic dataset. Experimental results show that our model outperforms the state-of-the-art models for both data imputation and forecasting tasks. We visualize the learned attention weights which validate the capability of CDSA to capture important cross-dimensional relationships.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a method for learning domain-invariant representations through bases decomposition with layer branching. The method introduces a small amount of additional trainable parameters to explicitly model each domain, while the majority of the parameters remain shared across domains. The decomposed filters reduce the overall computations significantly compared to previous works. The paper provides theoretical and empirical evidence that invariant representations across domains are achieved progressively by stacking the bases-decomposed branched layer. The proposed approach serves as an efficient way for domain invariant learning.",
        "Abstract": "Domain shifts are frequently encountered in real-world scenarios. In this paper, we consider the problem of domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a regular convolutional layer into a domain-specific basis layer and a domain-shared basis coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific basis to ``absorb\" domain variations, and then output channels are linearly combined using common basis coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples to show the framework's effectiveness in cross-domain performance and domain adaptation. With the proposed architecture, we need only a small set of basis elements to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.",
        "Introduction": "  INTRODUCTION Training supervised deep networks requires large amount of labeled training data; however, well- trained deep networks often degrade dramatically on testing data from a significantly different do- main. In real-world scenarios, such domain shifts are introduced by many factors, such as different illumination, viewing angles, and resolutions. Research topics such as transfer learning and domain adaptation are studied to promote invariant representations across domains with different levels of availabilities of annotated data. Recent efforts on learning cross-domain invariant representations using deep networks generally fall into two categories. The first one is to learn a common network with constraints encouraging invariant feature representations across different domains  Long et al. (2015 ;  2016b );  Tzeng et al. (2014) . The feature invariance is usually measured by feature statistics like Maximum Mean Dis- crepancy (MMD), or feature discriminators using adversarial training  Ganin et al. (2016) . While these methods introduce no additional model parameters, the effectiveness largely depends on the degree of domain shifts. The other direction is to explicitly model domain specific characteris- tics with a multi-stream network structure where different domains are modeled by corresponding sub-networks at the cost of significant extra parameters and computations  Rozantsev et al. (2018b) . Regularizations are imposed among sub-networks to encourage common semantics across domains. In this paper, we model domain shifts through domain-adaptive filter decomposition (DAFD) with layer branching. At a branched layer, we decompose each filter over a small set of domain-specific basis elements to model intrinsic domain characteristics, while enforcing shared cross-domain coef- ficients to align invariant semantics. A regular convolution is now decomposed into two steps. First, a domain-specific basis convolves spatially only each individual input channel for shift \"correction.\" Second, the \"corrected\" output channels are weighted summed using domain-shared basis coeffi- cients (1×1 convolution) to promote common semantics. When domain shifts happen in space, we rigorously prove that such layer-wise \"correction\" by the same spatial transform applied to bases suffices to align the learned features. Comparing to the existing subnetwork-based methods, the proposed method has several appealing properties: First, only a very small amount of additional trainable parameters are introduced to explicitly model each domain, i.e., domain-specific bases. The majority of the parameters in the Under review as a conference paper at ICLR 2020 network remain shared across domains, and learned from abundant training data to effectively avoid overfitting. Furthermore, the decomposed filters reduce the overall computations significantly com- pared to previous works, where computation typically grows linearly with the number of domains. We conduct extensive real-world face recognition, image classification, and segmentation experi- ments, and observe that, with the proposed method, invariant representations across domains are consistently achieved without compromising the performance of individual domain. Our main contributions are summarized as follows: • We propose domain-invariant representation learning through bases decomposition with layer branching, where domain-specific bases are learned to counter domain shifts, and semantic alignments are enforced with cross-domain common basis coefficients. • We both theoretically prove and empirically observe that by stacking the bases-decomposed branched layer, invariant representations across domains are achieved progressively. • The majority of network parameters remain shared across domains, which alleviates the demand for massive annotated data from every domain, and introduces only a small amount of additional computation and parameter overhead. Thus the proposed approach serves as an efficient way for domain invariant learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a RL based method to solve the city metro network expansion problem without expert knowledge. The method formulates the expansion of a metro line as a process of sequential stations selection, a Markov decision process (MDP). Feasibility rules are designed based on the selected station sequence to ensure the reasonable connection patterns of metro line. The reward function takes the final output station sequence as input only, which is friendly to objective changing. The method is tested on real city-scale human mobility information of 24,770,715 mobile phone users obtained from a citywide 3G cellular network in Xi'an, China. The results demonstrate the effectiveness of the method in considering social equity in transportation planning.",
        "Abstract": "This paper presents a method to solve the city metro network expansion problem using reinforcement learning (RL). In this method, we formulate the metro expansion as a process of sequential station selection, and design feasibility rules based on the selected station sequence to ensure the reasonable connection patterns of metro line. Following this formulation, we train an actor critic model to design the next metro line. The actor is a seq2seq network with attention mechanism to generate the parameterized policy which is the probability distribution over feasible stations. The critic is used to estimate the expected reward, which is determined by the output station sequences generated by the actor during training, in order to reduce the training variance. The learning procedure only requires the reward calculation, thus our general method can be extended to multi-factor cases easily. Considering origin-destination (OD) trips and social equity, we expand the current metro network in Xi'an, China, based on the real mobility information of 24,770,715 mobile phone users in the whole city. The results demonstrate the effectiveness of our method. ",
        "Introduction": "  INTRODUCTION City metro network plays an important role in public transportation system. With the development of city, new transportation demands have led to the expansion of metro network. The last few years have witnessed tremendous expansion of metro network ( Sun et al., 2018 ). On the other hand, the expansion of the metro network in turn has a profound impact on the city. The expanded lines may change the mobility trend of city population. Most previous research focuses on the design of metro network from scratch ( Gutiérrez-Jarpa et al., 2018 ;  Laporte & Pascoal, 2015 ). However, in the subsequent construction process, the dynamic of the city has been different from that in the initial stage. The original scheme may not be suitable for the current situation. Therefore, it is more reasonable to gradually design new lines to expand the metro network according to current city dynamic. Usually, transportation planning objectives are mobility-based, such as maximizing OD trips. As the society progressed, sustainability has increasingly become the demand of city development. The sustainability prompts governments to re-recognize the role of the transport system, and thereby influences their transport policy ( Manaugh et al., 2015 ). One conception of sustainability, social equity, which can be measured by the distributable benefit accessibility ( Behbahani et al., 2019 ), has been acknowledged important. There have been several real transportation plans considering social equity ( Arsenio et al., 2016 ). Metro network, an important transportation system, has a great influence on social equity. Therefore, in this work, we consider both OD trips and social equity to expand city metro network. However, the problem becomes difficult when the city becomes large. First, it is difficult to formulate the problem efficiently ( Laporte & Mesa, 2015 ). Existing studies formulate the problem as non- linear integer programming, and call for an exponential number of subtour elimination constraints to ensure the rationality of expanded metro lines ( Gutiérrez-Jarpa et al., 2013 ;  Wei et al., 2019 ), which hinders solving the problem efficiently. Second, the huge solution space makes it difficult to find a good solution effectively. Exact methods are inapplicable for integer programming problems with large solution space ( Farahani et al., 2013 ). One common method in existing studies is to limit the search space. Previous work ( Gutiérrez-Jarpa Under review as a conference paper at ICLR 2020 et al., 2013 ;  Wei et al., 2019 ;  Laporte & Pascoal, 2015 ;  Gutiérrez-Jarpa et al., 2018 ) predefines corridors based on expert knowledge, and only consider to design metro lines in these corridors. Their results depend on expert guidance, and it is possible that the best solution is left out. Therefore, we call for a method which does not require expert knowledge. Carefully handcrafted heuristics, embedded with problem-specific knowledge, are usually efficient for large search space ( Dufourd et al., 1996 ). However, the factors considered in the expansion of metro line vary with different cities and stages. Once the objectives of metro expansion change, heuristic methods need to be revised. Rather than handcrafting different heuristics for different objectives, a general method is necessary. In this paper, we propose a method without expert knowledge to solve the city metro network ex- pansion problem using RL. We formulate the metro line expansion as a process of sequential station selection, an MDP, and design feasibility rules based on the selected station sequence to ensure the reasonable connection patterns of the metro line. This formulation efficiently characterize the expansion of the metro line, without heavy constraints like existing studies ( Wei et al., 2019 ). Following this formulation, we propose an actor critic model ( Konda & Tsitsiklis, 2000 ) to generate the next metro line. The actor is based on the seq2seq model ( Sutskever et al., 2014 ). In the actor, an encoder characterizes the timely metro station information in the expansion process. After encoding, an RNN decoder is used to characterize the sequence information of the selected stations. Moreover, we employ an attention layer to produce the probability distribution over feasible candidate stations. Only requiring the reward calculation, we train the model with the critic reducing training variance, in order to find the high-priority metro line following feasibility rules. The reward function takes the final output station sequence as input only, which is friendly to objective changing. Therefore, our model is general for different objectives. Without expert knowledge, the learning procedure drives the policy to keep track of the better solution during the search and to search for better solutions. Its natural exploration mechanism determines that RL is suitable for large scale solution space. Based on real city-scale human mobility information of 24,770,715 mobile phone users obtained from a citywide 3G cellular network, we expand the current metro network in Xi'an, China. The results demonstrate the effectiveness of our method. Our contributions are as follows: 1. We formulate the expansion of a metro line as a process of sequential stations selection, a Markov decision process (MDP). We design feasibility rules based on the selected station sequence to ensure the reasonable connection patterns of metro line, which is more efficient to formulate the problem than integer programming models. 2. We firstly propose a RL based method to solve the city metro network expansion prob- lem. Without expert knowledge, our general method can be easily extended to the metro expansion considering multi-factors. 3. We incorporate social equity concerns into metro network expansion. Compared with the realistically planned lines, the results show the rationality of considering social equity in transportation planning. 4. We use real city-scale human mobility information to expand a metro network. The exper- imental results demonstrate the effectiveness of our method.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel model, DeepEnFM, which combines both Deep Neural Networks (DNNs) and an encoder enhanced Factorization Machine (FM) to predict the Click Through Rate (CTR). The encoder is endowed with bilinear attention and max-pooling power to learn a context-aware feature embedding. Experimental results on Criteo and Avazu datasets have demonstrated the efficacy of the proposed model, achieving state-of-the-art results. An ablation study has also been conducted to show the contribution of each component and the relations between DNN, encoder, and FM.",
        "Abstract": "Click Through Rate (CTR) prediction is a critical task in industrial applications, especially for online social and commerce applications. It is challenging to find a proper way to automatically discover the effective cross features in CTR tasks. We propose a novel model for CTR tasks, called Deep neural networks with Encoder enhanced Factorization Machine (DeepEnFM). Instead of learning the cross features directly, DeepEnFM adopts the Transformer encoder as a backbone to align the feature embeddings with the clues of other fields. The embeddings generated from encoder are beneficial for the further feature interactions. Particularly, DeepEnFM utilizes a bilinear approach to generate different similarity functions with respect to different field pairs. Furthermore, the max-pooling method makes DeepEnFM feasible to capture both the supplementary and suppressing information among different attention heads. Our model is validated on the Criteo and Avazu datasets, and achieves state-of-art performance.",
        "Introduction": "  INTRODUCTION This paper studies the problem of predicting the Click Through Rate (CTR), which is an essential task in industrial applications, such as online advertising, and e-commerce. To be exact, the adver- tisements of cost-per-click (CPC) advertising system are normally ranked by the eCPM (effective cost per mille), which is computed as the prodcut of bid price and CTR (click-through rate). To pre- dict CTR precisely, feature representation is an important step in extracting the good, interpretable patterns from training data. For example, the co-occurrence of \"Valentine's Day\", \"chocolate\" and \"male\" can be viewed as one meaningful indicator/feature for the recommendation. Such hand- crafted feature type is predominant in CTR prediction ( Lee et al., 2012 ), until the renaissance of Deep Neural Networks (DNNs). Recently, a more effective manner, i.e., representation learning has been investigated in CTR predic- tion with some works ( Guo et al., 2017 ;  Qu et al., 2016 ;  Wang et al., 2017 ;  Lian et al., 2018 ;  Song et al., 2018 ), which implicitly or explicitly learn the embeddings of high-order feature extractions among neurons or input elements by the expressive power of DNNs or FM. Despite their noticeable performance improvement, DNNs and explicit high order feature-based methods ( Wang et al., 2017 ;  Zhang et al., 2016 ;  Guo et al., 2017 ;  Lian et al., 2018 ) seek better feature interactions merely based on the naive feature embeddings. Few efforts have been made in addressing the task of holistically understanding and learning rep- resentations of inputs. This leads to many practical problems, such as \"polysemy\" in the learned feature embeddings existed in previous works. For example, the input feature 'chocolate' is much closer to the 'snack' than 'gift' in normal cases, while we believe 'chocolate' should be better paired with 'gift' if given the occurrence input as \"Valentine's Day\". This is one common polysemy prob- lem in CTR prediction. Towards fully understanding the inputs, we re-introduce to CTR, the idea of Transformer en- coder ( Vaswani et al., 2017 ), which is oriented in Natural Language Processing (NLP). Such an encoder can efficiently accumulate and extract patterns from contextual word embeddings in NLP, and thus potentially would be very useful in holistically representation learning in CTR. Critically, the Transformer encoder has seldom been applied to CTR prediction with the only one exception arxiv paper AutoInt ( Song et al., 2018 ), which, however, simply implements the multi-head self- attention (MHSA) mechanism of encoders, to directly extract high-order feature interactions. We Under review as a conference paper at ICLR 2020 argue that the output of MHSA/encoder should be still considered as first-order embedding influ- enced by the other fields, rather than a high-order interaction feature. To this end, our main idea is to apply the encoder to learn a context-aware feature embedding, which contains the clues from the content of other features. Thus the \"polysemy\" problem can be solved naturally, and the second-order interaction of such features can represent more meaning. Contrast to AutoInt( Song et al., 2018 ), which feeds the output of encoder directly to the prediction layer or a DNN, our work not only improves the encoder to be more suitable for CTR task, but also feeds the encoder output to FM, since both our encoder and FM are based on vector-wise learning mechanism. And we adopt DNN to learn the bit-wise high-order feature interactions in a parallel way, which avoids interweaving the vector-wise and bit-wise interactions in a stacked way. Formally, we propose a novel framework - Deep neural networks with Encoder enhanced Factoriza- tion Machine (DeepEnFM). DeepEnFM focuses on generating better contextual aligned vectors for FM and uses DNN as a bit-wise information supplement. The architecture adopting both Deep and FM part is inspired by DeepFM ( Guo et al., 2017 ). The encoder is endowed with bilinear attention and max-pooling power. First, we observed that unlike the random order of words in a sentence, the features in a transaction are in a fixed order of fields. For example, the fields of features are arranged in an order of {Gender, Age, Price ...}. When the features are embedded in dense vec- tors, the first and second vectors in a transaction always represent the field \"Gender\" and \"Age\". To make use of this advantage, we add a bilinear mechanism to the Transformer encoder. We use bilinear functions to replace the simple dot product in attention. In this way, feature similarity of different field pairs is modeled with different functions. The embedding size in CTR tasks is usu- ally around 10, which allows the application of bilinear functions without unbearable computing complexity. Second, the original multi-head outputs are merged by concatenation, which considers the outputs are complementary to each other. We argue that there are also suppressing information between different heads. We apply a max-pooling merge mechanism to extract both complementary and suppressing information from the multi-head outputs. Experimental results on Criteo and Avazu datasets have demonstrated the efficacy of our proposed model. To summarize, our contributions are three-fold. (1) We propose a novel model DeepEnFM, which combines both DNN and encoder enhanced FM. (2) The state-of-the-art results on Criteo and Avazu datasets have shown the efficacy of our proposed model. (3) Extensive ablation study has shown the contribution of each component, and revealed the relations between DNN, encoder, and FM by developing different architectures.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the robustness of the A-GEM model to adversarial examples in the lifelong learning setting. We propose a novel attack method to generate adversarial examples for A-GEM and evaluate the robustness of A-GEM on several benchmarks. Our results show that A-GEM is vulnerable to adversarial examples and the attack can reduce the performance of A-GEM significantly.\n\nAbstract: This paper investigates the robustness of the A-GEM model to adversarial examples in the lifelong learning setting. We propose a novel attack method to generate adversarial examples for A-GEM and evaluate the robustness of A-GEM on several benchmarks. Our results show that A-GEM is vulnerable to adversarial examples and the attack can reduce the performance of A-GEM significantly.",
        "Abstract": "Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.",
        "Introduction": "  INTRODUCTION Lifelong learning ( French, 1999 ;  Thrun & Mitchell, 1995 ;  Kirkpatrick et al., 2017 ) aims at improv- ing the continual learning ability of neural networks. Standard supervised learning methods suffer from the problem of catastrophic forgetting, in which case the models gradually forget previously learned knowledge while learning on a sequence of new tasks. In lifelong learning, neural networks are equipped with the capability to learn new tasks while maintaining the performance on the tasks trained previously. Lifelong learning models with continual learning ability can be deployed in complex environments with the aim to process a continuous stream of information. Several methodologies are proposed recently to address the catastrophic forgetting problem. In  Kirkpatrick et al. (2017) , the authors adopt Fisher information matrix to prevent important weights for old tasks from drastic changes while the model is trained on a new task. While in  Rusu et al. (2016) , a neural network that has lateral connections with old tasks is trained each time for the new task. Recently, lifelong learning methods based on episodic memory ( Lopez-Paz et al., 2017 ;  Chaudhry et al., 2018b ;  d'Autume et al., 2019 ) such as A-GEM ( Chaudhry et al., 2018b ) are shown to be able to achieve the state-of-the-art performance across several benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from old tasks. While the model is trained on a new task, a reference gradient is computed on a batch of the samples from the episodic memory to guide the current update direction. If the angle between the reference gradient and the current gradient computed on the new task is obtuse, the current gradient is projected to be perpendicular to the reference gradient. The strong continual learning ability of A-GEM relies on the episodic memory which can give a hint on the performance of the current model on old tasks. It has been known that in the standard super- Under review as a conference paper at ICLR 2020 vised learning setting, deep neural networks can be easily fooled by adversarial examples ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ). A natural question then arises:",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel algorithm, A MCTS, which combines A and MCTS to efficiently learn optimal policies in decision trees using value and policy networks. We provide theoretical guarantees for the sample complexity of the algorithm and validate the theoretical analysis with experiments. To our knowledge, this is the first work to study tree search for optimal actions in the presence of pre-trained value and policy networks.",
        "Abstract": "Combined with policy and value neural networks, Monte Carlos Tree Search (MCTS) is a critical component of the recent success of AI agents in learning to play board games like Chess and Go (Silver et al., 2017). However, the theoretical foundations of MCTS with policy and value networks remains open. Inspired by MCTS, we propose A⋆MCTS, a novel search algorithm that uses both the policy and value predictors to guide search and enjoys theoretical guarantees. Specifically, assuming that value and policy networks give reasonably accurate signals of the values of each state and action, the sample complexity (number of calls to the value network) to estimate the value of the current state, as well as the optimal one-step action to take from the current state, can be bounded. We apply our theoretical framework to different models for the noise distribution of the policy and value network as well as the distribution of rewards, and show that for these general models, the sample complexity is polynomial in D, where D is the depth of the search tree. Empirically, our method outperforms MCTS in these models.",
        "Introduction": "  INTRODUCTION Monte Carlo Tree Search (MCTS) is an online heuristic search algorithm commonly used to find op- timal policies for problems in reinforcement learning. It is a vital component of recent breakthroughs for AI in game play, including AlphaGo and AlphaZero (Silver et al., 2016; 2017). An interesting difference between the MCTS used in AlphaGo/AlphaZero and traditional MCTS is the adoption of neural networks to predict the value of the current state (value network), and to prioritize the next action to search (policy network). The value network replaces random rollouts used to evaluate a non-terminal state, which saves both online computation time and substantially reduces the variance of the estimate. The policy network (coupled with PUCT (Rosin, 2011)) prioritizes promising actions for more fruitful exploration. Both networks are trained on pre-existing datasets (either from human replays or self-play) to incorporate prior knowledge. This strategy has been applied in other domains including Neural Architecture Search (Wang et al., 2018; Wistuba, 2017; Negrinho & Gordon, 2017; Wang et al., 2019b). Despite the empirical success and recent popularity, theoretical foundations are lacking. (Kocsis & Szepesvári, 2006; Coquelin & Munos, 2007) analyze MCTS with rollouts using a multi-armed bandit framework and gives asymptotic results without finite sample complexity bounds. Powering MCTS with value/policy neural networks instead of rollouts is even more poorly understood. To better understand how to efficiently learn optimal policies in decision trees using value and policy networks, we propose a novel algorithm, A MCTS, which is a combination of A (Delling et al., 2009; Kanoulas et al., 2006) and MCTS. Like A , it uses a priority queue to store all leaves currently being explored and picks the most optimistic one to expand, according to an upper confidence bound heuristic function. Like MCTS, it uses policy and value networks to prioritize the next state to be explored. To facilitate the analysis, the policy and value networks are treated as black-box functions and a sta- tistical model is built the accuracy of the predictions. In particular, we assume the standard deviation of the noisy estimates of intermediate state values decays (either polynomially or exponentially) as a function of the depth of the tree, so that the values of near-terminal states are estimated to a greater degree of accuracy. With this statistical model, we provide theoretical guarantees for the sample Under review as a conference paper at ICLR 2020 complexity (expected number of state expansions / rollouts) to determine optimal actions. We apply our theoretical framework to simple models for rewards and show that our algorithm enjoys a sample complexity that is polynomial in depth D. Our experiments validate the theoretical analysis and demonstrate the effectiveness of A MCTS over benchmark MCTS algorithms with value and policy networks. To our knowledge, our work is the first that studies tree search for optimal actions in the presence of pre-trained value and policy networks, and we hope that it inspires further progress on this important problem.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new Hybrid-RL method, Risk Averse Value Expansion (RAVE), which is an extension of the model-based value expansion and stochastic ensemble value expansion. RAVE is designed to achieve both sample efficiency and robustness in stochastic and risky environments. RAVE utilizes a probabilistic ensemble environment model to capture both epistemic and aleatoric uncertainty, and adopts a dynamic confidence lower bound of the target value function to make the policy more risk-sensitive. Results show that RAVE not only yields SOTA expected performance, but also facilitates the robustness of the policy.",
        "Abstract": "Model-based Reinforcement Learning(RL) has shown great advantage in sample-efficiency, but suffers from poor asymptotic performance and high inference cost. A promising direction is to combine model-based reinforcement learning with model-free reinforcement learning, such as model-based value expansion(MVE). However, the previous methods do not take into account the stochastic character of the environment, thus still suffers from higher function approximation errors. As a result, they tend to fall behind the best model-free algorithms in some challenging scenarios. We propose a novel Hybrid-RL method, which is developed from MVE, namely the Risk Averse Value Expansion(RAVE). In the proposed method, we use an ensemble of probabilistic models for environment modeling to generate imaginative rollouts, based on which we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on different environments including MuJoCo and robo-school show that RAVE yields state-of-the-art performance. Also we found that it greatly prevented some catastrophic consequences such as falling down and thus reduced the variance of the rewards.",
        "Introduction": "  INTRODUCTION In contrast to the tremendous progress made by model-free reinforcement learning algorithms in the domain of games(Mnih et al., 2015; Silver et al., 2017; Vinyals et al., 2019), poor sample efficiency has risen up as a great challenge to RL, especially when interacting with the real world. Toward this challenge, a promising direction is to integrate the dynamics model to enhance the sample efficiency of the learning process(Sutton, 1991; Calandra et al., 2016; Kalweit & Boedecker, 2017; Oh et al., 2017; Racanière et al., 2017). However, classic model-based reinforcement learning(MBRL) meth- ods tend to lag behind the model-free methods(MFRL) asymptotically, especially in cases of noisy environments and long trajectories. The hybrid combination of MFRL and MBRL(Hybrid-RL for short) has attracted much attention due to this reason. A lot of efforts has been devoted to this field, including the dyna algorithm(Kurutach et al., 2018), model-based value expansion(Feinberg et al., 2018), I2A(Weber et al., 2017), etc. The robustness of the learned policy is another concern in RL. For stochastic environments, pol- icy can be vulnerable to tiny disturbance and occasionally drop into catastrophic consequences. In MFRL, off-policy RL(such as DQN, DDPG) typically suffers from such problems, which in the end leads to instability in the performance including sudden drop in the rewards. To solve such problem, risk sensitive MFRL not only maximize the expected return, but also try to reduce those catastrophic outcomes(Garcıa & Fernández, 2015; Dabney et al., 2018a; Pan et al., 2019). For MBRL and Hybrid-RL, without modeling the uncertainty in the environment(especially for contin- uous states and actions), it often leads to higher function approximation errors and poorer perfor- mances. It is proposed that complete modeling of uncertainty in transition can obviously improve the performance(Chua et al., 2018), however, reducing risks in MBRL and Hybrid-RL has not been sufficiently studied yet. In order to achieve both sample efficiency and robustness at the same time, we propose a new Hybrid-RL method more capable of solving stochastic and risky environments. The proposed method, namely Risk Averse Value Expansion(RAVE), is an extension of the model-based value ex- pansion(MVE)(Feinberg et al., 2018) and stochastic ensemble value expansion(STEVE)(Buckman Under review as a conference paper at ICLR 2020 et al., 2018). We systematically analyse the approximation errors of different methods in stochastic environments. We borrow ideas from the uncertainty modeling( Chua et al. (2018)) and risk averse reinforcement learning. The probabilistic ensemble environment model is used, which captures not only the variance in estimation(also called epistemic uncertainty), but also stochastic transition na- ture of the environment(also called aleatoric uncertainty). Utilizing the ensemble of estimations, we further adopt a dynamic confidence lower bound of the target value function to make the policy more risk-sensitive. We compare RAVE with prior MFRL and Hybrid-RL baselines, showing that RAVE not only yields SOTA expected performance, but also facilitates the robustness of the policy.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new attack mechanism, adversarial imitation attack, which is a two-player game between a generative model G and an imitation model D. The generative model G is designed to produce examples to make the predicted label of the attacked model T and D different, while the imitation model D fights for outputting the same label with T. Compared with current substitute attacks, adversarial imitation attack requires less training data of attacked models, but achieves an attack success rate close to the white-box attacks. The proposed attack mechanism requires the same information of attacked models with decision attacks on the training stage, but is query-independent on the testing stage.",
        "Abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models T. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the T. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the T by a two-player game like the generative adversarial networks (GANs). The objective of the generative model G is to generate examples which lead D returning different outputs with T. The objective of the discriminative model D is to output the same labels with T under the same inputs. Then, the adversarial examples generated by D are utilized to fool the T. Compared with the current substitute attacks, imitation attack can use less training data to produce a replica of T and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query. ",
        "Introduction": "  INTRODUCTION Deep neural networks are often vulnerable to imperceptible perturbations of their inputs, causing incorrect predictions ( Szegedy et al., 2014 ). Studies on adversarial examples developed attacks and defenses to assess and increase the robustness of models, respectively. Adversarial attacks include white-box attacks, where the attack method has full access to models, and black-box attacks, where the attacks do not need knowledge of models structures and weights. White-box attacks need training data and the gradient information of models, such as FGSM (Fast Gradient Sign Method) ( Goodfellow et al., 2015 ), BIM (Basic Iterative Method) ( Kurakin et al., 2017a ) and JSMA (Jacobian-based Saliency Map Attack) ( Papernot et al., 2016b ). However, the gradient information of attacked models is hard to access, the white-box attack is not practical in real-world tasks. Literature shows adversarial examples have transferability property and they can affect different models, even the models have different architectures ( Szegedy et al., 2014 ;  Papernot et al., 2016a ;  Liu et al., 2017 ). Such a phenomenon is closely related to linearity and over-fitting of models ( Szegedy et al., 2014 ;  Hendrycks & Gimpel, 2017 ;  Goodfellow et al., 2015 ;  Tramèr et al., 2018 ). Therefore, substitute attacks are proposed to attack models without the gradient information. Substitute black-box attacks utilize pre-trained models to generate adversarial examples and apply these examples to attacked models. Their attack success rates rely on the transferability of adver- sarial examples and are often lower than that of white-box attacks. Black-box score-based attacks ( Chen et al., 2017 ;  Ilyas et al., 2018a ; b ) do not need pre-trained models, they access the output probabilities of the attacked model to generate adversarial examples iteratively. Black-box decision- based attacks ( Brendel et al., 2017 ;  Cheng et al., 2018 ;  Chen et al., 2019 ) require less information than the score-based attacks. They utilize hard labels of the attacked model to generate adversarial examples. Adversarial attacks need knowledge of models. However, a practical attack method should require as little as possible knowledge of attacked models, which include training data and procedure, models weights and architectures, output probabilities and hard labels ( Athalye et al., 2018 ). The disadvan- tage of current substitute black-box attacks is that they need pre-trained substitute models trained by Under review as a conference paper at ICLR 2020 the same dataset with attacked model T ( Hendrycks & Gimpel, 2017 ;  Goodfellow et al., 2015 ;  Ku- rakin et al., 2017a ) or a number of images to imitate the outputs of T to produce substitute networks ( Papernot et al., 2017 ). Actually, the prerequisites of these attacks are hard to obtain in real-world tasks. The substitute models trained by limited images hardly generate adversarial examples with well transferability. The disadvantage of current decision-based and score-based black-box attacks is that every adversarial example is synthesized by numerous queries. Hence, developing a practical attack mechanism is necessary. In this paper, we propose an adver- sarial imitation training, which is a special two-player game. The game has a generative model G and a imitation model D. The G is designed to produce examples to make the predicted label of the attacked model T and D different, while the imitation model D fights for outputting the same label with T . The proposed imitation training needs much less training data than the T and does not need the labels of these data, and the data do not need to coincide with the training data. Then, the adversarial examples generated by D are utilized to fool the T like substitute attacks. We call this new attack mechanism as adversarial imitation attack. Compared with current substitute attacks, our adversarial imitation attack requires less training data. Score-based and decision-based attacks need a lot of queries to generate each adversarial attack. The similarity between the proposed method and current score-based and decision-based attacks is that adversarial imitation attack also needs to obtain a lot of queries in the training stage. The difference between these two kinds of attack is our method do not need any additional queries in the test stage like other substitute attacks. Experi- ments show that our proposed method achieves state-of-the-art performance compared with current substitute attacks and decision-based attack. We summarize our main contributions as follows: • The proposed new attack mechanism needs less training data of attacked models than cur- rent substitute attacks, but achieves an attack success rate close to the white-box attacks. • The proposed new attack mechanism requires the same information of attacked models with decision attacks on the training stage, but is query-independent on the testing stage.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a theoretical analysis of Deep Q-Network (DQN), a deep reinforcement learning algorithm, which is an extension of the classical Q-learning algorithm. The paper discusses the algorithmic and statistical properties of the classical Q-learning algorithm and the challenges of theoretical analysis of DQN due to its differences in two aspects.",
        "Abstract": "Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on a slight simplification of DQN that fully captures its key features. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by DQN. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players, which is deferred to the appendix due to space limitations.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) attacks the multi-stage decision-making problems by interacting with the environment and learning from the experiences. With the breakthrough in deep learning, deep reinforcement learning (DRL) demonstrates tremendous success in solving highly challenging problems, such as the game of Go ( Silver et al., 2016 ;  2017 ), robotics (Kober & Peters, 2012), and dialogue systems ( Chen et al., 2017 ). In DRL, the value or policy functions are often represented as deep neural networks and the related deep learning techniques can be readily applied. For example, deep Q-network (DQN) ( Mnih et al., 2015 ), asynchronous advantage actor-critic (A3C) and ( Mnih et al., 2016 ) demonstrate superhuman performance in various applications and become standard algorithms for artificial intelligence. Despite its great empirical success, there exists a gap between the theory and practice of DRL. In particular, most existing theoretical work on reinforcement learning focuses on the tabular case where the state and action spaces are finite, or the case where the value function is linear. Under these restrictive settings, the algorithmic and statistical perspectives of reinforcement learning are well-understood via the tools developed for convex optimization and linear regression. However, in presence of nonlinear function approximators such as deep neural network, the theoretical analysis of reinforcement learning becomes intractable as it involves solving a highly nonconvex statistical optimization problem. To bridge such a gap in DRL, we make the first attempt to theoretically understand DQN, which can be cast as an extension of the classical Q-learning algorithm (Watkins & Dayan, 1992) that uses deep neural network to approximate the action-value function. Although the algorithmic and statistical properties of the classical Q-learning algorithm are well-studied, theoretical analysis of DQN is highly challenging due to its differences in the following two aspects.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the performance of Sequential Neural Processes (SNP) under the under-fitting problem. We propose a new model, Attentive Sequential Neural Processes (ASNP), which augments attention to SNP and introduces the concept of imaginary context. Experiments on synthetic and real-world datasets demonstrate that ASNP significantly improves the standard SNP under the under-fitting settings.",
        "Abstract": "Sequential Neural Processes (SNP) is a new class of models that can meta-learn a temporal stochastic process of stochastic processes by modeling temporal transition between Neural Processes. As Neural Processes (NP) suffers from underfitting, SNP is also prone to the same problem, even more severely due to its temporal context compression. Applying attention which resolves the problem of NP, however, is a challenge in SNP, because it cannot store the past contexts over which it is supposed to apply attention. In this paper, we propose the Attentive Sequential Neural Processes (ASNP) that resolve the underfitting in SNP by introducing a novel imaginary context as a latent variable and by applying attention over the imaginary context. We evaluate our model on 1D Gaussian Process regression and 2D moving MNIST/CelebA regression. We apply ASNP to implement Attentive Temporal GQN and evaluate on the moving-CelebA task.",
        "Introduction": "  INTRODUCTION Neural Processes (NP) ( Garnelo et al., 2018b ) combine the strengths of neural networks and Gaus- sian processes (GP) such that, like Gaussian processes, it is flexible in learning a new stochastic process at test time and also provides fast O(1) prediction speed like neural networks. Learning from small datasets of multiple tasks (i.e., multiple stochastic processes), NP can be seen as a prob- abilistic latent variable framework for meta-learning. Sequential Neural Processes (SNP) ( Singh et al., 2019 ) extend the power of NP to a sequence of stochastic processes thus introducing a new class of sequential latent generative models. It targets a large class of problems where the sequence of stochastic processes is governed by underlying transition dynamics and where learning to transfer information between stochastic processes is useful. Despite these strengths, the remained question is whether NP and SNP can scale to more complex stochastic processes when the individual observations about the true process are highly partial, which prevails in many situations. For example, an agent in a large complex landscape can only obtain very limited information from its immediate surroundings and it is desirable that it still maintains an accurate global scene representation that could also be changing with time. Studying NP from this perspective,  Kim et al. (2019)  demonstrates that the under-fitting problem significantly deteriorates the performance of NP. To address this, ANP ( Kim et al., 2019 ) and the method in  Rosenbaum et al. (2018)  augment attention to NP and GQN ( Eslami et al., 2018 ), respectively. Given the severe performance degradation of NP due to the under-fitting, an important key question is whether SNP would also suffer from the under-fitting problem or not, and its follow-up questions such as if so, how severely and in which settings would it happen? and how can we resolve the problem-would the same attention mechanism that worked for NP work for SNP as well? In this paper, our goal is to answer these questions. We achieve this by not only showing that our proposed attention for SNP significantly improves the standard SNP under the under-fitting settings (see  Fig. 1 ), but also by making a stronger claim i.e., in a sequential setting like ours, it is not optimal to per- form attention on a memory buffer that simply stores all the observed contexts. Instead, through our experiments, we claim that attention should be performed on a memory that is also updated (learned) at every time-step. Consequently, a) this memory would learn to store an optimized representation of the past geared towards prediction and b) it would require fewer storage locations as it does not need to naively store every observed context points. To implement such a memory, we introduce the concept of imaginary context that augments the set of real context points observed at any time-step. The imaginary context contains a fixed number of context points and is modeled as a random variable sampled at every time-step. Given a query, we then apply attention on the union of the real and the imaginary contexts to generate predictions. We call the proposed model as Attentive Sequential Neural Processes (ASNP). In experiments on Under review as a conference paper at ICLR 2020",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the generalization of convolution to non-Euclidean data, such as point clouds, in order to enable the design of convolutional neural networks. It reviews existing definitions of convolutions on manifolds, graphs, and point clouds, and proposes a narrow-band parallel transport convolution. It also discusses the need to properly define pooling to enable networks to extract global features and save memory during training.",
        "Abstract": "Convolution plays a crucial role in various applications in signal and image processing, analysis and recognition. It is also the main building block of convolution neural networks (CNNs). Designing appropriate convolution neural networks on manifold-structured point clouds can inherit and empower recent advances of CNNs to analyzing and processing point cloud data. However, one of the major challenges is to define a proper way to \"sweep\" filters through the point cloud as a natural generalization of the planar convolution and to reflect the point cloud's geometry at the same time. In this paper, we consider generalizing convolution by adapting parallel transport on the point cloud. Inspired by a triangulated surface based method \\cite{DBLP:journals/corr/abs-1805-07857}, we propose the Narrow-Band Parallel Transport Convolution (NPTC) using a specifically defined connection on a voxelized narrow-band approximation of point cloud data. With that, we further propose a deep convolutional neural network based on NPTC (called NPTC-net) for point cloud classification and segmentation. Comprehensive experiments show that the proposed NPTC-net achieves similar or better results than current state-of-the-art methods on point clouds classification and segmentation.",
        "Introduction": "  INTRODUCTION Convolution is one of the most widely used operators in applied mathematics, computer science and engineering. It is also the most important building block of Convolutional Neural Netowrks (CNNs) which are the main driven force in the recent success of deep learning  LeCun et al. (2015) ;  Goodfellow et al. (2016) . In the Euclidean space R n , the convolution of function f with a kernel (or filter) k is defined as This operation can be easily calculated in Euclidean spaces due to the shift-invariance of the space so that the translates of the filter k, i.e. k(x−y) is naturally defined. With the rapid development of data science, more and more non-Euclidean data emerged in various fields including network data from social science, 3D shapes from medical images and computer graphics, data from recommending systems, etc. Therefore, geometric deep learning, i.e. deep learning of non-Euclidean data, is now a rapidly growing branch of deep learning  Bronstein et al. (2017) . In this paper, we will discuss how we can generalize the definition of convolution to (manifold-structured) point clouds in a way that it inherits desirable properties of the planar convolution, thus it enables to design convolutional neural networks on point clouds. One of the main challenges of defining convolution on manifolds and point clouds (a discrete form of manifolds) is to define translation x − y on the non-Euclidean domain. Other than convolutions, we also need to properly define pooling to enable networks to extract global features and to save memory during training. Multiple types of generalization of convolutions on manifolds, graphs and point clouds have been proposed in recent years. We shall recall some of them and discuss the re- lation between existing definitions of convolutions and the proposed narrow-band parallel transport convolution.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel off-policy Deep Reinforcement Learning (DRL) algorithm, Streamlined Off Policy (SOP), which is designed to improve sample efficiency and robustness performance on Mujoco benchmark environments. SOP does not employ the entropy term of maximum entropy RL algorithms, but instead uses a simple normalization scheme to address the bounded nature of the action spaces. Additionally, SOP is combined with a simple non-uniform sampling method, Emphasizing Recent Experience (ERE), to achieve state-of-the art performance on the Mujoco benchmarks. The paper also investigates the primary contribution of the entropy term of maximum entropy RL algorithms and provides anonymized code for reproducibility.",
        "Abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.",
        "Introduction": "  INTRODUCTION Off-policy Deep Reinforcement Learning (RL) algorithms aim to improve sample efficiency by reusing past experience. Recently a number of new off-policy Deep Reinforcement Learning algo- rithms have been proposed for control tasks with continuous state and action spaces, including Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG (TD3) ( Lillicrap et al., 2015 ;  Fu- jimoto et al., 2018 ). TD3, which introduced clipped double-Q learning, delayed policy updates and target policy smoothing, has been shown to be significantly more sample efficient than popular on-policy methods for a wide range of Mujoco benchmarks. The field of Deep Reinforcement Learning (DRL) has also recently seen a surge in the popularity of maximum entropy RL algorithms. Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In particular, Soft Actor Critic (SAC), which combines off-policy learning with maximum-entropy RL, not only has many attractive theoretical properties, but can also give superior performance on a wide-range of Mujoco environments, including on the high-dimensional environment Humanoid for which both DDPG and TD3 perform poorly ( Haarnoja et al., 2018a ;b;  Langlois et al., 2019 ). SAC has a similar structure to TD3, but also employs maximum entropy reinforcement learning. In this paper, we first seek to understand the primary contribution of the entropy term to the per- formance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that when using the standard objective without entropy along with standard additive noise exploration, there is often insufficient exploration due to the bounded nature of the action spaces. Specifically, the outputs of the policy network are often way outside the bounds of the action space, so that they need to be squashed to fit within the action space. The squashing results in actions persistently taking on their maximal values, so that there is insufficient exploration. In contrast, the entropy term in the SAC objective forces the outputs to have sensible values, so that even with squashing, exploration is maintained. We conclude that the entropy term in the objective for Soft Actor Critic principally addresses the bounded nature of the action spaces in the Mujoco environments. With this insight, we propose Streamlined Off Policy (SOP), a streamlined algorithm using the standard objective without the entropy term. SOP employs a simple normalization scheme to ad- dress the bounded nature of the action spaces, allowing satisfactory exploration throughout training. We also consider replacing the aforementioned normalization scheme with inverting gradients (IG) Under review as a conference paper at ICLR 2020  Hausknecht & Stone (2015) . Our results show that SOP and IG match the sample-efficiency and ro- bustness performance of SAC, including on the more challenging Ant and Humanoid environments. This demonstrates a need to revisit the importance of entropy maximization in DRL. Keeping with the theme of simplicity with the goal of meeting Occam's principle, we also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. In vanilla SOP (as well as in DDPG, TD3, and SAC), samples from the replay buffer are chosen uniformly at random during training. Our method, called Emphasizing Recent Experience (ERE), samples more aggressively recent experience while not neglecting past experience. Unlike Priority Experience Replay (PER) ( Schaul et al., 2015 ), a popular non-uniform sampling scheme for the Atari environments, ERE is only a few lines of code and does not rely on any sophisticated data structures. We show that SOP combined with ERE out-performs SAC and provides state of the art performance. For example, for Ant and Humanoid, it improves over SAC by 21% and 24%, respectively, with one million samples. Furthermore, we also investigate combining SOP with PER, and show SOP+ERE also out-performs the more complicated SOP+PER scheme. The contributions of this paper are thus threefold. First, we uncover the primary contribution of the entropy term of maximum entropy RL algorithms when the environments have bounded action spaces. Second, we propose a streamlined algorithm which do not employ entropy maximization but nevertheless matches the sampling efficiency and robustness performance of SAC for the Mu- joco benchmarks. And third, we combine our streamlined algorithms with a simple non-uniform sampling scheme to achieve state-of-the art performance for the Mujoco benchmarks. We provide anonymized code for reproducibility 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel effective solution to Deep Metric Learning (DML) and brings new insights from the perspective of learning theory. The proposed solution casts the problem of DML into a simple pairwise classification problem and focuses on addressing the sheer imbalance between positive pairs and negative pairs. A flexible distributionally robust optimization (DRO) framework is employed for defining a robust loss over pairs within a mini-batch. The DRO framework allows for the connection to advanced learning theories, the unification of pair sampling and loss-based methods, and the induction of simple and effective methods for DML. Experiments show that the proposed variants of DRO framework outperform state-of-the-art methods on several benchmark datasets.",
        "Abstract": "Deep metric learning (DML) has received much attention in deep learning due to its wide applications in computer vision. Previous studies have focused on designing complicated losses and hard example mining methods, which are mostly heuristic and lack of theoretical understanding. In this paper, we cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. It identifies the most critical issue in this problem---imbalanced data pairs. To tackle this issue, we propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the  {\\it uncertainty decision set} of the dual variable allows us to recover state-of-the-art complicated losses and also to induce novel variants.  Empirical studies on several benchmark data sets demonstrate that our simple and effective method outperforms the state-of-the-art results.",
        "Introduction": "  INTRODUCTION Metric Learning aims to learn a metric to measure the distance between examples that captures certain notion of human-defined similarity between examples. Deep metric learning (DML) has emerged as an effective approach for learning a metric by training a deep neural network. Simply speaking, a deep neural network can induce new feature embedding of examples and it is trained in such a way that the Euclidean distance between the induced feature embeddings of two similar examples shall be small and that between the induced feature embeddings of two dissimilar pairs shall be large. DML has been widely used in many tasks such as face recognition ( Fan et al. (2017) ), image retrieval (Chen & Deng (2019)), and classification ( Qian et al. (2015) ;  Li et al. (2019) ). However, unlike training a deep neural network by minimizing the classification error, training a deep neural network for metric learning is notoriously more difficult ( Qian et al. (2018) ;  Wang et al. (2017) ). Many studies have attempted to address this challenge by focusing on several issues. The first issue is how to define a loss function over pairs of examples. A variety of loss functions have been proposed such as contrastive loss ( Hadsell et al. (2006) ), binomial deviance loss (Yi et al.), margin loss ( Wu et al. (2017) ), lifted-structure (LS) loss ( Oh Song et al. (2016) ), N-pair loss ( Sohn (2016) ), triplet loss ( Schroff et al. (2015) ), multi-similarity (MS) loss ( Wang et al. (2019) . The major difference between these pair-based losses lies at how the pairs interact with each other in a mini-batch. In simple pairwise loss such as binomial deviance loss, contrastive loss, and margin loss, pairs are regarded as independent of each other. In triplet loss, a positive pair only interacts with one negative pair. In N-pair loss, a positive pair interacts with all negative pairs. In LS loss and MS loss, a positive pair interacts with all positive pairs and all negative pairs. The trend is that the loss functions become increasingly complicated but are difficult to understand. In parallel with the loss function, how to select informative pairs to construct the loss function has also received great attention. Traditional approaches that construct pairs or triplets over all examples before training suffer from prohibitive O(N 2 ) or O(N 3 ) sample complexity, where N is the total number of examples. To tackle this issue, constructing pairs within a mini-batch is widely used in practice. Although it helps mitigate the computational and storage burden, slow convergence and model degeneration with inferior performance still commonly exist when using all pairs in a mini- batch to update the model. To combat this issue, various pair mining methods have been proposed to complement the design of loss function, such as hard (semi-hard) mining for triplet loss (Schroff Under review as a conference paper at  ICLR 2020 et al. (2015) ), distance weighted sampling (DWS) for margin loss ( Wu et al. (2017) ), MS sampling for MS loss ( Wang et al. (2019) ). These sampling methods usually keep all positive (similar) pairs and select roughly the same order of negative (dissimilar) pairs according to some criterion. Regardless of these great efforts, existing studies either fail to explain the most fundamental problem in DML or fail to propose most effective approach towards addressing the fundamental challenge. It is evident that the loss functions become more and more complicated. But it is still unclear why these complicated losses are effective and how does the pair mining methods affect the overall loss within a mini-batch. In this paper, we propose a novel effective solution to DML and bring new insights from the perspective of learning theory that can guide the discovery of new methods. Our philosophy is simple: casting the problem of DML into a simple pairwise classification problem and focusing on addressing the most critical issue, i.e., the sheer imbalance between positive pairs and negative pairs. To this end, we employ simple pairwise loss functions (e.g., margin loss, binomial deviance loss) and propose a flexible distributionally robust optimization (DRO) framework for defining a robust loss over pairs within a mini-batch. The idea of DRO is to assign different weights to different pairs that are optimized by maximizing the weighted loss over an uncertainty set for the distributional variable. The model is updated by stochastic gradient descent with stochastic gradients computed based on the sampled pairs according to the found optimal distributional variable. The DRO framework allows us to (i) connect to advanced learning theories that already exhibit their power for imbalanced data, hence providing theoretical explanation for the proposed framework; (ii) to unify pair sampling and loss-based methods to provide a unified perspective for existing solutions; (iii) to induce simple and effective methods for DML, leading to state-of-the-art performance on several benchmark datasets. The contributions of our work are summarized as follows: • We propose a general solution framework for DML, i.e., by defining a robust overall loss based on the DRO formulation and updating the model based on pairs sampled according to the optimized sampling probabilities. We provide theoretical justification of the proposed framework from the perspective of advanced learning theories. • We show that the general DRO framework can recover existing methods based on complicated pair-based losses: LS loss and MS loss by specifying different uncertainty sets for the distribu- tional variable in DRO. It verifies that our method is general and brings a unified perspective regarding pair sampling and complicated loss over all pairs within a batch. • We also propose simple solutions under the general DRO framework for tackling DML. Exper- imental results show that our proposed variants of DRO framework outperform state-of-the-art methods on several benchmark datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the potential applications of Graph Neural Networks (GNNs) to 2-Quantified Boolean Formula (2QBF) problems. We design and evaluate GNN architectures for embedding 2QBF formulas, and demonstrate the potential of GNN-based heuristics for selecting candidates and counter-examples in the CEGAR-based solver framework. Our results pinpoint the limitation of GNN in learning logical decision procedures that need reasoning about a space of Boolean assignments, and uncover interesting challenges for GNN to generalize across graph structures.",
        "Abstract": "It is valuable yet remains challenging to apply neural networks in logical reasoning tasks. Despite some successes witnessed in learning SAT (Boolean Satisfiability) solvers for propositional logic via Graph Neural Networks (GNN),  there haven't been any successes in learning solvers for more complex predicate logic. In this paper, we target the QBF (Quantified Boolean Formula) satisfiability problem, the complexity of which is in-between propositional logic and predicate logic, and investigate the feasibility of learning GNN-based solvers and GNN-based heuristics for the cases with a universal-existential quantifier alternation (so-called 2QBF problems).\n\nWe conjecture, with empirical support, that GNNs have certain limitations in learning 2QBF solvers, primarily due to the inability to reason about a set of assignments. Then we show the potential of GNN-based heuristics in CEGAR-based solvers and explore the interesting challenges to generalize them to larger problem instances. In summary, this paper provides a comprehensive surveying view of applying GNN-based embeddings to 2QBF problems and aims to offer insights in applying machine learning tools to more complicated symbolic reasoning problems.\n",
        "Introduction": "  INTRODUCTION As deep learning makes astonishing achievements in the domain of image ( He et al., 2016 ) and audio ( Hannun et al., 2014 ) processing, natural languages ( Vaswani et al., 2017 ), and discrete heuristics decisions in games ( Silver et al., 2017 ), there is a profound interest in applying the relevant techniques in the field of logical reasoning. Logical reasoning problems span from simple propositional logic to complex predicate logic and high-order logic, with known theoretical complexities from NP-complete ( Cook, 1971 ) to semi-decidable and undecidable ( Church, 1936 ). Testing the ability and limitation of machine learning tools on logical reasoning problems leads to a fundamental understanding of the boundary of learnability and robust AI, and addresses the interesting questions in decision procedures in logic, symbolic reasoning, and program analysis and verification as defined in the programming language community. There have been some successes in learning propositional logic reasoning ( Selsam et al., 2019 ;  Amizadeh et al., 2019 ), which focus on SAT (Boolean Satisfiability) problems as defined below. A propositional logic formula is an expression composed of Boolean constants ( : true, ⊥ : false) , Boolean variables (x i ), and propositional connectives such as ∧, ∨, ¬ (for example (x 1 ∨ ¬x 2 ) ∧ (¬x 1 ∨ x 2 )). The SAT problem asks if a given formula can be satisfied (evaluated to ) by assigning proper Boolean values to the variables. A crucial feature of the logical reasoning domain (as is visible in the SAT problem) is that the inputs are often structural, where logical connections between entities (variables in SAT problems) are the key information. Accordingly, previous successes have used GNN (Graph Neural Networks) and message-passing based embeddings to solve SAT problems. However, it should be noted that logical decision procedures is more complex that just reading the formulas correctly. It is unclear if GNN embeddings (via simple message-passing) contain all the information needed to reason about complex logical questions on top of the graph structures derived from the formulas, or whether the complex embedding schemes can be learned from backpropagation. Previous successes on SAT problems argued for the power of GNN, which can handle NP-complete problems ( Selsam et al., 2019 ;  Amizadeh et al., 2019 ), but no successes have been reported for solving semi-decidable predicate logic problems via GNN. In order to find out where the limitation of GNN is and why, in learning logical reasoning problems, we decide to look at problems with complexity in- between SAT and predicate logic problems, for which QBF (Quantified Boolean Formula) problems serve as excellent middle steps. QBF is an extension of propositional formula, which allows quantifiers (∀ and ∃) over the Boolean variables (such as ∀x 1 ∃x 2 . (x 1 ∨ ¬x 2 ) ∧ (¬x 1 ∨ x 2 )). In general, a quantified Boolean formula in prenex normal form can be expressed as such: Q i X i Q i−1 X i−1 ...Q 0 X 0 . φ where Q i are quantifiers that always differ from their neighboring quantifiers, X i are disjoint sets of Boolean variables, and φ is a propositional formula with all Boolean variables bounded in Q i . Complexity-wise, QBF problems are PSPACE-complete ( Kleine Büning & Bubeck, 2009 ), which lies in-between the NP-completeness of SAT problems and the semi-decidability of predicate logic problems. Furthermore, 2-QBF (QBF with only two alternative quantifiers) is Σ P 2 -complete ( Kleine Büning & Bubeck, 2009 ). Another direction of addressing logical reasoning problems via machine learning is to learn heuristic decisions within traditional decision procedures. This direction is less appealing from a theoretical perspective, but more interesting from a practical one, since it has been shown to speed up SAT solvers in practical settings ( Selsam & Bjørner, 2019 ). In this direction, there is less concern about the embedding power of GNN, but more about the design of the training procedures (what is the data and label for training) and how to incorporate the trained models within the decision procedures. The embeddings captured via GNN is rather preferred to be lossy to prevent overfitting ( Selsam & Bjørner, 2019 ). In this paper we explore the potential applications of GNNs to 2QBF problems. In Section 2, we illustrate our designs of GNN architectures for embedding 2QBF formulas. In Section 3, we evaluate GNN-based 2QBF solvers, and conjecture with empirical evidences that the current GNN techniques are unable to learn complete SAT solvers or 2QBF solvers. In Section 4, we demonstrate the potential of our GNN-based heuristics for selecting candidates and counter-examples in the CEGAR-based solver framework. In Section 5, we discuss the related work and conclude in Section 6. Throughout the paper we redirect details to supplementary materials. We make the following contributions: 1. Design and test possible GNN architectures for embedding 2QBF. 2. Pinpoint the limitation of GNN in learning logical decision procedures that need reasoning about a space of Boolean assignments. 3. Learn GNN-based CEGAR solver heuristics via supervised learning and uncover interesting challenges for GNN to generalize across graph structures.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel perspective on the Transformer architecture, a commonly used neural network architecture in natural language processing. It is shown that the Transformer architecture is inherently related to the Multi-Particle Dynamic System (MPDS) in physics, and can be interpreted as a numerical ODE solver for a first-order convection-diffusion equation. This interpretation provides a new understanding of learning contextual representations of a sentence using the Transformer. Additionally, a new network structure, the Macaron Net, is proposed based on the Strang-Marchuk splitting scheme, which is theoretically more accurate than the Lie-Trotter splitting scheme. Experiments show that the Macaron Net can achieve higher accuracy than the Transformer on all tasks.",
        "Abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible code can be found on http://anonymized",
        "Introduction": "  INTRODUCTION The Transformer is one of the most commonly used neural network architectures in natural language processing. Variants of the Transformer have achieved state-of-the-art performance in many tasks including language modeling ( Dai et al., 2019 ;  Al-Rfou et al., 2018 ) and machine translation ( Vaswani et al., 2017 ;  Dehghani et al., 2018 ;  Edunov et al., 2018 ). Unsupervised pre-trained models based on the Transformer architecture also show impressive performance in many downstream tasks ( Radford et al., 2019 ;  Devlin et al., 2018 ). The Transformer architecture is mainly built by stacking layers, each of which consists of two sub-layers with residual connections: the self-attention sub-layer and the position-wise feed-forward network (FFN) sub-layer. For a given sentence, the self-attention sub-layer considers the semantics and dependencies of words at different positions and uses that information to capture the internal structure and representations of the sentence. The position-wise FFN sub-layer is applied to each position separately and identically to encode context at each position into higher-level representations. Although the Transformer architecture has demonstrated promising results in many tasks, its design principle is not fully understood, and thus the strength of the architecture is not fully exploited. As far as we know, there is little work studying the foundation of the Transformer or different design choices. In this paper, we provide a novel perspective towards understanding the architecture. In particular, we are the first to show that the Transformer architecture is inherently related to the Multi-Particle Under review as a conference paper at ICLR 2020 Dynamic System (MPDS) in physics. MPDS is a well-established research field which aims at modeling how a collection of particles move in the space using differential equations ( Moulton, 2012 ). In MPDS, the behavior of each particle is usu- ally modeled by two factors separately. The first factor is the convection which concerns the mech- anism of each particle regardless of other particles in the system, and the second factor is the diffu- sion which models the movement of the particle resulting from other particles in the system. Inspired by the relationship between the ODE and neural networks ( Lu et al., 2017 ;  Chen et al., 2018a ), we first show that the Transformer lay- ers can be naturally interpreted as a numerical ODE solver for a first-order convection-diffusion equation in MPDS. To be more specific, the self- attention sub-layer, which transforms the seman- tics at one position by attending over all other positions, corresponds to the diffusion term; The position-wise FFN sub-layer, which is applied to each position separately and identically, corresponds to the convection term. The number of stacked layers in the Transformer corresponds to the time dimension in ODE. In this way, the stack of self-attention sub-layers and position-wise FFN sub-layers with residual connections can be viewed as solving the ODE problem numerically using the Lie-Trotter splitting scheme ( Geiser, 2009 ) and the Euler's method ( Ascher & Petzold, 1998 ). By this interpretation, we have a novel understanding of learning contextual representations of a sentence using the Transformer: the feature (a.k.a, embed- ding) of words in a sequence can be considered as the initial positions of a collection of particles, and the latent representations abstracted in stacked Transformer layers can be viewed as the location of particles moving in a high-dimensional space at different time points. Such an interpretation not only provides a new perspective on the Transformer but also inspires us to design new structures by leveraging the rich literature of numerical analysis. The Lie-Trotter splitting scheme is simple but not accurate and often leads to high approximation error ( Geiser, 2009 ). The Strang-Marchuk splitting scheme ( Strang, 1968 ) is developed to reduce the approximation error by a simple modification to the Lie-Trotter splitting scheme and is theoretically more accurate. Mapped to neural network design, the Strang-Marchuk splitting scheme suggests that there should be three sub-layers: two position-wise feed-forward sub-layers with half-step residual connections and one self-attention sub-layer placed in between with a full-step residual connection. By doing so, the stacked layers will be more accurate from the ODE's perspective and will lead to better performance in deep learning. As the FFN-attention-FFN layer is \"Macaron-like\", we call it Macaron layer and call the network composed of Macaron layers the Macaron Net. We conduct extensive experiments on both supervised and unsupervised learning tasks. For each task, we replace Transformer layers by Macaron layers and keep the number of parameters to be the same. Experiments show that the Macaron Net can achieve higher accuracy than the Transformer on all tasks which, in a way, is consistent with the ODE theory.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel self-training framework, Dynamic Self-training, for semi-supervised node classification on graphs. The framework is general, flexible, and easy to use, and is based on Graph Convolutional Networks (GCN). It outperforms state-of-art methods including GNNs, self-trained GCN, and embedding based methods. The distinguishing features of the framework include augmenting the training set and recalculating the pseudo-labels after each epoch, using a threshold-based rule to insert unlabeled nodes, assigning a personalized weight to each active pseudo-label, and dynamically activating and deactivating pseudo-labels.",
        "Abstract": "Graph neural networks (GNN) such as GCN, GAT, MoNet have achieved state-of-the-art results on semi-supervised learning on graphs. However, when the number of labeled nodes is very small, the performances of GNNs downgrade dramatically. Self-training has proved to be effective for resolving this issue, however, the performance of self-trained GCN is still inferior to that of G2G and DGI for many settings. Moreover, additional model complexity make it more difficult to tune the hyper-parameters and do model selection. We argue that the power of self-training is still not fully explored for the node classification task. In this paper, we propose a unified end-to-end self-training framework called \\emph{Dynamic Self-traning}, which generalizes and simplifies prior work. A simple instantiation of the framework based on GCN is provided and empirical results show that our framework outperforms all previous methods including GNNs, embedding based method and  self-trained GCNs by a noticeable margin. Moreover, compared with standard self-training, hyper-parameter tuning for our framework is easier.",
        "Introduction": "  INTRODUCTION Graphs or networks can be used to model any interactions between entities such as social interactions (Facebook, Twitter), biological networks (protein-protein interaction), and citation networks. There has been an increasing research interest in deep learning on graph structured data, e.g., ( Bruna et al., 2014 ;  Defferrard et al., 2016 ;  Monti et al., 2017 ;  Kipf & Welling, 2017 ;  Hamilton et al., 2017 ;  Velickovic et al., 2018 ;  Tang et al., 2015 ;  Perozzi et al., 2014 ). Semi-supervised node classification on graphs is a fundamental learning task with many applications. Classic methods rely on some underly diffusion process to propagate label information. Recently, network embedding approaches have demonstrate outstanding performance on node classification ( Tang et al., 2015 ;  Grover & Leskovec, 2016 ;  Bojchevski & Günnemann, 2018 ). This approach first learns a lower-dimensional embedding for each node in an unsupervised manner, and then the embeddings are used to train a supervised classifier for node classification, e.g., logistic regression or multi-layer perceptron (MLP). Graph neural networks (GNN) are semi-supervised models and have achieved state-of-the-art performance on many benchmark data sets ( Monti et al., 2017 ;  Kipf & Welling, 2017 ;  Velickovic et al., 2018 ). GNNs generalize convolution to graph structured data and typically have a clear advantage when the number of training examples is reasonably large. However, when there are very few labeled nodes, GNNs is outperformed by embedding based method (as shown by our experimental results), e.g., G2G from ( Bojchevski & Günnemann, 2018 ) and DGI from ( Veličković et al., 2019 ). To overcome this limitation of GCNs ( Kipf & Welling, 2017 ), Li et al. ( Li et al., 2018 ) propose to apply self-training and co-training techniques ( Scudder, 1965 ). The idea of these techniques is to augment the original training set by adding in some unlabeled examples together with their label predictions. Such \"pseudo-label\" information is either from the base model trained on the original training set (self-training) or another learning algorithm (co-training). The results from ( Li et al., 2018 ) demonstrate the effectiveness of co-training and self-training. However, among the four variants implemented in ( Li et al., 2018 ), there is not a single one that achieves the best performance across different settings; and from our experiments, G2G and DGI outperforms all the four variants when the number of labels from each class is less than 10. There are clear restrictions in prior self-training approaches. First, the pseudo-label set is incremental only, i.e., after an unlabeled Under review as a conference paper at ICLR 2020 example is added to the training set, it will never be deleted and its pseudo-label will never change even if its prediction and/or the corresponding margin has changed drastically. Secondly, all the pseudo-labels are considered equal, although they may have very different classification margins. Furthermore, it introduces extra hyper-parameters such as the number of unlabeled nodes to be added into the training set and the total number of self-training iterations. The performance gain is sensitive to such parameters and their optimal values may differ for different data sets and label rates ( Buchnik & Cohen, 2018 ). To fully understand and explore the power of self-training on the node classification task, we propose a novel self-training framework, named Dynamic Self-training, which is general, flexible, and easy to use. We provide a simple instantiation of the framework based on GCN ( Kipf & Welling, 2017 ) and empirically show that it outperforms state-of-art methods including GNNs, self-trained GCN ( Li et al., 2018 ), and embedding based methods. Our framework has the following distinguishing features compared with ( Li et al., 2018 ;  Buchnik & Cohen, 2018 ). 1. We augment the training set and recalculate the pseudo-labels after each epoch. So the number self-training iterations is the same as the number of epochs and the pseudo-label assigned to an unlabeled example may change during the training process. 2. In stead of inserting a fixed number of new pseudo-labels with highest margin in each iteration, we use a threshold-based rule, i.e., insert an unlabeled node if and only if its classification margin is above the threshold. 3. The pseudo-label set is dynamic. When the margin of an unlabeled node is above the threshold, we activate it by adding it to the loss function, but if the margin of this node becomes lower than the threshold in a later epoch, we will deactivate it. 4. We assign a (dynamic) personalized weight to each active pseudo-label proportional to its current classification margin. The total pseudo-label loss is thus the weighted sum of losses corresponds to all pseudo-labels.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new and simple approach of image/tensor modeling which translates the ConvNet, and demonstrates its effectiveness and similarity to the Deep Image Prior (DIP). The proposed method, Manifold Modeling in Embedded Space (MMES), is characterized by a low-dimensional patch-manifold prior, and is a special case of deep neural networks. The MMES network is designed as simple as possible while keeping a essential ConvNet structure, and can be implemented by convolution and transposed convolution with one-hot-filters.",
        "Abstract": "Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet) structure itself as an image prior, has attracted huge attentions in computer vision community.  It empirically shows the effectiveness of ConvNet structure for various image restoration applications.  However, why the DIP works so well is still unknown, and why convolution operation is essential for image reconstruction or enhancement is not very clear. In this study, we tackle these questions. The proposed approach is dividing the convolution into ``delay-embedding'' and ``transformation (\\ie encoder-decoder)'', and proposing a simple, but essential, image/tensor modeling method which is closely related to dynamical systems and self-similarity. The proposed method named as manifold modeling in embedded space (MMES) is implemented by using a novel denoising-auto-encoder in combination with multi-way delay-embedding transform. In spite of its simplicity, the image/tensor completion and super-resolution results of MMES are quite similar even competitive to DIP in our extensive experiments, and these results would help us for reinterpreting/characterizing the DIP from a perspective of ``low-dimensional patch-manifold prior''.",
        "Introduction": "  INTRODUCTION The most important piece of information for image/tensor restoration would be the \"prior\" which usually converts the optimization problems from ill-posed to well-posed, and/or gives some robust- ness for specific noises and outliers. Many priors were studied in computer science problems such as low-rank representation (Pearson, 1901; Hotelling, 1933; Hitchcock, 1927; Tucker, 1966), smooth- ness (Grimson, 1981; Poggio et al., 1985; Li, 1994), sparseness (Tibshirani, 1996), non-negativity (Lee & Seung, 1999; Cichocki et al., 2009), statistical independence (Hyvarinen et al., 2004), and so on. Particularly in today's computer vision problems, total variation (TV) (Guichard & Malgouyres, 1998; Vogel & Oman, 1998), low-rank representation (Liu et al., 2013; Ji et al., 2010; Zhao et al., 2015; Wang et al., 2017), and non-local similarity (Buades et al., 2005; Dabov et al., 2007) priors are often used for image modeling. These priors can be obtained by analyzing basic properties of natural images, and categorized as \"unsupervised image modeling\". By contrast, the deep image prior (DIP) (Ulyanov et al., 2018) has been come from a part of \"super- vised\" or \"data-driven\" image modeling framework (i.e., deep learning) although the DIP itself is one of the state-of-the-art unsupervised image restoration methods. The method of DIP can be sim- ply explained to only optimize an untrained (i.e., randomly initialized) fully convolutional generator network (ConvNet) for minimizing squares loss between its generated image and an observed image (e.g., noisy image), and stop the optimization before the overfitting. Ulyanov et al. (2018) explained the reason why a high-capacity ConvNet can be used as a prior by the following statement: Net- work resists \"bad\" solutions and descends much more quickly towards naturally-looking images, and its phenomenon of \"impedance of ConvNet\" was confirmed by toy experiments. However, most researchers could not be fully convinced from only above explanation because it is just a part of whole. One of the essential questions is why is it ConvNet? or in more practical perspective, to explain what is \"priors in DIP\" with simple and clear words (like smoothness, sparseness, low-rank etc) is very important. In this study, we tackle the question why ConvNet is essential as an image prior, and try to translate the \"deep image prior\" with words. For this purpose, we divide the convolution operation into Under review as a conference paper at ICLR 2020 \"embedding\" and \"transformation\" (see Fig. 9 in Appendix). Here, the \"embedding\" stands for delay/shift-embedding (i.e., Hankelization) which is a copy/duplication operation of image-patches by sliding window of patch size (τ, τ ). The embedding/Hankelization is a preprocessing to capture the delay/shift-invariant feature (e.g., non-local similarity) of signals/images. This \"transformation\" is basically linear transformation in a simple convolution operation, and it also indicates some non- linear transformation from the ConvNet perspective. To simplify the complicated \"encoder-decoder\" structure of ConvNet used in DIP, we consider the following network structure: Embedding H (linear), encoding φ r (non-linear), decoding ψ r (non-linear), and backward embedding H † (linear) (see  Fig. 1 ). Note that its encoder-decoder part (φ r , ψ r ) is just a simple multi-layer perceptron along the filter domain (i.e., manifold learning), and it is sandwitched between forward and backward embedding (H, H † ). Hence, the proposed network can be characterized by Manifold Modeling in Embedded Space (MMES). The proposed MMES is designed as simple as possible while keeping a essential ConvNet structure. Some parameters τ and r in MMES are corresponded with a kernel size and a filter size in ConvNet. When we set the horizontal dimension of hidden tensor L with r, each τ 2 -dimensional fiber in H, which is a vectorization of each (τ, τ )-patch of an input image, is encoded into r-dimensional space. Note that the volume of hidden tensor L looks to be larger than that of input/output image, but representation ability of L is much lower than input/output image space since the first/last tensor (H,H ) must have Hankel structure (i.e., its representation ability is equivalent to image) and the hidden tensor L is reduced to lower dimensions from H. Here, we assume r < τ 2 , and its low- dimensionality indicates the existence of similar (τ, τ )-patches (i.e., self-similarity) in the image, and it would provide some \"impedance\" which passes self-similar patches and resist/ignore others. Each fiber of Hidden tensor L represents a coordinate on the patch-manifold of image. It should be noted that the MMES network is a special case of deep neural networks. In fact, the proposed MMES can be considered as a new kind of auto-encoder (AE) in which convolution operations have been replaced by Hankelization in pre-processing and post-processing. Compared with ConvNet, the forward and backward embedding operations can be implemented by convolution and transposed convolution with one-hot-filters (see Fig. 12 in Appendix for details). Note that the encoder-decoder part can be implemented by multiple convolution layers with kernel size (1,1) and non-linear activations. In our model, we do not use convolution explicitly but just do linear transform and non-linear activation for \"filter-domain\" (i.e., horizontal axis of tensors in  Fig. 1 ). The contributions in this study can be summarized as follow: (1) A new and simple approach of image/tensor modeling is proposed which translates the ConvNet, (2) effectiveness of the proposed method and similarity to the DIP are demonstrated in experiments, and (3) most importantly, there is a prospect for interpreting/characterizing the DIP as \"low-dimensional patch-manifold prior\".",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes Delayed Update and Temporally Sparse Update algorithms to enable scalable distributed training across different geographical locations with long distance and high latency network connections. These algorithms are designed to tolerate latency and reduce traffic congestion while preserving accuracy. Experiments show that these algorithms can achieve scalability of 0.72 with servers and data distributed in four different countries across the world, and can train ResNet-50 on ImageNet with scalability. To our best knowledge, this is the first work that can achieve scalable synchronous distributed training under such high latency.",
        "Abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90×speedup over traditional methods without loss of accuracy on ImageNet.",
        "Introduction": "  INTRODUCTION Bandwidth is easy to increase (e.g. stacking hardware) but latency is hard to improve (physical limits). For example, if we have two servers located at Shanghai and Boston respectively, even at the speed of light and direct air dis- tance, it still takes 78ms * to send and receive a packet. In real world scenario, the latency can be only worse (around 700ms) because indirect routing between internet service providers (ISP) and queuing delay in switches. Such high latency cause severe scalability † issue for distributed train- ing. As shown in  Fig. 1 , traditional distributed training algorithm scales poorly under such large latency. locations across public clouds, private clusters and even edge devices, it is impossible to setup a low latency network under long-distance connections and thereby hurts the scalability of training. In this work, we enable scalable distributed training across different geographical locations with long distance and high latency network connection. We propose Delayed Update to tolerate latency by putting off the synchronization barrier to a later iteration; we also propose Temporally Sparse Update to amortize the latency and alleviate congestion by reducing the synchronization frequency. To ensure no loss of accuracy, we design a novel error compensation to overcome the staleness for both vanilla and momentum SGD. We focus on the widely adopted synchronous update using data parallelism. As shown in  Fig. 1 , DTS can maintain high scalability even when latency is as high as 1000ms. This result is also better than existing state of art technologies such as ECD-PSGD (results copied directly from their original paper (Tang et al., 2018a)) We benchmark our DTS under a challenging settings: training a deep neural network on four AWS P3 instances located in different continents of the world ( Fig. 2 ). The measured latency is as large as 277ms (compared to internal latency 2us). In this case, the naive distributed synchronous SGD (SSGD) can only achieve a poor scalability of 0.008. It means in this setting, distributed training with 100 servers is even slower than single machine (0.8 v.s. 1.0). Meanwhile DTS achieves scalability of 0.72 without compromising the accuracy. In conclusion, our contributions are listed below: • We propose delayed update to tolerate the latency and temporally sparse update to amortize the latency. While preserving the accuracy, delayed update tolerates up to 6 seconds latency and temporally sparse update reduced the traffic congestion by 20×. • We theoretically justify the convergence rate of our proposed algorithms. We show that both algorithms can be as fast original SGD while scaling well under high latency. • With servers and data distributed in four different countries across the world, we can train ResNet-50 on ImageNet with scalability. To our best knowledge, DTS is the first work that can achieve scalable synchronous distributed training under such high latency.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel approach to deep reinforcement learning, in which agents are trained with a regulatory focus that is tailored to the environment. The regulatory fit theory (Higgins, 2000; Higgins et al., 2003) is used to determine the suitable regulatory focus for an environment, and two strategies are proposed to train an agent with a specified regulatory focus. The proposed strategies are tested on various environments, including sparse-reward environments, the Terrain RL Simulator for biped locomotion, the Atari games, and the set of MuJoCo environments for continuous control. Results show that the proposed strategies, when chosen appropriately, can substantially outperform the standard way. ",
        "Abstract": "The estimation of advantage is crucial for a number of reinforcement learning algorithms, as it directly influences the choices of future paths. In this work, we propose a family of estimates based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process in a promotion focus or prevention focus. On top of this formulation, we systematically study the impacts of different regulatory focuses. Our findings reveal that regulatory focus, when chosen appropriately, can result in significant benefits. In particular, for the environments with sparse rewards, promotion focus would lead to more efficient exploration of the policy space; while for those where individual actions can have critical impacts, prevention focus is preferable. On various benchmarks, including MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments, the proposed schemes consistently demonstrate improvement over mainstream methods, not only accelerating the learning process but also obtaining substantial performance gains.",
        "Introduction": "  INTRODUCTION It is not enough to do good; one must do it the right way. The best-known motivational principle that has been recognized in psychology ( Gray, 1982 ;  Atkinson, 1964 ) is the hedonic principle that people approach pleasure and avoid pain. However the principle does not explain how people approach and avoid.  Higgins (1997)  proposes the regulatory focus theory to address this question. A promotion focus is concerned with advancement, growth, and accomplishment, and a prevention focus is concerned with security, safety, and responsibility ( Crowe & Higgins, 1997 ). The theory hypothesized that the promotion focus inclination insures against errors of omission, and the prevention focus inclination insures against errors of commission. Furthermore, the regulatory fit theory ( Higgins, 2000 ;  Higgins et al., 2003 ) proposes that when people pursue a goal in a manner that fit their regulatory orientation, they are stronger motivated during goal pursuit. For example,  Lee & Aaker (2004)  showed that ads are more persuasive if there is a regulatory fit: use gain frames (e.g. \"great looks and exceptional engineering\") for individuals who are eager to approach positive outcomes, and use loss frames (e.g. \"don't be stranded with a disabled vehicle without an emergency safety kit\") for individuals who are vigilant to avoid negative outcomes. We study the phenomenon of regulatory fit in the context of reinforcement learning (RL). Be aware that humans own real emotions and are fundamentally more complex than RL agents, thus we borrow terminologies in psychology while trying to retain the resemblance. We hypothesize that different RL environments are more fit with agents of different regulatory focus. For example, in the environments with sparse rewards, it might be advisable to actively exploring those policies that show potentials without being discouraged by a few failed trials; while in the environments that are fragile, e.g. those where a wrong choice of action can lead to catastrophic consequences down the path, a more conservative, risk-averse stance might be preferable. There are three questions need to answer: (1) How to automatically determine the suitable regulatory focus for an environment; (2) How to train an agent with a specified regulatory focus; (3) Is there any benefit when training the agent with the regulatory focus that fits the environment? In this article, we aim to answer the second and third questions and leave the first question open. The research on deep reinforcement learning is gaining momentum in recent years. A number of learning methods, such as Proximal Policy Optimization algorithm (PPO) ( Schulman et al., 2017 ), Under review as a conference paper at ICLR 2020 Trust-Region Policy Optimization algorithm (TRPO) ( Schulman et al., 2015a ), and Advantage Actor- Critic algorithm (A2C) ( Mnih et al., 2016 ), have been developed. These methods and their variants have achieved great success in challenging problems, e.g. continuous control ( Schulman et al., 2015b ), locomotion ( Heess et al., 2017 ), and video games ( Vinyals et al., 2017 ). The core of all these methods is the estimation of the advantage A(s t , a t ), i.e. the gain in the expected cumulative reward relative to the state value V (s t ) if a certain action a t is taken. A common practice is to use the n-step estimate over the sampled trajectories. This way has been widely used in actor-critic algorithms such as the A2C. Schulman et al. presents a generalization called Generalized Advantage Estimator (GAE) ( Schulman et al., 2015b ), which blends 1 to n-step estimators with exponentially decayed weights, thus reducing the variance of policy gradients. For convenience, we refer to the set of 1 to n-step estimators along a trajectory as the path ensemble. We propose to take the maximum over path ensemble to implement the promotion focus inclination, and take the minimum over path ensemble to implement to prevention inclination during training. Intuitively, when taking the maximum or minimum, the agent is comparing a list of counterfactual speculations. When it looks at a k-step estimation where k ≤ n, it is thinking what if it didn't took those actions after step k? For a promotion focus, in order to insure hits and insure against omission, it would favor the estimation with largest value because it has the possibility of being that good. Similarly, for a prevention focus, it prefers the minimum to insure against errors of commission. We systematically studied different estimation schemes on various environments, including the sparse-reward environments ( Fu et al., 2017 ), the Terrain RL Simulator ( Berseth et al., 2018 ) for biped locomotion, the Atari games ( Bellemare et al., 2013 ), and the set of MuJoCo environments for continuous control ( Brockman et al., 2016 ). Our study shows that the proposed estimation strategies, when chosen appropriately, can substantially outperform the standard way. In particular, the promotion focus inclination, i.e. taking the maximum over path ensemble, greatly improves the learning efficiency in the environments with sparse rewards. On the other hand, the prevention focus inclination, i.e. taking the minimum, can effectively stabilize the learning by avoiding risky actions. It is noteworthy that the proposed strategic inclinations differ essentially from the recent two lines of work on robustness ( Smirnova et al., 2019 ;  Delage & Mannor, 2007 ) and risk-sensitivity ( Tamar et al., 2012 ;  Chow & Ghavamzadeh, 2014 ). Robust MDP optimizes for the worst case when there exist uncertainties in the parameters; risk-sensitive MDP optimizes the value of a risk measure. Our method does not modify the training objective. Instead, it controls the policy gradient towards policies with promotion or prevention inclinations through the alternative ways of estimating the advantage.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new approach to multi-task learning, which focuses on reducing task interference by regularizing the angle between gradients rather than their magnitudes. We empirically show that unregularized multi-task networks have high variation in the angles between task gradients, and that popular regularization methods such as Dropout and Batchnorm implicitly orthogonalize the task gradients. We propose a new gradient regularization term to the multi-task objective that explicitly minimizes the squared cosine between task gradients and show that our method obtains competitive results on the NYUv2 and SUN RGB-D datasets.",
        "Abstract": "Deep neural networks are a promising approach towards multi-task learning because of their capability to leverage knowledge across domains and learn general purpose representations. Nevertheless, they can fail to live up to these promises as tasks often compete for a model's limited resources, potentially leading to lower overall performance. In this work we tackle the issue of interfering tasks through a comprehensive analysis of their training, derived from looking at the interaction between gradients within their shared parameters. Our empirical results show that well-performing models have low variance in the angles between task gradients and that popular regularization methods implicitly reduce this measure. Based on this observation, we propose a novel gradient regularization term that minimizes task interference by enforcing near orthogonal gradients. Updating the shared parameters using this property encourages task specific decoders to optimize different parts of the feature extractor, thus reducing competition. We evaluate our method with classification and regression tasks on the multiDigitMNIST and NYUv2 dataset where we obtain competitive results. This work is a first step towards non-interfering multi-task optimization.",
        "Introduction": "  INTRODUCTION Deep neural networks have proven to be very successful at solving isolated tasks in a variety of fields ranging from computer vision to NLP. In contrast to this single task setup, multi-task learning aims to train one model on several problems simultaneously. This approach would incentivize it to transfer knowledge between tasks and obtain multi-purpose representations that are less likely to overfit to an individual problem. Apart from potentially achieving better overall performance ( Caruana, 1997 ), using a multi-task approach offers the additional benefit of being more efficient in memory usage and inference speed than training several single-task models ( Teichmann et al., 2018 ). A popular design for deep multi-task networks involves hard parameter sharing ( Ruder, 2017 ), where a model contains a common encoder, which is shared across all tasks and several problem specific decoders. Given a single input each of the decoders is then trained for a distinct task using a different objective function and evaluation metric. This approach allows the network to learn multi-purpose representations through the shared encoder which every decoder will then use differently according to the requirements of its task. Although this architecture has been successfully applied to multi-task learning ( Kendall et al., 2018 ;  Chen et al., 2017 ) it also faces some challenges. From an architec- tural point of view it is unclear how to choose the task specific network capacity ( Vandenhende et al., 2019 ; Misra et al., 2016) as well as the complexity of representations to share between tasks. Ad- ditionally, optimizing multiple objectives simultaneously introduces difficulties based on the nature of those tasks and the way their gradients interact with each other ( Sener & Koltun, 2018 ). The dis- similarity between tasks could cause negative transfer of knowledge ( Long et al., 2017 ;  Zhao et al., 2018 ;  Zamir et al., 2018 ) or having task losses of different magnitudes might bias the network in favor of a subset of tasks ( Chen et al., 2017 ;  Kendall et al., 2018 ). It becomes clear that the overall success of multi-task learning is reliant on managing the interaction between tasks, and implicitly their gradients with respect to the shared parameters of the model. This work focuses on the second category of challenges facing networks that employ hard param- eter sharing, namely the interaction between tasks when being jointly optimized. We concentrate on reducing task interference by regularizing the angle between gradients rather than their magni- Under review as a conference paper at ICLR 2020 tudes. Based on our empirical findings unregularized multi-task networks have high variation in the angles between task gradients, meaning gradients frequently point in similar or opposite directions. Additionally, well-performing models share the property that their distribution of cosines between task gradients is zero-centered and low in variance. Nearly orthogonal gradients will reduce task competition as individual task decoders learn to use different features of the encoder, thus not in- terfering with each other. Furthermore, we discover that popular regularization methods such as Dropout ( Srivastava et al., 2014 ) and Batchnorm ( Ioffe & Szegedy, 2015 ) implicitly orthogonalize the task gradients. We propose a new gradient regularization term to the multi-task objective that explicitly minimizes the squared cosine between task gradients and show that our method obtains competitive results on the NYUv2 ( Nathan Silberman & Fergus, 2012 ) and SUN RGB-D ( Song et al., 2015 ) datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to increasing behavioral diversity in Reinforcement Learning (RL) by introducing the concept of social influence. The proposed method, Interior Policy Differentiation (IPD), uses a policy distance metric to compare the similarity of agents and an optimization constraint to bring immediate feedback in the learning process. The method is benchmarked on several locomotion tasks and shows that it can learn various diverse and well-behaved policies for the given tasks.",
        "Abstract": "Animals develop novel skills not only through the interaction with the environment but also from the influence of the others. In this work we model the social influence into the scheme of reinforcement learning, enabling the agents to learn both from the environment and from their peers. Specifically, we first define a metric to measure the distance between policies then quantitatively derive the definition of uniqueness. Unlike previous precarious joint optimization approaches, the social uniqueness motivation in our work is imposed as a constraint to encourage the agent to learn a policy different from the existing agents while still solve the primal task. The resulting algorithm, namely Interior Policy Differentiation (IPD), brings about performance improvement as well as a collection of policies that solve a given task with distinct behaviors",
        "Introduction": "  INTRODUCTION The paradigm of Reinforcement Learning (RL), inspired by cognition and animal stud- ies ( Thorndike, 2017 ;  Schultz et al., 1997 ), can be described as learning by interacting with the environment to maximize a cumulative reward ( Sutton et al., 1998 ). From the perspective of ecol- ogy, biodiversity as well as the development of various skills are crucial to the continuation and evolution of species ( Darwin, 1859 ;  Pianka, 1970 ). Thus the behavioral diversity becomes a rising topic in RL. Previous works have tried to encourage the emergence of behavioral diversity in RL with two approaches: The first approach is to design interactive environments which contain suffi- cient richness and diversity. For example,  Heess et al. (2017)  show that rich environments enable agents to learn different locomotion skills even using the standard RL algorithms. Yet designing a complex environment requires manual efforts, and the diversity is limited by the obstacle classes. The second approach to increase behavioral diversity is to motivate agents to explore beyond just maximizing the reward for the given task.  Zhang et al. (2019)  proposed to maximize a heuristi- cally defined novelty metric between policies through task-novelty joint optimization, but the final performance of agents is not guaranteed. In this work, we address the topic of policy differentiation in RL, i.e., to improve the diversity of RL agents while keeping their ability to solve the primal task. We draw the inspiration from the Social Influence in animal society ( Rogoff, 1990 ;  Ryan & Deci, 2000 ;  van Schaik & Burkart, 2011 ; Henrich, 2017;  Harari, 2014 ) and formulate the concept of social influence in the reinforcement learning paradigm. Our learning scheme is illustrated in  Fig 1 . The target agent not only learns to interact with the environment to maximize the reward but also differentiate the actions it takes in order to be different from other existing agents. Since the social influence often acts on people passively as a sort of peer pressure, we implement the social influence in terms of social uniqueness motivation ( Chan et al., 2012 ) and consider it as a constrained optimization problem. In the following of our work, we first define a rigorous policy distance metric in the policy space to compare the similarity of the agents. Then we develop an op- timization constraint using the proposed metric, which brings immediate rather than episodic feed- back in the learning process. A novel method, namely Interior Policy Differentiation (IPD), is further Under review as a conference paper at ICLR 2020 proposed as a better solution for the constrained policy optimization problem. We benchmark our method on several locomotion tasks and show it can learn various diverse and well-behaved policies for the given tasks based on the standard Proximal Policy Optimization (PPO) algorithm ( Schulman et al., 2017 ).",
        "label": 0
    },
    {
        "Summary": "\nAbstract: Federated learning is a promising methodology for privacy-preserving distributed learning that reduces communication overhead and data privacy and security risks by enabling each learner to compute their local updates of each round for relatively many iterations. However, handling decentralized non-IID data still remains a statistical challenge in the field of federated learning. This paper presents an algorithm for federated learning that updates model parameters based on local training datasets and a global model parameters.",
        "Abstract": "Federated learning, where a global model is trained by iterative parameter averaging of locally-computed updates, is a promising approach for distributed training of deep networks; it provides high communication-efficiency and privacy-preservability, which allows to fit well into decentralized data environments, e.g., mobile-cloud ecosystems. However, despite the advantages, the federated learning-based methods still have a challenge in dealing with non-IID training data of local devices (i.e., learners). In this regard, we study the effects of a variety of hyperparametric conditions under the non-IID environments, to answer important concerns in practical implementations: (i) We first investigate parameter divergence of local updates to explain performance degradation from non-IID data. The origin of the parameter divergence is also found both empirically and theoretically. (ii) We then revisit the effects of optimizers, network depth/width, and regularization techniques; our observations show that the well-known advantages of the hyperparameter optimization strategies could rather yield diminishing returns with non-IID data. (iii) We finally provide the reasons of the failure cases in a categorized way, mainly based on metrics of the parameter divergence.",
        "Introduction": "  INTRODUCTION Over the recent years, federated learning ( McMahan et al., 2017 ) has been a huge success to re- duce the communication overhead in distributed training of deep networks. Guaranteeing compet- itive performance, the federated learning permits each learner to compute their local updates of each round for relatively many iterations (e.g., 1 epoch, 10 epochs, etc.), which provides much higher communication-efficiency compared to the conventional data parallelism approaches (for intra-datacenter environments, e.g.,  Dean et al. (2012) ;  Chen et al. (2016) ) that generally require very frequent gradient aggregation. Furthermore, the federated learning can also significantly re- duce data privacy and security risks by enabling to conceal on-device data of each learner from the server or other learners; thus the approach can be applied well to environments with highly private data (e.g., personal medical data), it is now emerging as a promising methodology for privacy- preserving distributed learning along with differential privacy-based methods ( Hard et al., 2018 ;  Yang et al., 2018 ;  Bonawitz et al., 2019 ;  Chen et al., 2019 ). On this wise, the federated learning takes a simple approach that performs iterative parameter aver- aging of local updates computed from each learners' own dataset, which suggests an efficient way to learn a shared model without centralizing training data from multiple sources; but hereby, since the local data of each device is created based on their usage pattern, the heterogeneity of training data distributions across the learners might be naturally assumed in real-world cases. Hence, each local dataset would not follow the population distribution, and handling the decentralized non-IID data still remains a statistical challenge in the field of federated learning ( Smith et al., 2017 ). For instance,  Zhao et al. (2018)  observed severe performance degradation in multi-class classification accuracy under highly skewed non-IID data; it was reported that more diminishing returns could be yielded as the probabilistic distance of learners' local data from the population distribution increases. Under review as a conference paper at ICLR 2020 Algorithm 1 Federated learning. w t k is the model parameters updated by learner k at communication round t, w t is the global model parameters at round t, K = {1, 2, · · · , K} is the universal set of learners, P k is the local training dataset of learner k, B is the local minibatch size, E is the number of the local epochs per round, η is the learning rate, and (·) is the loss function.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Dimensional reweighting Graph Convolutional Networks (DrGCNs), a novel approach to improve the performance of Graph Convolutional Networks (GCNs). DrGCNs apply a dimensional reweighting process to the node representations, which reduces the input covariance between dimensions and improves the stability of fully connected networks. Experiments on public datasets demonstrate that DrGCNs can achieve predictable improvements over the state-of-the-art GCNs, and offline evaluations on a company's recommendation system show performance improvements.",
        "Abstract": "In this paper, we propose a method named Dimensional reweighting Graph Convolutional Networks (DrGCNs), to tackle the problem of variance between dimensional information in the node representations of GCNs. We prove that DrGCNs can reduce the variance of the node representations by connecting our problem to the theory of the mean field. However, practically, we find that the degrees DrGCNs help vary severely on different datasets. We revisit the problem and develop a new measure K to quantify the effect. This measure guides when we should use dimensional reweighting in GCNs and how much it can help. Moreover, it offers insights to explain the improvement obtained by the proposed DrGCNs. The dimensional reweighting block is light-weighted and highly flexible to be built on most of the GCN variants. Carefully designed experiments, including several fixes on duplicates, information leaks, and wrong labels of the well-known node classification benchmark datasets, demonstrate the superior performances of DrGCNs over the existing state-of-the-art approaches. Significant improvements can also be observed on a large scale industrial dataset.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have been widely applied in various fields, including computer vi- sion ( He et al., 2016 ;  Hu et al., 2018 ), natural language processing ( Devlin et al., 2019 ), and speech recognition ( Abdel-Hamid et al., 2014 ), among many others. Graph neural networks (GNNs) is proposed for learning node presentations of networked data ( Scarselli et al., 2009 ), and later be ex- tended to graph convolutional network (GCN) that achieves better performance by capturing topo- logical information of linked graphs ( Kipf & Welling, 2017 ). Since then, GCNs begin to attract board interests. Starting from GraphSAGE ( Hamilton et al., 2017 ) defining the convolutional neu- ral network based graph learning framework as sampling and aggregation, many follow-up efforts attempt to enhance the sampling or aggregation process via various techniques, such as attention mechanism ( Veličković et al., 2018 ), mix-hop connection ( Abu-El-Haija et al., 2019 ) and adaptive sampling ( Huang et al., 2018 ). In this paper, we study the node representations in GCNs from the perspective of covariance between dimensions. Suprisingly, applying a dimensional reweighting process to the node representations may be very useful for the improvement of GCNs. As an instance, under our proposed reweighting scheme, the input covariance between dimensions can be reduced by 68% on the Reddit dataset, which is extremely useful since we also find that the number of misclassified cases reduced by 40%, compared with the previous SOTA method. We propose Dimensional reweighting Graph Convolutional Networks (DrGCNs), in which the input of each layer of the GCN is reweighted by global node representation information. Our discovery is that the experimental performance of GCNs can be greatly improved under this simple reweighting scheme. On the other hand, with the help of mean field theory ( Kadanoff, 2009 ;  Yang et al., 2019 ), this reweighting scheme is also proved to improve the stability of fully connected networks, provding insight to GCNs. To deepen the understanding to which extent the proposed reweighting scheme can help GCNs, we develop a new measure to quantify its effectiveness under different contexts (GCN variants and datasets). Experimental results verify our theoretical findings ideally that we can achieve predictable improve- ments on public datasets adopted in the literature over the state-of-the-art GCNs. While studying on Under review as a conference paper at ICLR 2020 these well-known benchmarks, we notice that two of them (Cora, Citeseer) suffer from duplicates and feature-label information leaks. We fix these problems and offer refined datasets for fair com- parisons. To further validate the effectiveness, we deploy the proposed DrGCNs on A* 1 company's recommendation system and clearly demonstrate performance improvements via offline evaluations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel framework to train a de-biased representation by encouraging it to be \"different\" from a set of representations that are biased by design. The proposed method is evaluated on datasets with shifted or removed bias, and results show improved accuracies in test data. This framework is intended to address the problem of cross-bias generalisation, where a model does not exploit its full capacity due to the \"sufficiency\" of bias cues for prediction of the target label in training data.",
        "Abstract": "Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led interesting advances, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles). Such biased models fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. Our experiments and analyses show that our method discourages models from taking bias shortcuts, resulting in improved performances on de-biased test data.",
        "Introduction": "  INTRODUCTION Most machine learning algorithms are trained and evaluated by randomly splitting a single source of data into training and test sets. Although this is a standard protocol, it is blind to a critical problem: the existence of dataset bias ( Torralba & Efros, 2011 ). For instance, many frog images are taken in swamp scenes, but swamp itself is not a frog. Nonetheless, a neural network will exploit this bias (i.e., take \"shortcuts\") if it yields correct predictions for the majority of training examples. If bias is sufficient to achieve high accuracy, there is little motivation for models to learn the complexity of the intended task, despite its full capacity to do so. Consequently, a model that relies on bias will achieve high in-distribution accuracy, yet fail to generalise when the bias shifts. We tackle this \"cross-bias generalisation\" problem where a model does not exploit its full capacity due to the \"sufficiency\" of bias cues for prediction of the target label in training data. For example, language models make predictions based on the presence of certain words (e.g., \"not\" for \"contra- diction\") ( Gururangan et al., 2018 ) without much reasoning on the actual meaning of sentences, even if they are in principle capable of sophisticated reasoning. Similarly, convolutional neural networks (CNNs) achieve high accuracies on image classification by using local texture cues as shortcut, as opposed to more reliable global shape cues ( Geirhos et al., 2019 ;  Brendel & Bethge, 2019 ). Existing methods attempt to remove a model's dependency on bias by de-biasing the training data through data augmentation ( Geirhos et al., 2019 ) or re-sampling tactics ( Li & Vasconcelos, 2019 ). Others have introduced a pre-defined set of biases that a model is trained to be independent against ( Wang et al., 2019 ). These prior works assume that bias can easily be defined or quantified, but real-world biases often do not (e.g., texture bias above). To address this limitation, we propose a novel framework to train a de-biased representation by encouraging it to be \"different\" from a set of representations that are biased by design. Our insight is that biased representations can easily be obtained by utilising models of smaller capacity (e.g., bag of words for word bias and CNNs of small receptive fields for texture bias). Experiments show that our method is effective in reducing a model's dependency on \"shortcuts\" in training data, as evidenced by improved accuracies in test data where the bias is either shifted or removed.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel method to improve the robustness of a model to label noise. The method combines probabilistic reasoning and data selecting to continuously improve the purity of the data labels by correcting the noise-corrupted ones. The label correction algorithm is based on statistical principles and is theoretically guaranteed to deliver a high quality label set. Experiments on various datasets with various noise patterns and levels show that the proposed method produces robust neural network models with superior performance.",
        "Abstract": "To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. A major challenge is to develop robust deep learning models that achieve high test performance despite training set label noise.  We introduce a novel approach that directly cleans labels in order to train a high quality model. Our method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness.  In particular, we use a likelihood ratio test(LRT) to flip the labels of training data.  We prove that our LRT label correction algorithm is guaranteed to flip the label so it is consistent with the true Bayesian optimal decision rule with high probability.  We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.",
        "Introduction": "  INTRODUCTION Label noise is ubiquitous in real world data. It may be caused by unintentional mistakes of manual or automatic annotators ( Yan et al., 2014 ;  Veit et al., 2017 ). It may also be introduced by malicious attackers ( Steinhardt et al., 2017 ). Noisy labels impair the performance of a model ( Smyth et al., 1994 ;  Brodley & Friedl, 1999 ), especially a deep neural network, which tends to have strong memo- rization power ( Frnay & Verleysen, 2014 ;  Zhang et al., 2017 ). Improving the robustness of a model to label noise is a crucial yet challenging task in many applications ( Mnih & Hinton, 2012 ;  Wu et al., 2018 ). Existing methods mainly follow two directions, probabilistic reasoning and data selecting. Probabilistic methods explicitly model a noise transition matrix, namely, the probability of one label being corrupted into another ( Goldberger & Ben-Reuven, 2017 ;  Patrini et al., 2017 ). The transition matrix is often estimated from the data, and is used to re-calibrate the training loss or to correct the prediction. Explicit estimation of the transition matrix can be problematic due to the large variation of noise patterns, e.g., uniform noise, asymmetric noise, or mixtures. Furthermore, the transition matrix size is quadratic to the number of classes, making the estimation task prohibitive when the data has hundreds or even thousands of classes. Data-selecting methods are agnostic of the underlying noise pattern. These methods gradually col- lect clean data whose labels are trustworthy ( Malach & Shalev-Shwartz, 2017 ;  Jiang et al., 2018 ;  Han et al., 2018 ). As more clean data are collected, the quality of the trained models improves. The major issue of these methods is the lack of a quantitative control of the quality of the collected clean data. Without a principled guideline, it is hard to find the correct data collection pace. An aggressive selection can unknowingly accumulate irreversible errors. On the other hand, an overly-conservative strategy can be very slow in training, or stops with insufficient clean data and mediocre models. We propose a novel method with the benefit from both the probabilistic and the data-selecting ap- proaches. Similar to data-selecting methods, our method continuously improves the purity of the data labels by correcting the noise-corrupted ones. Meanwhile, we improve the classifier using the updated labels. Our label correction algorithm is based on statistical principles and is theoretically guaranteed to deliver a high quality label set. Instead of explicitly estimating the transition matrix, the correction algorithm only depends on the prediction of the current model, denoted as f . Using an f -based likelihood ratio test, we determine whether the current label of each data should be cor- rected. Our main theorem proves that the label correction algorithm will clean a majority of noisy labels with high probability. In practice, we incorporate the label correction algorithm into the training of deep neural networks. Our method iteratively updates the labels of the data while continuously training a deep neural Under review as a conference paper at ICLR 2020 network. To ensure the deep neural network does not overfit with noise labels that are yet to be cor- rected, we introduce a new retroactive loss term that regulates the model by enforcing its consistency with models in previous epochs. The rationale is that the model in an earlier training stage tends to fit the true signal rather than noise, although its overall performance is sub-optimal. Through experiments on various datasets with various noise patterns and levels, we show that our method produces robust neural network models with superior performance. To the best of our knowledge, our method is the first to correct labels with theoretical guarantees. It is has advantages over both probabilistic methods and data-selecting methods. Compared with other data-selecting methods, it has a better quantitative control of the label quality and thus is less brittle when generalizing to different datasets and different noise patterns. Also note that we are not selecting clean data. Instead, we correct labels and always use the whole training set to train. This brings an additional advantage of fully leveraging the data. Compared with other probabilistic methods, our correction algorithm assumes a rather general family of underlying noise patterns and avoids an explicit estimation of the transition matrix.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new statistic to detect out-of-distribution (OoD) samples using deep generative models trained with batch normalization. We argue that using batch normalization not only improves optimization, but also challenges the i.i.d. assumption underlying typical likelihood-based objectives. We show that the training objective of generative models with batch normalization can be interpreted as maximum pseudo-likelihood over a different joint distribution that does not assume data in the same batch are i.i.d. samples. Empirically, we demonstrate that over this joint distribution, the estimated likelihood of a batch of OoD samples is much lower than that of in-distribution samples. This allows us to propose a permutation test which outperforms existing methods by a significant margin without modifying how the underlying generative model is trained.",
        "Abstract": "Likelihood from a generative model is a natural statistic for detecting out-of-distribution (OoD) samples. However, generative models have been shown to assign higher likelihood to OoD samples compared to ones from the training distribution, preventing simple threshold-based detection rules. We demonstrate that OoD detection fails even when using more sophisticated statistics based on the likelihoods of individual samples. To address these issues, we propose a new method that leverages batch normalization. We argue that batch normalization for generative models challenges the traditional \\emph{i.i.d.} data assumption and changes the corresponding maximum likelihood objective. Based on this insight, we propose to exploit in-batch dependencies for OoD detection. Empirical results suggest that this leads to more robust detection for high-dimensional images.",
        "Introduction": "  INTRODUCTION Modern neural network models are known to make poorly calibrated predictions ( Guo et al., 2017 ;  Kuleshov et al., 2018 ), and can be highly confident even for unrecognizable or irrelevant inputs ( Nguyen et al., 2015 ;  Moosavi-Dezfooli et al., 2017 ). This has serious implications for AI safety ( Amodei & Clark, 2016 ) in real world deployments, where a model could receive inputs that are beyond its training distribution. Detecting examples that are out of the training distribution becomes a viable solution: when encountering such samples, the model could choose to provide low confidence estimates or even abstain from making predictions (Cortes et al., 2017). Density estimation is one approach to detecting out-of-distribution (OoD) samples. A likelihood- based model is trained on the input samples; during evaluation, samples that have low likelihoods are assumed to be out-of-distribution. For high-dimensional inputs (such as images), deep generative models have been able to generate realistic samples as well as achieving good compression capabilities, which indicates high likelihoods on the training distribution (Ballé et al., 2016;  Kingma & Welling, 2013 ;  Kingma & Dhariwal, 2018 ;  van den Oord et al., 2016 ); thus, recent works have considered using deep generative models to detect out-of-distribution samples ( Li et al., 2018 ). However, contrary to popular belief, density estimates by deep generative models are highly inaccurate ( Nalisnick et al., 2018 ). For example, a Glow ( Kingma & Dhariwal, 2018 ) model trained on CIFAR10 gives higher likelihood estimates to SVHN samples than CIFAR10 ones, which makes accurate OoD detection impossible. While alternative statistics based on likelihood estimates have been proposed to alleviate this issue (Choi et al., 2018;  Song et al., 2017 ), they are not able to detect OoD samples consistently. In this paper, we propose a new statistic to detect OoD samples using deep generative models trained with batch normalization ( Ioffe & Szegedy, 2015 ). We argue that using batch normalization not only improves optimization (as argued in ( Kohler et al., 2018 ;  Santurkar et al., 2018 )), but also challenges the i.i.d. assumption underlying typical likelihood-based objectives. We show in that the training objective of generative models with batch normalization can be interpreted as maximum pseudo-likelihood over a different joint distribution that does not assume data in the same batch are i.i.d. samples. Empirically, we demonstrate that over this joint distribution, the estimated likelihood of a batch of OoD samples is much lower than that of in-distribution samples. This allows us to propose a permutation test which outperforms existing methods by a significant margin without modifying how the underlying generative model is trained. In particular, we achieve near-perfect performance even on cases such as Fashion MNIST vs. KMNIST, where the likelihood distributions for single samples are nearly identical (see  Figure 1 , left). While generative models trained with BatchNorm Under review as a conference paper at ICLR 2020 might not provide state-of-the-art likelihood numbers on the test set, it remains competitive and could be more suited for the OoD detection problem ( Kingma & Dhariwal, 2018 ;  Nalisnick et al., 2018 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a big-little dual-module inference method to reduce memory access and computation cost for recurrent neural networks (RNNs) in latency-sensitive scenarios. The method leverages the error resilience of non-linear activation functions in RNNs to reduce the expensive data access and computation of the big module, and uses a parameterized little module to approximate the big module in the insensitive region. Experiments on language modeling and neural machine translation show that our method can achieve 1.54x to 1.75x speedup with negligible impact on model quality.",
        "Abstract": "Using Recurrent Neural Networks (RNNs) in sequence modeling tasks is promising in delivering high-quality results but challenging to meet stringent latency requirements because of the memory-bound execution pattern of RNNs. We propose a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup RNN inference. Leveraging the error-resilient feature of nonlinear activation functions used in RNNs, we propose to use a lightweight little module that approximates the original RNN layer, which is referred to as the big module, to compute activations of the insensitive region that are more error-resilient. The expensive memory access and computation of the big module can be reduced as the results are only used in the sensitive region. Our method can reduce the overall memory access by 40% on average and achieve 1.54x to 1.75x speedup on CPU-based server platform with negligible impact on model quality.",
        "Introduction": "  INTRODUCTION Recurrent Neural Networks (RNNs) play a critical role in many natural language processing (NLP) tasks, such as machine translation ( Bahdanau et al., 2014 ;  Wu et al., 2016 ), speech recognition ( Graves et al., 2013 ;  He et al., 2019 ), and speech synthesis ( Wang et al., 2017 ), owing to the capabil- ity of modeling sequential data. These RNN-based services deployed in both data-center and edge devices often process inputs in a streaming fashion, which demands a real-time interaction. For instance, in cloud-based translation tasks, multiple requests need to be served with very stringent latency limit, where inference runs concurrently and individually ( Park et al., 2018 ). For on-device speech recognition as an automated assistant, latency is the primary concern to pursue a fast response ( He et al., 2019 ). However, serving RNN-based models in latency-sensitive scenarios is challenging due to the low data reuse, and thus low resource utilization as memory-bound General Matrix-Vector multiplica- tion (GEMV) is the core compute pattern of RNNs. Accessing weight matrix from off-chip memory is the bottleneck of GEMV-based RNN execution as the weight data almost always cannot fit in on-chip memory. Moreover, accessing weights repeatedly at each time-step, especially in sequence- to-sequence models, makes the memory-bound problem severer. Subsequently, the on-chip com- puting resources would be under-utilized. Although batching is a walk-around for low-utilization, using a large batch size is not favored in latency-sensitive scenarios such as speech recognition and translation. In essence, the RNN inference is not a simple GEMV. With non-linearity followed the GEMV op- eration as the activation functions, the RNN inference operation is \"activated\" GEMV. These non- linear activation functions as used in neural networks bring error resilience. As shown in  Figure 1 , sigmoid and tanh functions in Gated RNNs such as Long Short-Term Memory (LSTM) ( Hochreiter & Schmidhuber, 1997 ) and Gated Recurrent Unit (GRU) ( Cho et al., 2014 ) have insensitive regions - green shaded regions - where the outputs are saturated and resilient to errors in pre-activation accu- mulated results. In other words, not all computations in RNNs need to be accurate. Can we leverage this error resilience in RNNs to reduce the memory access and eventually achieve speedup? To this end, we propose a big-little dual-module inference that regarding the original RNN layer as the big module, and use a parameterized little module to approximate the big module to help reduce redundant weight accesses. The philosophy of dual-module inference is using approximated results computed by the memory-efficient little module in the insensitive region, and using accurate Under review as a conference paper at ICLR 2020 results computed by the memory-intensive big module in the sensitive region. For this reason, the final outputs are the mixture of the big-little module. With the memory-efficient little module computes for the insensitive region, we can reduce the expensive data access and computation of the big module and thus reduce overall memory access and computation cost. The (in)sensitive region is dynamically determined using the little module results. Because of the error resilience, using approximated results in the insensitive region has a negligible impact on the overall model quality but creates a significant acceleration potential. Given the trade-off between accuracy and efficiency, the little module needs to be sufficiently accu- rate while being as much lightweight as possible. To achieve this, we first use a dimension reduction method - random projection - to reduce the parameter size of the little module and thus reducing data accesses. Then, we quantize the weights of the little module to lower the overhead further. Because we only need the little module outputs in the insensitive region that is error-resilient, we can afford aggressively low bit-width. Compared with common sparsification schemes, our hybrid approach avoids indexing overheads and therefore successfully achieves practical speedup. We evaluate our method on language modeling and neural machine translation using RNN-based models and measure the performance, i.e., wall-clock execution time, on CPU-based server plat- form. With overall memory access data reduced by 40% on average, our method can achieve 1.54x to 1.75x speedup with negligible impact on model quality.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel few-shot classification algorithm to generalize across domains beyond the common assumption of meta-training and meta-testing within a single domain. The approach constructs a pool of multiple models and learns to select the best one given a novel task through meta-training over various domains. The model pool is designed to keep important domain-invariant features while having representational diversity as a whole. Experimental results show that the proposed selection scheme outperforms other state-of-the-art algorithms in few-shot classification tasks from many different domains, including previously unseen domains.",
        "Abstract": "Although few-shot learning research has advanced rapidly with the help of meta-learning, its practical usefulness is still limited because most of the researches assumed that all meta-training and meta-testing examples came from a single domain. We propose a simple but effective way for few-shot classification in which a task distribution spans multiple domains including previously unseen ones during meta-training.\nThe key idea is to build a pool of embedding models which have their own metric spaces and to learn to select the best one for a particular task through multi-domain meta-learning. This simplifies task-specific adaptation over a complex task distribution as a simple selection problem rather than modifying the model with a number of parameters at meta-testing time. Inspired by common multi-task learning techniques, we let all models in the pool share a base network and add a separate modulator to each model to refine the base network in its own way. This architecture allows the pool to maintain representational diversity and each model to have domain-invariant representation as well. \nExperiments show that our selection scheme outperforms other few-shot classification algorithms when target tasks could come from many different domains. They also reveal that aggregating outputs from all constituent models is effective for tasks from unseen domains showing the effectiveness of our framework.",
        "Introduction": "  INTRODUCTION Few-shot learning in the perspective of meta-learning aims to train models which can quickly solve novel tasks or adapt to new environments with limited number of examples. In case of few-shot classification, models are usually evaluated on a held-out dataset which does not have any common class with the training dataset. In the real world, however, we often face harder problems in which novel tasks arise arbitrarily from many different domains even including previously unseen ones. In this study, we propose a more practical few-shot classification algorithm to generalize across do- mains beyond the common assumption, i.e., meta-training and meta-testing within a single domain. Our approach to cover a complex multi-domain task distribution is to construct a pool of multi- ple models and learn to select the best one given a novel task through meta-training over various domains. This recasts task-specific adaption across domains as a simple selection problem, which could be much easier than manipulating high-dimensional parameters or representations of a single model to adapt to a novel task. Furthermore, we enforce all models to share some of the parameters and train per-model modula- tors with model-specific parameters on top of that. By doing so, each model could keep important domain-invariant features while the model pool has representational diversity as a whole without a significant increase of model parameters. We train and test our algorithms on various image classification datasets with different characteris- tics. Experimental results show that the proposed selection scheme outperforms other state-of-the- art algorithms in few-shot classification tasks from many different domains without being given any knowledge of the domain which the task belongs to. We also show that even few-shot clas- sification tasks from previously unseen domains, i.e., domains which have never appeared during meta-training, can be done successfully by averaging outputs of all models.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the security issues of deep neural networks, such as adversarial examples, and the methods proposed to make them robust. Adversarial training is one of the methods used to make neural networks robust, however, it is not compatible with recent fatal attack methods due to their high computational complexity. Fast gradient sign and projected gradient descent are used for practical adversarial training, but they might be susceptible to future attackers. This paper proposes a new method for generating adversarial examples that is not fixed in adversarial training, making the defense procedure more effective.",
        "Abstract": "Despite the remarkable development of recent deep learning techniques, neural networks are still vulnerable to adversarial attacks, i.e., methods that fool the neural networks with perturbations that are too small for human eyes to perceive. Many adversarial training methods were introduced as to solve this problem, using adversarial examples as a training data. However, these adversarial attack methods used in these techniques are fixed, making the model stronger only to attacks used in training, which is widely known as an overfitting problem. In this paper, we suggest a novel adversarial training approach. In addition to the classifier, our method adds another neural network that generates the most effective adversarial perturbation by finding the weakness of the classifier. This perturbation generator network is trained to produce perturbations that maximize the loss function of the classifier, and these adversarial examples train the classifier with a true label. In short, the two networks compete with each other, performing a minimax game. In this scenario, attack patterns created by the generator network are adaptively altered to the classifier, mitigating the overfitting problem mentioned above. We theoretically proved that our minimax optimization problem is equivalent to minimizing the adversarial loss after all. Beyond this, we proposed an evaluation method that could accurately compare a wide-range of adversarial algorithms. Experiments with various datasets show that our method outperforms conventional adversarial algorithms. ",
        "Introduction": "  INTRODUCTION Deep learning has shown the impressive performance in all areas of artificial intelligence, such as image classification and speech recognition ( Hinton et al., 2012 ;  Krizhevsky et al., 2012 ). These ad- vances lead to a broad application of deep neural networks in various real-life tasks. There are still, however, severe security issues such as adversarial examples, which hinder the use of machine learn- ing system until a complete defense is constructed against multiple adversarial attacks. Adversarial examples are data samples that are close to real data samples, which cause a given neural network to misclassify. The basic idea of adversarial examples is to find a sample that increases the loss value of a neural network in the neighborhood of training data ( Szegedy et al., 2014 ). The perturbation on the original training data is so small that it makes the adversarial examples indistinguishable from the original examples. Many authors proposed methods that make neural networks robust to adversarial examples ( Papernot et al., 2016 ;  Goodfellow et al., 2015 ;  Szegedy et al., 2014 ;  Miyato et al., 2016 ). One of the methods is an adversarial training, which re-trains the neural network with adversarial examples generated by adversarial attacks. Adversarial training with powerful attacks would guarantee robustness, but the recent fatal attack methods ( Szegedy et al., 2014 ;  Papernot et al., 2016 ;  Carlini & Wagner, 2017 ;  Moosavi-Dezfooli et al., 2016 ) require high computational complexity because of their iterative optimization. Therefore, they are not compatible with adversarial training. Methods that quickly produce adversarial examples, such as fast gradient sign ( Goodfellow et al., 2015 ) or projected gradient descent ( Kurakin et al. (2017) ;  Madry et al. (2018) ), have been used for practical adversarial training. While the above adversarial training methods are empirically successful, they might be susceptible to future attackers, and this makes the defense procedure useless. If an algorithm for generating an adversarial example is fixed in adversarial training, the network could overfit to the specific algorithm.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of a trusted execution environment (TEE) and trusted processors, such as Intel Software Guard Extension (SGX), to enable collaborative training and prediction of deep neural networks (DNNs) while preserving the privacy of data contributors. A baseline approach combining SCONE and Caffe is proposed and evaluated, demonstrating the potential of using SGX for training and prediction. The paper also examines the performance of the baseline approach and provides insight into possible improvements.",
        "Abstract": "Before we saw worldwide collaborative efforts in training machine-learning models or widespread deployments of prediction-as-a-service, we need to devise an efﬁcient privacy-preserving mechanism which guarantees the privacy of all stakeholders (data contributors, model owner, and queriers). Slaom (ICLR ’19) preserves privacy only for prediction by leveraging both trusted environment (e.g., Intel SGX) and untrusted GPU. The challenges for enabling private training are explicitly left open – its pre-computation technique does not hide the model weights and fails to support dynamic quantization corresponding to the large changes in weight magnitudes during training. Moreover, it is not a truly outsourcing solution since (ofﬂine) pre-computation for a job takes as much time as computing the job locally by SGX, i.e., it only works before all pre-computations are exhausted.\n\nWe propose Goten, a privacy-preserving framework supporting both training and prediction. We tackle all the above challenges by proposing a secure outsourcing protocol which 1) supports dynamic quantization, 2) hides the model weight from GPU, and 3) performs better than a pure-SGX solution even if we perform the precomputation online. Our solution leverages a non-colluding assumption which is often employed by cryptographic solutions aiming for practical efﬁciency (IEEE SP ’13, Usenix Security ’17, PoPETs ’19). We use three servers, which can be reduced to two if the pre-computation is done ofﬂine. Furthermore, we implement our tailor-made memory-aware measures for minimizing the overhead when the SGX memory limit is exceeded (cf., EuroSys ’17, Usenix ATC ’19). Compared to a pure-SGX solution, our experiments show that Goten can speed up linear-layer computations in VGG up to 40×, and overall speed up by 8.64× on VGG11.",
        "Introduction": "  INTRODUCTION While deep neural networks (DNN) can produce predictive models with unparalleled performance, its training phase requires enormous data as input. A single data owner may not possess enough data to train a good DNN. Multiple data owners, say, financial institutions, may want to collaborate in training DNNs. Yet, they are often expected to protect the privacy of the data contributors. This dis- courages any collaborative training over global-scale data that is otherwise promising ( Cheng et al., 2019 ). Moreover, to perform prediction using a trained model, queriers need to submit their own private data (e.g., medical history). Meanwhile, the model owners want to protect the confidentiality of the trained model in the prediction phase as well. The exposure of the (parameters of a) model (to queriers or a third-party cloud server) may reveal information about its training data ( Fredrik- son et al., 2015 ), deterring the participation of data contributors. Also, the model itself is of high commercial value. These concerns hinder the deployment of prediction as a service. An increasingly popular approach to ensure privacy is using a trusted execution environment (TEE) ( Cheng et al., 2019 ;  Tramèr & Boneh, 2019 ) and in particular, trusted processors, e.g., In- tel Software Guard Extension (SGX). When a data provider sends some private data to a server equipped with SGX, it can initialize an enclave to receive the data in a confidential and authenti- cated way and subsequently operate on them. Even the untrusted server, who physically owns the enclave, cannot read or tamper the data inside the enclave. This paper investigates the following questions: Can we support DNN training (and prediction) by using SGX and untrusted GPU while still preserving the privacy of all stakeholders? If so, how much speedup do we gain by using GPU? Under review as a conference paper at ICLR 2020 1.1 OUR BASELINE APPROACH: CAFFESCONE  Arnautov et al. (2016)  propose SCONE, a secure container mechanism that allows developers to directly run applications in an SGX enclave with almost zero code change 1 . We combine SCONE with Caffe ( Jia et al., 2014 ), an efficient open-source DNN framework, to build our baseline privacy- preserving DNN framework - CaffeSCONE. Beyond demonstrating what one can get by applying a generic solution that uses SGX (SCONE) for training (not supported by Slalom), our CaffeSCONE implementation enables more benchmarking for insight in possible improvements, which are even- tually achieved by our main result (hence further optimizing it is not our goal). For one, we show (in Section 4.2) that this baseline approach greatly suffers when the enclave's memory limit is reached. Specifically, it invokes a native paging mechanism to swap data in and out, which further requires en/decryption. Also, we found that using more threads and cores cannot improve performance.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel activation decomposition framework for visual explanation of deep metric learning architectures. The proposed method uncovers the point-to-point activation intensity between two given images, which is not explored by existing methods. Experiments are conducted to demonstrate the importance of the partial activation map on several new applications, such as cross-view pattern discovery and interactive retrieval. Additionally, two widely believed arguments about CAM and Grad-CAM are found to be inaccurate.",
        "Abstract": "This work explores the visual explanation for deep metric learning and its applications. As an important problem for learning representation, metric learning has attracted much attention recently, while the interpretation of such model is not as well studied as classification. To this end, we propose an intuitive idea to show where contributes the most to the overall similarity of two input images by decomposing the final activation. Instead of only providing the overall activation map of each image, we propose to generate point-to-point activation intensity between two images so that the relationship between different regions is uncovered. We show that the proposed framework can be directly deployed to a large range of metric learning applications and provides valuable information for understanding the model. Furthermore, our experiments show its effectiveness on two potential applications, i.e. cross-view pattern discovery and interactive retrieval. ",
        "Introduction": "  INTRODUCTION Learning the similarity metrics between arbitrary images is a fundamental problem for a variety of tasks, such as image retrieval ( Oh Song et al. (2016) ), verification ( Schroff et al. (2015) ;  Luo et al. (2019) ), localization ( Hu et al. (2018) ), video tracking (Bertinetto et al. (2016)), etc. Recently the deep Siamese network ( Chopra et al. (2005) ) based framework has become a standard architecture for metric learning and achieves exciting results on a wide range of applications ( Wang et al. (2019) ). However, there are surprisingly few papers conducting visual analyses to explain why the learned similarity of a given image pair is high or low. Specifically, which part contributes the most to the similarity is a straightforward question and the answer can reveal important hidden information about the model as well as the data. Previous visual explanation works mainly focus on the interpretation of deep neural network for clas- sification ( Springenberg et al. (2014) ;  Zhou et al. (2016 ;  2018 );  Fong & Vedaldi (2017) ). Guided back propagation (Guided BP) ( Springenberg et al. (2014) ) has been used for explanation by gen- erating the gradient from prediction to input, which shows how much the output will change with a little change in each dimension of the input. Another representative visual explanation, class activa- tion map (CAM)  Zhou et al. (2016) , generates the heatmap of discriminative regions corresponding to a specific class based on the linearity of global average pooling (GAP) and fully connected (FC) layer. However, the original method only works on this specific architecture configuration and needs retraining for visualizing other applications. Based on the gradient of the last convolutional layer instead of the input, Grad-CAM ( Selvaraju et al. (2017) ) is proposed to generate activation maps for all convolutional neural network (CNN) architectures. Besides, other existing methods explore network ablation ( Zhou et al. (2018) ), the winner-take-all strategy ( Zhang et al. (2018) ), inversion ( Mahendran & Vedaldi (2015) ), and perturbation ( Fong & Vedaldi (2017) ) for visual explanation. Since the verification applications like person re-identification (re-ID) ( Luo et al. (2019) ) usually train metric learning models along with classification, recent work (Yang et al. (2019)) starts to leverage the classification activation map to help improve the overall performance, but the activation map of metric learning is still not well explored. For two given images, a variant of Grad-CAM has been used for visualization of image retrieval ( Gordo & Larlus (2017) ) by computing the gradient from the cosine similarity of the embedding features to the last convolutional layers of both images. However, Grad-CAM only provides the overall highlighted regions of two input images, the rela- tionship between each activated region of two images is yet to be uncovered. Since the similarity is calculated from two images and possibly based on several similar patterns between them, the relationship between these patterns is critical for understanding the model. In this paper, we propose an activation decom- position framework for visual explanation of deep metric learning and explore the relation- ship between each activated region by point-to- point activation response between two images. As shown in  Fig. 1 , the overall activation map of the proposed method is generated by decom- posing the similarity along each image. In this example, the query image (A) has a high acti- vation on both the eyes and mouth areas, but the overall map of the retrieved image (B) only highlights the eyes. It is actually hard to under- stand how the model works only based on the overall maps. For image B, the mouth region (green point) has a low activation which means the activation between the mouth and the whole image A is low compared to the overall activa- tion (similarity). However, by further decom- posing this activation (green point) along image A, that is to give the activation between the mouth region of image B and each position in image A, the resulting partial (or point-specific) activation map (green box) reveals that the mouth region of image B still has a high response on the mouth region of image A. This partial activation map can be generated for each pixel, which renders the point-to-point activation intensity representing the relationship between regions in both images, e.g. eye-to-nose or mouth-to-mouth. The partial activation map provides much more refined information about the model which is crucial for explanation. The contributions are summarized as follows. • We propose a novel explanation framework for deep metric learning architectures and it may serve as an analysis tool for a host of applications, e.g. face recognition, person re-ID. • The proposed method uncovers the point-to-point activation intensity which is not explored by existing methods. Our experiments further show the importance of the partial activation map on several new applications, i.e. cross-view pattern discovery and interactive retrieval. • Our analysis suggests that two widely believed arguments (Section 2.1, 4) about CAM and Grad- CAM are inaccurate.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes VAENAS, a learnable and interactive architecture sampling module based on Variational Autoencoders (VAE) for Neural Architecture Search (NAS). VAENAS is applied to two mainstream NAS approaches, one-shot and gradient-based, and is shown to improve performance on various search spaces for image recognition tasks on CIFAR-10 and ImageNet datasets. VAENAS combined with one-shot approach achieves 2.26% test error on CIFAR-10 and 75.8% accuracy on ImageNet, outperforming state-of-the-art NAS methods. On a Shufflenet-like search space, VAENAS combined with one-shot achieves 77.4% top-1 accuracy with 365M FLOPs on ImageNet classification, outperforming state-of-the-art Efficient-B0 by 1.1% with 6.5% less computational complexity.",
        "Abstract": "Neural Architecture Search (NAS) aims at automatically finding neural network architectures within an enormous designed search space. The search space usually contains billions of network architectures which causes extremely expensive computing costs in searching for the best-performing architecture. One-shot and gradient-based NAS approaches have recently shown to achieve superior results on various computer vision tasks such as image recognition. With the weight sharing mechanism, these methods lead to efficient model search. Despite their success, however, current sampling methods are either fixed or hand-crafted and thus ineffective. In this paper, we propose a learnable sampling module based on variational auto-encoder (VAE) for neural architecture search (NAS), named as VAENAS, which can be easily embedded into existing weight sharing NAS framework, e.g., one-shot approach and gradient-based approach, and significantly improve the performance of searching results. VAENAS generates a series of competitive results on CIFAR-10 and ImageNet in NasNet-like search space. Moreover, combined with one-shot approach, our method achieves a new state-of-the-art result for ImageNet classification model under 400M FLOPs with 77.4\\% in ShuffleNet-like search space. Finally, we conduct a thorough analysis of VAENAS on NAS-bench-101 dataset, which demonstrates the effectiveness of our proposed methods.\n",
        "Introduction": "  INTRODUCTION Deep neural networks have greatly pushed the frontier of various influential applications by design- ing novel neural architectures ( Krizhevsky et al. (2012) ; Goodfellow et al. (2014);  He et al. (2016) ). Automatic model design of neural network architectures without human intervention, known as neu- ral architecture search (NAS), has drawn much attention of the community recently. It has resulted in state-of-the-art performance in the domain of image recognition ( Zoph et al. (2018) ;  Real et al. (2019) ), object detection ( Ghiasi et al. (2019) ;  Chen et al. (2019) ) and semantic segmentation ( Liu et al. (2019) ). Generally, the magnitude of search space for NAS tasks is enormous. For example, NasNet ( Zoph et al., 2018 ) presents a search space with 6 × 10 9 possible cells. Searching on such huge designed space cost 2400 GPU days. Weight sharing mechanism has shown to be a promising avenue for efficient NAS. Latest algorithms on efficient NAS fall into two categories: one-shot approaches ( Bender et al. (2018) ) and gradient-based approaches ( Liu et al. (2018b) ). In one-shot approaches, prior works focus on adopting a fixed sampling strategy ( Guo et al. (2019) ;  Bender et al. (2018) ;  Chu et al. (2019) ). In gradient-based approaches, search is typically performed without sampling proce- dure ( Liu et al. (2018b) ;  Xie et al. (2018) ) or hand-crafted sampling ( Liu et al. (2018a) ). Despite the success of these NAS methods on various benchmarks, however, these sampling approaches do not interactively learn the architecture distribution as the search process goes along, which makes the sampling procedure ineffective. In this paper, we propose a learnable and interactive architecture sampling module based on VAE for NAS, named as VAENAS. There are two advantages of VAENAS: 1) it learns the good-performing architecture distribution which could be used to reduce search space. 2) it can be embedded into existing NAS framework and improve the performance of current NAS methods. After developing the generic VAENAS approach, we study in detail the application of VAENAS module via two mainstream NAS approaches: one-shot approach ( Brock et al. (2017) ;  Guo et al. Under review as a conference paper at ICLR 2020  (2019)) and gradient-based approach ( Liu et al. (2018b) ;  Cai et al. (2018) ), both of which have obtained state-of-the-art performance on neural architecture search tasks. We show how VAENAS module can be embedded in both approaches and make performance improvement consistently. We validate VAENAS with various search space on two benchmark datasets (CIFAR-10 and Ima- geNet) for image recognition. On NASNET-like search space, we apply VAENAS on both gradient- based methods and one-shot methods. Specifically, combined with one-shot approach, VAENAS achieves 2.26% test error on CIFAR-10 and 75.8% accuracy on ImageNet, outperforming state-of- the-art NAS methods. On a Shufflenet-like search space, VAENAS combined with one-shot achieves 77.4% top-1 accuracy with 365M FLOPs on ImageNet classification, outperforming state-of-the-art Efficient-B0 by 1.1% with 6.5% less computational complexity. Finally, we perform a thorough analysis of VAENAS on NAS-101 benchmark, to show the effectiveness of our proposed architec- ture sampling methods.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a technique for generating synthetic data with properties similar to the original data, called model compatibility. Generative Adversarial Networks (GANs) are the most popular family of generative algorithms for this purpose, and have been used for applications such as image generation, style transfer, image processing, time series, text, point clouds, voxels, and tabular data.",
        "Abstract": "Generative Adversarial Networks (GANs) is a powerful family of models that learn an underlying distribution to generate synthetic data. Many existing studies of GANs focus on improving the realness of the generated image data for visual applications, and few of them concern about improving the quality of the generated data for training other classifiers---a task known as the model compatibility problem. As a consequence, existing GANs often prefer generating `easier' synthetic data that are far from the boundaries of the classifiers, and refrain from generating near-boundary data, which are known to play an important roles in training the classifiers. To improve GAN in terms of model compatibility, we propose Boundary-Calibration GANs (BCGANs), which leverage the boundary information from a set of pre-trained classifiers using the original data. In particular, we introduce an auxiliary Boundary-Calibration loss (BC-loss) into the generator of GAN to match the statistics between the posterior distributions of original data and generated data with respect to the boundaries of the pre-trained classifiers. The BC-loss is provably unbiased and can be easily coupled with different GAN variants to improve their model compatibility. Experimental results demonstrate that BCGANs not only generate realistic images like original GANs but also achieves superior model compatibility than the original GANs.",
        "Introduction": "  INTRODUCTION The success of machine learning relies on not only the advances of different models (e.g. deep learning) but also data with sufficient quality and quantity. Nowadays, companies spend tremendous efforts and expense collecting data to build their products. To better solve complicated real-world problems with public or third-party machine learning experts, many companies now needs release some data sets for competitions (e.g. Kaggle) or proof-of-concept purposes. However, considering the costs of collecting data, companies may not be willing to release the dataset if possible. As a result, a technique which can generate synthetic data with properties similar to the original data is in demand. To be specific, we are looking for generating a dataset with the property that machine learning models trained on the generated dataset can exhibit similar performance to ones trained on the original data. This property is called model compatibility ( Park et al., 2018 ) or machine learning efficacy ( Xu et al., 2019 ). The organizations can share the generated data with high model compatibility to the public and enjoy the solution derived from it without leaking the real dataset. When it comes to data generation, generative adversarial networks (GANs,  Goodfellow et al. 2014 ) is the most popular family of generative algorithms because of its impressive performance on gen- erating realistic images ( Karras et al., 2018 ). In GANs, the generator is trained via minimizing a neural network (discriminator) defined probability divergence ( Goodfellow et al., 2014 ;  Arjovsky et al., 2017 ;  Nowozin et al., 2016 ). In addition to image generation, GANs are also widely used in other applications, such as style transfer ( Isola et al., 2017 ;  Zhu et al., 2017 ;  Kim et al., 2017 ) and image processing ( Pathak et al., 2016 ;  Ledig et al., 2017 ;  Chang et al., 2017 ), and generating differ- ent types of data, including time series ( Luo et al., 2018 ;  Chang et al., 2019 ), text ( Yu et al., 2017 ;  Press et al., 2017 ), point clouds ( Li et al., 2018 ), voxels ( Wu et al., 2016 ) and tabular data ( Park et al., 2018 ;  Xu et al., 2019 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Amortized Nesterov's Momentum, a variant of Nesterov's momentum, which is designed to improve the robustness and reduce the iteration complexity of stochastic gradient descent. Through empirical evaluation and convex analysis, this paper identifies the benefits of the new variant, such as improved robustness, reduced iteration complexity, and faster convergence rate in the early stage of training. Additionally, the paper provides an understanding of the relation between convergence rate and the amortization length, and proves that the methods converge for any valid choice of the amortization length in the convex setting.",
        "Abstract": "Stochastic Gradient Descent (SGD) with Nesterov's momentum is a widely used optimizer in deep learning, which is observed to have excellent generalization performance. However, due to the large stochasticity, SGD with Nesterov's momentum is not robust, i.e., its performance may deviate significantly from the expectation. In this work, we propose Amortized Nesterov's Momentum, a special variant of Nesterov's momentum which has more robust iterates, faster convergence in the early stage and higher efficiency. Our experimental results show that this new momentum achieves similar (sometimes better) generalization performance with little-to-no tuning. In the convex case, we provide optimal convergence rates for our new methods and discuss how the theorems explain the empirical results. ",
        "Introduction": "  INTRODUCTION In recent years, Gradient Descent (GD) ( Cauchy, 1847 ) and its variants have been widely used to solve large scale machine learning problems. Among them, Stochastic Gradient Descent (SGD) ( Robbins & Monro, 1951 ), which replaces gradient with an unbiased stochastic gradient estimator, is a popular choice of optimizer especially for neural network training which requires lower precision.  Sutskever et al. (2013)  found that using SGD with Nesterov's momentum ( Nesterov, 1983 ; 2013b), which was originally designed to accelerate deterministic convex optimization, achieves substantial speedups for training neural networks. This finding essentially turns SGD with Nesterov's momen- tum into the benchmarking method of neural network design, especially for classification tasks ( He et al., 2016b ;a;  Zagoruyko & Komodakis, 2016 ;  Huang et al., 2017 ). It is observed that in these tasks, the momentum technique plays a key role in achieving good generalization performance. Adaptive methods ( Duchi et al., 2011 ;  Kingma & Ba, 2015 ;  Tieleman & Hinton, 2012 ;  Reddi et al., 2018 ), which are also becoming increasingly popular in the deep learning community, diagonally scale the gradient to speed up training. However,  Wilson et al. (2017)  show that these methods always generalize poorly compared with SGD with momentum (both classical momentum ( Polyak, 1964 ) and Nesterov's momentum). In this work, we introduce Amortized Nesterov's Momentum, which is a special variant of Nes- terov's momentum. From users' perspective, the new momentum has only one additional integer hyper-parameter m to choose, which we call the amortization length. Learning rate and momentum parameter of this variant are strictly aligned with Nesterov's momentum and by choosing m = 1, it recovers Nesterov's momentum. This paper conducts an extensive study based on both empirical evaluation and convex analysis to identify the benefits of the new variant (or from users' angle, to set m apart from 1). We list the advantages of Amortized Nesterov's Momentum as follows: • Increasing m improves robustness 1 . This is an interesting property since the new momentum not only provides acceleration, but also enhances the robustness. We provide an understanding of this property by analyzing the relation between convergence rate and m in the convex setting. • Increasing m reduces (amortized) iteration complexity. • A suitably chosen m boosts the convergence rate in the early stage of training and produces comparable final generalization performance. Under review as a conference paper at ICLR 2020 • It is easy to tune m. The performances of the methods are stable for a wide range of m and we prove that the methods converge for any valid choice of m in the convex setting. • If m is not too large, the methods obtain the optimal convergence rate in general convex setting, just like Nesterov's method. The new variant does have some minor drawbacks: it requires one more memory buffer, which is acceptable in most cases, and it shows some undesired behaviors when working with learning rate schedulers, which can be addressed by a small modification. Considering these pros and cons, we believe that the proposed variant can benefit many large-scale deep learning tasks. Our high level idea is simple: the stochastic Nesterov's momentum can be unreliable since it is pro- vided only by the previous stochastic iterate. The iterate potentially has large variance, which may lead to a false momentum that perturbs the training process. We thus propose to use the stochastic Nesterov's momentum based on several past iterates, which provides robust acceleration. In other words, instead of immediately using an iterate to provide momentum, we put the iterate into an \"amortization plan\" and use it later.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel view of adaptive algorithms for training deep neural networks (DNNs) by revisiting the direction angle between the descent direction and the momentum. It provides an interpretation of the fundamental mechanism of adaptive algorithms, such as Adagrad, Adadelta, RMSProp, Adam, and Amsgrad. It also discusses the theoretical regret analysis of these online learning algorithms in convex objective settings, and the generalization results of adaptive methods compared to SGD.",
        "Abstract": "In spite of the slow convergence, stochastic gradient descent (SGD) is still the most practical optimization method due to its outstanding generalization ability and simplicity. On the other hand, adaptive methods have attracted much more attention of optimization and machine learning communities, both for the leverage of life-long information and for the deep and fundamental mathematical theory. Taking the best of both worlds is the most exciting and challenging question in the field of optimization for machine learning. \n\nIn this paper, we take a small step towards such ultimate goal. We revisit existing adaptive methods from a novel point of view, which reveals a fresh understanding of momentum. Our new intuition empowers us to remove the second moments in Adam without the loss of performance. Based on our view, we propose a new method, named acute adaptive momentum (Acutum). To the best of our knowledge, Acutum is the first adaptive gradient method without second moments. Experimentally, we demonstrate that our method has a faster convergence rate than Adam/Amsgrad, and generalizes as well as SGD with momentum. We also provide a convergence analysis of our proposed method to complement our intuition. ",
        "Introduction": "  INTRODUCTION With the rapid development of neural network architectures ( Goodfellow et al., 2016 ), training algo- rithms have attracted much more attention in the modern machine learning community. Due to the network size and data amount increasing dramatically, calculating the full gradient of data and im- plementing the full gradient descent (GD) become computationally expensive. Therefore, stochastic gradient descent (SGD) ( Robbins & Monro, 1951 ) becomes the most practical optimization method for training deep neural networks (DNNs). In each iteration, SGD samples mini-batch data and computes the gradient corresponding to the mini-batch. Although SGD is computationally afford- able, it needs a mechanism of fine-tune the learning rate, e.g., linear decay or exponential decay, to converge efficiently. In fact, it is pretty brittle to tune the learning rate dynamically with SGD. To release the learning rate tuning burden of SGD and accelerate its convergence, several adaptive variants of SGD were proposed, including Adagrad ( Duchi et al., 2011 ), Adadelta ( Zeiler, 2012 ), RMSProp ( Hinton et al., 2012 ), Adam ( Kingma & Ba, 2014 ), etc. The descent direction is element- wise automatically adapted by the first moment and the second moment with an exponential average of gradients. Such a descent direction has achieved great improvements in practice, while the funda- mental interpretation is still unclear. Besides, the theoretical regret analysis of these online learning algorithms has become completed gradually in convex objective settings. Due to the fast decay of exponential moving average, Adam cannot converge even in simple convex cases as shown in ( Reddi et al., 2019 ). Amsgrad ( Reddi et al., 2019 ) addressed this issue by keeping an extra non-decreasing sequence to buffer the fast decay. On the other hand, although adaptive methods are easy to use and fast to converge, the generaliza- tion results cannot be as good as SGD ( Wilson et al., 2017 ). Various works ( Chen & Gu, 2018 ;  Luo et al., 2019 ) have been proposed to make algorithms not only converge faster but also has good generalization. However, these algorithms still inherit the second moment following Adam. Differ- ent from existing works, we provide a novel view for this problem by revisiting adaptive algorithms Through the view of direction angle between the descent direction and the momentum, we are able to demonstrate a fresh understanding of momentum.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes an Annealing Mechanism for Adversarial Training Acceleration (Amata) to reduce the computational burden associated with the inner maximization steps of adversarial training. Amata gradually increases the number of iterations and decreases the step size for the inner maximization, leading to comparable or even better robustness with much reduced computational overhead. We also develop a general formulation of adversarial training as an optimal control problem, from which an approximate optimality criterion is derived based on the Pontryagin's maximum principle. Experiments demonstrate that algorithms derived from this criterion lead to acceleration.",
        "Abstract": "Despite of the empirical success in various domains, it has been revealed that deep neural networks are vulnerable to maliciously perturbed input data that much degrade their performance. This is known as adversarial attacks. To counter adversarial attacks, adversarial training formulated as a form of robust optimization has been demonstrated to be effective. However, conducting adversarial training brings much computational overhead compared with standard training. In order to reduce the computational cost, we propose a simple yet effective modification to the commonly used projected gradient descent (PGD) adversarial training by increasing the number of adversarial training steps and decreasing the adversarial training step size gradually as training proceeds. We analyze the optimality of this annealing mechanism through the lens of optimal control theory, and we also prove the convergence of our proposed algorithm. Numerical experiments on standard datasets, such as MNIST and CIFAR10, show that our method can achieve similar or even better robustness with around 1/3 to 1/2 computation time compared with PGD.",
        "Introduction": "  INTRODUCTION Recently, the revival of deep neural networks has led to breakthroughs in various fields, including computer vision, natural language processing, game playing, etc. Despite these advancements, deep neural networks were found to be vulnerable to malicious perturbations on the original input data. While the perturbations remain almost imperceptible to humans, they can lead to wrong predictions over the perturbed examples ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ;  Akhtar & Mian, 2018 ). These maliciously crafted examples are known as adversarial examples, which have caused serious concerns over the reliability and security of deep learning systems, particularly when deployed in the life-critical scenarios, such as autonomous driving systems and health/medical domains. Several defense mechanisms have been proposed, such as input reconstruction ( Meng & Chen, 2017 ;  Song et al., 2018 ), input encoding ( Buckman et al., 2018 ), and adversarial training ( Goodfellow et al., 2014 ;  Tramèr et al., 2017 ;  He et al., 2017 ;  Madry et al., 2017 ). Among these methods, adversarial training is the most effective defense method so far. Adversarial training can be posed as a robust optimization problem ( Ben-Tal & Nemirovski, 1998 ), where a min-max optimization problem is solved ( Madry et al., 2017 ;  Kolter & Wong, 2017 ). For example, given a C-class dataset S = {(x 0 i , y i )} n i=1 with x 0 i ∈ R d as a normal or clean example in the d-dimensional input space and y i ∈ R C as its associated one-hot label, the objective of adversarial training is to solve the following min-max optimization problem: min θ 1 N N i=1 max zi−x 0 i ≤ (h θ (z i ), y i ) (1) where h θ : R d → R C is the deep neural network (DNN) function, is the loss function and is the maximum perturbation constraint. The inner maximization problem is to find an adversarial example x i , within the -ball around a given normal example x 0 i that maximizes the classification loss . On the other hand, the outer minimization problem is to find model parameters that minimizes the loss on the adversarial examples {x i } n i=1 that are generated from the inner maximization. The inner maximization problem is commonly solved by projected gradient descent (PGD). PGD perturbs a normal example x 0 by iteratively updating it in the steepest ascent direction for a total Under review as a conference paper at ICLR 2020 of K times. Each ascent step is modulated by a small step size and a projection step back onto the -ball of x 0 to prevent the updated value fall outside the -ball of x 0 ( Madry et al., 2017 ): x k = x k−1 + α · sign(∇ x (h θ (x k−1 ), y) (2) where α is the step size, (·) is the orthogonal projection function onto {x : x 0 − x ≤ }, and x k is the adversarial example at k-th step. However, a major problem prohibiting adversarial training to be practically applicable is the huge computational burden associated with the inner maximization steps: we need to iteratively solve the inner maximization problem to find good adversarial examples for DNN to be robust. Recently, to accelerate adversarial training, a few methods have been proposed. For example, YOPO estimat- ed the gradient on the input by only propagating the first layer ( Zhang et al., 2019a ), and parallel adversarial training utilized multiple graphics computation units (GPUs) for acceleration ( Bhat & Tsipras, 2019 ;  Shafahi et al., 2019 ). A common drawback of these methods are their implementa- tion complexity or their need for multiple GPUs for acceleration. On the other hand, an empirical observation made by ( Wang et al., 2019 ), indicated that we might not need to find good solutions to the inner maximization at the initial stages of adversarial training to achieve even better robustness. In line of such observations, in this paper we propose a simple yet effective Annealing Mechanism for Adversarial Training Acceleration, which we call Amata, that gradually controls the degree of which the inner maximization is solved as the training proceeds. This annealing algorithm only takes 1/3 to 1/2 the time to achieve comparable or even better robustness with the addition of only two lines of code. To motivate the present approach, we develop a general formulation of adversarial training as an optimal control problem, from which an approximate optimality criterion can be derived based on the Pontryagin's maximum principle. This criterion is related to but different from the empirical FOSC criterion ( Wang et al., 2019 ) that improves the robustness of adversarial training by determining the number of adversarial training steps according to the empirical criterion. We will also demonstrate in our experiments that unlike FOSC, algorithms derived from our proposed criterion leads to acceleration. Our contributions are as follows: 1. We propose an adversarial training algorithm, Amata, that gradually increases the number of iterations and decreases the step size for the inner maximization. We show that compara- ble or even better robustness can be achieved with much reduced computational overhead. We also prove the convergence of our algorithm. 2. To obtain more theoretical insight, we develop a general formulation of adversarial training subject to hyper-parameters in the inner maximization loop as an optimal control problem. This includes the PGD-based methods considered in this work, but potentially encompasses a larger class of annealed or adaptive adversarial training algorithms. 3. Using the Pontryagin's maximum principle from optimal control theory, we provide a prin- cipled criterion to measure the near-term optimality of an annealing schedule for inner max- imization, thereby guiding and validating our proposed algorithm in balancing the trade-off between robustness and computational cost.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents DeepReduce, a technique for cost-effective estimation of the performance of a deep learning (DL) model. DeepReduce can significantly reduce the amount of testing data required for testing a DL model while still achieving comparable performance as the whole testing data achieves. The proposed approach formulates the input reduction problem as a multi-objective optimization problem, which takes into account the efficiency, completeness, and effectiveness of testing in order to make the performance estimation more reliable. An extensive empirical evaluation is conducted to investigate the performance of DeepReduce on real-world DL models, and the results have confirmed that DeepReduce can significantly reduce DL testing cost by reducing the amount of input data required for testing a DL model.",
        "Abstract": "With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, especially when developers explore alternative designs of DL models and tune the hyperparameters. To reduce testing cost, we propose to use only a selected subset of testing data, which is small but representative enough for quick estimation of the performance of DL models. Our approach, called DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data in order to approximate the distribution between the whole testing data and the selected data leveraging relative entropy minimization.\nExperiments with various DL models and datasets show that our approach can reduce the whole testing data to 4.6\\% on average, and can reliably estimate the performance of DL models. Our approach significantly outperforms the random approach, and is more stable and reliable than the state-of-the-art approach.",
        "Introduction": "  INTRODUCTION In recent years, deep learning (DL) models have been successfully deployed in a variety of appli- cation domains. Erroneous DL models may lead to severe consequences. For example, a defect in a DL-based autonomous driving system resulted in the death of one pedestrian ( Amir, 2018 ). Therefore, sufficient testing for DL models is critical for ensuring system quality. However, the cost for testing deep learning models is not neglectable. For example, DeepFace ( Taigman et al., 2014 ), the face recognition system of Facebook, used about 0.22 million face images for testing.  Tian et al. (2018)  generated 254,221 images for testing a Chauffeur-CNN based autonomous driv- ing model and achieved a neuron coverage (the ratio of activated neurons) of 88%. The popular ImageNet dataset ( Russakovsky et al., 2015 ) contains 100,000 testing images (100 per class) for testing various image classification models. Clearly, it could take a long time to sufficiently test a DL model. Furthermore, in practice, developers always need to evaluate alternative designs of DL models and optimize the models. For example, they need to explore different deep neural network (DNN) structures (such as adding/deleting a layer) or different hyperparameters (such as learning rates and dropout rates), which all lead to increased testing cost. To reduce the testing cost, it is desirable to have an early estimation of the performance of a DL model by using only a small amount of testing data. After the first end-to-end system is established, developers can quickly test the performance of the DL model using a small subset of testing data, and start evaluating the design of the model. If the performance is acceptable, the developers can perform a full-scale testing. Otherwise, the developers can seek alternative designs of the model (e.g., adjust DNN structures or learning algorithms), or further tune the hyper-parameters (such as the learning rate). In this way, developers can quickly obtain an accurate DL model without having to perform costly full-scale testing many times. To achieve an early estimation of the performance of a DL model, the key challenge is to select a small yet effective subset of testing data for testing the model. The selected data should be able to achieve similar testing performance as what the entire testing data achieves, and should be small enough to reduce the testing cost. Random sampling is not an ideal solution as it tends to select more testing data in order to represent the whole testing data.  Li et al. (2019)  recently proposed a method to reduce the number of unlabelled instances required by DNN testing through a carefully Under review as a conference paper at ICLR 2020 designed sampling strategy. However, they require users to specify the number of data instances as the input to their approach. That is, without a proper value of such an input, they have no guarantee that the selected data is sufficient for DNN testing. Furthermore, being based on random sampling, their results could be unstable in different runs. In this paper, we propose DeepReduce, a technique for cost-effective estimation of the performance of a DL model. DeepReduce can significantly reduce the amount of testing data for testing a DL model but can still achieve comparable performance as the whole testing data achieves. We formu- late the input reduction problem in this work as a multi-objective optimization problem: 1) minimize the amount of testing data selected from the whole testing data; 2) maximize the testing adequacy achieved by the selected testing data; and 3) maximize the similarity of output distributions achieved by the selected and the whole testing data. These objectives take into account the efficiency, com- pleteness, and effectiveness of testing in order to make the performance estimation more reliable. Inspired by the usage of structural coverage in conventional software testing, we use neuron cover- age of a DL model to measure its testing adequacy in this work. We use the outputs of the neurons in the last hidden layer of a DL model to represent the output distribution. In order to maximize the similarity between the output distribution achieved by the selected data and the whole testing data, we design a heuristic-based algorithm to guide relative-entropy minimization. To evaluate our approach, we conduct an extensive experiment on six DL models and two datasets. The experiment results show that DeepReduce can reduce the amount of testing data required for a reliable testing from 10,000 to 455 on average, which means that over 95.5% testing cost can be saved by DeepReduce. Compared with the state-of-the-art approach ( Li et al., 2019 ), DeepReduce is more stable and reliable, and can reduce more testing data. The experiment results also shows that DeepReduce can be used in performance estimation in regression scenarios. In summary, we make the following contributions in this work: • A novel approach to cost-effective testing of a DL model by considering three different objectives: efficiency, completeness, and effectiveness. • An extensive empirical evaluation to investigate the performance of our approach on real- world DL models. The results have confirmed that the proposed approach can significantly reduce DL testing cost by reducing the amount of input data required for testing a DL model.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes AutoGrow, a method to automate the discovery of optimal depth for a deep neural network (DNN). AutoGrow starts with the shallowest backbone network and gradually grows sub-modules, stopping once a stopping policy is satisfied. Experiments show that AutoGrow is robust and adapts network depth to various datasets, including MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. AutoGrow also demonstrates high efficiency and scales up to ImageNet. Additionally, random initialization works equally or better than complicated Network Morphism when growing layers. Finally, it is beneficial to rapidly grow layers before a shallower net converges.",
        "Abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.",
        "Introduction": "  INTRODUCTION Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows ( Krizhevsky et al., 2012 ;  Simonyan & Zisserman, 2014 ;  Szegedy et al., 2015 ;  He et al., 2016 ;  Huang et al., 2017 ). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 ( He et al., 2016 ) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 × 56, 28 × 28, 14 × 14 and 7 × 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high compu- tational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures. There are some previous works which add or morph layers to increase the depth in DNNs. Vg- gNet ( Simonyan & Zisserman, 2014 ) and DropIn ( Smith et al., 2016 ) added new layers into shal- lower DNNs; Network Morphism ( Wei et al., 2016 ; 2017;  Chen et al., 2015 ) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net.  Table 1  summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works ap- plied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4.  Figure 1  illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point. Motivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval. Our contributions are: (1) We propose AutoGrow to automate DNN layer growing and depth discov- ery. AutoGrow is very robust. With the same hyper-parameters, it adapts network depth to various datasets including MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. Moreover, AutoGrow can also discover shallower DNNs when the dataset is a subset. (2) AutoGrow demon- strates high efficiency and scales up to ImageNet, because the layer growing is as fast as training a single DNN. On ImageNet, it discovers a new ResNets with better trade-off between accuracy and computation complexity. (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition. A trained neural network g(X0) with learned depth.  Figure 1  gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X 0 ) = l (f M −1 (f M −2 (· · · f 1 (f 0 (X 0 )) · · · ))), where X 0 is an input im- age, M is the number of sub-networks, l(·) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i ∈ R ci×hi×wi . Here, c i is the number of chan- nels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i Under review as a conference paper at ICLR 2020 is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M − 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks ( Simonyan & Zisserman, 2014 ), GoogLeNet ( Szegedy et al., 2015 ), ResNets ( He et al., 2016 ) and DenseNets ( Huang et al., 2017 ). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively. With above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimen- sion reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel deep learning framework, UMN, for training a model on limited annotated data with label noise. UMN includes two major components: a generative framework (VAE) to learn a latent feature representation using all the training data, and a semi-supervised learning component by conditional variational autoencoder to train the classifier. To handle the label noise, a label uncertainty estimation module is integrated, where reliable data contributes more to the model training and noisy data contributes less. Experiments demonstrate that UMN can identify mis-labeled data without any prior knowledge and improve model performance.",
        "Abstract": "In this work, we consider a new problem of training deep neural network on partially labeled data with label noise.  As far as we know, \nthere have been very few efforts to tackle such problems.\nWe present a novel end-to-end deep generative pipeline for improving classifier performance when dealing with such data problems.  We call it \nUncertainty Mining Net (UMN).  \n During the training stage, we utilize all the available data (labeled and unlabeled) to train the classifier via a semi-supervised generative framework. \n During training, UMN estimates the uncertainly of the labels’ to focus on clean data for  learning. More precisely, UMN applies the sample-wise label uncertainty estimation scheme. \n Extensive experiments and comparisons against state-of-the-art methods on several popular benchmark datasets demonstrate that UMN can reduce the effects of label noise and significantly improve classifier performance.",
        "Introduction": "  INTRODUCTION Deep Learning (DL), to learn powerful representations, it usually requires a large amount of training data. However, for many real world problems, it is not always possible to obtain sufficiently large training data. What we can usually get is limited training data with corrupted labels which heavily affect the model performance. Although acquiring large data is not hard, considering the information explosion on the internet, accurate labeling is usually an expensive and error-prone task which involves humans' interaction, especially experts with knowledge in the specific field. Most of the time, we have to build DL models using limited training data with corrupted data labels. It becomes very challenging to apply current popular deep learning frameworks to solve this problem. Training a deep learning model via noisily labeled data is a challenging task (Patrini et al., 2017; Li et al., 2017; Ding et al., 2018; Menon et al., 2015; Goldberger & Ben-Reuven, 2016; Jiang et al., 2017). Most of current explorations for dealing with such problems usually require large amount of labeled data as a prerequisite. It means that in these works, the designed pipelines and evaluations are based on having enough labeled training data, though they may include label noise. These approaches will suffer from the limited size of labeled training data as well as the label noise. As we know, the hierarchical representation learned by deep learning models mainly benefits from the amount of data. The deep learning model can easily overfit to the mislabeled samples especially when the data size is small (Tarvainen & Valpola, 2017; Ren et al., 2018). Unfortunately, there is no general solutions to solve the problem of training on limited annotated data with label noise. If we directly train the deep learning model only with the labeled data, the small portion of labeled training data will limit the learning capability. Meanwhile, the label noise makes it very challenging to utilize the labeled data directly. In this work, our framework can handle these problems simultaneously. UMN includes two major components. The first part is used to learn a latent feature representation via a generative framework (VAE) using all the training data. Then the learned embedding is applied as the input to its subsequent semi-supervised learning component by conditional variational autoencoder to train the classifier. To handle the label noise, we integrate label uncertainty estimation module where the reliable data contributes more to the model training and noisy data contributes less. Though recent works (Hataya & Nakayama, 2019; Ren et al., 2018) discuss such problems, they usually assume to have a separate small clean dataset to begin with. It may not be possible to have an independent clean dataset in real scenarios. Current supervised robust learning approaches cannot Under review as a conference paper at ICLR 2020 be directly adopted to our case since the labeled data is small and not reliable. The limited labeled training data makes the supervised model overfit very easily to the small size of noisy data. This will heavily hurt the model performance. The limited data labels and noise can also affect other related state-of-the-art semi-supervised learning approaches. We will present the comparisons with these approaches in the experiment section. In this paper, our major contributions can be summarized into the following aspects: First, our studied problem is very general in real scenarios: small mislabeled datasets with large amount of unlabeled data, and there are very few existing approaches to target this kind of problems. Second, instead of treating the label noise as the preknowledge, we explicitly use the deep generative architecture to model the noisy labels at the sample level precisely. This has been validated by the improved experimental results as well as the theoretical analysis. Third, we show how to use the moving average model to estimate the sample-wise uncertainty in labels. We have explained it in the theory analysis section and validated this assumption in the experiments. Furthermore, our experiments demonstrate that UMN can also help identify these mis-labeled data without any prior knowledge. Finally, we build one end-to-end deep learning pipeline to train against the noisy labeled data.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes LEX-GAN, a GAN-based layered explainable framework for text-level rumor detection. LEX-GAN is capable of discriminating between real-world and generated samples, and classifying real-world and generated fake samples. It also provides explainability for unverified rumor detection without requiring a verified news database. The layered structure of LEX-GAN avoids the function mixture and boosts the performance, and its mutation detection power is demonstrated by applying it to a gene classification and mutation detection task.",
        "Abstract": "Social media have emerged to be increasingly popular and have been used as tools for gathering and propagating information. However, the vigorous growth of social media contributes to the fast-spreading and far-reaching rumors. Rumor detection has become a necessary defense. Traditional rumor detection methods based on hand-crafted feature selection are replaced by automatic approaches that are based on Artificial Intelligence (AI). AI decision making systems need to have the necessary means, such as explainability to assure users their trustworthiness. Inspired by the thriving development of Generative Adversarial Networks (GANs) on text applications, we propose LEX-GAN, a GAN-based layered explainable rumor detector to improve the detection quality and provide explainability. Unlike fake news detection that needs a previously collected verified news database, LEX-GAN realizes explainable rumor detection based on only tweet-level text. LEX-GAN is trained with generated non-rumor-looking rumors. The generators produce rumors by intelligently inserting controversial information in non-rumors, and force the discriminators to detect detailed glitches and deduce exactly which parts in the sentence are problematic. The layered structures in both generative and discriminative model contributes to the high performance. We show LEX-GAN's mutation detection ability in textural sequences by performing a gene classification and mutation detection task.",
        "Introduction": "  INTRODUCTION Sequential synthetic data generation such as generating text and images that are indistinguishable to human generated data have become an important problem in the era of Artificial Intelligence (AI). Generative models, e.g., Variational AutoEncoders (VAEs) ( Kingma & Welling, 2013 ), Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ), Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) cells ( Hochreiter & Schmidhuber, 1997 ), have shown outstand- ing generation power of fake faces, fake videos, etc. Consequently, we require discriminative models capable of detecting AI-generated fake data with explainability in order to manage the malicious vi- ral information ( Knight, 2019 ). A black box decision maker without explainability that does not shed light into how the decision are made may hence lose the trust of its users. GANs estimate generative models via an adversarial training process ( Goodfellow et al., 2014 ). Powerful real-valued generators have found applications in image and video generation. However, GANs face challenges when the goal is to generate sequences of discrete tokens such as text ( Yu et al., 2017 ). Given the discrete nature of text, backpropagating the gradient from the discriminator to the generator becomes infeasible ( Fedus et al., 2018 ). Training instability is a common problem of GANs, especially those with discrete settings. Unlike image generation, the autoregressive prop- erty in text generation exacerbates the training instability since the loss from discriminator is only observed after a sentence has been generated completely ( Fedus et al., 2018 ). In addition to the recent development in GAN-based text generation, discriminator-oriented GAN- style approaches are proposed for detection and classification applications, such as rumor detection ( Ma et al., 2019 ). Differently from the original generator-oriented GANs, discriminator-oriented GAN-based models take real data as input to the generator instead of noise. Hence fundamentally, Under review as a conference paper at ICLR 2020 the detector may get high performance through the adversarial training technique. Current adversar- ial training strategies improve the robustness against adversarial samples. However, these methods lead to reduction of accuracy when the input samples are clean ( Raghunathan et al., 2019 ). On a related note, social media and micro-blogging have become increasingly popular ( Yazdanifard et al., 2011 ;  Viviani & Pasi, 2017 ). The convenient and fast-spreading nature of micro-blogs fosters the emergence of various rumors. Commercial giants, government authorities, and academic re- searchers take great effort in diminishing the negative impacts of rumors ( Cao et al., 2018 ). Rumor detection has been formulated into a binary classification problem by a lot of researchers. Tradi- tional approaches based on hand-crafted features describe the distribution of rumors ( Castillo et al., 2011 ;  Kwon et al., 2013 ). More recently, Deep Neural Network (DNN)-based methods extract and learn features automatically, and achieve significantly high accuracy on rumor detection ( Chen et al., 2018 ). Generative models have also been used to improve the performance of rumor detectors ( Ma et al., 2019 ). However, binary rumor classification lacks explanation since it only provides a binary result without expressing which parts of a sentence could be the source of problem. The majority of the literature defines rumors as \"an item of circulating information whose veracity status is yet to be verified at the time of posting\" ( Zubiaga et al., 2018 ). Providing explainability for unverified rumor detection is challenging. A related research area works with fake news is more well-studied since fake news has a verified veracity. Attribute information, linguistic features, and semantic meaning of post ( Yang et al., 2019 ) and/or comments ( Shu et al., 2019 ) have been used to provide explain- ability for fake news detection. A verified news database has to be established for these approaches. However, for rumor detection, sometimes a decision has to be made based on the current tweet only. Text-level models with explainability that recognize rumors by feature extraction should be developed to tackle this problem. In this work, we propose LEX-GAN, a GAN-based layered explainable framework for text-level rumor detection. LEX-GAN keeps the ability of discriminating between real-world and generated samples, and also serves as a discriminator-oriented model that classifies real-world and generated fake samples. We overcome the infeasibility of propagating the gradient from discriminator back to the generator by applying Reinforcement Learning (RL) to train the layered generators. The training instability of long sentence generation is lowered by selectively replacing words in the sentence. We solve the per time step error attribution difficulty by word-level generation and evaluation. We show that our model outperforms the baselines in terms of addressing the degraded accuracy problem with clean samples only. The major contributions of this work are listed as follows: • LEX-GAN delivers an explainable rumor detection without requiring a verified news database. Rumors could stay unverified for a long period of time because of information insufficiency. Providing explainability of which words in the sentence could be problem- atic is critical especially when there is no verified fact. When a verified news database is achievable, LEX-GAN can be applied to fake news detection with minor modification. • The layered structure of LEX-GAN avoids the function mixture and boosts the perfor- mance. During framework design, we found that using one layer to realize two functions either in generative or discriminative model causes function mixture and hurts the perfor- mance. LEX-GAN generates high-quality rumors by first intelligently selecting words to be replaced, then choosing appropriate substitutes to replace. The explanation generation and rumor detection are realized separately by two layers in the discriminative model. • LEX-GAN is a powerful framework in textural mutation detection. We demonstrate the mutation detection power by applying LEX-GAN to a gene classification and mutation detection task. LEX-GAN accurately identifies tokens in the gene sequences that are likely from the mutation, and classifies mutated gene sequences with high precision.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes Variance Reduced Local SGD (VRL-SGD), a novel distributed optimization algorithm to reduce the communication complexity of stochastic gradient descent (SGD) for large-scale machine learning problems. VRL-SGD eliminates the extra assumption about bounded gradient variance among workers in previous studies based on Local SGD, and reduces the communication complexity from O(T 3 4 N 3 4 ) to O(T 1 2 N 3 2 ) for the non-identical case. Theoretical analysis and experimental results show that VRL-SGD has a linear iteration speedup with respect to the number of workers, and performs significantly better than Local SGD if data distribution in workers is different, while maintaining the same convergence rate as Local SGD if all workers access identical datasets.",
        "Abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.",
        "Introduction": "  INTRODUCTION With the expansion of data and model scale, the training of machine learning models, especially deep learning models has become increasingly time-consuming. To accelerate the training process, distributed parallel optimization has attracted widespread interests recently, which encourages mul- tiple workers to cooperatively optimize the model. For large-scale machine learning problems, stochastic gradient descent (SGD) is a fundamental tool. It can be easily parallelized by collecting stochastic gradient from different workers and hence it is widely adopted. Previous studies ( Dekel et al., 2012 ;  Ghadimi & Lan, 2013 ) justify that synchronous stochastic gradient descent (S-SGD) has a linear iteration speedup for both general convex and non- convex objectives, which means that the total number of iterations is reduced by N times with N workers. However, S-SGD suffers from a major drawback: the communication cost among workers is expensive when the number of workers is large, which prevents S-SGD from achieving a linear time speedup. Therefore, it is crucial to overcome the communication bottleneck. To reduce communication cost, several studies ( Wang & Joshi, 2018 ;  Zhou & Cong, 2018 ;  Stich, 2019 ;  Yu et al., 2019b ;  Shen et al., 2019 ) have managed to lower the communication frequency. Among them, Local SGD ( Stich, 2019 ) is a representative distributed algorithm, where workers can conduct SGD locally and average model with each other every k iterations. Compared with S-SGD, the algorithms based on Local SGD reduce the communication rounds from O(T ) to O(T /k). To deal with the gradient variance among workers, previous studies require at least one of the follow- ing extra assumptions: (1) the bounded gradient variance among workers; (2) an upper bound for gradients; (3) identical data on all workers. When the data distribution on workers is identical, which is the so-called identical case, the algorithms based on Local SGD can exhibit superior per- formance. Nevertheless, the identical data assumption is not always valid in real cases. When the data distribution on workers is non-identical, which is the so-called non-identical case, these algo- rithms would encounter a significant degradation in the convergence rate due to the gradient variance among workers. We seek to eliminate the gradient variance among workers, which may make the algorithm converge much faster than the vanilla Local SGD. In this paper, we propose Variance Reduced Local SGD (VRL-SGD), a novel distributed optimiza- tion algorithm to further reduce the communication complexity. Benefiting from an additional vari- Under review as a conference paper at ICLR 2020 ance reduction component, VRL-SGD eliminates the extra assumption about bounded gradient vari- ance among workers in previous studies based on Local SGD ( Yu et al., 2019a ;b;  Shen et al., 2019 ). Thus the communication complexity can be reduced from O(T 3 4 N 3 4 ) to O(T 1 2 N 3 2 ) in VRL-SGD for the non-identical case, which is crucial for achieving a better time speedup. Therefore, VRL- SGD is more suitable than Local SGD for the scenarios, such as federated learning ( Konečnỳ et al., 2016 ), where the gradient variance across workers might be large. Contributions of this paper are summarized as follows: • We propose VRL-SGD, a novel distributed optimization algorithm with a state-of-the-art communication complexity. Specifically, the communication complexity is reduced from O(T 3 4 N 3 4 ) to O(T 1 2 N 3 2 ) for the non-identical case. To the best of our knowledge, this is the first time that an algorithm based on Local SGD possesses such a communication complexity for the non-identical case. Meanwhile, VRL-SGD also achieves the optimal communication complexity for the identical case. • We provide a theoretical analysis and prove that VRL-SGD has a linear iteration speedup with respect to the number of workers. Our method does not require the extra assumptions, e.g. the gradient variance across workers is bounded. • We validate the effectiveness of VRL-SGD on three standard machine learning tasks. And experimental results show that the proposed algorithm performs significantly better than Local SGD if data distribution in workers is different, while maintains the same conver- gence rate as Local SGD if all workers access identical datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a model-based reinforcement learning (MBRL) method called Model Imitation (MI) which enforces the learned transition model to generate similar rollouts to the real one so that policy gradient is accurate. Theoretically, it is shown that the transition can be learned by MI in the sense that T → T by consistency and the difference in cumulative rewards can be bounded by the training error of WGAN. Experiments show that MI is more sample efficient than state-of-the-art MBRL and MFRL methods and outperforms them on four standard tasks.",
        "Abstract": "Model-based reinforcement learning (MBRL) aims to learn a dynamic model to reduce the number of interactions with real-world environments. However, due to estimation error, rollouts in the learned model, especially those of long horizon, fail to match the ones in real-world environments. This mismatching has seriously impacted the sample complexity of MBRL. The phenomenon can be attributed to the fact that previous works employ supervised learning to learn the one-step transition models, which has inherent difficulty ensuring the matching of distributions from multi-step rollouts. Based on the claim, we propose to learn the synthesized model by matching the distributions of multi-step rollouts sampled from the synthesized model and the real ones via WGAN. We theoretically show that matching the two can minimize the difference of cumulative rewards between the real transition and the learned one. Our experiments also show that the proposed model imitation method outperforms the state-of-the-art in terms of sample complexity and average return.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) has become of great interest because plenty of real-world problems can be modeled as a sequential decision-making problem. Model-free reinforcement learning (MFRL) is favored by its capability of learning complex tasks when interactions with environments are cheap. However, in the majority of real-world problems, such as autonomous driving, interactions are ex- tremely costly, thus MFRL becomes infeasible. One critique about MFRL is that it does not fully exploit past queries over the environment, and this motivates us to consider the model-based rein- forcement learning (MBRL). In addition to learning an agent policy, MBRL also uses the queries to learn the dynamics of the environment that our agent is interacting with. If the learned dynamic is accurate enough, the agent can acquire the desired skill by simply interacting with the simulated environment, so that the number of samples to collect in the real world can be greatly reduced. As a result, MBRL has become one of the possible solutions to reduce the number of samples required to learn an optimal policy. Most previous works of MBRL adopt supervised learning with 2 -based errors ( Luo et al., 2019 ;  Kurutach et al., 2018 ;  Clavera et al., 2018 ) or maximum likelihood ( Janner et al., 2019 ), to obtain an environment model that synthesizes real transitions. These non-trivial developments imply that optimizing a policy on a synthesized environment is a challenging task. Because the estimation error of the model accumulates as the trajectory grows, it is hard to train a policy on a long synthesized trajectory. On the other hand, training on short trajectories makes the policy short-sighted. This issue is known as the planning horizon dilemma ( Wang et al., 2019 ). As a result, despite having a strong intuition at first sight, MBRL has to be designed meticulously. Intuitively, we would like to learn a transition model in a way that it can reproduce the trajectories that have been generated in the real world. Since the attained trajectories are sampled according to a certain policy, directly employing supervised learning may not necessarily lead to the mentioned result especially when the policy is stochastic. The resemblance in trajectories matters because we estimate policy gradient by generating rollouts; however, the one-step model learning adopted by many MBRL methods do not guarantee this. Some previous works propose multi-step training ( Luo et al., 2019 ;  Asadi et al., 2019 ;  Talvitie, 2017 ); however, experiments show that model learning fails to benefit much from the multi-step loss. We attribute this outcome to the essence of super- Under review as a conference paper at ICLR 2020 vised learning, which elementally preserves only one-step transition and the similarity between real trajectories and the synthesized ones cannot be guaranteed. In this work, we propose to learn the transition model via distribution matching. Specifically, we use WGAN ( Arjovsky et al., 2017 ) to match the distributions of state-action-next-state triple (s, a, s ) in real/learned models so that the agent policy can generate similar trajectories when interacting with either the true transition or the learned transition.  Figure 1  illustrates the difference between meth- ods based on supervised learning and distribution matching. Different from the ensemble methods proposed in previous works, our method is capable of generalizing to unseen transitions with only one dynamic model because merely incorporating multiple models does not alter the essence that one-step (or few-step) supervised learning fails to imitate the distribution of multi-step rollouts. Concretely, we gather some transitions in the real world according to a policy. To learn the real transition T , we then sample fake transitions from our synthesized model T with the same policy. The synthesized model serves as the generator in the WGAN framework and there is a critic that discriminates the two transition data. We update the generator and the critic alternatively until the synthesized data cannot be distinguished from the real one, which we will show later that it gives T → T theoretically. Our contributions are summarized below: • We propose an MBRL method called model imitation (MI), which enforces the learned transition model to generate similar rollouts to the real one so that policy gradient is accu- rate; • We theoretically show that the transition can be learned by MI in the sense that T → T by consistency and the difference in cumulative rewards can be bounded by the training error of WGAN; • To stabilize model learning, we deduce guarantee for our sampling technique and investi- gate training across WGANs; • We experimentally show that MI is more sample efficient than state-of-the-art MBRL and MFRL methods and outperforms them on four standard tasks.",
        "label": 0
    }
]