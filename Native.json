[
    {
        "Summary": "\n\nThis paper presents a learning framework which leverages underlying physics to improve generalization in deep neural networks. The framework incorporates inductive bias by promoting representations which are simple in some sense, and exploits the Hamiltonian dynamics to design the computation graph. Results show that incorporating physics-based inductive bias offers insight about relevant physical properties of the system, such as inertia, potential energy, and total conserved energy, which in turn enable more accurate prediction of future behavior and improvement in out-of-sample behavior. Furthermore, learning a physically-consistent model of the underlying dynamics can enable usage of model-based controllers which can provide performance guarantees for complex, nonlinear systems.",
        "Abstract": "In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
        "Introduction": "  INTRODUCTION In recent years, deep neural networks ( Goodfellow et al., 2016 ) have become very accurate and widely used in many application domains, such as image recognition ( He et al., 2016 ), language comprehension ( Devlin et al., 2019 ), and sequential decision making ( Silver et al., 2017 ). To learn underlying patterns from data and enable generalization beyond the training set, the learning ap- proach incorporates appropriate inductive bias ( Haussler, 1988 ;  Baxter, 2000 ) by promoting repre- sentations which are simple in some sense. It typically manifests itself via a set of assumptions, which in turn can guide a learning algorithm to pick one hypothesis over another. The success in predicting an outcome for previously unseen data then depends on how well the inductive bias cap- tures the ground reality. Inductive bias can be introduced as the prior in a Bayesian model, or via the choice of computation graphs in a neural network. In a variety of settings, especially in physical systems, wherein laws of physics are primarily re- sponsible for shaping the outcome, generalization in neural networks can be improved by leveraging underlying physics for designing the computation graphs. Here, by leveraging a generalization of the Hamiltonian dynamics, we develop a learning framework which exploits the underlying physics in the associated computation graph. Our results show that incorporation of such physics-based in- ductive bias offers insight about relevant physical properties of the system, such as inertia, potential energy, total conserved energy. These insights, in turn, enable a more accurate prediction of future behavior and improvement in out-of-sample behavior. Furthermore, learning a physically-consistent model of the underlying dynamics can subsequently enable usage of model-based controllers which can provide performance guarantees for complex, nonlinear systems. In particular, insight about Published as a conference paper at ICLR 2020 kinetic and potential energy of a physical system can be leveraged to synthesize appropriate control strategies, such as the method of controlled Lagrangian ( Bloch et al., 2001 ) and interconnection & damping assignment ( Ortega et al., 2002 ), which can reshape the closed-loop energy landscape to achieve a broad range of control objectives (regulation, tracking, etc.).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a decentralized approach to hierarchical reinforcement learning, which learns a set of low-level primitives without learning an explicit high-level controller. The primitives directly compete with one another to determine which one should be active at any given time, based on the degree to which their state encoders \"recognize\" the current state input. This approach enables the individual primitive mechanisms to be recombined in a plug-and-play fashion, and the primitives can be transferred seamlessly to new environments. The proposed approach is formalized as an information-driven objective based on the variational information bottleneck.",
        "Abstract": "Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states.\nIn this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state.\nWe use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization. ",
        "Introduction": "  INTRODUCTION Learning policies that generalize to new environments or tasks is a fundamental challenge in re- inforcement learning. While deep reinforcement learning has enabled training powerful policies, which outperform humans on specific, well-defined tasks ( Mnih et al., 2015 ), their performance often diminishes when the properties of the environment or the task change to regimes not encountered during training. This is in stark contrast to how humans learn, plan, and act: humans can seamlessly switch between different aspects of a task, transfer knowledge to new tasks from remotely related but essentially distinct prior experience, and combine primitives (or skills) used for distinct aspects of different tasks in meaningful ways to solve new problems. A hypothesis hinting at the reasons for this discrepancy is that the world is inherently compositional, such that its features can be described by compositions of small sets of primitive mechanisms ( Parascandolo et al., 2017 ). Since humans seem to benefit from learning skills and learning to combine skills, it might be a useful inductive bias for the learning models as well. This is addressed to some extent by the hierarchical reinforcement learning (HRL) methods, which focus on learning representations at multiple spatial and temporal scales, thus enabling better explo- ration strategies and improved generalization performance ( Dayan & Hinton, 1993 ;  Sutton et al., 1999b ;  Dietterich, 2000 ;  Kulkarni et al., 2016 ). However, hierarchical approaches rely on some form of learned high-level controller, which decides when to activate different components in the hierarchy. While low-level sub-policies can specialize to smaller portions of the state space, the top-level controller (or master policy) needs to know how to deal with any given state. That is, it should provide optimal behavior for the entire accessible state space. As the master policy is trained Published as a conference paper at ICLR 2020 on a particular state distribution, learning it in a way that generalizes to new environments effectively becomes the bottleneck for such approaches ( Sasha Vezhnevets et al., 2017 ;  Andreas et al., 2017 ). We argue, and empirically show, that in order to achieve better generalization, the interaction between the low-level primitives and the selection thereof should itself be performed without requiring a single centralized network that understands the entire state space. We, therefore, propose a decentralized approach as an alternative to standard HRL, where we only learn a set of low-level primitives without learning an explicit high-level controller. In particular, we construct a factorized representation of the policy by learning simple primitive policies, which focus on distinct regions of the state space. Rather than being gated by a single meta-policy, the primitives directly compete with one another to determine which one should be active at any given time, based on the degree to which their state encoders \"recognize\" the current state input. While, technically, the competition between primitives implicitly realizes a global selection mechanism, we consider our model decentralized in the sense that individual primitives can function on their own, and can be combined in new ways, without relying on an explicit high-level controller. We frame the problem as one of information transfer between the current state and a dynamically selected primitive policy. Each policy can, by itself, decide to request information about the current state, and the amount of information requested is used to determine which primitive acts in the current state. Since the amount of state information that a single primitive can access is limited, each primitive is encouraged to use its resources wisely. Constraining the amount of accessible information in this way naturally leads to a decentralized competition and decision mechanism where individual primitives specialize in smaller regions of the state space. We formalize this information-driven objective based on the variational information bottleneck. The resulting set of competing primitives achieves both a meaningful factorization of the policy and an effective decision mechanism for which primitives to use. Importantly, not relying on a centralized meta-policy enables the individual primitive mechanisms can be recombined in a plug-and-play fashion, and the primitives can be transferred seamlessly to new environments.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the extent to which current practice in deep reinforcement learning (RL) reflects the principles informing its development, with a specific focus on deep policy gradient methods. It is found that state-of-the-art implementations of these methods suffer from oversensitivity to hyperparameter choices, lack of consistency, and poor reproducibility, suggesting that it may be necessary to re-examine the conceptual underpinnings of deep RL methodology.",
        "Abstract": "    We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the \"true\" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of current methods, and indicates the need to move beyond current benchmark-centric evaluation methods.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (RL) is behind some of the most publicized achievements of modern machine learning ( Silver et al., 2017 ;  OpenAI, 2018 ;  Dayarathna et al., 2016 ;  OpenAI et al., 2018 ). In fact, to many, this framework embodies the promise of the real-world impact of machine learning. However, the deep RL toolkit has not yet attained the same level of engineering stability as, for example, the current deep (supervised) learning framework. Indeed, recent studies demonstrate that state-of-the-art deep RL algorithms suffer from oversensitivity to hyperparameter choices, lack of consistency, and poor reproducibility ( Henderson et al., 2017 ). This state of affairs suggests that it might be necessary to re-examine the conceptual underpinnings of deep RL methodology. More precisely, the overarching question that motivates this work is: To what degree does current practice in deep RL reflect the principles informing its development? Our specific focus is on deep policy gradient methods, a widely used class of deep RL algorithms. Our goal is to explore the extent to which state-of-the-art implementations of these methods succeed at realizing the key primitives of the general policy gradient framework.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a meta-learning objective that exploits the assumption of localized change in the data distribution to optimize the knowledge representation of a machine learning model. This objective measures the speed of online adaptation to out-of-distribution data, and can be used to guide the inference of the true causal structure of the problem. Experiments are conducted to verify the effectiveness of the proposed approach.",
        "Abstract": "We propose to use a meta-learning objective that maximizes the speed of transfer on a modified distribution to learn how to modularize acquired knowledge. In particular, we focus on how to factor a joint distribution into appropriate conditionals, consistent with the causal directions. We explain when this can work, using the assumption that the changes in distributions are localized (e.g. to one of the marginals, for example due to an intervention on one of the variables). We prove that under this assumption of localized changes in causal mechanisms, the correct causal graph will tend to have only a few of its parameters with non-zero gradient, i.e. that need to be adapted (those of the modified variables). We argue and observe experimentally that this leads to faster adaptation, and use this property to define a meta-learning surrogate score which, in addition to a continuous parametrization of graphs, would favour correct causal graphs. Finally, motivated by the AI agent point of view (e.g. of a robot discovering its environment autonomously), we consider how the same objective can discover the causal variables themselves, as a transformation of observed low-level variables with no causal meaning. Experiments in the two-variable case validate the proposed ideas and theoretical results.",
        "Introduction": "  INTRODUCTION The data used to train our models is often assumed to be independent and identically distributed (iid.), according to some unknown distribution. Likewise, the performance of a machine learning model is typically evaluated using test samples from the same distribution, assumed to be representative of the learned system's usage. While these assumptions are well analyzed from a statistical point of view, they are rarely satisfied in many real-world applications. For example, an accident on a major highway could completely perturb the trajectories of cars, and a driving policy trained in a static way might not be robust to such changes. Ideally, we would like our models to generalize well and adapt quickly to out-of-distribution data. However, this comes at a price - in order to successfully transfer to a novel distribution, one might need additional information about these distributions. In this paper, we are not considering assumptions on the data distribution itself, but rather on how it changes (e.g., when going from a training distribution to a transfer distribution, possibly resulting from some agent's actions). We focus on the assumption that the changes are sparse when the knowledge is represented in an appropriately modularized way, with only one or a few of the modules having changed. This is especially relevant when the distributional change is due to actions by one or more agents, because agents intervene at a particular place and time, and this is reflected in the form of the interventions discussed in the causality literature ( Pearl, 2009 ;  Peters et al., 2016 ), where a single causal variable is clamped to a particular value or a random variable. In general, it is difficult for agents to influence many underlying causal variables at a time, and although this paper is not about agent learning as such, this is a property of the world that we propose to exploit here, to help discovering these variables Published as a conference paper at ICLR 2020 and how they are causally related to each other. In this context, the causal graph is a powerful tool because it tells us how perturbations in the distribution of intervened variables will propagate to all other variables and affect their distributions. As expected, it is often the case that the causal structure is not known in advance. The problem of causal discovery then entails obtaining the causal graph, a feat which is in general achievable only with strong assumptions. One such assumption is that a learner that has learned to capture the correct structure of the true underlying data-generating process should still generalize to the case where the structure has been perturbed in a certain, restrictive way. This can be illustrated by considering the example of temperature and altitude from  Peters et al. (2017) : a learner that has learned to capture the mechanisms of atmospheric physics by learning that it makes more sense to predict temperature from the altitude (rather than vice versa) given training data from (say) Switzerland, will still remain valid when tested on out-of-distribution data from a less mountainous country like (say) the Netherlands. It has therefore been suggested that the out-of-distribution robustness of predictive models can be used to guide the inference of the true causal structure ( Peters et al., 2016 ; 2017). How can we exploit the assumption of localized change? As we explain theoretically and verify experimentally here, if we have the right knowledge representation, then we should get fast adaptation to the transfer distribution when starting from a model that is well trained on the training distribution. This arises because of our assumption that the ground truth data generative process is obtained as the composition of independent mechanisms, and that very few ground truth mechanisms and parameters need to change when going from the training distribution to the transfer distribution. A model capturing a corresponding factorization of knowledge would thus require just a few updates, a few examples, for this adaptation to the transfer distribution. As shown below, the expected gradient on the unchanged parameters would be near 0 (if the model was already well trained on the training distribution), so the effective search space during adaptation to the transfer distribution would be greatly reduced, which tends to produce faster adaptation, as found experimentally. Thus, based on the assumption of small change in the right knowledge representation space, we can define a meta-learning objective that measures the speed of online adaptation in order to optimize the way in which knowledge should be represented, factorized and structured. This is the core idea presented in this paper. Returning to the example of temperature and altitude: when presented with out-of-distribution data from the Netherlands, we expect the correct model to adapt faster given a few transfer samples of actual weather data collected in the Netherlands. Analogous to the case of robustness, the adaptation speed can then be used to guide the inference of the true causal structure of the problem at hand, possibly along with other sources of signal about causal structure.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to quantizing deep neural networks for real-world deployment on mobile phones, embedded systems, and IoT devices. We analyze the theoretical properties of noise introduced by quantization and derive a straightforward regularization scheme to control the maximum first-order induced loss and learn networks that are inherently more robust against post-training quantization. We show that applying this regularization at the final stages of training, or as a fine-tuning step after training, improves post-training quantization across different bit-widths at the same time for commonly used neural network architectures.",
        "Abstract": "We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths as energy and memory requirements of the application change. Unlike quantization-aware training using the straight-through estimator that only targets a specific bit-width and requires access to training data and pipeline, our regularization-based method paves the way for ``on the fly'' post-training quantization to various bit-widths. We show that by modeling quantization as a $\\ell_\\infty$-bounded perturbation, the first-order term in the loss expansion can be regularized using the $\\ell_1$-norm of gradients. We experimentally validate our method on different vision architectures on CIFAR-10 and ImageNet datasets and show that the regularization of a neural network using our method improves robustness against quantization noise.",
        "Introduction": "  INTRODUCTION Deep neural networks excel across a variety of tasks, but their size and computational requirements often hinder their real-world deployment. The problem is more challenging for mobile phones, embedded systems, and IoT devices, where there are stringent requirements in terms of memory, compute, latency, and energy consumption. Quantization of parameters and activations is often used to reduce the energy and computational requirements of neural networks. Quantized neural networks allow for more speed and energy efficiency compared to floating-point models by using fixed-point arithmetic. However, naive quantization of pre-trained models often results in severe accuracy degradation, especially when targeting bit-widths below eight ( Krishnamoorthi, 2018 ). Performant quantized models can be obtained via quantization-aware training or fine-tuning, i.e., learning full-precision shadow weights for each weight matrix with backpropagation using the straight-through estimator (STE) ( Bengio et al., 2013 ), or using other approximations ( Louizos et al., 2018 ). Alternatively, there have been successful attempts to recover the lost model accuracy without requiring a training pipeline ( Banner et al., 2018 ;  Meller et al., 2019 ;  Choukroun et al., 2019 ;  Zhao et al., 2019 ) or representative data ( Nagel et al., 2019 ). But these methods are not without drawbacks. The shadow weights learned through quantization- aware fine-tuning often do not show robustness when quantized to bit-widths other than the one they were trained for (see  Table 1 ). In practice, the training procedure has to be repeated for each quantization target. Furthermore, post-training recovery methods require intimate knowledge of the relevant architectures. While this may not be an issue for the developers training the model in the first Published as a conference paper at ICLR 2020 place, it is a difficult step for middle parties that are interested in picking up models and deploying them to users down the line, e.g., as part of a mobile app. In such cases, one might be interested in automatically constraining the computational complexity of the network such that it conforms to specific battery consumption requirements, e.g. employ a 4-bit variant of the model when the battery is less than 20% but the full precision one when the battery is over 80%. Therefore, a model that can be quantized to a specific bit-width \"on the fly\" without worrying about quantization aware fine-tuning is highly desirable. In this paper, we explore a novel route, substantially different from the methods described above. We start by investigating the theoretical properties of noise introduced by quantization and analyze it as a ∞ -bounded perturbation. Using this analysis, we derive a straightforward regularization scheme to control the maximum first-order induced loss and learn networks that are inherently more robust against post-training quantization. We show that applying this regularization at the final stages of training, or as a fine-tuning step after training, improves post-training quantization across different bit-widths at the same time for commonly used neural network architectures.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a study of group distributionally robust optimization (DRO) applied to overparameterized neural networks in three applications: natural language inference, facial attribute recognition, and bird photograph recognition. We show that strongly-regularized group DRO models can significantly outperform both regularized and unregularized empirical risk minimization (ERM) models, improving worst-case test accuracies by 10-40 percentage points while maintaining high average test accuracies. We introduce a new stochastic optimizer for group DRO that is stable and scales to large models and datasets, and provide convergence guarantees for our algorithm in the convex case.",
        "Abstract": "Overparameterized neural networks can be highly accurate on average on an i.i.d. test set, yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---stronger-than-typical L2 regularization or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm for the group DRO setting and provide convergence guarantees for the new algorithm.\n",
        "Introduction": "  INTRODUCTION Machine learning models are typically trained to minimize the average loss on a training set, with the goal of achieving high accuracy on an independent and identically distributed (i.i.d.) test set. However, models that are highly accurate on average can still consistently fail on rare and atypical examples (Hovy & Sgaard, 2015; Blodgett et al., 2016; Tatman, 2017; Hashimoto et al., 2018; Duchi et al., 2019). Such models are problematic when they violate equity considerations (Jurgens et al., 2017; Buolamwini & Gebru, 2018) or rely on spurious correlations: misleading heuristics that work for most training examples but do not always hold. For example, in natural language inference (NLI)-determining if two sentences agree or contradict-the presence of negation words like 'never' is strongly correlated with contradiction due to artifacts in crowdsourced training data (Gururangan et al., 2018; McCoy et al., 2019). A model that learns this spurious correlation would be accurate on average on an i.i.d. test set but suffer high error on groups of data where the correlation does not hold (e.g., the group of contradictory sentences with no negation words). To avoid learning models that rely on spurious correlations and therefore suffer high loss on some groups of data, we instead train models to minimize the worst-case loss over groups in the training data. The choice of how to group the training data allows us to use our prior knowledge of spurious correlations, e.g., by grouping together contradictory sentences with no negation words in the NLI example above. This training procedure is an instance of distributionally robust optimization (DRO), Published as a conference paper at ICLR 2020 which optimizes for the worst-case loss over potential test distributions (Ben-Tal et al., 2013; Duchi et al., 2016). Existing work on DRO has focused on models that cannot approach zero training loss, such as generative models (Oren et al., 2019) or convex predictive models with limited capacity (Maurer & Pontil, 2009; Shafieezadeh-Abadeh et al., 2015; Namkoong & Duchi, 2017; Duchi & Namkoong, 2018; Hashimoto et al., 2018). We study group DRO in the context of overparameterized neural networks in three applications ( Fig- ure 1 )-natural language inference with the MultiNLI dataset (Williams et al., 2018), facial attribute recognition with CelebA (Liu et al., 2015), and bird photograph recognition with our modified ver- sion of the CUB dataset (Wah et al., 2011). The problem with applying DRO to overparameterized models is that if a model achieves zero training loss, then it is optimal on both the worst-case (DRO) and the average training objectives (Zhang et al., 2017; Wen et al., 2014). In the vanishing-training- loss regime, we indeed find that group DRO models do no better than standard models trained to minimize average loss via empirical risk minimization (ERM): both models have high average test accuracies and worst-group training accuracies, but low worst-group test accuracies (Section 3.1). In other words, the generalization gap is small on average but large for the worst group. In contrast, we show that strongly-regularized group DRO models that do not attain vanishing train- ing loss can significantly outperform both regularized and unregularized ERM models. We con- sider 2 penalties, early stopping (Section 3.2), and group adjustments that minimize a risk measure which accounts for the differences in generalization gaps between groups (Section 3.3). Across the three applications, regularized group DRO improves worst-case test accuracies by 10-40 percentage points while maintaining high average test accuracies. These results give a new perspective on gen- eralization in neural networks: regularization might not be important for good average performance (e.g., models can \"train longer and generalize better\" on average (Hoffer et al., 2017)) but it appears important for good worst-case performance. Finally, to carry out the experiments, we introduce a new stochastic optimizer for group DRO that is stable and scales to large models and datasets. We derive convergence guarantees for our algorithm in the convex case and empirically show that it behaves well in our non-convex models (Section 5).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a differentiable linear quadratic MPC algorithm that can be learned using gradient-based methods. The learning method uses an MPC controller where the terminal cost and terminal policy are the solution of an unconstrained infinite-horizon Linear Quadratic Regulator (LQR). A closed-form solution for the derivative of the Discrete-time Algebraic Riccati Equation (DARE) associated with the LQR is presented, allowing analytical results from control theory to be used to determine the stabilizing properties of the learned controller when implemented in closed-loop. The proposed algorithm successfully learns an MPC with both local stability and intrinsic robustness guarantees under small model uncertainties.",
        "Abstract": "This paper proposes a differentiable linear quadratic Model Predictive Control (MPC) framework for safe imitation learning. The infinite-horizon cost is enforced using a terminal cost function obtained from the discrete-time algebraic Riccati equation (DARE), so that the learned controller can be proven to be stabilizing in closed-loop. A central contribution is the derivation of the analytical derivative of the solution of the DARE, thereby allowing the use of differentiation-based learning methods. A further contribution is the structure of the MPC optimization problem: an augmented Lagrangian method ensures that the MPC optimization is feasible throughout training whilst enforcing hard constraints on state and input, and a pre-stabilizing controller ensures that the MPC solution and derivatives are accurate at each iteration. The learning capabilities of the framework are demonstrated in a set of numerical studies. ",
        "Introduction": "  INTRODUCTION Imitation Learning ( IL, Osa et al., 2018 ) aims at reproducing an existing control policy by means of a function approximator and can be used, for instance, to hot-start reinforcement learning. Effective learning and generalisation to unseen data are paramount to IL success, especially in safety critical applications. Model Predictive Control ( MPC, Maciejowski, 2000 ; Camacho & Bordons, 2007;  Rawlings & Mayne, 2009 ;  Kouvaritakis & Cannon, 2015 ;  Gallieri, 2016 ;  Borrelli et al., 2017 ;  Raković & Levine, 2019 ) is the most successful advanced control methodology for systems with hard safety constraints. At each time step, a finite horizon forecast is made from a predictive model of the system and the optimal actions are computed, generally relying on convex constrained Quadratic Programming ( QP, Boyd & Vandenberghe, 2004 ;  Bemporad et al., 2000 ). Stability of the MPC in closed loop with the physical system requires the solution of a simpler unconstrained infinite horizon control problem ( Mayne et al., 2000 ) which results in a value function (terminal cost and constraint) and a candidate terminal controller to be accounted for in the MPC forecasting. For Linear Time Invariant (LTI) models and quadratic costs, this means solving (offline) a Riccati equation ( Kalman, 2001 ) or a linear matrix inequality ( Boyd et al., 1994 ). Under these conditions, an MPC controller will effectively control a system, up to a certain accuracy, provided that uncertainties in the model dynamics are limited ( Limon et al., 2009 ). Inaccuracies in the MPC predictions can reduce its effectiveness (and robustness) as the forecast diverges from the physical system trajectory over long horizons. This is particularly critical in applications with both short and long-term dynamics and it is generally addressed, for instance in robust MPC ( Richards, 2004 ;  Raković et al., 2012 ), by using a controller to pre-stabilise the predictions. This paper presents an infinite-horizon differentiable linear quadratic MPC that can be learned using gradient-based methods. In particular, the learning method uses an MPC controller where the terminal cost and terminal policy are the solution of an unconstrained infinite-horizon Linear Quadratic Regulator (LQR). A closed-form solution for the derivative of the Discrete-time Algebraic Riccati Equation (DARE) associated with the LQR is presented so that the stationary solution of the forward pass is fully differentiable. This method allows analytical results from control theory Published as a conference paper at ICLR 2020 to be used to determine the stabilizing properties of the learned controller when implemented in closed-loop. Once the unconstrained LQR is computed, the predictive model is pre-stabilised using a linear state-feedback controller to improve the conditioning of the QP and the numerical accuracy of the MPC solution and gradients. The proposed algorithm successfully learns an MPC with both local stability and intrinsic robustness guarantees under small model uncertainties.",
        "label": 1
    },
    {
        "Summary": " (1) How does the quality of the posterior approximation affect the performance of the learning system? (2) How does the computational cost of the hypermodel affect its performance? (3) How does the hypermodel architecture affect its performance?\n\nThis paper investigates the use of hypermodels to efficiently represent and resolve epistemic uncertainty in deep learning. It examines the effects of hypermodel quality, computational cost, and architecture on the performance of a learning system in bandit problems of varying complexity. Results suggest that hypermodels can enable approximate Thompson sampling in complex systems where exact posterior inference is intractable, and that more sophisticated hypermodel architectures can substantially improve exploration.",
        "Abstract": "We study the use of hypermodels to represent epistemic uncertainty and guide exploration.\nThis generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost of training an ensemble grows with its size, and as such, prior work has typically been limited to ensembles with tens of elements. We show that alternative hypermodels can enjoy dramatic efficiency gains, enabling behavior that would otherwise require hundreds or thousands of elements, and even succeed in situations where ensemble methods fail to learn regardless of size.\nThis allows more accurate approximation of Thompson sampling as well as use of more sophisticated exploration schemes.  In particular, we consider an approximate form of information-directed sampling and demonstrate performance gains relative to Thompson sampling.  As alternatives to ensembles, we consider linear and neural network hypermodels, also known as hypernetworks.\nWe prove that, with neural network base models, a linear hypermodel can represent essentially any distribution over functions, and as such, hypernetworks do not extend what can be represented.",
        "Introduction": "  INTRODUCTION Consider the sequential decision problem of an agent interacting with an uncertain environment, aiming to maximize cumulative rewards. Over each time period, the agent must balance between exploiting existing knowledge to accrue immediate reward and investing in exploratory behavior that may increase subsequent rewards. In order to select informative exploratory actions, the agent must have some understanding of what it is uncertain about. As such, an ability to represent and resolve epistemic uncertainty is a core capability required of the intelligent agents. The efficient representation of epistemic uncertainty when estimating complex models like neural networks presents an important research challenge. Techniques include variational inference ( Blun- dell et al., 2015 ), dropout 1 ( Gal & Ghahramani, 2016 ) and MCMC ( Andrieu et al., 2003 ). Another approach has been motivated by the nonparametric bootstrap ( Efron & Tibshirani, 1994 ) and trains an ensemble of neural networks with random perturbations applied to each dataset ( Lu & Van Roy, 2017 ). The spirit is akin to particle filtering, where each element of the ensemble approximates a sample from the posterior and variation between models reflects epistemic uncertainty. Ensem- bles have proved to be relatively effective and to address some shortcomings of alternative posterior approximation schemes ( Osband et al., 2016 ; 2018). When training a single large neural network is computationally intensive, training a large ensem- ble of separate models can be prohibitively expensive. As such, ensembles in deep learning have typically been limited to tens of models ( Riquelme et al., 2018 ). In this paper, we show that this parsimony can severely limit the quality of the posterior approximation and ultimately the quality of the learning system. Further, we consider more general approach based on hypermodels that can realize the benefits of large ensembles without the prohibitive computational requirements. A hypermodel maps an index drawn from a reference distribution to a base model. An ensemble is one type of hypermodel; it maps a uniformly sampled base model index to that independently trained base model. We will consider additional hypermodel classes, including linear hypermodels, which we will use to map a Gaussian-distributed index to base model parameters, and hypernetworks, for which the mapping is a neural network ( Ha et al., 2016 ). Our motivation is that intelligent Published as a conference paper at ICLR 2020 hypermodel design might be able to amortize computation across the entire distribution of base models, and in doing so, offer large gains in computational efficiency. We train our hypermodels to estimate a posterior distribution over base models conditioned on ob- served data, in a spirit similar to that of the Bayesian hypermodel literature (Krueger et al., 2017). Unlike typical variational approximations to Bayesian deep learning, this approach allows com- putationally efficient training with complex multimodal distributions. In this paper, we consider hypermodels trained through stochastic gradient descent on perturbed data (see Section 2.1 for a full description). Training procedures for hypermodels are an important area of research, and it may be possible to improve on this approach, but that is not the focus of this paper. Instead, we aim to understand whether more sophisticated hypermodel architectures can substantially improve explo- ration. To do this we consider bandit problems of varying degrees of complexity, and investigate the computational requirements to achieve low regret over a long horizon. To benchmark the quality of posterior approximations, we compare their efficacy when used for Thompson sampling ( Thompson, 1933 ;  Russo et al., 2018 ). In its ideal form, Thompson sampling (TS) selects each action by sampling a model from the posterior distribution and optimizing over actions. For some simple model classes, this approach is computationally tractable. Hypermodels enable approximate TS in complex systems where exact posterior inference is intractable. Our results address three questions:",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the potential of non-normal recurrent neural networks (RNNs) in sequential processing tasks. Non-normal RNNs have a non-normal recurrent connectivity matrix, which allows them to display interesting transient behaviors that are not available in normal systems. Empirical results show that non-normal vanilla RNNs significantly outperform their orthogonal counterparts in a diverse range of benchmarks.",
        "Abstract": "Training recurrent neural networks (RNNs) is a hard problem due to degeneracies in the optimization landscape, a problem also known as vanishing/exploding gradients. Short of designing new RNN architectures, previous methods for dealing with this problem usually boil down to orthogonalization of the recurrent dynamics, either at initialization or during the entire training period. The basic motivation behind these methods is that orthogonal transformations are isometries of the Euclidean space, hence they preserve (Euclidean) norms and effectively deal with vanishing/exploding gradients. However, this ignores the crucial effects of non-linearity and noise. In the presence of a non-linearity, orthogonal transformations no longer preserve norms, suggesting that alternative transformations might be better suited to non-linear networks. Moreover, in the presence of noise, norm preservation itself ceases to be the ideal objective. A more sensible objective is maximizing the signal-to-noise ratio (SNR) of the propagated signal instead. Previous work has shown that in the linear case, recurrent networks that maximize the SNR display strongly non-normal, sequential dynamics and orthogonal networks are highly suboptimal by this measure. Motivated by this finding, here we investigate the potential of non-normal RNNs, i.e. RNNs with a non-normal recurrent connectivity matrix, in sequential processing tasks. Our experimental results show that non-normal RNNs outperform their orthogonal counterparts in a diverse range of benchmarks. We also find evidence for increased non-normality and hidden chain-like feedforward motifs in trained RNNs initialized with orthogonal recurrent connectivity matrices. ",
        "Introduction": "  INTRODUCTION Modeling long-term dependencies with recurrent neural networks (RNNs) is a hard problem due to degeneracies inherent in the optimization landscapes of these models, a problem also known as the vanishing/exploding gradients problem (Hochreiter, 1991; Bengio et al., 1994). One approach to addressing this problem has been designing new RNN architectures that are less prone to such difficulties, hence are better able to capture long-term dependencies in sequential data (Hochreiter & Schmidhuber, 1997; Cho et al., 2014; Chang et al., 2017; Bai et al., 2018). An alternative approach is to stick with the basic vanilla RNN architecture instead, but to constrain its dynamics in some way so as to eliminate or reduce the degeneracies that otherwise afflict the optimization landscape. Previous proposals belonging to this second category generally boil down to orthogonalization of the recurrent dynamics, either at initialization or during the entire training period (Le et al., 2015; Arjovsky et al., 2016; Wisdom et al., 2016). The basic idea behind these methods is that orthogonal transformations are isometries of the Euclidean space, hence they preserve distances and norms, which enables them to deal effectively with the vanishing/exploding gradients problem. However, this idea ignores the crucial effects of non-linearity and noise. Orthogonal transformations no longer preserve distances and norms in the presence of a non-linearity, suggesting that alternative transformations might be better suited to non-linear networks (this point was noted by Pennington et al. (2017) and Chen et al. (2018) before, where isometric initializations that take the non-linearity into account were proposed). Similarly, in the presence of noise, norm preservation itself ceases to be the ideal objective. One must instead maximize the signal-to-noise ratio (SNR) of the propagated signal. In neural networks, noise comes in both through the stochasticity of the stochastic gradient descent (SGD) algorithm and sometimes also through direct noise injection for regularization purposes, as Published as a conference paper at ICLR 2020 in dropout (Srivastava et al., 2014). Previous work has shown that even in a simple linear setting, recurrent networks that maximize the SNR display strongly non-normal, sequential dynamics and orthogonal networks are highly suboptimal by this measure (Ganguli et al., 2008). Motivated by these observations, in this paper, we investigate the potential of non-normal RNNs, i.e. RNNs with a non-normal recurrent connectivity matrix, in sequential processing tasks. Recall that a normal matrix is a matrix with an orthonormal set of eigenvectors, whereas a non-normal matrix does not have an orthonormal set of eigenvectors. This property allows non-normal systems to display interesting transient behaviors that are not available in normal systems. This kind of transient behavior, specifically a particular kind of transient amplification of the signal in certain non-normal systems, underlies their superior memory properties (Ganguli et al., 2008), as will be discussed further below. Our empirical results show that non-normal vanilla RNNs significantly outperform their orthogonal counterparts in a diverse range of benchmarks. 1",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new discretization algorithm, vq-wav2vec, which combines two lines of research to learn discrete representations of speech via a context prediction task. The algorithm utilizes the wav2vec loss and architecture to learn discrete representations of fixed length segments of audio signal. Gumbel-Softmax and online k-means clustering are used to choose the discrete variables. The representations are then used to train a Deep Bidirectional Transformer (BERT) on the discretized unlabeled speech data and input these representations to a standard acoustic model. Experiments show that BERT representations perform better than log-mel filterbank inputs as well as dense wav2vec representations on both TIMIT and WSJ benchmarks. Additionally, a standard sequence to sequence model from the NLP literature is used to perform speech recognition over discrete audio tokens.",
        "Abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
        "Introduction": "  INTRODUCTION Learning discrete representations of speech has gathered much recent interest ( Versteegh et al., 2016 ;  Dunbar et al., 2019 ). A popular approach to discover discrete units is via autoencoding ( Tjandra et al., 2019 ;  Eloff et al., 2019 ;  Chorowski et al., 2019 ) sometimes coupled with an autoregressive model ( Chung et al., 2019 ). Another line of research is to learn continuous speech representations in a self-supervised way via predicting context information ( Chung & Glass, 2018 ;  van den Oord et al., 2018 ;  Schneider et al., 2019 ). In this paper, we combine these two lines of research by learning discrete representations of speech via a context prediction task instead of reconstructing the input. This enables us to directly apply well performing NLP algorithms to speech data (Figure 1a). Our new discretization algorithm, vq-wav2vec, learns discrete representations of fixed length seg- ments of audio signal by utilizing the wav2vec loss and architecture ( Schneider et al, 2019 ; §2). To Published as a conference paper at ICLR 2020 choose the discrete variables, we consider a Gumbel-Softmax approach ( Jang et al., 2016 ) as well as online k-means clustering, similar to VQ-VAE ( Oord et al., 2017 ;  Eloff et al., 2019 ; §3). We then train a Deep Bidirectional Transformer (BERT;  Devlin et al., 2018 ; Liu et al., 2019) on the discretized unlabeled speech data and input these representations to a standard acoustic model (Figure 1b; §4). Our experiments show that BERT representations perform better than log-mel filterbank inputs as well as dense wav2vec representations on both TIMIT and WSJ benchmarks. Discretization of audio enables the direct application of a whole host of algorithms from the NLP literature to speech data. For example, we show that a standard sequence to sequence model from the NLP literature can be used to perform speech recognition over discrete audio tokens (§5, §6). wav2vec ( Schneider et al., 2019 ) learns representations of audio data by solving a self-supervised context-prediction task with the same loss function as word2vec ( Mikolov et al., 2013 ;  van den Oord et al., 2018 ). The model is based on two convolutional neural networks where the the encoder produces a representation z i for each time step i at a rate of 100 Hz and the aggregator combines multiple encoder time steps into a new representation c i for each time step i. Given an aggregated representation c i , the model is trained to distinguish a sample z i+k that is k steps in the future from distractor samplesz drawn from a distribution p n , by minimizing the contrastive loss for steps k = 1, . . . , K: L wav2vec k = − T −k i=1 log σ(z i+k h k (c i )) + λ Ẽ z∼pn [log σ(−z h k (c i ))] (1) where T is the sequence length, σ(x) = 1/(1 + exp(−x)), and where σ(z i+k h k (c i )) is the probability of z i+k being the true sample. We consider a step-specific affine transformation h k (c i ) = W k c i + b k that is applied to c i ( van den Oord et al., 2018 ). We optimize the loss L = K k=1 L k , summing (1) over different step sizes. After training, the representations produced by the context network c i are input to the acoustic model instead of log-mel filterbank features.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel technique to generate insightful visualizations for pre-trained deep reinforcement learning (RL) agents. The technique involves learning a generative model of the environment as an input to the agent, which allows for probing the agent's behavior in novel states created by an optimization scheme to induce specific actions in the agent. Visualizing such states allows for observing the agent's interaction with the environment in critical scenarios to understand its shortcomings. The method does not affect and does not depend on the training of the agent and is applicable to a wide variety of RL algorithms.",
        "Abstract": "As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",
        "Introduction": "  INTRODUCTION Humans can naturally learn and perform well at a wide variety of tasks, driven by instinct and practice; more importantly, they are able to justify why they would take a certain action. Artificial agents should be equipped with the same capability, so that their decision making process is interpretable by researchers. Following the enormous success of deep learning in various domains, such as the application of convolutional neural networks (CNNs) to computer vision ( LeCun et al., 1998 ;  Krizhevsky et al., 2012 ;  Long et al., 2015 ;  Ren et al., 2015 ), a need for understanding and analyzing the trained models has arisen. Several such methods have been proposed and work well in this domain, for example for image classification ( Simonyan et al., 2013 ;  Zeiler & Fergus, 2014 ;  Fong & Vedaldi, 2017 ), sequential models ( Karpathy et al., 2016 ) or through attention ( Xu et al., 2015 ). Deep reinforcement learning (RL) agents also use CNNs to gain perception and learn policies directly from image sequences. However, little work has been so far done in analyzing RL networks. We found that directly applying common visualization techniques to RL agents often leads to poor results. In this paper, we present a novel technique to generate insightful visualizations for pre-trained agents. Currently, the generalization capability of an agent is-in the best case-evaluated on a validation set of scenarios. However, this means that this validation set has to be carefully crafted to encompass as many potential failure cases as possible. As an example, consider the case of a self-driving agent, where it is near impossible to exhaustively model all interactions of the agent with other drivers, pedestrians, cyclists, weather conditions, even in simulation. Our goal is to extrapolate from the training scenes to novel states that induce a specified behavior in the agent. In our work, we learn a generative model of the environment as an input to the agent. This allows us to probe the agent's behavior in novel states created by an optimization scheme to induce specific Published as a conference paper at ICLR 2020 actions in the agent. For example we could optimize for states in which the agent sees the only option as being to slam on the brakes; or states in which the agent expects to score exceptionally low. Visualizing such states allows to observe the agent's interaction with the environment in critical scenarios to understand its shortcomings. Furthermore, it is possible to generate states based on an objective function specified by the user. Lastly, our method does not affect and does not depend on the training of the agent and thus is applicable to a wide variety of reinforcement learning algorithms.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel probabilistic deep learning approach for modeling uncertain orientations. The proposed approach is based on the Bingham distribution, an antipodally symmetric distribution on the sphere, and uses a loss based on the Bingham distribution. The paper also provides a methodology for making the proposed loss and its normalization constant computationally tractable in a deep learning pipeline, and demonstrates how the approach outperforms the state-of-the-art on challenging pose and orientation estimation tasks.",
        "Abstract": "Reasoning about uncertain orientations is one of the core problems in many perception tasks such as object pose estimation or motion estimation. In these scenarios, poor illumination conditions, sensor limitations, or appearance invariance may result in highly uncertain estimates. In this work, we propose a novel learning-based representation for orientation uncertainty. By characterizing uncertainty over unit quaternions with the Bingham distribution, we formulate a loss that naturally captures the antipodal symmetry of the representation. We discuss the interpretability of the learned distribution parameters and demonstrate the feasibility of our approach on several challenging real-world pose estimation tasks involving uncertain orientations.",
        "Introduction": "  INTRODUCTION Reasoning about uncertain poses and orientations, specifically 3-dimensional (3d) positions and 3-axes orientations, is one of the main inference tasks in computer vision ( Sattler et al., 2019 ), robotics ( Glover et al., 2011 ), aerospace ( Crassidis & Markley, 2003 ), and other fields. Proper representation and estimation of uncertainty is im- portant, e.g. when dealing with structural ambiguities in object pose estimation or coping with sensor corruption. In vision and robotics tasks, high levels of pose uncer- tainty may occur due to potentially adversarial conditions that arise in real-world scenarios. A principled approach to uncertainty quantification allows for better execution of planning and situation-awareness tasks such as grasping, tracking, and motion estimation. When representing uncertainties over poses, the position can be modeled using a Gaussian distribution. This ap- proach is well-motivated by the Central Limit Theorem and widely used in probabilistic deep learning models. However, this paradigm cannot be as easily applied to modeling periodic quantities, such as the orientation of an object. Therefore, Gaussian models become unsuit- able particularly in learning regimes involving high un- certainties where one cannot assume local linearity of the underlying space. In this work, we set out to develop a principled probabilistic deep learning approach capable of coping with uncertain orientations. Currently, most deep learning approaches that predict poses or rigid-body motions suffer from at least one of three drawbacks: 1) they do not model the uncertainty at all and merely focus on the accuracy of the predicted pose, 2) they make simplifying assumptions not taking into account that the orientation is defined on a periodic manifold, making the approach Published as a conference paper at ICLR 2020 only suitable in low-noise regimes, or 3) even when trying to account for periodicity, no dependency is assumed between the orientation axes and usually an Euler angle-based representation is required. To this point, there are no probabilistic deep learning models for uncertainty of orientations that take the geometry of the underlying domain into account. In this work, we close this research gap by proposing a probabilistic deep learning model inspired by Directional Statistics ( Mardia & Jupp, 1999 ). We present a loss based on the Bingham distri- bution ( Bingham, 1974 ), an antipodally symmetric distribution on the sphere. With this loss, we represent uncertain orientations by modeling uncertainty over unit quaternions. Our contributions involve Bingham parameter learning using backpropagation through a Gram-Schmidt method to en- sure orthonormalization, efficient approximate evaluation of the normalization constant of the Bing- ham distribution from a lookup table, and backpropagating through an interpolation scheme during learning. We also discuss interpretability of the Bingham distribution parameters and establish the feasibility of the approach through extensive evaluations. In summary, this work makes the following contributions: 1) We propose the Bingham loss, a novel loss function for deep learning-based predictions of orientations and their uncertainty. 2) We provide a methodology for making the newly proposed loss and its normalization constant computationally tractable in a deep learning pipeline. 3) We demonstrate multi-modal orientation prediction using a Bingham variant of Mixture Density Networks. 4) We demonstrate how our approach outperforms the state-of-the-art on challenging pose and orientation estimation tasks 1 .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the use of multiplicative interactions in neural networks, and how they can be used to represent meaningful classes of functions, improve data-efficiency, and lead to better generalization and stronger performance. Through controlled synthetic scenarios and large-scale reinforcement learning and sequence modelling tasks, the paper demonstrates the efficacy of multiplicative interactions and argues for their wider use in neural networks.",
        "Abstract": "We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others.\nMultiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should be considered in many situation where multiple compute or information paths need to be combined, in place of the simple and oft-used concatenation operation. Finally, we back up our claims and demonstrate the potential of multiplicative interactions by applying them in large-scale complex RL and sequence modelling tasks, where their use allows us to deliver state-of-the-art results, and thereby provides new evidence in support of multiplicative interactions playing a more prominent role when designing new neural network architectures.",
        "Introduction": "  INTRODUCTION Much attention has recently turned toward the design of custom neural network architectures and components in order to increase efficiency, maximise performance, or otherwise introduce desirable inductive biases. While there have been a plethora of newer, intricate architectures proposed, in this work we train our sights instead on an older staple of the deep learning toolkit: multiplicative interactions. Although the term itself has fallen somewhat out of favour, multiplicative interactions have reap- peared in a range of modern architectural designs. We start this work by considering multiplicative interactions as an object of study in their own right. We describe various formulations and how they relate to each other as well as connect more recent architectural developments (e.g. hypernetworks  Ha et al. (2017) , dynamic convolutions  Wu et al. (2019) ) to the rich and longer-standing literature on multiplicative interactions. We hypothesise that multiplicative interactions are suitable for representing certain meaningful classes of functions needed to build algorithmic operations such as conditional statements or similarity metrics, and more generally as an effective way of integrating contextual information in a network in a way that generalizes effectively. We show this empirically in controlled synthetic scenarios, and also demonstrate significant performance improvement on a variety of challenging, large-scale reinforcement learning (RL) and sequence modelling tasks when a conceptually simple multiplicative interaction module is incorporated. Such improvements are consistent with our hypothesis that the use of appropriately applied mul- tiplicative interactions can provide a more suitable inductive bias over function classes leading to more data-efficient learning, better generalization, and stronger performance. We argue that these operations should feature more widely in neural networks in and of themselves, especially in the increasingly important setting of integrating multiple streams of information (including endogenously created streams e.g. in branching architectures). Our contributions are thus: (i) to re-explore multiplicative interactions and their design principles; (ii) to aid the community's understanding of other models (hypernetworks, gating, multiplicative RNNs) through them; (iii) to show their efficacy at representing certain solutions; and (iv) to empirically apply them to large scale sequence modeling and reinforcement learning problems, where we demonstrate state-of-the-art results.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a simple baseline for few-shot learning, which outperforms all state-of-the-art algorithms on standard benchmarks and few-shot protocols. The baseline is based on pre-training a model on the meta-training dataset using the standard cross-entropy loss, and then fine-tuning on the few-shot dataset. The approach is evaluated on a variety of benchmark datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100, and is also tested on the ImageNet-21k dataset. The paper also proposes a metric to quantify the hardness of few-shot episodes and a way to systematically report performance for different few-shot protocols.",
        "Abstract": "Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the \"hardness\" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",
        "Introduction": "  INTRODUCTION As image classification systems begin to tackle more and more classes, the cost of annotating a massive number of images and the difficulty of procuring images of rare categories increases. This has fueled interest in few-shot learning, where only few labeled samples per class are available for training.  Fig. 1  displays a snapshot of the state-of-the-art. We estimated this plot by using published numbers for the estimate of the mean accuracy, the 95% confidence interval of this estimate and the number of few-shot episodes. For MAML ( Finn et al., 2017 ) and MetaOpt SVM ( Lee et al., 2019 ), we use the number of episodes in the author's Github implementation. The field appears to be progressing steadily albeit slowly based on  Fig. 1 . However, the variance of the estimate of the mean accuracy is not the same as the variance of the accuracy. The former can be zero (e.g., asymptotically for an unbiased estimator), yet the latter could be arbitrarily large. The variance of the accuracies is extremely large in  Fig. 1 . This suggests that progress in the past few years may be less significant than it seems if one only looks at the mean accuracies. To compound the problem, many algorithms report results using different models for different number of ways (classes) and shots (number of labeled samples per class), with aggressive hyper-parameter optimization. 1 Our goal is to develop a simple baseline for few-shot learning, one that does not require specialized training depending on the number of ways or shots, nor hyper-parameter tuning for different protocols. The simplest baseline we can think of is to pre-train a model on the meta-training dataset using the standard cross-entropy loss, and then fine-tune on the few-shot dataset. Although this approach is basic and has been considered before ( Vinyals et al., 2016 ;  Chen et al., 2018 ), it has gone unnoticed that it outperforms many sophisticated few-shot algorithms. Indeed, with a small twist of performing fine-tuning transductively, this baseline outperforms all state-of-the-art algorithms on all standard benchmarks and few-shot protocols (cf.  Table 1 ). Our contribution is to develop a transductive fine-tuning baseline for few-shot learning, our approach works even for a single labeled example and a single test datum per class. Our baseline outperforms the state-of-the-art on a variety of benchmark datasets such as Mini-ImageNet ( Vinyals et al., 2016 ), Tiered-ImageNet ( Ren et al., 2018 ), CIFAR-FS ( Bertinetto et al., 2018 ) and FC-100 ( Oreshkin et al., 2018 ), all with the same hyper-parameters. Current approaches to few-shot learning are hard to scale to large datasets. We report the first few-shot learning results on the ImageNet-21k dataset ( Deng et al., 2009 ) which contains 14.2 million images across 21,814 classes. The rare classes in ImageNet-21k form a natural benchmark for few-shot learning. The empirical performance of this baseline, should not be understood as us suggesting that this is the right way of performing few-shot learning. We believe that sophisticated meta-training, understanding taxonomies and meronomies, transfer learning, and domain adaptation are necessary for effective few-shot learning. The performance of the simple baseline however indicates that we need to interpret existing results 2 with a grain of salt, and be wary of methods that tailor to the benchmark. To facilitate that, we propose a metric to quantify the hardness of few-shot episodes and a way to systematically report performance for different few-shot protocols.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the notion of an arrow of time, a function that monotonically increases as a system evolves in time, and its implications for reinforcement learning. It posits that an arrow of time can be used to quantify the amount of disorder in the environment, analogous to the entropy for isolated thermodynamical systems. It further suggests that an arrow of time can be used to detect and preempt side-effects, penalizing policies that significantly increment the arrow of time by executing difficult-to-reverse transitions. Additionally, it proposes that a directed measure of reachability afforded by an arrow of time can be utilized for deriving an intrinsic reward signal to enable agents to learn complex skills in the absence of external rewards.",
        "Abstract": "We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a well known notion of an arrow of time due to Jordan, Kinderlehrer and Otto (1998). ",
        "Introduction": "  INTRODUCTION The asymmetric progression of time has a profound effect on how we, as agents, perceive, process and manipulate our environment. Given a sequence of observations of our familiar surroundings (e.g. as video frames), we possess the innate ability to predict whether the said observations are ordered correctly. We use this ability not just to perceive, but also to act: for instance, we know to be cautious about dropping a vase, guided by the intuition that the act of breaking a vase cannot be undone. This profound intuition reflects some fundamental properties of the world in which we dwell, and in this work we ask whether and how these properties can be exploited to learn a representation that functionally mimics our understanding of the asymmetric nature of time. The term Arrow of Time was coined by the British astronomer  Eddington (1929)  to denote this inherent asymmetry, which he attributed to the non-decreasing nature of the total thermodynamic entropy of an isolated system, as required by the second law of thermodynamics. Since then, the notion of an arrow of time has been formalized and explored in various contexts, spanning not only physics, but also algorithmic information theory (Zurek, 1989), causal inference ( Janzing et al., 2016 ) and time-series analysis ( Janzing, 2010 ;  Bauer et al., 2016 ). Broadly, an arrow of time can be thought of as a function that monotonously increases as a system evolves in time. Expectedly, the notion of irreversibility plays a central role in the discourse. In sta- tistical physics, it is posited that the arrow of time (i.e. entropy production) is driven by irreversible processes ( Prigogine, 1978 ; Seifert, 2012). To understand how a notion of an arrow of time can be useful in the reinforcement learning context, consider the example of a cleaning robot tasked with moving a box across a room ( Amodei et al., 2016 ). The optimal way of successfully completing the task might involve the robot doing something disruptive, like knocking a vase over ( Fig 1 ). Now on the one hand, such disruptions - or side-effects - might be difficult to recover from. In the extreme case, they might be virtually irreversible - say when the vase is broken. On the other hand, irre- versibility implies that states with a larger number of broken vases tend to occur in the future, and one should therefore expect an arrow of time (as a scalar function of the state) to assign larger values Disorder Time Disorder Time Disorder Time Disorder Time to states with larger number of broken vases. An arrow of time should therefore quantify the amount of disorder in the environment, analogous to the entropy for isolated thermodynamical systems. Now, one possible application could be to detect and preempt such side-effects, for instance by penalizing policies that significantly increment the arrow of time by executing difficult-to-reverse transitions. But the utility of an arrow of time is more general: it serves as a directed measure of reachability. This can be seen by observing that it is more difficult to obtain order from disorder: it is, after all, difficult to reach a state with a vase intact from one with it broken, rather than vice versa. In this sense, we may say that a state is relatively unreachable from another state if an arrow of time assigns a lower value to the former. Further, a directed measure of reachability afforded by an arrow of time can be utilized for deriving an intrinsic reward signal to enable agents to learn complex skills in the absence of external rewards. To see how, consider that an agent tasked with reversing the arrow of time (by creating order from disorder) must in general learn complex skills to achieve its goal. Indeed, gluing together a broken vase will require the agent to learn an array of complex planning and motor skills, which is the ultimate goal of such intrinsic rewards.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a new representation for programs and their specifications, based on the principle that to represent a program, we can use a set of simpler programs. This representation, called property signatures, can be used for program synthesis, algorithm induction, improving code readability, and program analysis. Experiments on a test set of 185 functional programs demonstrate that the use of property signatures leads to a dramatic improvement in the performance of the synthesizer, allowing it to synthesize over twice as many programs in less than one-tenth of the time. The paper also introduces a specialized programming language, called Searcho, which is designed to enable rapid execution of many similar programs.",
        "Abstract": "We introduce the notion of property signatures, a representation for programs and\nprogram specifications meant for consumption by machine learning algorithms.\nGiven a function with input type τ_in and output type τ_out, a property is a function\nof type: (τ_in, τ_out) → Bool that (informally) describes some simple property\nof the function under consideration. For instance, if τ_in and τ_out are both lists\nof the same type, one property might ask ‘is the input list the same length as the\noutput list?’. If we have a list of such properties, we can evaluate them all for our\nfunction to get a list of outputs that we will call the property signature. Crucially,\nwe can ‘guess’ the property signature for a function given only a set of input/output\npairs meant to specify that function. We discuss several potential applications of\nproperty signatures and show experimentally that they can be used to improve\nover a baseline synthesizer so that it emits twice as many programs in less than\none-tenth of the time.",
        "Introduction": "  INTRODUCTION Program synthesis is a longstanding goal of computer science research ( Manna & Waldinger, 1971 ;  Waldinger et al., 1969 ;  Summers, 1977 ; Shaw;  Pnueli & Rosner, 1989 ;  Manna & Waldinger, 1975 ), arguably dating to the 1940s and 50s ( Copeland, 2012 ;  Backus et al., 1957 ). Deep learning methods have shown promise at automatically generating programs from a small set of input-output examples ( Balog et al., 2016 ;  Devlin et al., 2017 ;  Ellis et al., 2018b ;  2019b ). In order to deliver on this promise, we believe it is important to represent programs and specifications in a way that supports learning. Just as computer vision methods benefit from the inductive bias inherent to convolutional neural networks ( LeCun et al., 1989 ), and likewise with LSTMs for natural language and other sequence data ( Hochreiter & Schmidhuber, 1997 ), it stands to reason that ML techniques for computer programs will benefit from architectures with a suitable inductive bias. We introduce a new representation for programs and their specifications, based on the principle that to represent a program, we can use a set of simpler programs. This leads us to introduce the concept of a property, which is a program that computes a boolean function of the input and output of another program. For example, consider the problem of synthesizing a program from a small set of input-output examples. Perhaps the synthesizer is given a few pairs of lists of integers, and the user hopes that the synthesizer will produce a sorting function. Then useful properties might include functions that check if the input and output lists have the same length, if the input list is a subset of the output, if element 0 of the output list is less than element 42, and so on. The outputs of a set of properties can be concatenated into a vector, yielding a representation that we call a property signature. Property signatures can then be used for consumption by machine learning algorithms, essentially serving as the first layer of a neural network. In this paper, we demonstrate the utility of property signatures for program synthesis, using them to perform a type of premise selection as in  Balog et al. (2016) . More broadly, however, we envision that property signatures could be useful across a broad range of problems, including algorithm induction ( Devlin et al., 2017 ), improving code readability ( Allamanis et al., 2014 ), and program analysis ( Heo et al., 2019 ). More specifically, our contributions are: • We introduce the notion of property signatures, which are a general purpose way of featurizing both programs and program specifications (Section 3). • We demonstrate how to use property signatures within a machine-learning based synthesizer for a general-purpose programming language. This allows us to automatically learn a useful set of property signatures, rather than choosing them manually (Sections 3.2 and 4). • We show that a machine learning model can predict the signatures of individual functions given the signature of their composition, and describe several ways this could be used to improve existing synthesizers (Section 5). • We perform experiments on a new test set of 185 functional programs of varying difficulty, designed to be the sort of algorithmic problems that one would ask on an undergraduate computer science examination. We find that the use of property signatures leads to a dramatic improvement in the performance of the synthesizer, allowing it to synthesize over twice as many programs in less than one-tenth of the time (Section 4). An example of a complex program that was synthesized only by the property signatures method is shown in Listing 1. For our experiments, we created a specialized programming language, called Searcho 1 (Section 2), based on strongly-typed functional languages such as Standard ML and Haskell. Searcho is designed so that many similar programs can be executed rapidly, as is needed during a large-scale distributed search during synthesis. We release 2 the programming language, runtime environment, distributed search infrastructure, machine learning models, and training data from our experiments so that they can be used for future research. Listing 1: A program synthesized by our system, reformatted and with variables renamed for readability. This program returns the sub-list of all of the elements in a list that are distinct from their previous value in the list.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents BADGE, a batch active learning algorithm for deep neural networks that creates diverse batches of examples about which the current model is uncertain. BADGE measures uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, and captures diversity by collecting a batch of examples where these gradients span a diverse set of directions. Experiments show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across a variety of conditions.",
        "Abstract": "We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.",
        "Introduction": "  INTRODUCTION In recent years, deep neural networks have produced state-of-the-art results on a variety of important super- vised learning tasks. However, many of these successes have been limited to domains where large amounts of labeled data are available. A promising approach for minimizing labeling effort is active learning, a learning protocol where labels can be requested by the algorithm in a sequential, feedback-driven fashion. Active learning algorithms aim to identify and label only maximally-informative samples, so that a high-performing classifier can be trained with minimal labeling effort. As such, a robust active learning algorithm for deep neural networks may considerably expand the domains in which these models are applicable. How should we design a practical, general-purpose, label-efficient active learning algorithm for deep neural networks? Theory for active learning suggests a version-space-based approach (Cohn et al., 1994;  Balcan et al., 2006 ), which explicitly or implicitly maintains a set of plausible models, and queries examples for which these models make different predictions. But when using highly expressive models like neural networks, these algorithms degenerate to querying every example. Further, the computational overhead of training deep neural networks precludes approaches that update the model to best fit data after each label query, as is often done (exactly or approximately) for linear methods ( Beygelzimer et al., 2010 ;  Cesa-Bianchi et al., 2009 ). Unfortunately, the theory provides little guidance for these models. One option is to use the network's uncertainty to inform a query strategy, for example by labeling samples for which the model is least confident. In a batch setting, however, this creates a pathological scenario where data in the batch are nearly identical, a clear inefficiency. Remedying this issue, we could select samples to maximize batch diversity, but this might choose points that provide little new information to the model. For these reasons, methods that exploit just uncertainty or diversity do not consistently work well across model architectures, batch sizes, or datasets. An algorithm that performs well when using a ResNet, for Published as a conference paper at ICLR 2020 example, might perform poorly when using a multilayer perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Further, what even constitutes a \"large\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use. Moreover, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. As a result, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning. Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization ( Arthur and Vassilvitskii, 2007 ), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE). We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3 and experiments in Section 4. We defer our discussion of related work to Section 5.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper provides an overview of the current state of reinforcement learning (RL) and its potential applications. It discusses the three main challenges of RL: generalization, exploration, and long-term consequences, and how they can be addressed. It also highlights the recent successes of RL in a variety of tasks, from playing games to decision-making systems, and discusses the potential of RL to create artificial general intelligence (AGI).",
        "Abstract": "This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",
        "Introduction": "  Introduction The reinforcement learning (RL) problem describes an agent interacting with an environ- ment with the goal of maximizing cumulative reward through time ( Sutton & Barto, 2017 ). Unlike other branches of control, the dynamics of the environment are not fully known to the agent, but can be learned through experience. Unlike other branches of statistics and ma- chine learning, an RL agent must consider the effects of its actions upon future experience. An efficient RL agent must address three challenges simultaneously: 1. Generalization: be able to learn efficiently from data it collects. 2. Exploration: prioritize the right experience to learn from. 3. Long-term consequences: consider effects beyond a single timestep. The great promise of reinforcement learning are agents that can learn to solve a wide range of important problems. According to some definitions, an agent that can learn to perform at or above human level across a wide variety of tasks is an artificial general intelligence (AGI) ( Minsky, 1961 ;  Legg et al., 2007 ). Interest in artificial intelligence has undergone a resurgence in recent years. Part of this interest is driven by the constant stream of innovation and success on high profile challenges previously deemed impossible for computer systems. Improvements in image recognition are a clear example of these accomplishments, progressing from individual digit recognition ( Le- Cun et al., 1998 ), to mastering ImageNet in only a few years ( Deng et al., 2009 ;  Krizhevsky et al., 2012 ). The advances in RL systems have been similarly impressive: from checkers ( Samuel, 1959 ), to Backgammon ( Tesauro, 1995 ), to Atari games ( Mnih et al., 2015a ), to competing with professional players at DOTA ( Pachocki et al., 2019 ) or StarCraft ( Vinyals et al., 2019 ) and beating world champions at Go ( Silver et al., 2016 ). Outside of playing games, decision systems are increasingly guided by AI systems ( Evans & Gao, 2016 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new theory of nonlinear independent component analysis (ICA) which allows for unknown intrinsic problem dimension and a new architecture, the General Incompressible-flow Network (GIN), for its implementation. The theory is demonstrated on artificial data and the EMNIST dataset, where it is able to extract 22 meaningful variables from EMNIST, encoding both global and local features.",
        "Abstract": "A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.",
        "Introduction": "  INTRODUCTION Deep latent-variable models promise to unlock the key factors of variation within a dataset, opening a window to interpretation and granting the power to manipulate data in an intuitive fashion. The theory of identifiability in linear independent component analysis (ICA) ( Comon, 1994 ) tells us when this is possible, if we restrict the model to a linear transformation, but until recently there was no corresponding theory for the highly nonlinear models needed to manipulate complex data. This changed with the recent breakthrough work by  Khemakhem et al. (2019) , which showed that under relatively mild conditions, it is possible to recover the joint data and latent space distribution, up to a simple transformation in the latent space. The key requirement is that the generating process is conditioned on a variable which is observed along with the data. This condition could be a class label, time index of a time series, or any other piece of information additional to the data. They interpret their theory as a nonlinear version of ICA. This work extends this theory in a direction relevant for application to real-world data. The existing theory assumes knowledge of the intrinsic problem dimension, but this is unrealistic for anything but artificially generated datasets. Here, we show that in the special case of Gaussian latent space distributions, the intrinsic problem dimension can be discovered. The important latent variables are organically separated from noise variables by the estimating model. Furthermore, the variables dis- covered correspond to the true generating latent variables, up to a trivial component-wise translation and scaling. Very similar results exist for other members of the exponential family with two param- eters, such as the beta and gamma distributions. We introduce a variant of the RealNVP ( Dinh et al., 2016 ) invertible neural network: the General Incompressible-flow Network (GIN). The flow is called incompressible in reference to fluid dynam- ics, since it preserves volumes: the Jacobian determinant is simply unity. We emphasise its generality and increased expressive power in comparison to previous volume-preserving flows, such as NICE ( Dinh et al., 2014 ). As already noted in  Khemakhem et al. (2019) , flow-based generative models are a natural fit for the theory of nonlinear ICA, as are the variational autoencoders (VAEs) ( Kingma & Published as a conference paper at ICLR 2020 Welling, 2013 ) used in that work. For us, major advantages of invertible architectures over VAEs are the ability to specify volume preservation and directly optimize the likelihood, and freedom from the requirement to specify the dimension of the model's latent space. An INN always has a latent space of the same dimension as the data. In addition, the forward and backward models share parameters, saving the effort of learning separate models for each direction. In summary, our work makes the following contributions: • We extend the theory of nonlinear ICA to allow for unknown intrinsic problem dimension. Doing so, we find that this dimension can be discovered and a one-to-one correspondence between generating and estimated latent variables established. • We propose as an implementation an invertible neural network obtained by modifying the RealNVP architecture. We call our new architecture GIN: the General Incompressible-flow Network. • We demonstrate the viability of the model on artificial data and the EMNIST dataset. We extract 22 meaningful variables from EMNIST, encoding both global and local features.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents CHAMELEON, an automated compilation framework for Deep Neural Networks (DNNs) that utilizes reinforcement learning and clustering to reduce the optimization time and enable far more diverse tensor operations. Experiments on modern DNNs (AlexNet, VGG-16, and ResNet-18) on a high-end GPU (Titan Xp) show that CHAMELEON yields 4.45×speedup over the leading framework, AutoTVM.",
        "Abstract": "Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that Chameleon provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%.",
        "Introduction": "  INTRODUCTION The enormous computational intensity of Deep Neural Networks (DNNs) have resulted in develop- ing either hand-optimized kernels, such as NVIDIA cuDNN or Intel MKL that serve as backend for a variety of programming environment such as TensorFlow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019). However, the complexity of the tensor operations in DNNs and the volatility of algo- rithms, which has led to unprecedented rate of innovation (LeCun, 2019), calls for developing auto- mated compilation frameworks. To imitate or even surpass the success of hand-optimized libraries, recent research has developed stochastic optimization passes: for general code, STOKE (Schkufza et al., 2013), and neural network code, TVM (Chen et al., 2018a) and TensorComprehensions (Vasi- lache et al., 2018). TVM and TensorComprehensions are based on random or genetic algorithms to search the space of optimized code for neural networks. AutoTVM (Chen et al., 2018b) builds on top of TVM and leverage boosted trees (Chen & Guestrin, 2016) as part of the search cost model to avoid measuring the fitness of each solution (optimized candidate neural network code), and instead predict its fitness. However, even with these innovations the optimizing compilation time can be around 10 hours for ResNet-18 (He et al., 2016), and even more for deeper or wider networks. Since the general objective is to unleash new possibilities by developing automatic optimization passes, long compilation time hinders innovation and could put the current solutions in a position of questionable utility. To solve this problem, we first question the very statistical guarantees which the aforementioned optimization passes rely on. The current approaches are oblivious to the patterns in the design space of schedules that are available for exploitation, and causes inefficient search or even converges to solutions that may even be suboptimal. Also, we notice that current approaches rely on greedy sampling that neglects the distribution of the candidate solutions (configurations). While greedy sampling that passively filter samples based on the fitness estimations from the cost models work, many of their hardware measurements (required for optimization) tend to be redundant and wasteful. Moreover, we found that current solutions that rely on greedy sampling lead to significant fractions of the candidate configurations being redundant over iterations, and that any optimizing Published as a conference paper at ICLR 2020 compiler are prone to invalid configurations which significantly prolongs the optimization time. As such, this work sets out to present an Adaptive approach dubbed CHAMELEON to significantly reduce the compilation time and offer automation while avoiding dependence to hand-optimization, enabling far more diverse tensor operations in the next generation DNNs. We tackle this challenge from two fronts with the following contributions: (1) Devising an Adaptive Exploration module that utilizes reinforcement learning to adapt to unseen design space of new networks to reduce search time yet achieve better performance. (2) Proposing an Adaptive Sampling algorithm that utilizes clustering to adaptively reduce the num- ber of costly hardware measurements, and devising a domain-knowledge inspired Sample Syn- thesis to find configurations that would potentially yield better performance. Real hardware experimentation with modern DNNs (AlexNet, VGG-16, and ResNet-18) on a high- end GPU (Titan Xp), shows that the combination of these two innovations, dubbed CHAMELEON, yields 4.45×speedup over the leading framework, AutoTVM. CHAMELEON is publicly available in the project page: https://bitbucket.org/act-lab/chameleon.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Stochastic Weight Averaging in Parallel (SWAP), a simple strategy to accelerate deep neural network (DNN) training by better utilizing available compute resources. SWAP produces models with good generalization performance by averaging the weights of a set of models sampled from the final stages of a training run. Experiments on popular computer vision datasets (CIFAR10, CIFAR100, and ImageNet) show that SWAP achieves generalization performance comparable to models trained with small-batches but does so in time similar to that of a training run with large-batches. SWAP is able to substantially reduce training times and beat the state of the art for CIFAR10, training in 68% of the time of the winning entry of the DAWNBench competition.",
        "Abstract": "We propose Stochastic Weight Averaging in Parallel (SWAP), an algorithm to accelerate DNN training. Our algorithm uses large mini-batches to compute an approximate solution quickly and then refines it by averaging the weights of multiple models computed independently and in parallel. The resulting models generalize equally well as those trained with small mini-batches but are produced in a substantially shorter time. We demonstrate the reduction in training time and the good generalization performance of the resulting models on the computer vision datasets CIFAR10, CIFAR100, and ImageNet.",
        "Introduction": "  INTRODUCTION Stochastic gradient descent (SGD) and its variants are the de-facto methods to train deep neural networks (DNNs). Each iteration of SGD computes an estimate of the objective's gradient by sam- pling a mini-batch of the available training data and computing the gradient of the loss restricted to the sampled data. A popular strategy to accelerate DNN training is to increase the mini-batch size together with the available computational resources. Larger mini-batches produce more precise gradient estimates; these allow for higher learning rates and achieve larger reductions of the training loss per iteration. In a distributed setting, multiple nodes can compute gradient estimates simul- taneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates, with one synchronization event per iteration. Training with larger mini-batches requires fewer updates, thus fewer synchronization events, yielding good overall scaling behavior. Even though the training loss can be reduced more efficiently, there is a maximum batch size after which the resulting model tends to have worse generalization performance ( McCandlish et al., 2018 ;  Keskar et al., 2016 ;  Hoffer et al., 2017 ;  Golmant et al., 2018 ;  Shallue et al., 2018 ). This phenomenon forces practitioners to use batch sizes below those that achieve the maximum throughput and limits the usefulness of large-batch training strategies. Stochastic Weight Averaging (SWA) ( Izmailov et al., 2018 ) is a method that produces models with good generalization performance by averaging the weights of a set of models sampled from the final stages of a training run. As long as the models all lie in a region where the population loss is mostly convex, the average model can behave well, and in practice, it does. We have observed that if instead of sampling multiple models from a sequence generated by SGD, we generate multiple independent SGD sequences and average models from each, the resulting model achieves similar generalization performance. Furthermore, if all the independent sequences use small-batches, but start from a model trained with large-batches, the resulting model achieves generalization performance comparable with a model trained solely with small-batches. Using these observations, we derive Stochastic Weight Averaging in Parallel (SWAP): A simple strategy to ac- celerate DNN training by better utilizing available compute resources. Our algorithm is simple to implement, fast and produces good results with minor tuning. For several image classification tasks on popular computer vision datasets (CIFAR10, CIFAR100, and ImageNet), we show that SWAP achieves generalization performance comparable to models trained with small-batches but does so in time similar to that of a training run with large-batches. We use SWAP on some of the most efficient publicly available models to date, and show that it's Published as a conference paper at ICLR 2020 able to substantially reduce their training times. Furthermore, we are able to beat the state of the art for CIFAR10 and train in 68% of the time of the winning entry of the DAWNBench competition.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the relation between generalization error and model and dataset sizes in vision and language tasks. Through empirical exploration, the authors observe the emergence of power-law behavior approximating the error with respect to data size and model size. They propose an intuitive candidate for a function approximating the error landscape and evaluate its quality in explaining the observed error landscapes and in extrapolating from small scale (seen) to large scale (unseen) errors. The results show that the proposed function leads to a high quality fit and extrapolation, with mean and standard deviation of the relative errors under 2% when fitting across all scales investigated and under 5% when extrapolating from a slimmed-down model.",
        "Abstract": "The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.",
        "Introduction": "  INTRODUCTION With the success and heightened adoption of neural networks for real world tasks, some questions remain poorly answered. For a given task and model architecture, how much data would one require to reach a prescribed performance level? How big a model would be needed? Addressing such questions is made especially difficult by the mounting evidence that large, deep neural networks trained on large-scale data outperform their smaller counterparts, rendering the training of high performance models prohibitively costly. Indeed, in the absence of practical an- swers to the above questions, surrogate approaches have proven useful. One such common approach is model scaling, where one designs and compares small-scale models, and applies the obtained ar- chitectural principles at a larger scale (e.g.,  Liu et al., 2018 ;  Real et al., 2018 ;  Zoph et al., 2018 ). Despite these heuristics being widely used to various degrees of success, the relation between the performance of a model in the small- and large-scale settings is not well understood. Hence, explor- ing the limitations or improving the efficiency of such methods remains subject to trial and error. In this work we circle back to the fundamental question: what is the (functional) relation between generalization error and model and dataset sizes? Critically, we capitalize on the concept of model scaling in its strictest form: we consider the case where there is some given scaling policy that completely defines how to scale up a model from small to large scales. We include in this context all model parameters, such that traversing from one scale (in which all parameters are known) to another requires no additional resources for specifying the model (e.g., architecture search/design). We empirically explore the behavior of the generalization error over a wide range of datasets and models in vision and language tasks. While the error landscape seems fairly complex at first glance, we observe the emergence of several key characteristics shared across benchmarks and domains. Chief among these characteristics is the emergence of regions where power-law behavior approxi- mates the error well both with respect to data size, when holding model size fixed, and vice versa. Motivated by these observations, we establish criteria which a function approximating the error landscape should meet. We propose an intuitive candidate for such a function and evaluate its quality, both in explaining the observed error landscapes and in extrapolating from small scale (seen) to large scale (unseen) errors. Critically, our functional approximation of the error depends on both Published as a conference paper at ICLR 2020 model and data sizes. We find that this function leads to a high quality fit and extrapolation. For instance, the mean and standard deviation of the relative errors are under 2% when fitting across all scales investigated and under 5% when extrapolating from a slimmed-down model (1/16 of the parameters) on a fraction of the training data (1/8 of the examples) on the ImageNet ( Russakovsky et al., 2015 ) and WikiText-103 ( Merity et al., 2016 ) datasets, with similar results for other datasets. To the best of our knowledge, this is the first work that provides simultaneously: • A joint functional form of the generalization error landscape-as dependent on both data and model size-with few, interpretable degrees of freedom (section 5). • Direct and complete specification (via the scaling policy) of the model configuration attain- ing said generalization error across model and dataset sizes. • Highly accurate approximation of error measurements across model and data scales via the functional form, evaluated on different models, datasets, and tasks (section 6 ). • Highly accurate error prediction from small to large model and data (section 7). We conclude with a discussion of some implications of our findings as a practical and principled tool for understanding network design at small scale and for efficient computation and trade-off design in general. We hope this work also provides a useful empirical leg to stand on and an invitation to search for a theory of generalization error which accounts for our findings.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Transfer learning is a popular method for training deep neural networks on small datasets, but it does not address the issue of model robustness. Adversarial training is often used to improve robustness, but it increases training time and can lead to over-fitting on small datasets. This paper proposes a novel method for improving model robustness in transfer learning scenarios.",
        "Abstract": "Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly.  When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome.\nWe consider robust transfer learning, in which we transfer not only performance but also robustness from a source model to a target domain.  We start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, we produce new models that inherit the robustness of their parent networks. We then consider the case of \"fine tuning\" a network by re-training end-to-end in the target domain. When using lifelong learning strategies, this process preserves the robustness of the source network while achieving high accuracy. By using such strategies, it is possible to produce accurate and robust models with little data, and without the cost of adversarial training. Additionally, we can improve the generalization of adversarially trained models, while maintaining their robustness.",
        "Introduction": "  INTRODUCTION Deep neural networks achieve human-like accuracy on a range of tasks when sufficient training data and computing power is available. However, when large datasets are unavailable for training, or pracitioners require a low-cost training strategy, transfer learning methods are often used. This process starts with a source network (pre-trained on a task for which large datasets are available), which is then re-purposed to act on the target problem, usually with minimal re-training on a small dataset ( Yosinski et al., 2014 ;  Pan & Yang, 2009 ). While transfer learning greatly accelerates the training pipeline and reduces data requirements in the target domain, it does not address the important issue of model robustness. It is well-known that naturally trained models often completely fail under adversarial inputs ( Biggio et al., 2013 ;  Szegedy et al., 2013 ). As a result, researchers and practitioners often resort to adversarial training, in which adversarial examples are crafted on-the-fly during network training and injected into the training set. This process greatly exacerbates the problems that transfer learning seeks to avoid. The high cost of creating adversarial examples increases training time (often by an order of magnitude or more). Furthermore, robustness is known to suffer when training on a small dataset ( Schmidt et al., 2018 ). To make things worse, high-capacity models are often needed to achieve good robustness ( Madry et al., 2017 ;  Kurakin et al., 2016 ;  Shafahi et al., 2019b ), but these models may over-fit badly on small datasets.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel transfer learning method for global black-box optimization in the framework of Bayesian optimization. The method meta-learns a neural acquisition function, which implicitly encodes the task structure, to increase data-efficiency on new task instances. The meta-training is performed using reinforcement learning, making the proposed approach applicable to the standard BO setting without requiring objective function gradients. The efficiency and practical applicability of the approach is demonstrated on a challenging simulation-to-real control task, two hyperparameter optimization problems, and a set of synthetic functions.",
        "Abstract": "Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.",
        "Introduction": "  INTRODUCTION Global optimization of black-box functions is highly relevant for a wide range of real-world tasks. Examples include the tuning of hyperparameters in machine learning, the identification of control parameters, or the optimization of system designs. Such applications oftentimes require the optimiza- tion of relatively low-dimensional ( 10D) functions where each function evaluation is expensive in either time or cost. Furthermore, there is typically no gradient information available. In this context of data-efficient global black-box optimization, Bayesian optimization (BO) has emerged as a powerful solution (Močkus, 1975; Brochu et al., 2010; Snoek et al., 2012; Shahriari et al., 2016). BO's data efficiency originates from a probabilistic surrogate model which is used to generalize over information from individual data points. This model is typically given by a Gaussian process (GP), whose well-calibrated uncertainty prediction allows for an informed exploration- exploitation trade-off during optimization. The exact manner of performing this trade-off, however, is left to be encoded in an acquisition function (AF). There is a wide range of AFs available in the literature which are designed to yield universal optimization strategies and therefore come with minimal assumptions about the class of target objective functions. To achieve optimal data-efficiency on new instances of previously seen tasks, however, it is crucial to incorporate the information obtained from these tasks into the optimization. Therefore, transfer Published as a conference paper at ICLR 2020 learning is an important and active field of research. Indeed, in many practical applications, op- timizations are repeated numerous times in similar settings, underlining the need for specialized optimizers. Examples include hyperparameter optimization which is repeatedly done for the same machine learning model on varying datasets or the optimization of control parameters for a given system with varying physical configurations. Following recent approaches (Swersky et al., 2013; Feurer et al., 2018; Wistuba et al., 2018), we argue that it is beneficial to perform transfer learning for global black-box optimization in the framework of BO to retain the proven generalization capabilities of its underlying GP surrogate model. To not restrict the expressivity of this model, we propose to implicitly encode the task structure in a specialized AF, i.e., in the optimization strategy. We realize this encoding via a novel method which meta-learns a neural AF, i.e., a neural network representing the AF, on a set of source tasks. The meta-training is performed using reinforcement learning, making the proposed approach applicable to the standard BO setting, where we do not assume access to objective function gradients. Our contributions are (1) a novel transfer learning method allowing the incorporation of implicit structural knowledge about a class of objective functions into the framework of BO through learned neural AFs to increase data-efficiency on new task instances, (2) an automatic and practical meta- learning procedure for training such neural AFs which is fully compatible with the black-box optimization setting, i.e, not requiring objective function gradients, and (3) the demonstration of the efficiency and practical applicability of our approach on a challenging simulation-to-real control task, on two hyperparameter optimization problems, as well as on a set of synthetic functions.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to integrating backpropagation into spiking neural networks (SNNs) by using a second accumulation compartment to discretize the error into spikes. This allows for dynamic precision computation and the exploitation of the sparsity of the gradient. The system is mapped to an integer activation artificial neural network (ANN) whose activations are equivalent to the accumulated neuron responses for both the forward and the backward propagation phase. This method is demonstrated to achieve competitive classification performances on the CIFAR10 and CIFAR100 datasets, and is the first to show how the sparsity of the gradient during backpropagation can be exploited within a large-scale SNN processing structure.",
        "Abstract": "Event-based neuromorphic systems promise to reduce the energy consumption of deep neural networks by replacing expensive floating point operations on dense matrices by low energy, sparse operations on spike events. While these systems can be trained increasingly well using approximations of the backpropagation algorithm, this usually requires high precision errors and is therefore incompatible with the typical communication infrastructure of neuromorphic circuits. In this work, we analyze how the gradient can be discretized into spike events when training a spiking neural network. To accelerate our simulation, we show that using a special implementation of the integrate-and-fire neuron allows us to describe the accumulated activations and errors of the spiking neural network in terms of an equivalent artificial neural network, allowing us to largely speed up training compared to an explicit simulation of all spike events. This way we are able to demonstrate that even for deep networks, the gradients can be discretized sufficiently well with spikes if the gradient is properly rescaled. This form of spike-based backpropagation enables us to achieve equivalent or better accuracies on the MNIST and CIFAR10 datasets than comparable state-of-the-art spiking neural networks trained with full precision gradients. The algorithm, which we call SpikeGrad, is based on only accumulation and comparison operations and can naturally exploit sparsity in the gradient computation, which makes it an interesting choice for a spiking neuromorphic systems with on-chip learning capacities.",
        "Introduction": "  INTRODUCTION Spiking neural networks (SNNs) are a new generation of artificial neural network models ( Maass, 1997 ) that try to harness properties of biological neurons to build energy efficient spiking neuro- morphic systems. Processing in traditional artificial neural networks (ANNs) is based on parallel processing of operations on dense tensors of fixed length. In contrast to this, spiking neuromorphic systems communicate with asynchronous events, which allows dynamic, data dependent computation that can exploit high temporal and spatial sparsity. The recent years have seen a large number of approaches devoted to optimization of spiking neural networks with the backpropagation algorithm, either by converting ANNs to SNNs ( Diehl et al., 2015 ;  Esser et al., 2016 ;  Rueckauer et al., 2017 ;  Sengupta et al., 2019 ) or by simulating spikes explicitly in the forward pass and optimizing these dynamics with floating point gradients ( Lee et al., 2016 ;  Yin et al., 2017 ;  Wu et al., 2018b ; c ;  Severa et al., 2019 ;  Jin et al., 2018 ;  Bellec et al., 2018 ;  Zenke & Ganguli, 2018 ;  Shrestha & Orchard, 2018 ). These methods aim to optimize SNNs for efficient inference, and backpropagation is performed offline on a standard computing system. It would however be desirable to also enable on-chip learning in neuromorphic chips using the power of the backpropagation algorithm, and to maintain the advantages of spike-based processing also in the error propagation phase. Previous work on the implementation of backpropagation with spikes is mostly concerned with biological plausibility. A non-spiking version of biologically inspired backpropagation is presented Published as a conference paper at ICLR 2020 by  Sacramento et al. (2018) .  Guerguiev et al. (2017) ,  Neftci et al. (2017)  and  Samadi et al. (2017)  introduce spike-based versions of the backpropagation algorithm using variants of (direct) feedback alignment ( Lillicrap et al., 2016 ;  Nøkland, 2016 ). The exact backpropagation algorithm, which backpropagates through symmetric weights, might however be required to achieve good inference performance on large-scale deep neural networks ( Baldi & Sadowski, 2016 ;  Bartunov et al., 2018 ).  O'Connor & Welling (2016)  and  Thiele et al. (2019)  present implementations of standard backpropa- gation where the gradient is coded into spikes and propagated through symmetric weights. In the same spirit, our work is mostly concerned with exploiting spike based information encoding for energy efficient processing, which means that inference performance and operational simplicity will be preferred over biological plausibility and complex neuron models. We demonstrate how backpropagation can be seamlessly integrated into the spiking neural network framework by using a second accumulation compartment that discretizes the error into spikes. By additionally weighting the activity counters by the learning rate, we obtain a system that is able to perform learning and inference based on accumulations and comparisons alone. As for the forward pass, this allows us to use the dynamic precision computation provided by the discretization of all operations into spike events, and to exploit the sparsity of the gradient. Using a similar reasoning as  Binas et al. (2016)  and  Wu et al. (2019)  have applied to forward propagation in SNNs, we show that the system obtained in this way can be mapped to an integer activation ANN whose activations are equivalent to the accumulated neuron responses for both the forward and the backward propagation phase. This allows us to simulate training of large-scale SNNs efficiently on graphics processing units (GPUs), using their equivalent ANN. Additionally, in contrast to conversion methods that approximate pre-trained ANNs with SNNs, this method guarantees that the inference precision of the SNN will be equivalent to the ANN. In contrast to  O'Connor & Welling (2016) , this is true for any number of spikes and arbitrary spike order. We demonstrate classification accuracies equivalent or superior to existing implementations of SNNs trained with full precision gradients, and comparable to the precision of standard ANNs using similar topologies. This is the first time competitive classification performances are reported on the CIFAR10 and CIFAR100 datasets using a large-scale SNN where both training and inference are fully implemented with spikes. To the best of our knowledge, our work provides for the first time a demonstration of how the sparsity of the gradient during backpropagation could be exploited within a large-scale SNN processing structure.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the success of unsupervised representation learning, which is a fundamental problem in machine learning. It is shown that the success of these methods cannot be attributed to the properties of mutual information alone, and a connection to deep metric learning is established. Empirical evidence is provided to support these claims.",
        "Abstract": "Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",
        "Introduction": "  INTRODUCTION Unsupervised representation learning is a fundamental problem in machine learning. Intuitively, one aims to learn a function g which maps the data into some, usually lower-dimensional, space where one can solve some (generally a priori unknown) target supervised tasks more efficiently, i.e. with fewer labels. In contrast to supervised and semi-supervised learning, the learner has access only to unlabeled data. Even though the task seems ill-posed as there is no natural objective one should optimize, by leveraging domain knowledge this approach can be successfully applied to a variety of problem areas, including image ( Kolesnikov et al., 2019 ;  van den Oord et al., 2018 ;  Hénaff et al., 2019 ;  Tian et al., 2019 ;  Hjelm et al., 2019 ;  Bachman et al., 2019 ) and video classification ( Wang and Gupta, 2015 ;  Sun et al., 2019 ), and natural language understanding ( van den Oord et al., 2018 ;  Peters et al., 2018 ;  Devlin et al., 2019 ). Recently, there has been a revival of approaches inspired by the InfoMax principle ( Linsker, 1988 ): Choose a representation g(x) maximizing the mutual information (MI) between the input and its representation, possibly subject to some structural constraints. MI measures the amount of information obtained about a random variable X by observing some other random variable Y 1 Formally, the MI between X and Y , with joint density p(x, y) and marginal densities p(x) and p(y), is defined as the Kullback-Leibler (KL) divergence between the joint and the product of the marginals The fundamental properties of MI are well understood and have been extensively studied (see e.g.  Kraskov et al. (2004) ). Firstly, MI is invariant under reparametrization of the variables - namely, if X = f 1 (X) and Y = f 2 (Y ) are homeomorphisms (i.e. smooth invertible maps), then I(X; Y ) = I(X ; Y ). Secondly, estimating MI in high-dimensional spaces is a notoriously difficult task, and in practice one often maximizes a tractable lower bound on this quantity ( Poole et al., 2019 ). Nonetheless, any distribution-free high-confidence lower bound on entropy requires a sample size exponential in the size of the bound ( McAllester and Statos, 2018 ). Despite these fundamental challenges, several recent works have demonstrated promising empirical results in representation learning using MI maximization ( van den Oord et al., 2018 ;  Hénaff et al., 2019 ;  Tian et al., 2019 ;  Hjelm et al., 2019 ;  Bachman et al., 2019 ;  Sun et al., 2019 ). In this work we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone. In fact, we show that maximizing tighter bounds on MI can result in worse representations. In addition, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation of the success of the recently introduced methods.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel approach to active contour segmentation, using a 2-channel displacement field inferred directly from the input image. This approach is demonstrated to outperform classical active contour methods, deep active contour methods, and modern semantic segmentation methods across a variety of benchmarks. The tool used to facilitate this approach is a neural mesh renderer, which is used to propagate the intuitive loss back to the displacement of the polygon vertices.",
        "Abstract": "We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images.",
        "Introduction": "  INTRODUCTION The importance of automatic segmentation methods is growing rapidly in a variety of fields, such as medicine, autonomous driving and satellite image analysis, to name but a few. In addition, with the advent of deep semantic segmentation networks, there is a growing interest in the segmentation of common objects with applications in augmented reality and seamless video editing. Since the current semantic segmentation methods often capture the objects well, except for occa- sional inaccuracies along some of the boundaries, fitting a curve to the image boundaries seems to be an intuitive solution. Active contours, is a set of techniques that given an initial contour (which can be provided by an existing semantic segmentation solution) grow iteratively to fit an image boundary. Active contour may also be appropriate in cases, such as medical imaging, where the training dataset is too limited to support the usage of a high-capacity segmentation network. Despite their potential, the classical active contours fall behind the latest semantic segmentation solutions with respect to accuracy. The recent learning-based active contour approaches were not yet demonstrated to outperform semantic segmentation methods across both medical datasets and real world images, despite having success in specific settings. In this work, we propose to evolve an active contour based on a 2-channel displacement field (cor- responding to 2D image coordinates) that is inferred directly and only once from the input image. This is, perhaps, the simplest approach, since unlike the active contour solutions in literature, it does not involve and balance multiple forces, and the displacement is given explicitly. Moreover, the architecture of the method is that of a straightforward encoder-decoder with two decoding networks. The loss is also direct, and involves the comparison of two mostly binary images. The tool that facilitates this explicit and direct approach is a neural mesh renderer. It allows for the propagation of the intuitive loss, back to the displacement of the polygon vertices. While such renderers have been discovered multiple times in the past, and were demonstrated to be powerful solutions in multiple reconstruction problems, this is the first time, as far as we can ascertain that this tool is used for image segmentation. Our empirical results demonstrate state of the art performance in a wide variety of benchmarks, showing a clear advantage over classical active contour methods, deep active contour methods, and modern semantic segmentation methods.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the use of first-order methods in solving large-scale optimization problems. It focuses on the application of online optimization tools and techniques beyond the standard Lipschitz framework, and explores the potential of these methods in various fields such as machine learning, signal processing, wireless communications, and nuclear medicine. It is shown that, if the optimizer faces a sequence of G-Lipschitz convex losses, the incurred min-max regret after T rounds is Ω(GT 1/2 ), and this bound can be achieved by inexpensive first-order methods.",
        "Abstract": "Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann–Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem’s loss functions. In this way, we are able to tackle cases beyond the Lipschitz framework provided by a global norm, and we derive optimal regret bounds and last iterate convergence results through the use of regularized learning methods (such as online mirror descent). These results are subsequently validated in a class of stochastic Poisson inverse problems that arise in imaging science.",
        "Introduction": "  Introduction The surge of recent breakthroughs in machine learning and artificial intelligence has reaffirmed the prominence of first-order methods in solving large-scale optimization problems. One of the main reasons for this is that the computation of higher-order derivatives of functions with thousands - if not millions - of variables quickly becomes prohibitive; another is that gradient calculations are typically easier to distribute and parallelize, especially in large-scale problems. In view of this, first-order methods have met with prolific success in many diverse fields, from machine learning and signal processing to wireless communications, nuclear medicine, and many others [ 10 , 34, 37]. This success is especially pronounced in the field of online optimization, i.e., when the optimizer faces a sequence of time-varying loss functions f t , t = 1, 2, . . . , one at a time - for instance, when drawing different sample points from a large training set [ 11 , 35]. In this general framework, first-order methods have proven extremely flexible and robust, and the attained performance guarantees are well known to be optimal [ 1 ,  11 , 35]. Specifically, if the optimizer faces a sequence of G-Lipschitz convex losses, the incurred min-max regret after T rounds is Ω(GT 1/2 ), and this bound can be achieved by inexpensive first-order methods - such as online mirror descent and its variants [ 11 , 35, 36, 41]. Nevertheless, in many machine learning problems (support vector machines, Poisson inverse problems, quantum tomography, etc.), the loss landscape is not Lipschitz continuous, so the results mentioned above do not apply. Thus, a natural question that emerges is the following: Is it possible to apply online optimization tools and techniques beyond the standard Lipschitz framework? And, if so, how?",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach for optimizing model parallelism decisions, such as placement and scheduling, for neural network computation graphs. We propose an approach that uses a graph neural network to predict mutant sampling distributions of a genetic algorithm, specifically BRKGA, for the input graph to be optimized. This directs BRKGA's search in an input-dependent way, improving solution quality for the same search budget. We evaluate our approach on 372 unique real-world TensorFlow graphs, and show that it significantly outperforms all baseline algorithms on two separate tasks of minimizing runtime and peak memory usage. We also demonstrate that our approach is robust to hyperparameter choices, and that it generalizes to unseen graphs from different tasks, architectures, and datasets.",
        "Abstract": "We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks. ",
        "Introduction": "  INTRODUCTION Deep Learning frameworks such as MXNet ( Chen et al., 2015 ), PyTorch ( Paszke et al., 2017 ), and TensorFlow ( TensorFlow Authors, 2016a ) represent neural network models as computation graphs. Efficiently executing such graphs requires optimizing discrete decisions about how to map the computations in a graph onto hardware so as to minimize a relevant cost metric (e.g., running time, peak memory). Given that execution efficiency is critical for the success of neural networks, there is growing interest in the use of optimizing static compilers for neural network computation graphs, such as Glow ( Rotem et al., 2018 ), MLIR (MLIR Authors, 2018), TVM ( Chen et al., 2018a ), and XLA ( XLA team, 2017 ). Here we consider the model parallelism setting where a computation graph can be executed using multiple devices in parallel. Nodes of the graph are computational tasks, and directed edges denote dependencies between them. We consider jointly optimizing over placement, i.e., which nodes are executed on which devices, and schedule, i.e., the node execution order on each device. These decisions are typically made in either one or two passes in the compiler. We consider two different objectives: 1) minimize running time, subject to not exceeding device memory limits, and 2) minimize peak memory usage. In the optimization literature, such problems are studied under the class of task scheduling, which is known to be NP-hard in typical settings ( Sinnen, 2007 ;  Kwok & Ahmad, 1999 ). As scheduling and placement are just a few of the many complex decisions made in a compiler, it is essential in a production setting that a solution 1) produce solutions of acceptable quality fast, even on large graphs (e.g., thousands of nodes) and decision spaces, and 2) handle diverse graphs from various types of applications, neural network architectures, and users. In this work we consider Published as a conference paper at ICLR 2020 learning an optimizer that satisfies these requirements. Crucially, we aim to learn an optimizer that generalizes to a broad set of previously unseen computation graphs, without the need for training on such graphs, thus allowing it to be fast at test time. Previous works on learning to optimize model parallelism decisions ( Mirhoseini et al., 2017 ;  2018 ;  Addanki et al., 2019 ) have not considered generalization to a broad set of graphs nor joint optimization of placement and scheduling. In  Mirhoseini et al. (2017 ;  2018 ), learning is done from scratch for each computation graph and for placement decisions only, requiring hours (e.g., 12 to 27 hours per graph). This is too slow to be broadly useful in a general-purpose production compiler. We propose an approach that takes only seconds to optimize similar graphs. In concurrent work to ours,  Addanki et al. (2019)  shows generalization to unseen graphs, but they are generated artificially by architecture search for a single learning task and dataset. In contrast, we collect real user-defined graphs spanning a broad set of tasks, architectures, and datasets. In addition, both  Mirhoseini et al. (2017 ;  2018 ) and  Addanki et al. (2019)  consider only placement decisions and rely on TensorFlow's dynamic scheduler; they do not address the static compiler setting where it is natural to jointly optimize scheduling and placement. The key idea of our approach ( Figure 1 ) is to learn a neural network that, conditioned on the input graph to be optimized, directs an existing optimization algorithm's search such that it finds a better solution in the same search budget. We choose the Biased Random-Key Genetic Algorithm (BRKGA ( Gonçalves & Resende, 2011 )) as the optimization algorithm after an extensive evaluation of several choices showed that it gives by far the best speed-vs-quality trade-off for our application. BRKGA produces good solutions in just a few seconds even for real-world TensorFlow graphs with thousands of nodes, and we use learning to improve the solution quality significantly at similar speed. We train a graph neural network ( Battaglia et al., 2018 ) to take a computation graph as input and output node-specific proposal distributions to use in the mutant generation step of BRKGA's inner loop. BRKGA is then run to completion with those input-dependent distribution choices, instead of input- agnostic default choices, to compute execution decisions. The distributions are predicted at each node, resulting in a high-dimensional prediction problem. There is no explicit supervision available, so we use the objective value as a reward signal in a contextual bandit approach with REINFORCE ( Williams, 1992 ). Our approach, \"Reinforced Genetic Algorithm Learning\" (REGAL), uses the network's ability to generalize to new graphs to significantly improve the solution quality of the genetic algorithm for the same objective evaluation budget. We follow the static compiler approach of constructing a coarse static cost model to evaluate execution decisions and optimizing them with respect to it, as done in ( Addanki et al., 2018 ;  Jia et al., 2018 ). This is in contrast to evaluating the cost by executing the computation graph on hardware ( Mirhoseini Published as a conference paper at ICLR 2020 et al., 2017 ;  2018 ). A computationally cheap cost model enables fast optimization. It is also better suited for distributed training of RL policies since a cost model is cheap to replicate in parallel actors, while hardware environments are not. Our cost model corresponds to classical NP-hard scheduling problems, so optimizing it is difficult. In this paper we focus fully on learning to optimize this cost model, leaving integration with a compiler for future work. We structure the neural network's task as predicting proposal distributions to use in the search over execution decisions, rather than the decisions themselves directly. Empirically we have found the direct prediction approach to be too slow at inference time for our application and generalizes poorly. Our approach potentially allows the network to learn a more abstract policy not directly tied to detailed decisions that are specific to particular graphs, which may generalize better to new graphs. It can also make the learning task easier as the search may succeed even with sub-optimal proposal distribution predictions, thus smoothening the reward function and allowing the network to incrementally learn better proposals. The node-specific proposal distribution choices provide a rich set of knobs for the network to flexibly direct the search. Combining learning with a search algorithm has been shown to be successful (e.g., ( Silver et al., 2017 ;  2018 )), and our work can be seen as an instance of the same high-level idea. This paper makes several contributions: • We are the first to demonstrate learning a policy for jointly optimizing placement and scheduling that generalizes to a broad set of real-world TensorFlow graphs. REGAL significantly outperforms all baseline algorithms on two separate tasks of minimizing runtime and peak memory usage (section 5.3) on datasets constructed from 372 unique real-world TensorFlow graphs, the largest dataset of its kind in the literature and at least an order of magnitude larger than the ones in previous works ( Mirhoseini et al., 2017 ;  2018 ;  Chen et al., 2018b ;  Addanki et al., 2018 ;  2019 ). • We use a graph neural network to predict mutant sampling distributions of a genetic algo- rithm, specifically BRKGA, for the input graph to be optimized. This directs BRKGA's search in an input-dependent way, improving solution quality for the same search budget. • We compare extensively to classical optimization algorithms, such as enumerative search, local search, genetic search, and other heuristics, and analyze room-for-improvement in the objective value available to be captured via learning. Both are missing in previous works.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a clustering method to investigate the functional organization of excitatory neurons in primary visual cortex (V1). A rotation-equivariant convolutional neural network model of V1 is used to map convolutional features to neural responses, and a two-stage procedure is used to split neurons into functional types. Results suggest that V1 neurons may be organized into functional clusters, with around 100 clusters that can be grouped into a smaller number of 10-20 groups. Analysis of the resulting clusters via their maximally exciting inputs (MEIs) shows that many of these functional clusters correspond to distinct computations.",
        "Abstract": "Similar to a convolutional neural network (CNN), the mammalian retina encodes visual information into several dozen nonlinear feature maps, each formed by one ganglion cell type that tiles the visual space in an approximately shift-equivariant manner. Whether such organization into distinct cell types is maintained at the level of cortical image processing is an open question. Predictive models building upon convolutional features have been shown to provide state-of-the-art performance, and have recently been extended to include rotation equivariance in order to account for the orientation selectivity of V1 neurons. However, generally no direct correspondence between CNN feature maps and groups of individual neurons emerges in these models, thus rendering it an open question whether V1 neurons form distinct functional clusters. Here we build upon the rotation-equivariant representation of a CNN-based V1 model and propose a methodology for clustering the representations of neurons in this model to find functional cell types independent of preferred orientations of the neurons. We apply this method to a dataset of 6000 neurons and visualize the preferred stimuli of the resulting clusters. Our results highlight the range of non-linear computations in mouse V1.",
        "Introduction": "  INTRODUCTION A compact description of the nonlinear computations in primary visual cortex (V1) is still elusive. Like in the retina ( Baden et al., 2016 ;  Sanes & Masland, 2015 ), such understanding could come from a functional classification of neurons. However, it is currently unknown if excitatory neurons in V1 are organized into functionally distinct cell types. It has recently been proposed that predictive models of neural responses based on convolutional neu- ral networks could help answer this question ( Klindt et al., 2017 ;  Ecker et al., 2019 ). These models are based on a simple principle ( Fig. 1 - 1 ): learn a core (e.g. a convolutional network) that is shared among all neurons and provides nonlinear features Φ(x), which are turned into predictions of neural responses by a linear readout for each neuron ( Antolík et al., 2016 ). Models based on this basic ar- chitecture exploit aspects of our current understanding of V1 processing. First, convolutional weight sharing allows us to characterize neurons performing the same computation but with differently lo- cated receptive fields by the same feature map ( Klindt et al., 2017 ;  McIntosh et al., 2016 ;  Kindel et al., 2019 ;  Cadena et al., 2019 ). Second, V1 neurons can extract local oriented features such as edges at different orientations, and most low-level image features can appear at arbitrary orienta- tions. Therefore,  Ecker et al. (2019)  proposed a rotation-equivariant convolutional neural network model of V1 that extends the convolutional weight sharing to the orientations domain. The basic idea of previous work ( Klindt et al., 2017 ;  Ecker et al., 2019 ) is that each convolutional feature map could correspond to one cell type. While this idea is conceptually appealing, it hinges on the assumption that V1 neurons are described well by individual units in the shared feature space. However, existing models do not tend to converge to such solutions. Instead, V1 neurons are better described by linearly combining units from the same spatial location in multiple different feature maps ( Ecker et al., 2019 ). Whether or not there are distinct functional cell types in V1 is therefore still an open question. Here, we address this question by introducing a clustering method on rotation-equivariant spaces. We treat the feature weights ( Fig. 1 - 1 ) that map convolutional features to neural responses as an approximate low-dimensional vector representation of this neuron's input-output function. We then split neurons into functional types using a two-stage procedure: first, because these feature weights have a rotation-equivariant structure, we find an alignment that rotates them into a canonical orientation ( Fig. 1 - 2 ); in a second step, we cluster them using standard approaches such as k-means or Gaussian mixture models ( Fig. 1 - 3 ). We apply our method to the published model and data of  Ecker et al. (2019)  that contains recordings of around 6000 neurons in mouse V1 under stimulation with natural images. Our results suggest that V1 neurons might indeed be organized into functional clusters. The dataset is best described by a GMM with around 100 clusters, which are to some extent redundant but can be grouped into a smaller number of 10-20 groups. We analyse the resulting clusters via their maximally exciting inputs (MEIs) ( Walker et al., 2018 ) to show that many of these functional clusters do indeed correspond to distinct computations.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a technique to train piecewise affine models, such as ReLU networks, to be simultaneously provably robust to all l p -norms with p ∈ [1, ∞]. The technique is based on having guarantees on the l 1 - and l ∞ -distance to the decision boundary and region boundaries, and is shown to be effective with experiments on four datasets. The networks trained with this method are the first ones having non-trivial provable robustness wrt l 1 -, l 2 - and l ∞ -perturbations.",
        "Abstract": "In recent years several adversarial attacks and defenses have been proposed. Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees. While provably robust models for specific $l_p$-perturbation models have been developed, we show that they do not come with any guarantee against other $l_q$-perturbations. We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt $l_1$- \\textit{and} $l_\\infty$-perturbations and show how that leads to the first provably robust models wrt any $l_p$-norm for $p\\geq 1$.",
        "Introduction": "  Introduction The vulnerability of neural networks against adversarial manipulations ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ) is a problem for their deployment in safety critical systems such as autonomous driving and medical applications. In fact, small perturbations of the input which appear irrelevant or are even imperceivable to humans change the decisions of neural networks. This questions their reliability and makes them a target of adversarial attacks. To mitigate the non-robustness of neural networks many empirical defenses have been proposed, e.g. by  Gu & Rigazio (2015) ; Zheng et al. (2016);  Papernot et al. (2016) ;  Huang et al. (2016) ;  Bastani et al. (2016) ;  Madry et al. (2018) , but at the same time more sophisticated attacks have proven these defenses to be ineffective ( Carlini & Wagner, 2017 ;  Athalye et al., 2018 ;  Mosbach et al., 2018 ), with the exception of the adversarial training of  Madry et al. (2018) . However, even these l ∞ -adversarially trained models are not more robust than normal ones when attacked with perturbations of small l p -norms with p = ∞ ( Sharma & Chen, 2019 ;  Schott et al., 2019 ;  Croce et al., 2019b ;  Kang et al., 2019 ). The situation becomes even more complicated if one extends the attack models beyond l p -balls to other sets of perturbations ( Brown et al., 2017 ;  Engstrom et al., 2017 ;  Hendrycks & Dietterich, 2019 ;  Geirhos et al., 2019 ). Another approach, which fixes the problem of overestimating the robustness of a model, is provable guarantees, which means that one certifies that the decision of the network does not change in a certain l p -ball around the target point. Along this line, current state-of-the- art methods compute either the norm of the minimal perturbation changing the decision at a point (e.g.  Katz et al. (2017) ; Tjeng et al. (2019)) or lower bounds on it ( Hein & Andriushchenko, 2017 ;  Raghunathan et al., 2018 ; Wong & Kolter, 2018). Several new training schemes like ( Hein & Andriushchenko, 2017 ;  Raghunathan et al., 2018 ; Wong & Kolter, 2018;  Mirman et al., 2018 ;  Croce et al., 2019a ; Xiao et al., 2019;  Gowal et al., 2018 ) aim at both enhancing the robustness of networks and producing models more amenable to verification techniques. However, all of them are only able to prove robustness against a single kind of perturbations, typically either l 2 - or l ∞ -bounded, and not wrt all the l p -norms simultaneously, as shown in Section 5. Some are also designed to work for a specific p ( Mirman et al., 2018 ;  Gowal et al., 2018 ), and it is not clear if they can be extended to other norms. The only two papers which have shown, with some limitations, non-trivial empirical robustness against multiple types of adversarial examples are  Schott et al. (2019)  and Tramèr & Boneh Published as a conference paper at ICLR 2020 (2019), which resist to l 0 - resp. l 1 -, l 2 - and l ∞ -attacks. However, they come without provable guarantees and  Schott et al. (2019)  is restricted to MNIST. In this paper we aim at robustness against all the l p -bounded attacks for p ≥ 1. We study the non-trivial case where none of the l p -balls is contained in another. If p is the radius of the l p -ball for which we want to be provably robust, this requires: d 1 p − 1 q q > p > q for p < q and d being the input dimension. We show that, for normally trained models, for the l 1 - and l ∞ -balls we use in the experiments none of the adversarial examples constrained to be in the l 1 -ball (i.e. results of an l 1 -attack) belongs to the l ∞ -ball, and vice versa. This shows that certifying the union of such balls is significantly more complicated than getting robust in only one of them, as in the case of the union the attackers have a much larger variety of manipulations available to fool the classifier. We propose a technique which allows to train piecewise affine models (like ReLU networks) which are simultaneously provably robust to all the l p -norms with p ∈ [1, ∞]. First, we show that having guarantees on the l 1 - and l ∞ -distance to the decision boundary and region boundaries (the borders of the polytopes where the classifier is affine) is sufficient to derive meaningful certificates on the robustness wrt all l p -norms for p ∈ (1, ∞). In particular, our guarantees are independent of the dimension of the input space and thus go beyond a naive approach where one just exploits that all l p -metrics can be upper- and lower-bounded wrt any other l q -metric. Then, we extend the regularizer introduced in  Croce et al. (2019a)  so that we can directly maximize these bounds at training time. Finally, we show the effectiveness of our technique with experiments on four datasets, where the networks trained with our method are the first ones having non-trivial provable robustness wrt l 1 -, l 2 - and l ∞ -perturbations.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents GraN-DAG, a score-based method for structure learning of causal graphical models using neural networks. It extends the framework of Zheng et al. (2018) to deal with nonlinear relationships between variables and uses an argument similar to what is used in NOTEARS (Zheng et al., 2018) to adapt the acyclicity constraint to the nonlinear model. This approach improves upon popular methods while avoiding the design of greedy algorithms based on heuristics. The paper also discusses the application of GraN-DAG to real-world datasets and its potential to improve machine intelligence.",
        "Abstract": "We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks while being competitive with existing greedy search methods on important metrics for causal inference.",
        "Introduction": "  INTRODUCTION Structure learning and causal inference have many important applications in different areas of sci- ence such as genetics ( Koller & Friedman, 2009 ;  Peters et al., 2017 ), biology ( Sachs et al., 2005 ) and economics ( Pearl, 2009 ). Bayesian networks (BN), which encode conditional independencies using directed acyclic graphs (DAG), are powerful models which are both interpretable and computation- ally tractable. Causal graphical models (CGM) ( Peters et al., 2017 ) are BNs which support inter- ventional queries like: What will happen if someone external to the system intervenes on variable X? Recent work suggests that causality could partially solve challenges faced by current machine learn- ing systems such as robustness to out-of-distribution samples, adaptability and explainability ( Pearl, 2019 ;  Magliacane et al., 2018 ). However, structure and causal learning are daunting tasks due to both the combinatorial nature of the space of structures (the number of DAGs grows super expo- nentially with the number of nodes) and the question of structure identifiability (see Section 2.2). Nevertheless, these graphical models known qualities and promises of improvement for machine intelligence renders the quest for structure/causal learning appealing. The typical motivation for learning a causal graphical model is to predict the effect of various in- terventions. A CGM can be best estimated when given interventional data, but interventions are often costly or impossible to obtained. As an alternative, one can use exclusively observational data and rely on different assumptions which make the graph identifiable from the distribution (see Section 2.2). This is the approach employed in this paper. We propose a score-based method ( Koller & Friedman, 2009 ) for structure learning named GraN- DAG which makes use of a recent reformulation of the original combinatorial problem of finding an optimal DAG into a continuous constrained optimization problem. In the original method named NOTEARS ( Zheng et al., 2018 ), the directed graph is encoded as a weighted adjacency matrix which represents coefficients in a linear structural equation model (SEM) ( Pearl, 2009 ) (see Section 2.3) and enforces acyclicity using a constraint which is both efficiently computable and easily differen- tiable, thus allowing the use of numerical solvers. This continuous approach improved upon popular methods while avoiding the design of greedy algorithms based on heuristics. Our first contribution is to extend the framework of  Zheng et al. (2018)  to deal with nonlinear relationships between variables using neural networks (NN) ( Goodfellow et al., 2016 ). To adapt the acyclicity constraint to our nonlinear model, we use an argument similar to what is used in 2 BACKGROUND Before presenting GraN-DAG, we review concepts relevant to structure and causal learning.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an unbiased gradient estimator for expectations over discrete random variables based on unordered sets of samples without replacement. The estimator reduces variance by avoiding duplicate samples and further reduces variance by deriving a built-in control variate. The estimator is applicable to tasks such as reinforcement learning, discrete latent variable modelling, structured prediction, hard attention, and many other tasks that use models with discrete operations in their computational graphs.",
        "Abstract": "We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators in both high and low entropy settings.",
        "Introduction": "  INTRODUCTION Put replacement in your basement! We derive the unordered set estimator 1 : an unbiased (gradi- ent) estimator for expectations over discrete random variables based on (unordered sets of) samples without replacement. In particular, we consider the problem of estimating (the gradient of) the expectation of f (x) where x has a discrete distribution p over the domain D, i.e. This expectation comes up in reinforcement learning, discrete latent variable modelling (e.g. for compression), structured prediction (e.g. for translation), hard attention and many other tasks that use models with discrete operations in their computational graphs (see e.g.  Jang et al. (2016) ). In general, x has structure (such as a sequence), but we can treat it as a 'flat' distribution, omitting the bold notation, so x has a categorical distribution over D given by p(x), x ∈ D. Typically, the distribution has parameters θ, which are learnt through gradient descent. This requires estimating the gradient ∇ θ E x∼p θ (x) [f (x)], using a set of samples S. A gradient estimate e(S) is unbiased if The samples S can be sampled independently or using alternatives such as stratified sampling which reduce variance to increase the speed of learning. In this paper, we derive an unbiased gradient esti- mator that reduces variance by avoiding duplicate samples, i.e. by sampling S without replacement. This is challenging as samples without replacement are dependent and have marginal distributions that are different from p(x). We further reduce the variance by deriving a built-in control variate, which maintains the unbiasedness and does not require additional samples.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a methodology grounded in counterfactual reasoning to empirically evaluate the explanations generated using saliency maps in deep reinforcement learning (RL). We survey the ways in which saliency maps have been used as evidence in explanations of deep RL agents, describe a new interventional method to evaluate the inferences made from saliency maps, and experimentally evaluate how well the pixel-level inferences of saliency maps correspond to the semantic-level inferences of humans.",
        "Abstract": "Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",
        "Introduction": "  INTRODUCTION Saliency map methods are a popular visualization technique that produce heatmap-like output high- lighting the importance of different regions of some visual input. They are frequently used to explain how deep networks classify images in computer vision applications (Simonyan et al., 2014; Zeiler & Fergus, 2014; Springenberg et al., 2015; Ribeiro et al., 2016; Dabkowski & Gal, 2017; Fong & Vedaldi, 2017; Selvaraju et al., 2017; Shrikumar et al., 2017; Smilkov et al., 2017; Zhang et al., 2018) and to explain how agents choose actions in reinforcement learning (RL) applications (Bog- danovic et al., 2015; Wang et al., 2016; Zahavy et al., 2016; Greydanus et al., 2017; Iyer et al., 2018; Sundar, 2018; Yang et al., 2018; Annasamy & Sycara, 2019). Saliency methods in computer vision and reinforcement learning use similar procedures to generate these maps. However, the temporal and interactive nature of RL systems presents a unique set of opportunities and challenges. Deep models in reinforcement learning select sequential actions whose effects can interact over long time periods. This contrasts strongly with visual classification tasks, in which deep models merely map from images to labels. For RL systems, saliency maps are often used to assess an agent's internal representations and behavior over multiple frames in the environment, rather than to assess the importance of specific pixels in classifying images. Despite their common use to explain agent behavior, it is unclear whether saliency maps provide useful explanations of the behavior of deep RL agents. Some prior work has evaluated the ap- plicability of saliency maps for explaining the behavior of image classifiers (Samek et al., 2017; Adebayo et al., 2018; Kindermans et al., 2019), but there is not a corresponding literature evaluating the applicability of saliency maps for explaining RL agent behavior. In this work, we develop a methodology grounded in counterfactual reasoning to empirically evalu- ate the explanations generated using saliency maps in deep RL. Specifically, we: C1 Survey the ways in which saliency maps have been used as evidence in explanations of deep RL agents. C2 Describe a new interventional method to evaluate the inferences made from saliency maps. C3 Experimentally evaluate how well the pixel-level inferences of saliency maps correspond to the semantic-level inferences of humans.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces SEED (Scalable, Efficient, Deep-RL), a modern reinforcement learning agent that is designed to scale well, be flexible, and efficiently utilize available resources. SEED is a distributed agent where model inference is done centrally combined with fast streaming RPCs to reduce the overhead of inference calls. We show that with simple methods, one can achieve state-of-the-art results faster on a number of tasks. We also analyze the cost of running SEED against IMPALA, a commonly used state-of-the-art distributed RL algorithm, and show cost reductions of up to 80% while being significantly faster. The implementation is open-sourced and examples of running it at scale on Google Cloud are provided.",
        "Abstract": "We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost. of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 twice as fast in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.",
        "Introduction": "  INTRODUCTION The field of reinforcement learning (RL) has recently seen impressive results across a variety of tasks. This has in part been fueled by the introduction of deep learning in RL and the introduction of accelerators such as GPUs. In the very recent history, focus on massive scale has been key to solve a number of complicated games such as AlphaGo ( Silver et al., 2016 ), Dota ( OpenAI, 2018 ) and StarCraft 2 ( Vinyals et al., 2017 ). The sheer amount of environment data needed to solve tasks trivial to humans, makes distributed machine learning unavoidable for fast experiment turnaround time. RL is inherently comprised of heterogeneous tasks: running environments, model inference, model training, replay buffer, etc. and current state-of-the-art distributed algorithms do not efficiently use compute resources for the tasks. The amount of data and inefficient use of resources makes experiments unreasonably expensive. The two main challenges addressed in this paper are scaling of reinforcement learning and optimizing the use of modern accelerators, CPUs and other resources. We introduce SEED (Scalable, Efficient, Deep-RL), a modern RL agent that scales well, is flexible and efficiently utilizes available resources. It is a distributed agent where model inference is done centrally combined with fast streaming RPCs to reduce the overhead of inference calls. We show that with simple methods, one can achieve state-of-the-art results faster on a number of tasks. For optimal performance, we use TPUs (cloud.google.com/tpu/) and TensorFlow 2 ( Abadi et al., 2015 ) to simplify the implementation. The cost of running SEED is analyzed against IMPALA ( Espeholt et al., 2018 ) which is a commonly used state-of-the-art distributed RL algorithm ( Veeriah et al. (2019) ;  Li et al. (2019) ;  Deverett et al. (2019) ;  Omidshafiei et al. (2019) ;  Vezhnevets et al. (2019) ;  Hansen et al. (2019) ;  Schaarschmidt et al.; Tirumala et al. (2019) , ...). We show cost reductions of up to 80% while being significantly faster. When scaling SEED to many accelerators, it can train on millions of frames per second. Finally, the implementation is open-sourced together with examples of running it at scale on Google Cloud (see Appendix A.4 for details) making it easy to reproduce results and try novel ideas.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a theoretical and empirical analysis of Multi-Task Reinforcement Learning (MTRL), which aims to learn multiple tasks jointly instead of learning them separately. The paper proposes a neural network architecture that allows for the extraction of a common representation among tasks, and provides theoretical guarantees that exploit the framework of Maurer et al. (2016). The paper also proposes a variant of Fitted Q-Iteration (FQI) called Multi Fitted Q-Iteration (MFQI) and performs an empirical evaluation in challenging RL problems, showing significant performance improvements compared to the single-task version of the algorithms.",
        "Abstract": "We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.",
        "Introduction": "  INTRODUCTION Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them separately, leveraging the assumption that the considered tasks have common properties which can be exploited by Machine Learning (ML) models to generalize the learning of each of them. For instance, the features extracted in the hidden layers of a neural network trained on multiple tasks have the advantage of being a general representation of structures common to each other. This translates into an effective way of learning multiple tasks at the same time, but it can also improve the learning of each individual task compared to learning them separately ( Caruana, 1997 ). Furthermore, the learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary knowledge to learn a new similar task resulting in a more effective and faster learning than learning the new task from scratch ( Baxter, 2000 ;  Thrun & Pratt, 2012 ). The same benefits of extraction and exploitation of common features among the tasks achieved in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single agent on multiple Reinforcement Learning (RL) problems with common structures ( Taylor & Stone, 2009 ; Lazaric, 2012). In particular, in MTRL an agent can be trained on multiple tasks in the same Published as a conference paper at ICLR 2020 domain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar domains, e.g. balancing a pendulum or balancing a double pendulum 1 . Considering recent advances in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental benchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular and effective way to extract common features among tasks in MTRL algorithms ( Rusu et al., 2015 ;  Liu et al., 2016 ;  Higgins et al., 2017 ). However, despite the high representational capacity of DL models, the extraction of good features remains challenging. For instance, the performance of the learning process can degrade when unrelated tasks are used together ( Caruana, 1997 ;  Baxter, 2000 ); another detrimental issue may occur when the training of a single model is not balanced properly among multiple tasks ( Hessel et al., 2018 ). Recent developments in MTRL achieve significant results in feature extraction by means of algorithms specifically developed to address these issues. While some of these works rely on a single deep neural network to model the multi-task agent ( Liu et al., 2016 ;  Yang et al., 2017 ;  Hessel et al., 2018 ;  Wulfmeier et al., 2019 ), others use multiple deep neural networks, e.g. one for each task and another for the multi-task agent ( Rusu et al., 2015 ;  Parisotto et al., 2015 ;  Higgins et al., 2017 ;  Teh et al., 2017 ). Intuitively, achieving good results in MTRL with a single deep neural network is more desirable than using many of them, since the training time is likely much less and the whole architecture is easier to implement. In this paper we study the benefits of shared representations among tasks. We theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that exploit the theoretical framework provided by  Maurer et al. (2016) , in which the authors present upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a single shared representation. The significancy of this result is that the cost of learning the shared representation decreases with a factor O( 1 / √ T ), where T is the number of tasks for many function approximator hypothesis classes. The main contribution of this work is twofold. 1. We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate Policy-Iteration (API) 2 ( Farahmand, 2011 ) in the MTRL setting, and we extend the approx- imation error bounds in  Maurer et al. (2016)  to the case of multiple tasks with different dimensionalities. Then, we show how to combine these results resulting in, to the best of our knowledge, the first proposed extension of the finite-time bounds of AVI/API to MTRL. Despite being an extension of previous works, we derive these results to justify our approach showing how the error propagation in AVI/API can theoretically benefit from learning multiple tasks jointly. 2. We leverage these results proposing a neural network architecture, for which these bounds hold with minor assumptions, that allow us to learn multiple tasks with a single regressor extracting a common representation. We show an empirical evidence of the consequence of our bounds by means of a variant of Fitted Q-Iteration (FQI) ( Ernst et al., 2005 ), based on our shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI). Then, we perform an empirical evaluation in challenging RL problems proposing multi- task variants of the Deep Q-Network (DQN) ( Mnih et al., 2015 ) and Deep Deterministic Policy Gradient (DDPG) ( Lillicrap et al., 2015 ) algorithms. These algorithms are practical implementations of the more general AVI/API framework, designed to solve complex problems. In this case, the bounds apply to these algorithms only with some assumptions, e.g. stationary sampling distribution. The outcome of the empirical analysis joins the theoretical results, showing significant performance improvements compared to the single- task version of the algorithms in various RL problems, including several MuJoCo ( Todorov et al., 2012 ) domains.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper studies the effect of the choice of activation function on the training of overparametrized neural networks. It is known that any continuous function on a bounded domain can be approximated arbitrarily well by a finite neural network with one hidden layer, and ReLU activation generally performs better than the traditional choices in terms of training and generalization. This paper explores the possibility of a universality phenomenon for training neural networks similar to the one for expressive power.",
        "Abstract": "It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but no clear understanding has emerged. This state of affairs underscores the importance of theoretically understanding the impact of activation functions on training. In the present paper, we provide theoretical results about the effect of activation function on the training of highly overparametrized 2-layer neural networks. A crucial property that governs the performance of an activation is whether or not it is smooth: \n• For non-smooth activations such as ReLU, SELU, ELU, which are not smooth because there is a point where either the ﬁrst order or second order derivative is discontinuous, all eigenvalues of the associated Gram matrix are large under minimal assumptions on the data. \n• For smooth activations such as tanh, swish, polynomial, which have derivatives of all orders at all points, the situation is more complex: if the subspace spanned by the data has small dimension then the minimum eigenvalue of the Gram matrix can be small leading to slow training. But if the dimension is large and the data satisﬁes another mild condition, then the eigenvalues are large. If we allow deep networks, then the small data dimension is not a limitation provided that the depth is sufﬁcient. \nWe discuss a number of extensions and applications of these results.",
        "Introduction": "  INTRODUCTION It is now well-known that overparametrized feedforward neural networks trained using gradient-based algorithms with appropriate hyperparameter choices reliably achieve near-zero training error, e.g., Neyshabur et al. (2015). Importantly, overparametrization also often helps with generalization; but our central concern here is the training error which is an important component in understanding generalization. We study the effect of the choice of activation function (we often just say activation) on the training of overparametrized neural networks. By overparametrized setting we roughly mean that the number of parameters or weights in the networks is much larger than the number of data samples. The well-known universal approximation theorem for feedforward neural networks states that any continuous function on a bounded domain can be approximated arbitrarily well by a finite neural network with one hidden layer. This theorem is generally stated for specific activation functions such as sigmoid or ReLU. A more general form of the theorem shows this for essentially all non- polynomial activations (Leshno et al., 1993; Pinkus, 1999; Sonoda & Murata, 2017). This theorem Published as a conference paper at ICLR 2020 concerns only the expressive power and does not address how the training and generalization of neural networks are affected by the choice of activation, nor does it provide quantitative information about the size of the network needed for the task. Traditionally, sigmoid and tanh had been the popular activations but a number of other activations have also been considered including linear and polynomial activations (Arora et al., 2019a; Du & Lee, 2018; Kileel et al., 2019). One of the many innovations in the resurgence of neural networks in the last decade or so has been the realization that ReLU activation generally performs better than the traditional choices in terms of training and generalization. ReLU is now the de facto standard for activation functions for neural networks but many other activations are also used which may be advantageous depending on the situation (e.g. (Goodfellow et al., 2016, Chapter 6)). In practice, most activation functions often achieve reasonable performance. To quote Goodfellow et al. (2016): In general, a wide variety of differentiable functions perform perfectly well. Many unpublished activation functions perform just as well as the popular ones. Concretely, Ramachandran et al. (2018) provides a list of ten non-standard functions which all perform close to the state of the art at some tasks. This hints at the possibility of a universality phenomenon for training neural networks similar to the one for expressive power mentioned above.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new algorithm, DRIL (Disagreement-Regularized Imitation Learning), to address the covariate shift problem in imitation learning. DRIL operates by training an ensemble of policies on the demonstration data, and using the disagreement in their predictions as a cost which is optimized through reinforcement learning together with a supervised behavioral cloning cost. Theoretical results show that DRIL obtains a O( κT ) regret bound, where κ is a measure which quantifies a tradeoff between the concentration of the demonstration data and the diversity of the ensemble outside the demonstration data. Empirical results show that DRIL matches or significantly outperforms behavioral cloning and generative adversarial imitation learning, often recovering expert performance with only a few trajectories.",
        "Abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.",
        "Introduction": "  INTRODUCTION Training artificial agents to perform complex tasks is essential for many applications in robotics, video games and dialogue. If success on the task can be accurately described using a reward or cost function, reinforcement learning (RL) methods offer an approach to learning policies which has proven to be successful in a wide variety of applications ( Mnih et al., 2015 ; 2016;  Lillicrap et al., 2016 ;  Hessel et al., 2018 ). However, in other cases the desired behavior may only be roughly specified and it is unclear how to design a reward function to characterize it. For example, training a video game agent to adopt more human-like behavior using RL would require designing a reward function which characterizes behaviors as more or less human-like, which is difficult. Imitation learning (IL) offers an elegant approach whereby agents are trained to mimic the demon- strations of an expert rather than optimizing a reward function. Its simplest form consists of training a policy to predict the expert's actions from states in the demonstration data using supervised learn- ing. While appealingly simple, this approach suffers from the fact that the distribution over states observed at execution time can differ from the distribution observed during training. Minor errors which initially produce small deviations become magnified as the policy encounters states further and further from its training distribution. This phenomenon, initially noted in the early work of ( Pomerleau, 1989 ), was formalized in the work of ( Ross & Bagnell, 2010 ) who proved a quadratic O( T 2 ) bound on the regret and showed that this bound is tight. The subsequent work of ( Ross et al., 2011 ) showed that if the policy is allowed to further interact with the environment and make queries to the expert policy, it is possible to obtain a linear bound on the regret. However, the ability to query an expert can often be a strong assumption. In this work, we propose a new and simple algorithm called DRIL (Disagreement-Regularized Im- itation Learning) to address the covariate shift problem in imitation learning, in the setting where the agent is allowed to interact with its environment. Importantly, the algorithm does not require any additional interaction with the expert. It operates by training an ensemble of policies on the demonstration data, and using the disagreement in their predictions as a cost which is optimized through RL together with a supervised behavioral cloning cost. The motivation is that the policies in the ensemble will tend to agree on the set of states covered by the expert, leading to low cost, but are more likely to disagree on states not covered by the expert, leading to high cost. The RL cost Published as a conference paper at ICLR 2020 thus guides the agent back towards the distribution of the expert, while the supervised cost ensures that it mimics the expert within the expert's distribution. Our theoretical results show that, subject to realizability and optimization oracle assumptions 1 , our algorithm obtains a O( κT ) regret bound, where κ is a measure which quantifies a tradeoff between the concentration of the demonstration data and the diversity of the ensemble outside the demon- stration data. We evaluate DRIL empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning, often recovering expert performance with only a few trajectories.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper revisits the loss function of neural networks from a geometric perspective, focusing on the relationship between the functional space of the network and its parameterization. It defines pure and spurious critical points for arbitrary networks, and investigates in detail the classification of critical points in the case of linear networks. It proves that non-global local minima are necessarily pure critical points for convex losses, and emphasizes that even for linear networks it is possible to find many smooth convex losses with non-global local minima. This paper provides a framework for explaining existing results in a unified way, and sets the stage for studying gradient dynamics on determinantal varieties.",
        "Abstract": "The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network's weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the landscape of linear networks with different loss functions and different parameterizations. Our analysis clearly illustrates that the absence of \"bad\" local minima in the loss landscape of linear networks is due to two distinct phenomena that apply in different settings: it is true for arbitrary smooth convex losses in the case of architectures that can express all linear maps (\"filling architectures\") but it holds only for the quadratic loss when the functional space is a determinantal variety (\"non-filling architectures\"). Without any assumption on the architecture, smooth convex losses may lead to landscapes with many bad minima.",
        "Introduction": "  INTRODUCTION A fundamental goal in the theory of deep learning is to explain why the optimization of the non- convex loss function of a neural network does not seem to be affected by the presence of non- global local minima. Many papers have addressed this issue by studying the landscape of the loss function ( Baldi & Hornik, 1989 ;  Choromanska et al., 2015 ;  Kawaguchi, 2016 ;  Venturi et al., 2018 ). These papers have shown that, in certain situations, any local minimum for the loss is in fact always a global minimum. Unfortunately, it is also known that this property does not apply in more general realistic settings ( Yun et al., 2018 ;  Venturi et al., 2018 ). More recently, researchers have begun to search for explanations based on the dynamics of optimization. For example, in certain limit situations, the gradient flow of over-parameterized networks will avoid local minimizers ( Chizat & Bach, 2018 ;  Mei et al., 2018 ). We believe however that the study of the static properties of the loss function (the structure of its critical locus) is not settled. Even in the case of linear networks, the existing literature paints a purely analytical picture of the loss, and provides no sort of explanation as to \"why\" such architectures exhibit no bad local minima. A complete understanding of the critical locus should be a prerequisite for investigating the dynamics of the optimization. The goal of this paper is to revisit the loss function of neural networks from a geometric perspective, focusing on the relationship between the functional space of the network and its parameterization. In particular, we view the loss as a composition {parameter space} µ → {functional space} → R. In this setting, the function is almost always convex, however the composition L = • µ is not. Critical points for L can in fact arise for two distinct reasons: either because we are applying to a non-convex functional space, or because the parameterizing map µ is locally degenerate. We distinguish these two types of critical points by referring to them, respectively, as pure and spurious. Intuitively, pure critical points actually reflect the geometry of the functional space associated with the network, while spurious critical points arise as \"artifacts\" from the parameterization. After defining pure and critical points for arbitrary networks, we investigate in detail the classification of critical points in the case of linear networks. The functional space for such networks can be identified with a family of linear maps, and we can describe its geometry using algebraic tools. Many of our statements rely on a careful analysis of the differential of the matrix multiplication map. In particular, we prove that non-global local minima are necessarily pure critical points for convex losses, which means that many properties of the loss landscape can be read from the functional space. On the other hand, we emphasize that even for linear networks it is possible to find many smooth convex losses with non-global local minima. This happens when the functional space is a determinantal variety, i.e., a (non-smooth and non-convex) family of matrices with bounded rank. In this setting, the absence of non-global minima actually holds in the particular case of the quadratic loss, because of very special geometric properties of determinantal varieties that we discuss. Related Work.  Baldi & Hornik (1989)  first proved the absence of non-global (\"bad\") local min- ima for linear networks with one hidden layer (autoencoders). Their result was generalized to the case of deep linear networks by  Kawaguchi (2016) . Many papers have since then studied the loss landscape of linear networks under different assumptions ( Hardt & Ma, 2016 ;  Yun et al., 2017 ;  Zhou & Liang, 2017 ;  Laurent & von Brecht, 2017 ;  Lu & Kawaguchi, 2017 ;  Zhang, 2019 ). In particular,  Laurent & von Brecht (2017)  showed that linear networks with \"no bottlenecks\" have no bad local minima for arbitrary smooth loss functions.  Lu & Kawaguchi (2017)  and  Zhang (2019)  argued that \"depth does not create local minima\", meaning that the absence of local minima of deep linear net- works is implied by the same property of shallow linear networks. Our study of pure and spurious critical points can be used as a framework for explaining all these results in a unified way. The opti- mization dynamics of linear networks are also an active area of research ( Arora et al., 2019 ; 2018), and our analysis of the landscape in function space sets the stage for studying gradient dynamics on determinantal varieties, as in  Bah et al. (2019) . Our work is also closely related to objects of study in applied algebraic geometry, particularly determinantal varieties and ED discriminants ( Draisma et al., 2013 ;  Ottaviani et al., 2013 ). Finally, we mention other recent works that study neural net- works using algebraic-geometric tools ( Mehta et al., 2018 ;  Kileel et al., 2019 ;  Jaffali & Oeding, 2019 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces Learned Step Size Quantization (LSQ), a new approach for learning the quantization mapping for each layer in a deep network. LSQ provides a simple way to approximate the gradient to the quantizer step size that is sensitive to quantized state transitions, and a heuristic to bring the magnitude of step size updates into better balance with weight updates. Using LSQ to train several network architectures on the ImageNet dataset, the paper demonstrates significantly better accuracy than prior quantization approaches and, for the first time, 3-bit quantized networks reaching full precision network accuracy.",
        "Abstract": "Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code.",
        "Introduction": "  INTRODUCTION Deep networks are emerging as components of a number of revolutionary technologies, including image recognition ( Krizhevsky et al., 2012 ), speech recognition ( Hinton et al., 2012 ), and driving assistance ( Xu et al., 2017 ). Unlocking the full promise of such applications requires a system perspective where task performance, throughput, energy-efficiency, and compactness are all critical considerations to be optimized through co-design of algorithms and deployment hardware. Current research seeks to develop methods for creating deep networks that maintain high accuracy while reducing the precision needed to represent their activations and weights, thereby reducing the computation and memory required for their implementation. The advantages of using such algorithms to create networks for low precision hardware has been demonstrated in several deployed systems ( Esser et al., 2016 ;  Jouppi et al., 2017 ;  Qiu et al., 2016 ). It has been shown that low precision networks can be trained with stochastic gradient descent by updating high precision weights that are quantized, along with activations, for the forward and backward pass ( Courbariaux et al., 2015 ;  Esser et al., 2016 ). This quantization is defined by a mapping of real numbers to the set of discrete values supported by a given low precision representation (often integers with 8-bits or less). We would like a mapping for each quantized layer that maximizes task performance, but it remains an open question how to optimally achieve this. To date, most approaches for training low precision networks have employed uniform quantizers, which can be configured by a single step size parameter (the width of a quantization bin), though more complex nonuniform mappings have been considered ( Polino et al., 2018 ). Early work with low precision deep networks used a simple fixed configuration for the quantizer ( Hubara et al., 2016 ;  Esser et al., 2016 ), while starting with  Rastegari et al. (2016) , later work focused on fitting the quantizer to the data, either based on statistics of the data distribution ( Li & Liu, 2016 ;  Zhou et al., 2016 ;  Cai et al., 2017 ;  McKinstry et al., 2018 ) or seeking to minimize quantization error during training ( Choi et al., 2018c ;  Zhang et al., 2018 ). Most recently, work has focused on using backpropagation with Published as a conference paper at ICLR 2020 While attractive for their simplicity, fixed mapping schemes based on user settings place no guarantees on optimizing network performance, and quantization error minimization schemes might perfectly minimize quantization error and yet still be non optimal if a different quantization mapping actually minimizes task error. Learning the quantization mapping by seeking to minimize task loss is appealing to us as it directly seeks to improve on the metric of interest. However, as the quantizer itself is discontinuous, such an approach requires approximating its gradient, which existing methods have done in a relatively coarse manner that ignore the impact of transitions between quantized states ( Choi et al., 2018b ;a;  Jung et al., 2018 ). Here, we introduce a new way to learn the quantization mapping for each layer in a deep network, Learned Step Size Quantization (LSQ), that improves on prior efforts with two key contributions. First, we provide a simple way to approximate the gradient to the quantizer step size that is sensitive to quantized state transitions, arguably providing for finer grained optimization when learning the step size as a model parameter. Second, we propose a simple heuristic to bring the magnitude of step size updates into better balance with weight updates, which we show improves convergence. The overall approach is usable for quantizing both activations and weights, and works with existing methods for backpropagation and stochastic gradient descent. Using LSQ to train several network architectures on Published as a conference paper at ICLR 2020 the ImageNet dataset, we demonstrate significantly better accuracy than prior quantization approaches ( Table 1 ) and, for the first time that we are aware of, demonstrate the milestone of 3-bit quantized networks reaching full precision network accuracy ( Table 4 ).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the Reformer model, a new architecture for Transformer models which solves the problems of large-scale long-sequence models by using reversible layers, splitting activations inside feed-forward layers, and approximate attention computation based on locality-sensitive hashing. Experiments on a synthetic task, a text task, and an image generation task show that Reformer matches the results obtained with full Transformer but runs much faster, especially on the text task, and with orders of magnitude better memory efficiency.",
        "Abstract": "Large Transformer models routinely achieve state-of-the-art results on\na number of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve\nthe efficiency of Transformers. For one, we replace dot-product attention\nby one that uses locality-sensitive hashing, changing its complexity\nfrom O($L^2$) to O($L \\log L$), where $L$ is the length of the sequence.\nFurthermore, we use reversible residual layers instead of the standard\nresiduals, which allows storing activations only once in the training\nprocess instead of N times, where N is the number of layers.\nThe resulting model, the Reformer, performs on par with Transformer models\nwhile being much more memory-efficient and much faster on long sequences.",
        "Introduction": "  INTRODUCTION The Transformer architecture ( Vaswani et al., 2017 ) is widely used in natural language processing and yields state-of-the-art results on a number of tasks. To obtain these results, researchers have resorted to training ever larger Transformer models. The number of parameters exceeds 0.5B per layer in the largest configuration reported in ( Shazeer et al., 2018 ) while the number of layers goes up to 64 in ( Al-Rfou et al., 2018 ). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music ( Huang et al., 2018 ) and images ( Parmar et al., 2018 ), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research 1 . Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply ineffi- cient? Consider the following calculation: the 0.5B parameters used in the largest reported Trans- former layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K × 1K × 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. • Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. • Since the depth d f f of intermediate feed-forward layers is often much larger than the depth d model of attention activations, it accounts for a large fraction of memory use. • Attention on sequences of length L is O(L 2 ) in both computational and memory complex- ity, so even for a single sequence of 64K tokens can exhaust accelerator memory. We introduce the Reformer model which solves these problems using the following techniques: • Reversible layers, first introduced in  Gomez et al. (2017) , enable storing only a single copy of activations in the whole model, so the N factor disappears. • Splitting activations inside feed-forward layers and processing them in chunks removes the d f f factor and saves memory inside feed-forward layers. • Approximate attention computation based on locality-sensitive hashing replaces the O(L 2 ) factor in attention layers with O(L log L) and so allows operating on long sequences. We study these techniques and show that they have negligible impact on the training process com- pared to the standard Transformer. Splitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. Applying reversible residuals instead of the standard ones does change the model but has a negligible effect on training in all configurations we experimented with. Finally, locality-sensitive hashing in attention is a more major change that can influence the training dynamics, depending on the number of concurrent hashes used. We study this parameter and find a value which is both efficient to use and yields results very close to full attention. We experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K. In both cases we show that Reformer matches the results obtained with full Transformer but runs much faster, especially on the text task, and with orders of magnitude better memory efficiency.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the fundamental open question of Model Agnostic Meta-Learning (MAML) - does the meta-initialization learned by the outer loop result in rapid learning on unseen test tasks or is the success primarily due to feature reuse? Through layer freezing experiments and latent representational analysis, the authors find that feature reuse is the predominant reason for efficient learning. Based on these results, they propose the ANIL (Almost No Inner Loop) algorithm, a significant simplification to MAML that removes the inner loop updates for all but the head (final layer) of a neural network during training and inference. The authors also study the effect of the head of the network, finding that once training is complete, the head can be removed, and the representations can be used without adaptation to perform unseen tasks. Finally, they discuss rapid learning and feature reuse in the context of other meta-learning approaches.",
        "Abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse,  with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.",
        "Introduction": "  INTRODUCTION A central problem in machine learning is few-shot learning, where new tasks must be learned with a very limited number of labelled datapoints. A significant body of work has looked at tackling this challenge using meta-learning approaches (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Finn et al., 2017; Santoro et al., 2016; Ravi and Larochelle, 2016; Nichol and Schulman, 2018). Broadly speaking, these approaches define a family of tasks, some of which are used for training and others solely for evaluation. A proposed meta-learning algorithm then looks at learning properties that generalize across the different training tasks, and result in fast and efficient learning of the evaluation tasks. One highly successful meta-learning algorithm has been Model Agnostic Meta-Learning (MAML) (Finn et al., 2017). At a high level, the MAML algorithm is comprised of two optimization loops. The outer loop (in the spirit of meta-learning) aims to find an effective meta-initialization, from which the inner loop can perform efficient adaptation - optimize parameters to solve new tasks with very few labelled examples. This algorithm, with deep neural networks as the underlying model, has been highly influential, with significant follow on work, such as first order variants (Nichol and Schulman, 2018), probabilistic extensions (Finn et al., 2018), augmentation with generative modelling (Rusu et al., 2018), and many others (Hsu et al., 2018; Finn and Levine, 2017; Grant et al., 2018; Triantafillou et al., 2019). Despite the popularity of MAML, and the numerous followups and extensions, there remains a fundamental open question on the basic algorithm. Does the meta-initialization learned by the outer loop result in rapid learning on unseen test tasks (efficient but significant changes in the representations) or is the success primarily due to feature reuse (with the meta-initialization already providing high quality representations)? In this paper, we explore this question and its many surprising consequences. Our main contributions are: • We perform layer freezing experiments and latent representational analysis of MAML, finding that feature reuse is the predominant reason for efficient learning. • Based on these results, we propose the ANIL (Almost No Inner Loop) algorithm, a significant simplification to MAML that removes the inner loop updates for all but the head (final layer) of a neural network during training and inference. ANIL performs identically to MAML on standard benchmark few-shot classification and RL tasks and offers computational benefits over MAML. • We study the effect of the head of the network, finding that once training is complete, the head can be removed, and the representations can be used without adaptation to perform unseen tasks, which we call the No Inner Loop (NIL) algorithm. • We study different training regimes, e.g. multiclass classification, multitask learning, etc, and find that the task specificity of MAML/ANIL at training facilitate the learning of better features. We also find that multitask training, a popular baseline with no task specificity, performs worse than random features. • We discuss rapid learning and feature reuse in the context of other meta-learning approaches.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an approach to discover the shared space of motor programs underlying a variety of manipulation tasks. The approach is based on the insight that an abstraction of a demonstration into a sequence of motor programs or primitives, each of which correspond to an implied movement sequence, and must yield back the demonstration when the inferred primitives are 'recomposed'. The paper demonstrates that the method allows for learning a primitive space that captures the shared motions required across diverse skills, and that these motor programs can be adapted and composed to further perform specific tasks. Furthermore, it is shown that these motor programs are semantically meaningful, and can be recombined to solve robotic tasks using reinforcement learning, resulting in 2 orders of magnitude faster training than reinforcement learning in the low-level control space.",
        "Abstract": "In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing demonstrations into primitives often assume manually defined primitives and bypass the difficulty of discovering these primitives. On the other hand, approaches in primitive discovery put restrictive assumptions on the complexity of a primitive, which limit applicability to narrow tasks. Our approach attempts to circumvent these challenges by jointly learning both the underlying motor primitives and recomposing these primitives to form the original demonstration. Through constraints on both the parsimony of primitive decomposition and the simplicity of a given primitive, we are able to learn a diverse set of motor primitives, as well as a coherent latent representation for these primitives. We demonstrate both qualitatively and quantitatively, that our learned primitives capture semantically meaningful aspects of a demonstration. This allows us to compose these primitives in a hierarchical reinforcement learning setup to efficiently solve robotic manipulation tasks like reaching and pushing. Our results may be viewed at https://sites.google.com/view/discovering-motor-programs. ",
        "Introduction": "  INTRODUCTION We have seen impressive progress over the recent years in learning based approaches to perform a plethora of manipulation tasks ( Levine et al., 2016 ;  Andrychowicz et al., 2018 ;  Levine et al., 2018 ;  Pinto & Gupta, 2016 ; Agrawal et al., 2016). However, these systems are typically task-centric savants - able to only execute a single task that they were trained for. This is because these systems, whether leveraging demonstrations or environmental rewards, attempt to learn each task tabula rasa, where low to high level motor behaviours, are all acquired from scratch in context of the specified task. In contrast, we humans are adept at a variety of basic manipulation skills e.g. picking, pushing, grasping etc., and can effortlessly perform these diverse tasks via a unified manipulation system. How can we step-away from the paradigm of learning task-centric savants, and move towards building similar unified manipulation systems? We can begin by not treating these tasks independently, but via instead exploiting the commonalities across them. One such commonality relates to the primitive actions executed to accomplish the tasks - while the high-level semantics of tasks may differ significantly, the low and mid-level motor programs across them are often shared e.g. to either pick or push an object, one must move the hand towards it. This concept of motor programs can be traced back to the work of Lashley, who noted that human motor movements consist of 'orderly sequences' that are not simply sequences of stimulus-response patterns. The term 'motor programs' is however better attributed to  Keele (1968)  as being representative of 'muscle commands that execute a movement sequence uninfluenced by peripheral feedback', though later works shifted the focus from muscle commands to the movement itself, while allowing for some feedback ( Adams, 1971 ). More directly relevant to our motivation is Schmidt's notion of 'generalized' motor programs ( Schmidt, 1975 ) that can allow abstracting a class of movement patterns instead of a singular one. In this work, we present an approach to discover the shared space of (generalized) motor programs underlying a variety of tasks, and show that elements from this space can be composed to accomplish diverse tasks. Not only does this allow understanding the commonalities and shared structure across diverse skills, the discovered space of motor programs can provide a high-level abstraction using which new skills can be acquired quickly by simply learning the set of desired motor programs to compose. We are not the first to advocate the use of such mid-level primitives for efficient learning or gener- alization, and there have been several reincarnations of this idea over the decades, from 'operators' in the classical STRIPS algorithm ( Fikes & Nilsson, 1971 ), to 'options' ( Sutton et al., 1999 ) or 'primitives' ( Schaal et al., 2005 ) in modern usage. These previous approaches however assume a set of manually defined/programmed primitives and therefore bypass the difficulty of discovering them. While some attempts have been made to simultaneously learn the desired skill and the underlying primitives, learning both from scratch is difficult, and are therefore restricted to narrow tasks. Towards overcoming this difficulty, we observe that instead of learning the primitives from scratch in the context of a specific task, we can instead discover them using demonstrations of a diverse set of tasks. Concretely, by leveraging demonstrations for different skills e.g. pouring, grasping, opening etc., we discover the motor programs (or movement primitives) that occur across these. We present an approach to discover movement primitives from a set of unstructured robot demon- stration i.e. demonstrations without additional parsing or segmentation labels available. This is a challenging task as each demonstration is composed of a varying number of unknown primitives, and therefore the process of learning entails both, learning the space of primitives as well as un- derstanding the available demonstrations in context of these. Our approach is based on the insight that an abstraction of a demonstrations into a sequence of motor programs or primitives, each of which correspond to an implied movement sequence, and must yield back the demonstration when the inferred primitives are 'recomposed'. We build on this and formulate an unsupervised approach to jointly learn the space of movement primitives, as well as a parsing of the available demonstrations into a high-level sequence of these primitives. We demonstrate that our method allows us to learn a primitive space that captures the shared motions required across diverse skills, and that these motor programs can be adapted and composed to further perform specific tasks. Furthermore, we show that these motor programs are semantically meaningful, and can be recombined to solved robotic tasks using reinforcement learning. Specifically, solving reaching and pushing tasks with reinforcement learning over the space of primitives achieves 2 orders of magnitude faster training than reinforcement learning in the low-level control space.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the current state of few-shot learning and the limitations of the two most commonly used datasets, Omniglot and mini-ImageNet. It argues that a more challenging and realistic benchmark is needed to further progress in this area, and proposes three criteria for such a benchmark: homogeneity, within-dataset generalization, and class relationships.",
        "Abstract": "Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models’ ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.",
        "Introduction": "  INTRODUCTION Few-shot learning refers to learning new concepts from few examples, an ability that humans naturally possess, but machines still lack. Improving on this aspect would lead to more efficient algorithms that can flexibly expand their knowledge without requiring large labeled datasets. We focus on few-shot classification: classifying unseen examples into one of N new 'test' classes, given only a few reference examples of each. Recent progress in this direction has been made by considering a meta-problem: though we are not interested in learning about any training class in particular, we can exploit the training classes for the purpose of learning to learn new classes from few examples, thus acquiring a learning procedure that can be directly applied to new few-shot learning problems too. This intuition has inspired numerous models of increasing complexity (see Related Work for some examples). However, we believe that the commonly-used setup for measuring success in this direction is lacking. Specifically, two datasets have emerged as de facto benchmarks for few-shot learning: Omniglot (Lake et al., 2015), and mini-ImageNet (Vinyals et al., 2016), and we believe that both of them are approaching their limit in terms of allowing one to discriminate between the merits of different approaches. Omniglot is a dataset of 1623 handwritten characters from 50 different alphabets and contains 20 examples per class (character). Most recent methods obtain very high accuracy on Omniglot, rendering the comparisons between them mostly uninformative. mini-ImageNet is formed out of 100 ImageNet (Russakovsky et al., 2015) classes (64/16/20 for train/validation/test) and contains 600 examples per class. Albeit harder than Omniglot, it has the same property that most recent methods trained on it present similar accuracy when controlling for model capacity. We advocate that a more challenging and realistic benchmark is required for further progress in this area. More specifically, current benchmarks: 1) Consider homogeneous learning tasks. In contrast, real-life learning experiences are heterogeneous: they vary in terms of the number of classes and examples per class, and are unbalanced. 2) Measure only within-dataset generalization. However, we are eventually after models that can generalize to entirely new distributions (e.g., datasets). 3) Ignore the relationships between classes when forming episodes. Specifically, the coarse-grained classification of dogs and chairs may present different difficulties than the fine-grained classification of dog breeds, and current benchmarks do not establish a distinction between the two.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a method for automated discovery of diverse self-organized patterns in high-dimensional and complex dynamical systems, such as Conway's Game of Life. The proposed method leverages and transposes population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs) algorithms to address the joint challenge of learning to represent interesting patterns and discovering them in a sample efficient manner. The paper compares various approaches to define or learn goal space representations for the sample efficient discovery of diverse patterns in a continuous GOL testbed, and shows that an extension of a state-of-the-art POP-IMGEP algorithm, with incremental learning of a goal space using a deep auto-encoder, is equally efficient than a system pretrained on a hand-made database of patterns.",
        "Abstract": "In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the interesting features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts.",
        "Introduction": "  INTRODUCTION Self-organization of patterns that emerge from local rules is a pervasive phenomena in natural and artificial dynamical systems ( Ball, 1999 ). It ranges from the formation of snow flakes, spots and rays on animal's skin, to spiral galaxies. Understanding these processes has boosted progress in many fields, ranging from physics to biology ( Camazine et al., 2003 ). This progress relied importantly on the use of powerful and rich abstract computational models of self-organization ( Kauffman, 1993 ). For example, cellular automata like Conway's Game of Life (GOL) have been used to study the emergence of spatially localized patterns (SLPs) ( Gardener, 1970 ), informing theories of the origins of life ( Gardener, 1970 ;  Beer, 2004 ). SLPs, such as the famous glider in GOL ( Gardner et al., 1983 ), are self-organizing patterns that have a local extension and can exist independently of other patterns. However, finding such novel self-organized patterns, and mapping the space of possible emergent patterns, has so far relied heavily on manual tuning of parameters and initial states. Moreover, the dependence of this exploration process on the human eye to identify \"interesting\" patterns is strongly limiting further advances. We formulate here the problem of automated discovery of a diverse set of self-organized patterns in such high-dimensional, complex dynamical systems. This involves several challenges. A first Published as a conference paper at ICLR 2020 challenge consists in determining a representation of patterns, possibly through learning, enabling to incentivize the discovery of diverse \"interesting\" patterns. Such a representation guides exploration by providing a measure of (di-)similarity between patterns. This problem is particularly challenging in domains where patterns are high-dimensional as in GOL. In such cases, scientists have a limited intuition about what useful features are and how to represent them. Moreover, low-dimensional representations of patterns are needed for human browsing and the visualization of the discoveries. Representation learning shall both guide exploration, and be fed by self-collected data. A second challenge consists in how to automate exploration of high-dimensional, continuous initial- ization parameters to discover efficiently \"interesting\" patterns, such as SLPs, with a limited budget of experiments. Sample efficiency is important to enable the later use of such discovery algorithms for physical systems ( Grizou et al., 2020 ), where experimental time and costs are strongly bounded. For example, in the continuous GOL used in this paper as a testbed, initialization consists in deter- mining the values of a real-valued, high-dimensional 256×256 matrix besides 7 additional dynamics parameters. The possible variations of this matrix are too large for a simple random sampling to be efficient. More structured methods are needed. To address these challenges, we propose to leverage and transpose recent intrinsically motivated learning algorithms, within the family of population-based Intrinsically Motivated Goal Exploration Processes (POP-IMGEPs - denoted simply as IMGEPs below, Baranes & Oudeyer (2013);  Péré et al. (2018) ). They were initially designed to enable autonomous robots to explore and learn what effects can be produced by their actions, and how to control these effects. IMGEPs self-define goals in a goal space that represents important features of the outcomes of actions, such as the position reached by an arm. This allows the discovery of diverse novel effects within their goal representations. It was recently shown how deep neuronal auto-encoders enabled unsupervised learning of goal repre- sentations in IMGEPs from raw pixel perception of a robot's visual scene ( Laversanne-Finot et al., 2018 ). We propose to use a similar mechanism for automated discovery of patterns by unsupervised learning of a low-dimensional representation of features of self-organized patterns. This removes the need for human expert knowledge to define such representations. Moreover, a key ingredient for sample efficient exploration of IMGEPs for robotics has been the use of structured motion primitives to encode the space of body motions ( Pastor et al., 2013 ). We propose to use a similar mechanism to handle the generation of structured initial states in GOL-like complex systems, based on specialized recurrent neural networks (CPPNs) ( Stanley, 2006 ). In summary, we provide in this paper the following contributions: • We formulate the problem of automated discovery of diverse self-organized patterns in high-dimensional and complex game-of-life types of dynamical systems. • We show how to transpose POP-IMGEPs algorithms to address the associated joint chal- lenge of learning to represent interesting patterns and discovering them in a sample efficient manner. • We compare various approaches to define or learn goal space representations for the sample efficient discovery of diverse SLPs in a continuous GOL testbed. • We show that an extension of a state-of-the-art POP-IMGEP algorithm, with incremen- tal learning of a goal space using a deep auto-encoder, is equally efficient than a system pretrained on a hand-made database of patterns.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new bound on the parameter curvature of deep neural networks, and investigates the relation between the singular value distribution of the input-output Jacobian and locally maximal curvature of the parameter space. It also examines the effects of different orthogonal, nearly isometric initializations on the optimization rate, and investigates whether constraining the spectrum of the Jacobian matrix of each layer affects optimization rate. Results show that Euclidean networks with a carefully designed initialization reduce the test misclassification error at approximately the same rate as their manifold constrained counterparts, and overall attain a higher accuracy.",
        "Abstract": "    Recently mean field theory has been successfully used to analyze properties\n    of wide, random neural networks. It gave rise to a prescriptive theory for\n    initializing feed-forward neural networks with orthogonal weights, which\n    ensures that both the forward propagated activations and the backpropagated\n    gradients are near \\(\\ell_2\\) isometries and as a consequence training is\n    orders of magnitude faster. Despite strong empirical performance, the\n    mechanisms by which critical initializations confer an advantage in the\n    optimization of deep neural networks are poorly understood. Here we show a\n    novel connection between the maximum curvature of the optimization landscape\n    (gradient smoothness) as measured by the Fisher information matrix (FIM) and\n    the spectral radius of the input-output Jacobian, which partially explains\n    why more isometric networks can train much faster. Furthermore, given that\n    orthogonal weights are necessary to ensure that gradient norms are\n    approximately preserved at initialization, we experimentally investigate the\n    benefits of maintaining orthogonality throughout training, and we conclude\n    that manifold optimization of weights performs well regardless of the\n    smoothness of the gradients. Moreover, we observe a surprising yet robust\n    behavior of highly isometric initializations --- even though such networks\n    have a lower FIM condition number \\emph{at initialization}, and therefore by\n    analogy to convex functions should be easier to optimize, experimentally\n    they prove to be much harder to train with stochastic gradient descent. We\n    conjecture the FIM condition number plays a non-trivial role in the optimization.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNN) have shown tremendous success in computer vision problems, speech recognition, amortized probabilistic inference, and the modelling of neural data. Despite their performance, DNNs face obstacles in their practical application, which stem from both the excessive computational cost of running gradient descent for a large number of epochs, as well as the inherent brittleness of gradient descent applied to very deep models. A number of heuristic approaches such as batch normalization, weight normalization and residual connections (He et al., 2016;  Ioffe & Szegedy, 2015 ;  Salimans & Kingma, 2016 ) have emerged in an attempt to address these trainability issues. Recently mean field theory has been successful in developing a more principled analysis of gradients of neural networks, and has become the basis for a new random initialization principle. The mean field approach postulates that in the limit of infinitely wide random weight matrices, the distribution of pre-activations converges weakly to a Gaussian. Using this approach, a series of works proposed to initialize the networks in such a way that for each layer the input-output Jacobian has mean singular values of 1 ( Schoenholz et al., 2017 ). This requirement was further strengthened to suggest that the spectrum of singular values of the input-output Jacobian should concentrate on 1, and it was shown that this can only be achieved with random orthogonal weight matrices. Under these conditions the backpropagated gradients are bounded in 2 norm ( Pennington et al., 2017 ) irrespective of depth, i.e., they neither vanish nor explode. It was shown experimentally in ( Pennington et al., 2017 ;  Xiao et al., 2018 ;  Chen et al., 2018 ) that networks with these critical initial conditions train orders of magnitude faster than networks with arbitrary initializations. The empirical success invites questions from an optimization perspective on how the spectrum of the hidden layer Published as a conference paper at ICLR 2020 input-output Jacobian relates to notions of curvature of the parameters space, and consequentially to convergence rate. The largest effective (initial) step size η 0 for stochastic gradient descent is inversely proportional to the local gradient smoothness M ( Bottou et al., 2018 ;  Boyd & Vandenberghe, 2004 ). Intuitively, the gradient step can be at most as large as the fastest change in the parameter landscape. Recent attempts have been made to analyze the mean field geometry of the optimization using the Fisher information matrix (FIM) (Amari et al., 2019; Karakida et al., 2019). The theoretical and practical appeal of measuring curvature with the FIM is due to among other reasons the fact that the FIM is necessarily positive (semi-)definite even for non-convex objectives, and due to it its intimate relationship with the Hessian matrix. Karakida et al. (2019) derived an upper bound on the maximum eigenvalue, however this bound is not satisfactory since it is agnostic of the entire spectrum of singular values and therefore cannot differentiate between Gaussian and orthogonal weight initalizations. In this paper, we develop a new bound on the parameter curvature M given the maximum eigenvalue of the Fisher information λ max (G) which holds for random neural networks with both Gaussian and orthogonal weights. We derive this quantity to inspect the relation between the singular value distribution of the input-output Jacobian and locally maximal curvature of the parameter space. We use this result to probe different orthogonal, nearly isometric initializations, and observe that, broadly speaking, networks with a smaller initial curvature train faster and generalize better, as expected. However, consistent with a previous report ( Pennington et al., 2018 ), we also observe highly isometric networks perform worse despite having a slowly varying loss landscape ( i.e. small initial λ max (G)). We conjecture that the long term optimization behavior is depends on trivially on the smallest eigenvalue m and therefore surprisingly there is a sweetspot with the condition number being m M > 1. We then investigate whether constraining the spectrum of the Jacobian matrix of each layer affects optimization rate. We do so by training networks using Riemannian optimization to constrain their weights to be orthogonal, or nearly orthogonal and we find that manifold constrained networks are insensitive to the maximal curvature at the beginning of training unlike the unconstrained gradient descent (hereafter \"Euclidean\"). In particular, we observe that the advantage conferred by optimizing over manifolds cannot be explained by the improvement of the gradient smoothness as measured by λ max (G). Finally, we observe that contrary to  Bansal et al. (2018) 's results Euclidean networks with a carefully designed initialization reduce the test misclassification error at approximately the same rate as their manifold constrained counterparts, and overall attain a higher accuracy.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates exploration in procedurally-generated sparse-reward environments using a novel intrinsic reward, Rewarding Impact-Driven Exploration (RIDE). RIDE encourages the agent to take actions which result in impactful changes to its representation of the environment state. Experiments show that RIDE outperforms state-of-the-art exploration methods, particularly in procedurally-generated environments, and does not suffer from diminishing intrinsic rewards during training.",
        "Abstract": "Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (RL) is one of the most popular frameworks for developing agents that can solve a wide range of complex tasks ( Mnih et al., 2016 ;  Silver et al., 2016 ;  2017 ). RL agents learn to act in new environments through trial and error, in an attempt to maximize their cumulative reward. However, many environments of interest, particularly those closer to real-world problems, do not provide a steady stream of rewards for agents to learn from. In such settings, agents require many episodes to come across any reward, often rendering standard RL methods inapplicable. Inspired by human learning, the use of intrinsic motivation has been proposed to encourage agents to learn about their environments even when extrinsic feedback is rarely provided ( Schmidhuber, 1991b ; 2010;  Oudeyer et al., 2007 ;  Oudeyer & Kaplan, 2009 ). This type of exploration bonus emboldens the agent to visit new states ( Bellemare et al., 2016 ;  Burda et al., 2019b ;  Ecoffet et al., 2019 ) or to improve its knowledge and forward prediction of the world dynamics ( Pathak et al., 2017 ;  Burda et al., 2019a ), and can be highly effective for learning in hard exploration games such as Montezuma's Revenge ( Mnih et al., 2016 ). However, most hard exploration environments used in previous work have either a limited state space or an easy way to measure similarity between states ( Ecoffet et al., 2019 ) and generally use the same \"singleton\" environment for training and evaluation ( Mnih et al., 2016 ;  Burda et al., 2019a ). Deep RL agents trained in this way are prone to overfitting to a specific environment and often struggle to generalize to even slightly different settings ( Rajeswaran et al., 2017 ;  Zhang et al., 2018a ;b). As a first step towards addressing this problem, a number of procedurally-generated environments have been recently released, for example DeepMind Lab ( Beattie et al., 2016 ), Sokoban ( Racanière et al., 2017 ), Malmö ( Johnson et al., 2016 ), CraftAssist ( Jernite et al., 2019 ), Sonic ( Nichol et al., 2018 ), CoinRun ( Cobbe et al., 2019 ), Obstacle Tower ( Juliani et al., 2019 ), or Capture the Flag ( Jaderberg et al., 2019 ). In this paper, we investigate exploration in procedurally-generated sparse-reward environments. Throughout the paper, we will refer to the general problem that needs to be solved as the task (e.g. find a goal inside a maze) and to the particular instantiation of this task as the environment (e.g. maze layout, colors, textures, locations of the objects, environment dynamics etc.). The environment can be singleton or procedurally-generated. Singleton environments are those in which the agent has to solve the same task in the same environment in every episode, i.e.., the environment does not change between episodes. A popular example of a hard exploration environment that falls into that category is Montezuma's Revenge. In procedurally-generated environments, the agent needs to solve the same task, but in every episode the environment is constructed differently (e.g. resulting in a different maze layout), making it unlikely for an agent to ever visit the same state twice. Thus, agents in such environments have to learn policies that generalize well across a very large state space. We demonstrate that current exploration methods fall short in such environments as they (i) make strong assumptions about the environment (deterministic or resettable to previous states) ( Ecoffet et al., 2019 ;  Aytar et al., 2018 ), (ii) make strong assumptions about the state space (small number of different states or easy to determine if two states are similar) ( Ecoffet et al., 2019 ;  Burda et al., 2019b ;  Bellemare et al., 2016 ;  Ostrovski et al., 2017 ;  Machado et al., 2018a ), or (iii) provide intrinsic rewards that can diminish quickly during training ( Pathak et al., 2017 ;  Burda et al., 2019a ). To overcome these limitations, we propose Rewarding Impact-Driven Exploration (RIDE), a novel intrinsic reward for exploration in RL that encourages the agent to take actions which result in impactful changes to its representation of the environment state (see  Figure 1  for an illustration). We compare against state-of-the-art intrinsic reward methods on singleton environments with high- dimensional observations (i.e. visual inputs), as well as on hard-exploration tasks in procedurally- generated grid-world environments. Our experiments show that RIDE outperforms state-of-the-art exploration methods, particularly in procedurally-generated environments. Furthermore, we present a qualitative analysis demonstrating that RIDE, in contrast to prior work, does not suffer from diminishing intrinsic rewards during training and encourages agents substantially more to interact with objects that they can control (relative to other state-action pairs).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents Warped Gradient Descent (WarpGrad), a novel framework for meta-learning a gradient-based update rule that preconditions gradients. WarpGrad leverages insights from memory-based methods to define a trajectory agnostic meta-objective over a joint parameter search space, and introduces non-linearity to model preconditioning beyond the block-diagonal structure of prior works. Empirical results show that WarpGrad surpasses baseline gradient-based meta-learners on standard few-shot learning tasks, scales beyond few-shot learning to standard supervised settings, and outperforms competing methods in a reinforcement learning setting.",
        "Abstract": "Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.",
        "Introduction": "  INTRODUCTION Learning (how) to learn implies inferring a learning strategy from some set of past experiences via a meta-learner that a task-learner can leverage when learning a new task. One approach is to directly parameterise an update rule via the memory of a recurrent neural network (Andrychowicz et al., 2016; Ravi & Larochelle, 2016; Li & Malik, 2016; Chen et al., 2017). Such memory-based methods can, in principle, represent any learning rule by virtue of being universal function approximators (Cybenko, 1989; Hornik, 1991; Schäfer & Zimmermann, 2007). They can also scale to long learning processes by using truncated backpropagation through time, but they lack an inductive bias as to what constitutes a reasonable learning rule. This renders them hard to train and brittle to generalisation as their parameter updates have no guarantees of convergence. An alternative family of approaches defines a gradient-based update rule and meta-learns a shared initialisation that facilitates task adaptation across a distribution of tasks (Finn et al., 2017; Nichol et al., 2018; Flennerhag et al., 2019). Such methods are imbued with a strong inductive bias- gradient descent-but restrict knowledge transfer to the initialisation. Recent work has shown that it is beneficial to more directly control gradient descent by meta-learning an approximation of a parameterised matrix (Li et al., 2017; Lee & Choi, 2018; Park & Oliva, 2019) that preconditions gradients during task training, similarly to second-order and Natural Gradient Descent methods (Nocedal & Wright, 2006; Amari & Nagaoka, 2007). To meta-learn preconditioning, these methods backpropagate through the gradient descent process, limiting them to few-shot learning. In this paper, we propose a novel framework called Warped Gradient Descent (WarpGrad) 1 , that relies on the inductive bias of gradient-based meta-learners by defining an update rule that preconditions gradients, but that is meta-learned using insights from memory-based methods. In particular, we leverage that gradient preconditioning is defined point-wise in parameter space and can be seen as a recurrent operator of order 1. We use this insight to define a trajectory agnostic meta-objective over a joint parameter search space where knowledge transfer is encoded in gradient preconditioning. To achieve a scalable and flexible form of preconditioning, we take inspiration from works that embed preconditioning in task-learners (Desjardins et al., 2015; Lee & Choi, 2018), but we relax the assumption that task-learners are feed-forward and replace their linear projection with a generic neural network ω, referred to as a warp layer. By introducing non-linearity, preconditioning is rendered data-dependent. This allows WarpGrad to model preconditioning beyond the block-diagonal structure of prior works and enables it to meta-learn over arbitrary adaptation processes. We empirically validate WarpGrad and show it surpasses baseline gradient-based meta-learners on standard few-shot learning tasks (miniImageNet, tieredImageNet; Vinyals et al., 2016; Ravi & Larochelle, 2016; Ren et al., 2018), while scaling beyond few-shot learning to standard supervised settings on the \"multi\"-shot Omniglot benchmark (Flennerhag et al., 2019) and a multi-shot version of tieredImageNet. We further find that WarpGrad outperforms competing methods in a reinforce- ment learning (RL) setting where previous gradient-based meta-learners fail (maze navigation with recurrent neural networks (Miconi et al., 2019)) and can be used to meta-learn an optimiser that prevents catastrophic forgetting in a continual learning setting.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a generalization of the Transformer architecture, the 2-simplicial Transformer, which incorporates both 2- and 3-way interactions. This architecture is used to model higher-order relationships between entities, such as the 3-way relationship between entities in a BoxWorld environment. Experiments show that the simplicial agent confers an advantage over the relational agent as an inductive bias in the reasoning task. The paper also provides motivation from neuroscience for a simplicial inductive bias for abstract reasoning.",
        "Abstract": "We introduce the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning.\n",
        "Introduction": "  INTRODUCTION Deep learning contains many differentiable algorithms for computing with learned representations. These representations form vector spaces, sometimes equipped with additional structure. A recent example is the Transformer (Vaswani et al., 2017) in which there is a vector space V of value vectors and an inner product space H of query and key vectors. This structure supports a kind of message- passing, where a value vector v j ∈ V derived from entity j is propagated to update an entity i with weight q i · k j , where q i ∈ H is a query vector derived from entity i, k j ∈ H is a key vector derived from entity j, and the inner product on H is written as a dot product. The Transformer therefore represents a relational inductive bias, where a relation from entity j to entity i is perceived to the extent that q i · k j is large and positive. However, the real world has structure beyond entities and their direct relationships: for example, the three blocks in  Figure 1  are arranged in such a way that if either of the supporting blocks is removed, the top block will fall. This is a simple 3-way relationship between entities i, j, k that is complex to represent as a system of 2-way relationships. It is natural to make the hypothesis that such higher-order relationships are essential to extracting the full predictive power of data, across many domains. In accordance with this hypothesis, we introduce a generalisation of the Transformer architecture, the 2-simplicial Transformer, which incorporates both 2- and 3-way interactions. Mathematically, the key observation is that higher-order interactions between entities can be understood using al- gebras. This is nothing but Boole's insight (Boole, 1847) which set in motion the development of modern logic. In our situation, an appropriate algebra is the Clifford algebra Cl(H) of the space H of queries and keys, which contains that space H ⊆ Cl(H) and in which queries and keys can be multiplied. To represent a 3-way interaction we map each entity i to a triple (p i , l 1 i , l 2 i ) of vectors in H consisting of a query vector p i , a (first) key vector l 1 i and a (second) key vector l 2 i . Given a triple i, j, k we first form the product p i l 1 j l 2 k in the Clifford algebra, and then extract a scalar quantity η(p i l 1 j l 2 k ) using a natural continuous function η : Cl(H) −→ R associated to the Z-grading of Cl(H). This scalar Published as a conference paper at ICLR 2020 measures how strongly the network perceives a 3-way interaction involving i, j, k. In summary, the 2-simplicial Transformer learns how to represent entities in its environment as vectors v ∈ V , and how to transform those entities to queries and (pairs of) keys in H, so that the signals provided by the scalars q i · k j and η(p i l 1 j l 2 k ) are informative about higher-order structure in the environment. As a toy example of higher-order structure, we consider the reinforcement learning problem in a variant of the BoxWorld environment from (Zambaldi et al., 2019). The original BoxWorld is played on a rectangular grid populated by keys and locked boxes of varying colours, with the goal being to open the box containing the \"Gem\". In our variant of the BoxWorld environment, bridge BoxWorld, the agent must use two keys simultaneously to obtain the Gem; this structure in the environment creates many 3-way relationships between entities, including for example the relationship between the locked boxes j, k providing the two keys and the Gem entity i. This structure in the environment is fundamentally logical in nature, and encodes a particular kind of conjunction; see Appendix I. The architecture of our deep reinforcement learning agent largely follows (Zambaldi et al., 2019) and the details are given in Section 4. The key difference between our simplicial agent and the relational agent of (Zambaldi et al., 2019) is that in place of a standard Transformer block we use a 2-simplicial Transformer block. Our experiments show that the simplicial agent confers an advantage over the relational agent as an inductive bias in our reasoning task. Motivation from neuroscience for a simplicial inductive bias for abstract reasoning is contained in Appendix J. Our use of tensor products of value vectors is inspired by the semantics of linear logic in vector spaces (Girard, 1987; Melliès, 2009; Clift & Murfet, 2017; Wallbridge, 2018) in which an algorithm with multiple inputs computes on the tensor product of those inputs, but this is an old idea in natural language processing, used in models including the second-order RNN (Giles et al., 1989; Pollack, 1991; Goudreau et al., 1994; Giles et al., 1991), multiplicative RNN (Sutskever et al., 2011; Irsoy & Cardie, 2015), Neural Tensor Network (Socher et al., 2013) and the factored 3-way Restricted Boltzmann Machine (Ranzato et al., 2010), see Appendix A. Tensors have been used to model pred- icates in a number of neural network architectures aimed at logical reasoning (Serafini & Garcez, 2016; Dong et al., 2019). The main novelty in our model lies in the introduction of the 2-simplicial attention, which allows these ideas to be incorporated into the Transformer architecture.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a policy iteration algorithm for batch reinforcement learning (RL) in continuous control domains. The proposed algorithm learns a prior that gives information about which candidate policies are potentially supported by the data, while ensuring that the prior focuses on relevant trajectories. This prior biases the RL policy towards previously experienced actions that also have a high chance of being successful in the current task. The proposed method enables stable learning from conflicting data sources and is shown to improve on competitive baselines in a variety of RL tasks, including standard continuous control benchmarks and multi-task learning for simulated and real-world robots.",
        "Abstract": "Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be seen as an extension of recent work on batch-RL that enables stable learning from conflicting data-sources. We find  improvements on competitive baselines in a variety of RL tasks -- including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. ",
        "Introduction": "  INTRODUCTION Batch reinforcement learning (RL) ( Ernst et al., 2005 ;  Lange et al., 2011 ) is the problem of learning a policy from a fixed, previously recorded, dataset without the opportunity to collect new data through interaction with the environment. This is in contrast to the typical RL setting which alternates between policy improvement and environment interaction (to acquire data for policy evaluation). In many real world domains collecting new data is laborious and costly, both in terms of experimentation time and hardware availability but also in terms of the human labour involved in supervising experiments. This is especially evident in robotics applications (see e.g.  Riedmiller et al. 2018 ;  Haarnoja et al. 2018b ;  Kalashnikov et al. 2018  for recent examples learning on robots). In these settings where gathering new data is expensive compared to the cost of learning, batch RL promises to be a powerful solution. There exist a wide class of off-policy algorithms for reinforcement learning designed to handle data generated by a behavior policy µ which might differ from π, the policy that we are interested in learning (see e.g.  Sutton & Barto (2018)  for an introduction). One might thus expect solving batch RL to be a straightforward application of these algorithms. Surprisingly, for batch RL in continuous control domains, however,  Fujimoto et al. (2018)  found that policies obtained via the naïve application of off-policy methods perform dramatically worse than the policy that was used to generate the data. This result highlights the key challenge in batch RL: we need to exhaustively exploit the information that is in the data but avoid drawing conclusions for which there is no evidence (i.e. we need to avoid over-valuing state-action sequences not present in the training data). As we will show in this paper, the problems with existing methods in the batch learning setting are further exacerbated when the provided data contains behavioral trajectories from different policies µ 1 , . . . , µ N which solve different tasks, or the same task in different ways (and thus potentially execute conflicting actions) that are not necessarily aligned with the target task that π should accomplish. We empirically show that previously suggested adaptations for off-policy learning Published as a conference paper at ICLR 2020 ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ) can be led astray by behavioral patterns in the data that are consistent (i.e. policies that try to accomplish a different task or a subset of the goals for the target task) but not relevant for the task at hand. This situation is more damaging than learning from noisy or random data where the behavior policy is sub-optimal but is not predictable, i.e. the randomness is not a correlated signal that will be picked up by the learning algorithm. We propose to solve this problem by restricting our solutions to 'stay close to the relevant data'. This is done by: 1) learning a prior that gives information about which candidate policies are potentially supported by the data (while ensuring that the prior focuses on relevant trajectories), 2) enforcing the policy improvement step to stay close to the learned prior policy. We propose a policy iteration algorithm in which the prior is learned to form an advantage-weighted model of the behavior data. This prior biases the RL policy towards previously experienced actions that also have a high chance of being successful in the current task. Our method enables stable learning from conflicting data sources and we show improvements on competitive baselines in a variety of RL tasks - including standard continuous control benchmarks and multi-task learning for simulated and real-world robots. We also find that utilizing an appropriate prior is sufficient to stabilize learning; demonstrating that the policy evaluation step is implicitly stabilized when a policy iteration algorithm is used - as long as care is taken to faithfully evaluate the value function within temporal difference calculations. This results in a simpler algorithm than in previous work ( Fujimoto et al., 2018 ;  Kumar et al., 2019 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel algorithm, Projection-based Constrained Policy Optimization (PCPO), for learning control policies that optimize a reward function while satisfying predefined constraints. PCPO performs policy updates in two stages: first, maximizing reward using a trust region optimization method without constraints, and second, reconciling any constraint violation by projecting the policy back onto the constraint set. Theoretical analysis of PCPO is provided, including performance bounds for the algorithm. Empirical results on four different control tasks demonstrate the ability of PCPO to robustly learn constraint-satisfying policies, achieving 3.5 times fewer constraint violations and around 15% more reward than prior approaches.",
        "Abstract": "We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm - Projection-Based Constrained Policy Optimization (PCPO), an iterative method for optimizing policies in a two-step process - the first step performs an unconstrained update while the second step reconciles the constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on reward improvement, as well as an upper bound on constraint violation for each policy update. We further characterize the convergence of PCPO with projection based on two different metrics - L2 norm and Kullback-Leibler divergence. Our empirical results over several control tasks demonstrate that our algorithm achieves superior performance, averaging more than 3.5 times less constraint violation and around 15% higher reward compared to state-of-the-art methods.",
        "Introduction": "  INTRODUCTION Recent advances in deep reinforcement learning (RL) have demonstrated excellent performance on several domains ranging from games like Go ( Silver et al., 2017 ) and StarCraft ( AlphaStar, 2019 ) to robotic control ( Levine et al., 2016 ). In these settings, agents are allowed to explore the entire state space and experiment with all possible actions during training. However, in many real- world applications such as self-driving cars and unmanned aerial vehicles, considerations of safety, fairness and other costs prevent the agent from having complete freedom to explore. For instance, an autonomous car, while optimizing its driving policies, must not take any actions that could cause harm to pedestrians or property (including itself). In effect, the agent is constrained to take actions that do not violate a specified set of constraints on state-action pairs. In this work, we address the problem of learning control policies that optimize a reward function while satisfying predefined constraints. The problem of policy learning with constraints is more challenging since directly optimizing for the reward, as in Q-Learning ( Mnih et al., 2013 ) or policy gradient ( Sutton et al., 2000 ), will usu- ally violate the constraints. One approach is to incorporate constraints into the learning process by forming a constrained optimization problem. Then perform policy updates using a conditional gradient descent with line search to ensure constraint satisfaction ( Achiam et al., 2017 ). However, the base optimization problem can become infeasible if the current policy violates the constraints. Another approach is to add a hyperparameter weighted copy of the constraints to the objective func- tion ( Tessler et al., 2018 ). However, this incurs the cost of extensive hyperparameter tuning. To address the above issues, we propose projection-based constrained policy optimization (PCPO). This is an iterative algorithm that performs policy updates in two stages. The first stage maximizes reward using a trust region optimization method (e.g., TRPO ( Schulman et al., 2015a )) without Published as a conference paper at ICLR 2020 constraints. This might result in a new intermediate policy that does not satisfy the constraints. The second stage reconciles the constraint violation (if any) by projecting the policy back onto the constraint set, i.e., choosing the policy in the constraint set that is closest to the selected interme- diate policy. This allows efficient updates to ensure constraint satisfaction without requiring a line search ( Achiam et al., 2017 ) or adjusting a weight ( Tessler et al., 2018 ). Further, due to the projec- tion step, PCPO offers efficient recovery from infeasible (i.e., constraint-violating) states (e.g., due to approximation errors), which existing methods do not handle well. We analyze PCPO theoretically and derive performance bounds for the algorithm. Specifically, based on information geometry and policy optimization theory, we construct a lower bound on reward improvement, and an upper bound on constraint violations for each policy update. We find that with a relatively small step size for each policy update, the worst-case constraint violation and reward degradation are tolerable. We further analyze two distance measures for the projection step onto the constraint set. We find that the convergence of PCPO is affected by the smallest and largest singular values of the Fisher information matrix used during training. By observing these singular values, we can choose the appropriate projection best suited to the problem. Empirically, we compare PCPO with state-of-the-art algorithms on four different control tasks, in- cluding two Mujoco environments with safety constraints introduced by  Achiam et al. (2017)  and two traffic management tasks with fairness constraints introduced by  Vinitsky et al. (2018) . In all cases, the proposed algorithm achieves comparable or superior performance to prior approaches, averaging more reward with fewer cumulative constraint violations. For instance, across the above tasks, PCPO achieves 3.5 times fewer constraint violations and around 15% more reward. This demonstrates the ability of PCPO robustly learn constraint-satisfying policies, and represents a step towards reliable deployment of RL in real problems.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a novel domain adaptation paradigm that allows different domains to undergo different computations, not only in terms of layer weights but also in terms of number of operations, while selectively sharing subsets of these computations. This strategy, called Domain Adaptive Multibranch Networks (DAMNets), is implemented in conjunction with the popular domain classifier-based method of Ganin & Lempitsky (2015). Experiments demonstrate that DAMNets outperform the original technique of Ganin & Lempitsky (2015) and the state-of-the-art strategy for untying the source and target weights of Rozantsev et al. (2019).",
        "Abstract": "We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others.\nThis contrasts with  state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared.\nAs evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it allows us to handle any number of domains simultaneously.",
        "Introduction": "  INTRODUCTION While deep learning has ushered in great advances in automated image understanding, it still suffers from the same weaknesses as all other machine learning techniques: when trained with images obtained under specific conditions, deep networks typically perform poorly on images acquired under different ones. This is known as the domain shift problem: the changing conditions cause the statistical properties of the test, or target, data, to be different from those of the training, or source, data, and the network's performance degrades accordingly. Domain adaptation aims to address this problem, especially when annotating images from the target domain is difficult, expensive, or downright infeasible. The dominant trend is to map images to features that are immune to the domain shift, so that the classifier works equally well on the source and target domains ( Fernando et al., 2013 ;  Ganin & Lempitsky, 2015 ;  Sun & Saenko, 2016 ). In the context of deep learning, the standard approach is to find those features using a single architecture for both domains ( Tzeng et al., 2014 ;  Ganin & Lempitsky, 2015 ;  Sun & Saenko, 2016 ;  Yan et al., 2017 ;  Zhang et al., 2018 ). Intuitively, however, as the domains have different properties, it is not easy to find one network that does this effectively for both. A better approach is to allow domains to undergo different transformations to arrive at domain-invariant features. This has been the focus of recent work ( Tzeng et al., 2017 ;  Bermúdez-Chacón et al., 2018 ;  Rozantsev et al., 2018 ;  2019 ), where source and target data pass through two different networks with the same architecture but different weights, nonetheless related to each other. In this paper, we introduce a novel, even more flexible paradigm for domain adaptation, that allows the different domains to undergo different computations, not only in terms of layer weights but also in terms of number of operations, while selectively sharing subsets of these computations. This enables the network to automatically adapt to situations where, for example, one domain depicts simpler images, such as synthetic ones, which may not need as much processing power as those coming from more complex domains, such as images taken in-the-wild. Our formulation reflects the intuition that source and target domain networks should be similar because they solve closely related problems, but should also perform domain-specific computations to offset the domain shift. To turn this intuition into a working algorithm, we develop a multibranch architecture that sends the data through multiple network branches in parallel. What gives it the necessary flexibility are trainable gates that are tuned to modulate and combine the outputs of these branches, as shown in  Fig. 1 . Assigning to each domain its own set of gates allows the global network to learn what set of Published as a conference paper at ICLR 2020 Source Domain (labeled) Target Domain (unlabeled) Bike! Bike! f ( 1 ) f ( ) N f computations should be carried out for each one. As an additional benefit, in contrast to previous strategies for untying the source and target streams ( Rozantsev et al., 2018 ;  2019 ), our formulation naturally extends to more than two domains. In other words, our contribution is a learning strategy that adaptively adjusts the specific compu- tation to be performed for each domain. To demonstrate that it constitutes an effective approach to extracting domain-invariant features, we implement it in conjunction with the popular domain classifier-based method of  Ganin & Lempitsky (2015) . Our experiments demonstrate that our Do- main Adaptive Multibranch Networks, which we will refer to as DAMNets, not only outperform the original technique of  Ganin & Lempitsky (2015) , but also the state-of-the-art strategy for untying the source and target weights of  Rozantsev et al. (2019) , which relies on the same domain classifier. We will make our code publicly available upon acceptance of the paper.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new task, Paired Associative Inference (PAI), to investigate how neural networks can be enhanced to support inferential reasoning. It introduces MEMO, an external memory architecture that uses a linear projection paired with a powerful recurrent attention mechanism to enable flexible weighting of individual elements in memory. Additionally, a REINFORCE loss component is used to learn the optimal number of iterations required to solve a task. Results on three tasks demonstrate the effectiveness of MEMO and the REINFORCE loss component: PAI, shortest path finding, and bAbI.",
        "Abstract": "Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding the shortest path between nodes in a path. We therefore developed a novel architecture, MEMO, endowed with the capacity to reason over longer distances. This was accomplished with the addition of two novel components. First, it introduces a separation between memories/facts stored in external memory and the items that comprise these facts in external memory. Second, it makes use of an adaptive retrieval mechanism, allowing a variable number of ‘memory hops’ before the answer is produced. MEMO is capable of solving our novel reasoning tasks, as well as all 20 tasks in bAbI.",
        "Introduction": "  INTRODUCTION During our every day life we need to make several judgments that require connecting facts which were not experienced together, but acquired across experiences at different points in time. For instance, imagine walking your daughter to a coding summer camp and encountering another little girl with a woman. You might conclude that the woman is the mother of the little girl. Few weeks later, you are at a coffee shop near your house and you see the same little girl, this time with a man. Based on these two separated episodes you might infer that there is a relationship between the woman and the man. This flexible recombination of single experiences in novel ways to infer unobserved relationships is called inferential reasoning and is supported by the hippocampus ( Zeithamova et al., 2012 ). Interestingly, it has been shown that the hippocampus is storing memories independently of each other through a process called pattern separation ( Yassa & Stark, 2011 ;  Marr et al., 1991 ). The reason hippocampal memories are kept separated is to minimize interference between experiences, which allows us to recall specific events in the form of 'episodic' memories ( Eichenbaum & Cohen, 2004 ;  Squire et al., 2004 ). Clearly, this separation is in conflict with the above mentioned role of the hippocampus in generalisation - i.e. how can separated memories be chained together? Interestingly, a recent line of research ( Kumaran & McClelland, 2012 ;  Banino et al., 2016 ;  Schapiro et al., 2017 ;  Koster et al., 2018 ) sheds lights on this tension by showing that the integration of separated experiences emerges at the point of retrieval through a recurrent mechanism. This allows Published as a conference paper at ICLR 2020 multiple pattern separated codes to interact, and therefore support inference. In this paper we rely on these findings to investigate how we can take inspiration from neuroscience models to investigate and enhance inferential reasoning in neural networks. Neural networks augmented with external memory, like the Differential Neural Computer ( Graves et al., 2016 , DNC), and end to end memory networks ( Sukhbaatar et al., 2015 , EMN) have shown remarkable abilities to tackle difficult computational and reasoning tasks. Also, more powerful attention mechanisms ( Vaswani et al., 2017 ;  Dehghani et al., 2018 ) or the use of context ( Seo et al., 2016 ) have recently allowed traditional neural networks to tackle the same set of tasks. However, some of these tasks - e.g. bAbI ( Weston et al., 2015 ) - present repetitions and commonalities between the train and the test set that neural networks can exploit to come up with degenerate solutions. To overcome this limitation we introduced a new task, called Paired Associative Inference (PAI - see below), which is derived from the neuroscientific literature ( Bunsey & Eichenbaum, 1996 ;  Banino et al., 2016 ). This task is meant to capture the essence of inferential reasoning - i.e. the appreciation of distant relationships among elements distributed across multiple facts or memories. PAI is fully procedurally generated and so it is designed to force neural networks to learn abstractions to solve previously unseen associations. We then use the PAI task, followed by a task involving finding the shortest path and finally bAbi to investigate what kind of memory representations effectively support memory based reasoning. The EMN and other similar models ( Sukhbaatar et al., 2015 ;  Santoro et al., 2017 ;  Pavez et al., 2018 ) have used fixed memory representations based on combining word embeddings with a positional encoding transformation. A similar approach has been recently implemented by current state of the art language model ( Vaswani et al., 2017 ;  Devlin et al., 2018 ). By contrast our approach, called MEMO, retains the full set of facts into memory, and then learns a linear projection paired with a powerful recurrent attention mechanism that enable greater flexibility in the use of these memories. MEMO is based on the same basic structure of the external memory presented in EMN ( Sukhbaatar et al., 2015 ), but its new architectural components can potentially allow for flexible weighting of individual elements in memory and so supporting the form of the inferential reasoning outlined above. Next, we tackle the problem of prohibitive computation time. In standard neural networks, the computation grows as a function of the size of the input, instead of the complexity of the problem being learnt. Sometimes the input is padded with a fixed number of extra values to provide greater computation ( Graves et al., 2016 ), in other cases, input values are systematically dropped to reduce the amount of computation (e.g., frame dropping in reinforcement learning ( Mnih et al., 2016 )). Critically, these values are normally hand tuned by the experimenter; instead, here we are interested in adapting the amount of compute time to the complexity of the task. To do so we drawn inspiration from a model of human associative memory called REMERGE ( Kumaran & McClelland, 2012 ). In this model, the content retrieved from memory is recirculated back as the new query, then the difference between the content retrieved at different time steps in the re-circulation process is used to calculate if the network has settled into a fixed point, and if so this process terminates. To implement this principle in a neural network, we were inspired by techniques such as adaptive computation time ( Graves, 2016 ). In our architecture, the network outputs an action (in the reinforce- ment learning sense) that indicates whether it wishes to continue computing and querying its memory, or whether it is able to answer the given task. We call this the halting policy as the network learns the termination criteria of a fixed point operator. Like ACT, the network outputs a probability of halting, but unlike ACT, the binary halting random variable is trained using REINFORCE ( Williams, 1992 ). Thus we use reinforcement learning to adjust weights based upon the counterfactual problem: what would be the optimal number of steps of computation, given a particular number of steps was taken this time? The use of REINFORCE to perform variable amount of computation has been investigated already (e.g.  Shen et al., 2017 ;  Louizos et al., 2017 ) however our approach differs in that we added an extra term to the REINFORCE loss that, by exploiting the mathematical properties of binary random variables, naturally minimizes the expected number of computation steps. Thus we directly encourage our network to explicitly prefer representations and computation that minimize the amount of required computation. To sum up, our contributions are: 1. A new task that stresses the essence of reasoning - i.e. the appreciation of distant relation- ships among elements distributed across multiple facts. 2. An in depth investigation of the memory representation that support inferential reasoning, and extensions to existing memory architectures that show promising results on these reasoning tasks. 3. A REINFORCE loss component that learn the optimal number of iterations required to learn to solve a task. 4. Significant empirical results on three tasks demonstrating the effectiveness of the above two contributions: paired associative inference, shortest path finding, and bAbI ( Weston et al., 2015 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new algorithm for extreme classification, which is a problem of training classifiers over an enormous number of classes. The algorithm is based on a non-uniform sampling scheme for scalably approximating a softmax classification scheme, which reduces the gradient noise of the algorithm and minimizes the gradient variance. An adversarial auxiliary model is proposed to generate 'fake' labels that are more realistic by taking the input features of the data into account. The paper also presents mathematical proofs that the best signal-to-noise ratio in the gradient is obtained if the auxiliary model best reflects the true dependencies between input features and labels, and that the involved bias to the softmax approximation can be exactly quantified and cheaply removed at test time. Experiments on two classification data sets show that the proposed method outperforms all baselines by at least one order of magnitude in training speed.",
        "Abstract": "Training a classifier over a large number of classes, known as 'extreme classification', has become a topic of major interest with applications in technology, science, and e-commerce. Traditional softmax regression induces a gradient cost proportional to the number of classes C, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due a poor signal-to-noise ratio. In this paper, we propose a simple training method for drastically enhancing the gradient signal by drawing negative samples from an adversarial model that mimics the data distribution. Our contributions are three-fold: (i) an adversarial sampling mechanism that produces negative samples at a cost only logarithmic in C, thus still resulting in cheap gradient updates; (ii) a mathematical proof that this adversarial sampling minimizes the gradient variance while any bias due to non-uniform sampling can be removed; (iii) experimental results on large scale data sets that show a reduction of the training time by an order of magnitude relative to several competitive baselines.\n",
        "Introduction": "  INTRODUCTION In many problems in science, healthcare, or e-commerce, one is interested in training classifiers over an enormous number of classes: a problem known as 'extreme classification' ( Agrawal et al., 2013 ;  Jain et al., 2016 ;  Prabhu & Varma, 2014 ;  Siblini et al., 2018 ). For softmax (aka multinomial) regression, each gradient step incurs a cost proportional to the number of classes C. As this may be prohibitively expensive for large C, recent research has explored more scalable softmax approximations which circumvent the linear scaling in C. Progress in accelerating the training procedure and thereby scaling up extreme classification promises to dramatically improve, e.g., advertising ( Prabhu et al., 2018 ), recommender systems, ranking algorithms ( Bhatia et al., 2015 ;  Jain et al., 2016 ), and medical diagnostics ( Bengio et al., 2019 ;  Lippert et al., 2017 ;  Baumel et al., 2018 ) While scalable softmax approximations have been proposed, each one has its drawbacks. The most popular approach due to its simplicity is 'negative sampling' ( Mnih & Hinton, 2009 ; Mikolov et al., 2013), which turns the problem into a binary classification between so-called 'positive samples' from the data set and 'negative samples' that are drawn at random from some (usually uniform) distribution over the class labels. While negative sampling makes the updates cheaper since computing the gradient no longer scales with C, it induces additional gradient noise that leads to a poor signal- to-noise ratio of the stochastic gradient estimate. Improving the signal-to-noise ratio in negative sampling while still enabling cheap gradients would dramatically enhance the speed of convergence. In this paper, we present an algorithm that inherits the cheap gradient updates from negative sampling while still preserving much of the gradient signal of the original softmax regression problem. Our approach rests on the insight that the signal-to-noise ratio in negative sampling is poor since there is no association between input features and their artificial labels. If negative samples were harder to discriminate from positive ones, a learning algorithm would obtain a better gradient signal close to the optimum. Here, we make these arguments mathematically rigorous and propose a non-uniform sampling scheme for scalably approximating a softmax classification scheme. Instead of sampling labels uniformly, our algorithm uses an adversarial auxiliary model to draw 'fake' labels that are more realistic by taking the input features of the data into account. We prove that such procedure Published as a conference paper at ICLR 2020 reduces the gradient noise of the algorithm, and in fact minimizes the gradient variance in the limit where the auxiliary model optimally mimics the data distribution. A useful adversarial model should require only little overhead to be fitted to the data, and it needs to be able to generate negative samples quickly in order to enable inexpensive gradient updates. We propose a probabilistic version of a decision tree that has these properties. As a side result of our approach, we show how such an auxiliary model can be constructed and efficiently trained. Since it is almost hyperparameter-free, it does not cause extra complications when tuning models. The final problem that we tackle is to remove the bias that the auxiliary model causes relative to our original softmax classification. Negative sampling is typically described as a softmax approximation; however, only uniform negative sampling correctly approximates the softmax. In this paper, we show that the bias due to non-uniform negative sampling can be easily removed at test time. The stucture of our paper reflects our main contributions as follows: 1. We present a new scalable softmax approximation (Section 2). We show that non-uniform sampling from an auxiliary model can improve the signal-to-noise ratio. The best perfor- mance is achieved when this sampling mechanism is adversarial, i.e., when it generates fake labels that are hard to discriminate from the true ones. To allow for efficient training, such adversarial samples need to be generated at a rate sublinear (e.g., logarithmic) in C. 2. We design a new, simple adversarial auxiliary model that satisfies the above requirements (Section 3). The model is based on a probabilistic version of a decision tree. It can be efficiently pre-trained and included into our approach, and requires only minimal tuning. 3. We present mathematical proofs that (i) the best signal-to-noise ratio in the gradient is obtained if the auxiliary model best reflects the true dependencies between input features and labels, and that (ii) the involved bias to the softmax approximation can be exactly quantified and cheaply removed at test time (Section 4). 4. We present experiments on two classification data sets that show that our method outperforms all baselines by at least one order of magnitude in training speed (Section 5). We discuss related work in Section 6 and summarize our approach in Section 7.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel algorithm for solving the weight transport problem in biologically realistic, spiking neural networks. The algorithm takes advantage of a neuron's spiking discontinuity to infer the causal effect of its spiking on the activity of downstream neurons, and uses this causal effect to align the feedback synapses with the reciprocal feedforward weights. Results demonstrate that this leads to the reduction of a cost function which measures the weight symmetry, better weight symmetry in spiking neural networks than other algorithms, and better learning in deep neural networks in comparison to the use of fixed feedback weights.",
        "Abstract": "In artificial neural networks trained with gradient descent, the weights used for processing stimuli are also used during backward passes to calculate gradients. For the real brain to approximate gradients, gradient information would have to be propagated separately, such that one set of synaptic weights is used for processing and another set is used for backward passes. This produces the so-called \"weight transport problem\" for biological models of learning, where the backward weights used to calculate gradients need to mirror the forward weights used to process stimuli. This weight transport problem has been considered so hard that popular proposals for biological learning assume that the backward weights are simply random, as in the feedback alignment algorithm. However, such random weights do not appear to work well for large networks. Here we show how the discontinuity introduced in a spiking system can lead to a solution to this problem. The resulting algorithm is a special case of an estimator used for causal inference in econometrics, regression discontinuity design. We show empirically that this algorithm rapidly makes the backward weights approximate the forward weights. As the backward weights become correct, this improves learning performance over feedback alignment on tasks such as Fashion-MNIST and CIFAR-10. Our results demonstrate that a simple learning rule in a spiking network can allow neurons to produce the right backward connections and thus solve the weight transport problem.",
        "Introduction": "  INTRODUCTION Any learning system that makes small changes to its parameters will only improve if the changes are correlated to the gradient of the loss function. Given that people and animals can also show clear behavioral improvements on specific tasks ( Shadmehr et al., 2010 ), however the brain determines its synaptic updates, on average, the changes in must also correlate with the gradients of some loss function related to the task ( Raman et al., 2019 ). As such, the brain may have some way of calculating at least an estimator of gradients. To-date, the bulk of models for how the brain may estimate gradients are framed in terms of setting up a system where there are both bottom-up, feedforward and top-down, feedback connections. The feedback connections are used for propagating activity that can be used to estimate a gradient ( Williams, 1992 ;  Lillicrap et al., 2016 ;  Akrout et al., 2019 ;  Roelfsema & Ooyen, 2005 ;  Lee et al., 2015 ;  Scellier & Bengio, 2017 ; Sacramento et al., 2018). In all such models, the gradient estimator is less biased the more the feedback connections mirror the feedforward weights. For example, in the REINFORCE algorithm ( Williams, 1992 ), and related algorithms like AGREL ( Roelfsema & Published as a conference paper at ICLR 2020 Ooyen, 2005 ), learning is optimal when the feedforward and feedback connections are perfectly symmetric, such that for any two neurons i and j the synaptic weight from i to j equals the weight from j to i, e.g. W ji = W ij ( Figure 1 ). Some algorithms simply assume weight symmetry, such as Equilibrium Propagation ( Scellier & Bengio, 2017 ). The requirement for synaptic weight symmetry is sometimes referred to as the \"weight transport problem\", since it seems to mandate that the values of the feedforward synaptic weights are somehow transported into the feedback weights, which is not biologically realistic ( Crick, 1989 -01-12;  Grossberg, 1987 ). Solving the weight transport problem is crucial to biologically realistic gradient estimation algorithms ( Lillicrap et al., 2016 ), and is thus an important topic of study. Several solutions to the weight transport problem have been proposed for biological models, includ- ing hard-wired sign symmetry (Moskovitz et al., 2018), random fixed feedback weights ( Lillicrap et al., 2016 ), and learning to make the feedback weights symmetric ( Lee et al., 2015 ; Sacramento et al., 2018;  Akrout et al., 2019 ;  Kolen & Pollack, 1994 ). Learning to make the weights symmetric is promising because it is both more biologically feasible than hard-wired sign symmetry (Moskovitz et al., 2018) and it leads to less bias in the gradient estimator (and thereby, better training results) than using fixed random feedback weights ( Bartunov et al., 2018 ;  Akrout et al., 2019 ). However, of the current proposals for learning weight symmetry some do not actually work well in practice ( Bar- tunov et al., 2018 ) and others still rely on some biologically unrealistic assumptions, including scalar value activation functions (as opposed to all-or-none spikes) and separate error feedback pathways with one-to-one matching between processing neurons for the forward pass and error propagation neurons for the backward pass  Akrout et al. (2019) ;  Sacramento et al. (2018) . Interestingly, learning weight symmetry is implicitly a causal inference problem-the feedback weights need to represent the causal influence of the upstream neuron on its downstream partners. As such, we may look to the causal infererence literature to develop better, more biologically realis- tic algorithms for learning weight symmetry. In econometrics, which focuses on quasi-experiments, researchers have developed various means of estimating causality without the need to actually ran- domize and control the variables in question  Angrist & Pischke (2008) ;  Marinescu et al. (2018) . Among such quasi-experimental methods, regression discontinuity design (RDD) is particularly promising. It uses the discontinuity introduced by a threshold to estimate causal effects. For ex- ample, RDD can be used to estimate the causal impact of getting into a particular school (which is a discontinuous, all-or-none variable) on later earning power. RDD is also potentially promising for estimating causal impact in biological neural networks, because real neurons communicate with discontinuous, all-or-none spikes. Indeed, it has been shown that the RDD approach can produce unbiased estimators of causal effects in a system of spiking neurons  Lansdell & Kording (2019) . Given that learning weight symmetry is fundamentally a causal estimation problem, we hypothe- sized that RDD could be used to solve the weight transport problem in biologically realistic, spiking neural networks. Here, we present a learning rule for feedback synaptic weights that is a special case of the RDD algo- rithm previously developed for spiking neural networks ( Lansdell & Kording, 2019 ). Our algorithm takes advantage of a neuron's spiking discontinuity to infer the causal effect of its spiking on the activity of downstream neurons. Since this causal effect is proportional to the feedforward synaptic weight between the two neurons, by estimating it, feedback synapses can align their weights to be symmetric with the reciprocal feedforward weights, thereby overcoming the weight transport prob- lem. We demonstrate that this leads to the reduction of a cost function which measures the weight symmetry (or the lack thereof), that it can lead to better weight symmetry in spiking neural networks than other algorithms for weight alignment ( Akrout et al., 2019 ) and it leads to better learning in deep neural networks in comparison to the use of fixed feedback weights ( Lillicrap et al., 2016 ). Al- together, these results demonstrate a novel algorithm for solving the weight transport problem that takes advantage of discontinuous spiking, and which could be used in future models of biologically plausible gradient estimation.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates algorithms for combining supervised learning with self-play - which we call supervised self-play (S2P) algorithms - using two classic emergent communication tasks. We empirically investigate several supervised-first S2P methods in our environments and propose the use of population-based methods for S2P. Our findings highlight the need for further work in combining supervised learning and self-play to develop more sample-efficient language learners.",
        "Abstract": "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.",
        "Introduction": "  INTRODUCTION Language is one of the most important aspects of human intelligence; it allows humans to coordinate and share knowledge with each other. It is also crucial for human-machine interaction, as human language is a natural means by which to exchange information, give feedback, and specify goals. A promising approach for training agents to solve problems with natural language is to have a \"human in the loop\", meaning we collect problem-specific data from humans interacting directly with our agents for learning. However, human-in-the-loop data is expensive and time-consuming to obtain as it requires continuously collecting human data as the agent's policy improves, and recent work suggests that current machine learning methods (e.g. from deep reinforcement learning) are too data-inefficient to be trained in this way from scratch ( Chevalier-Boisvert et al., 2019 ). Thus, an important open problem is: how can we make human-in-the-loop training as data efficient as possible? To maximize data efficiency, it is important to fully leverage all available training signals. In this paper, we study two categories of such training methods: imitating human data via supervised learning, and self-play to maximize reward in a multi-agent environment, both of which provide rich signals for endowing agents with language-using capabilities. However, these are potentially competing objectives, as maximizing environmental reward can lead to the resulting communication protocol drifting from natural language ( Lewis et al., 2017 ;  Lee et al., 2019 ). The crucial question, then, is how do we best combine self-play and supervised updates? This question has received surprisingly little attention from the emergent communication literature, where the question of how to bridge the gap from emergent protocols to natural language is generally left for future work ( Mordatch & Abbeel, 2018 ;  Lazaridou et al., 2018 ;  Cao et al., 2018 ). Our goal in this paper is to investigate algorithms for combining supervised learning with self-play - which we call supervised self-play (S2P) algorithms - using two classic emergent communication tasks: a Lewis signaling game with symbolic inputs, and a more complicated image-based referential game with natural language descriptions. Our first finding is that supervised learning followed by self-play outperforms emergent communication with supervised fine-tuning in these environments, and we provide three reasons for why this is the case. We then empirically investigate several supervised-first S2P methods in our environments. Existing approaches in this area have used various ad-hoc schedules for alternating between the two kinds of updates ( Lazaridou et al., 2017 ), but to our knowledge there has been no systematic study that has compared these approaches. Lastly, we propose the use of population-based methods for S2P, and find that it leads to improved performance in the more challenging image-based referential game. Our findings highlight the need for further work in combining supervised learning and self-play to develop more sample-efficient language learners.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the use of semi-supervised methods to autonomously generate pseudo-labelled data sets from large unlabelled data sets. We propose a combination of ensemble of deep networks with a custom graph clustering algorithm to create a high accuracy pseudo-labelled data set. We then use this data set to train a semi-supervised model, which is augmented with information maximization and a dot product loss. We evaluate our method on several standard image and text datasets, and show that it is able to achieve higher clustering accuracy compared to current state-of-the-art deep unsupervised clustering techniques.",
        "Abstract": "In this paper, we propose a framework that leverages semi-supervised models to improve unsupervised clustering performance. To leverage semi-supervised models, we first need to automatically generate labels, called pseudo-labels. We find that prior approaches for generating pseudo-labels hurt clustering performance because of their low accuracy. Instead, we use an ensemble of deep networks  to construct a similarity graph, from which we extract high accuracy pseudo-labels. The approach of finding high quality pseudo-labels using ensembles and training the semi-supervised model is iterated, yielding continued improvement. We show that our approach outperforms state of the art clustering results for multiple image and text datasets. For example, we achieve 54.6% accuracy for CIFAR-10 and 43.9% for 20news, outperforming state of the art by 8-12% in absolute terms.",
        "Introduction": "  INTRODUCTION Semi-supervised methods, which make use of large unlabelled data sets and a small labelled data set, have seen recent success, e.g., ladder networks Rasmus et al. (2015) achieves 99% accuracy in MNIST using only 100 labelled samples. These approaches leverage the unlabelled data to help the network learn an underlying representation, while the labelled data guides the network towards separating the classes. In this paper, we ask two questions: is it possible to create the small labelled data set required by semi-supervised methods purely using unsupervised techniques? If so, can semi-supervised methods leverage this autonomously generated pseudo-labelled data set to deliver higher performance than state-of-the-art unsupervised approaches? We answer both these questions in the affirmative. We first find that prior approaches for identifying pseudo-labels Caron et al. (2018); Chen (2018); Lee (2013) perform poorly because of their low accuracy (Section 2). To create a high accuracy pseudo-labelled data set autonomously, we use a combination of ensemble of deep networks with a custom graph clustering algorithm (Section 4). We first train an ensemble of deep networks in an unsupervised manner. Each network independently clusters the input. We then compare two input data points. If all of the networks agree that these two data points belong to the same cluster, we can be reasonably sure that these data points belong to the same class. In this way, we identify all input data pairs belonging to the same class with high precision in a completely unsupervised manner. In the next step, we use these high quality input pairs to generate a similarity graph, with the data points as nodes and edges between data points which are deemed to be similar by our ensemble. From this graph, we extract tight clusters of data points, which serve as pseudo-labels. Note that, in this step, we do not cluster the entire dataset, but only a small subset on which we can get high Published as a conference paper at ICLR 2020 precision. Extracting high quality clusters from this graph while ensuring that the extracted clusters correspond to different classes is challenging. We discuss our approach in Section 4.2.1 for solving this problem. In this way, our method extracts unambiguous samples belonging to each class, which serves as pseudo-labels for semi-supervised learning. For semi-supervised learning using the labels generated above, one could use ladder networks Rasmus et al. (2015). However, we found that ladder networks is unsuitable for the initial unsupervised clustering step as it can degenerate to outputting constant values for all inputs in the absence of unsupervised loss. To enable unsupervised clustering, we augment ladder networks using information maximization Krause et al. (2010) to create the Ladder-IM, and with a dot product loss to create Ladder-Dot. We show in Section 5 that Ladder-IM and Ladder-Dot, by themselves, also provide improvements over previous state of the art. We use the same models for both the first unsupervised learning step as well as the subsequent pseudo-semi-supervised iterations. Finally, the approach of finding high quality clusters using an ensemble, and using them as labels to train a new ensemble of semi-supervised models, is iterated, yielding continued improvements. The large gains of our method mainly come from this iterative approach, which can in some cases, yield upto 17% gains in accuracy over the base unsupervised models (see section 5.4). We name our pseudo-semi-supervised learning approach Kingdra 1 . Kingdra is independent of the type of data set; we show examples of its use on both image and text data sets in Section 5. This is in contrast to some previous approaches using CNNs, e.g. Chang et al. (2017), Caron et al. (2018), which are specialized for image data sets. We perform unsupervised classification using Kingdra on several standard image (MNIST, CIFAR10, STL) and text (reuters, 20news) datasets. On all these datasets, Kingdra is able to achieve higher clustering accuracy compared to current state-of-the-art deep unsupervised clustering techniques. For example, on the CIFAR10 and 20news datasets, Kingdra is able to achieve classification accuracy of 54.6% and 43.9%, respectively, delivering 8-12% absolute gains over state of the art results Hu et al. (2017); Xie et al. (2016). Several techniques have been proposed in the literature for generating pseudo-labels (Caron et al. (2018); Chen (2018); Lee (2013). In Lee (2013), the output class with the highest softmax value (Argmax) is taken to be the pseudo-label. In Caron et al. (2018), the authors perform K-means clustering on the feature vector and use the K-means clusters as pseudo-labels. Finally, authors in Chen (2018) treat the softmax output as confidence and only label those items whose confidence value is above a high threshold. Note that none of these techniques for identifying pseudo-labels have been applied in our context, i.e., for unsupervised clustering using semi-supervised models. In this section, we evaluate if pseudo-labels created by these prior techniques can be leveraged by semi-supervised models to improve clustering accuracy. We start with a semi-supervised model based on Ladder networks (Rasmus et al. (2015)) called Ladder-IM (see Section 4.1 for model details) and train using only its unsupervised loss terms on MNIST and CIFAR10 datasets. We use each of the above three pseudo-labelling approaches on the trained model to provide an initial set of pseudo-labels to the datasets (e.g., using K-means clustering on the feature vector of the model as in Caron et al. (2018), etc.). We call the accuracy of these pseudo-labels the initial pseudo-label accuracy. We then use these generated pseudo-labels along with the datasets to train the model again, Published as a conference paper at ICLR 2020 now with a supervised loss term (based on the pseudo-labels) and the earlier unsupervised loss terms. We again run the pseudo-labelling approaches on the newly trained model to derive an updated set of pseudo-labels. We iterate this process of training and pseudo-labelling until the pseudo-label accuracy stabilizes. We call this the final clustering accuracy. The initial pseudo-label accuracy and the final clustering accuracy results for the three approaches are shown in  Table 1 . First, consider MNIST. The unsupervised clustering accuracy of Ladder-IM is 95.4%. Argmax simply assigns pseudo-labels based on the model's output and since this doesn't add any new information for subsequent iterations, the final accuracy remains at 95.4%. On the other hand, the pseudo-labels identified by both the K-means and threshold approaches result in worse initial label accuracy (75.4% and 88.6%). When this low-accuracy pseudo-label is used as supervision to train the model further, it results in a low final clustering accuracy of 60.9% and 91.6%, respectively. CIFAR10 results are similar. Ladder-IM clustering accuracy is 49% which remains the same under Argmax as before. Pseudo-label accuracy using the K-means approach is worse and results in pulling down the final accuracy to 44.8%. Interestingly, threshold results in a slightly higher initial accuracy of 60.5% but even this is not high enough to improve the final clustering accuracy for CIFAR10. From these results, we arrive at the following two conclusions. First, if the initial pseudo-label accuracy is not high, using pseudo-labels as supervision can result in bringing down the final clustering accuracy. Thus, high accuracy of initial pseudo-labels is crucial for improving clustering accuracy. Second, current approaches for identifying pseudo-labels do not deliver high accuracy and hence are unable to help improve overall clustering accuracy.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a generalization of the Transformer architecture using three-dimensional, block-local self-attention for video generation. This is combined with a three-dimensional generalization of methods from Menick & Kalchbrenner (2019) to reduce the memory footprint. The model is evaluated on popular benchmarks and produces high fidelity video continuations on the BAIR robot pushing dataset. Results on down-sampled videos from the Kinetics-600 dataset are also presented, showing encouraging video continuations for a more limited subset of cooking videos.",
        "Abstract": "Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models attempt to address these issues by combining sometimes complex, often video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple, autoregressive video generation models based on a three-dimensional self-attention mechanism achieve highly competitive results across multiple metrics on popular benchmark datasets for which they produce continuations of high fidelity and realism. Furthermore, we find that our models are capable of producing diverse and surprisingly realistic continuations on a subset of videos from Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. To our knowledge, this is the first promising application of video-generation models to videos of this complexity.",
        "Introduction": "  INTRODUCTION Generative modeling of video holds promise for applications such as content creation, forecasting, transfer learning and model-based reinforcement learning ( Srivastava et al., 2015 ;  Carl Vondrick, 2016 ;  Oh et al., 2015 ;  Kaiser et al., 2019 ). While recently there has been a lot of progress on generative models for text, audio and images, video generation remains challenging. To some extent this is simply due to the large amount of data that needs to be produced. Autoregressive models suffer from this particularly in their generation speed. On the other hand, they have a number of desirable attributes, such as their conceptual simplicity and tractable likelihood, which enables straightforward evaluation of their ability to model the entire data distribution. Moreover, recent results on image generation by  Menick & Kalchbrenner (2019)  show that pixel- level autoregressive models are capable of generating images with high fidelity. These findings motivate the question of how far one can push such autoregressive models in the more general task of video generation when scaling recent advances in neural architectures to modern hardware accelerators. In this work, we introduce a generalization of the Transformer architecture of  Vaswani et al. (2017)  using three-dimensional, block-local self-attention. In contrast to the block-local attention mecha- nism of  Parmar et al. (2018) , our formulation can be implemented efficiently on Tensor Processing Units, or TPUs ( Jouppi et al., 2017 ). To further reduce the memory footprint, we combine this with a three-dimensional generalization of methods from  Menick & Kalchbrenner (2019) , who generate images as sequences of smaller, sub-scaled image slices. Together, these techniques allow us to efficiently model videos as 3D volumes instead of sequences of still image frames, with direct interactions between representations of pixels across the spatial and temporal dimensions. We obtain strong results on popular benchmarks (Section 4.2, Appendix A) and produce high fidelity video continuations on the BAIR robot pushing dataset ( Ebert et al., 2017 ) exhibiting plausible ob- ject interactions. Furthermore, our model achieves an almost 50% reduction in perplexity compared to prior work on autoregressive models on another robot pushing dataset. Finally, we apply our models to down-sampled videos from the Kinetics-600 dataset ( Carreira et al., 2018 ) (Section 4.3). While modeling the full range of Kinetics-600 videos still poses a major chal- lenge, we see encouraging video continuations for a more limited subset, namely cooking videos. These feature camera movement, complex object interactions and still cover diverse subjects. We hope that these initial results will encourage future video generation work to evaluate models on more challenging datasets such as Kinetics.",
        "label": 1
    },
    {
        "Summary": "\nAbstract: This paper focuses on parameter transfer via gradient-based meta-learning (GBML) and how to ensure data privacy in this process. We propose a task-global differential privacy (DP) setting to balance the trade-off between privacy and model utility. We provide a DP GBML algorithm with provable learning guarantees in convex settings and demonstrate its application in federated learning with personalization. Empirically, we show that our proposed privacy setting allows for strong performance on federated language-modeling and few-shot image classification tasks.",
        "Abstract": "Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, with personalization, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.",
        "Introduction": "  INTRODUCTION The field of meta-learning offers promising directions for improving the performance and adaptability of machine learning methods. At a high level, the key assumption leveraged by these approaches is that the sharing of knowledge gained from individual learning tasks can help catalyze the learning of similar unseen tasks. However, the collaborative nature of this process, in which task-specific information must be sent to and used by a meta-learner, also introduces inherent data privacy risks. In this work, we focus on a popular and flexible meta-learning approach, parameter transfer via gradient-based meta-learning (GBML). This set of methods, which includes well-known algorithms such as MAML ( Finn et al., 2017 ) and Reptile ( Nichol et al., 2018 ), tries to learn a common initialization φ over a set of tasks t = 1, . . . , T such that a high-performance model can be learned in only a few gradient-steps on new tasks. Notably, information flows constantly between training tasks and the meta-learner as learning progresses; to make iterative updates, the meta-learner obtains feedback on the current φ by having task-specific modelsθ t trained with it. Meanwhile, in many settings amenable to meta-learning, it is crucial to ensure that sensitive informa- tion in each task's dataset stays private. Examples of this include learning models for word prediction on cell phone data ( McMahan et al., 2018 ), clinical predictions using hospital records ( Zhang et al., 2019 ), and fraud detectors for competing credit card companies ( Stolfo et al., 1997 ). In such cases, each data-owner can benefit from information learned from other tasks, but each also desires, or is legally required, to keep their raw data private. Thus, it is not sufficient to learn a well-performing φ; it is equally imperative to ensure that a task's sensitive information is not obtainable by anyone else. While parameter transfer algorithms can move towards this goal by peforming task-specific opti- mization locally, thus preventing direct access to private data, this provision is far from fail-safe in terms of privacy. A wealth of work has shown in the single-task setting that it is possible for an adversary with only access to the model to learn detailed information about the training set, such as the presence or absence of specific records ( Shokri et al., 2017 ) or the identities of sensitive features given other covariates ( Fredrikson et al., 2015 ). Furthermore,  Carlini et al. (2018)  showed that deep Published as a conference paper at ICLR 2020 neural networks can effectively memorize user-unique training examples, which can be recovered even after only a single epoch of training. As such, in parameter-transfer methods, the meta-learner or any downstream participant can potentially recover data from a previous task. However, despite these serious risks, privacy-preserving meta-learning has remained largely an unstudied problem. Our work aims to address this issue by applying differential privacy (DP), a well-established definition of privacy with rich theoretical guarantees and consistent empirical success at preventing leakages of data ( Carlini et al., 2018 ;  Fredrikson et al., 2015 ;  Jayaraman and Evans, 2019 ). Crucially, although there are various threat models and degrees of DP one could consider in the meta-learning setting (as we outline in Section 2), we balance the well-documented trade-off between privacy and model utility by formalizing and focusing on a setting that we call task-global DP. This setting provides a strong privacy guarantee for each task-owner that sharingθ t with the meta-learner will not reliably reveal anything about specific training examples to any downstream agent. It also allows us to use the framework of  Khodak et al. (2019a)  to provide a DP GBML algorithm that enjoys provable learning guarantees in convex settings. Finally, we show an application of our work by drawing connections to federated learning (FL) ( Li et al., 2019 ). While standard methods for FL, such as FedAvg ( McMahan et al., 2017 ), have inspired many works also concerning DP in a multi-user setup ( Agarwal et al., 2018 ; Bhowmick et al., 2019;  Geyer et al., 2018 ;  McMahan et al., 2018 ;  Truex et al., 2019 ), we are the first to consider task-global DP as a useful variation on standard DP settings. Moreover, these works fundamentally differ from ours in that they do not consider a task-based notion of learnability, instead focusing on the global federated learning problem to learn a single global model. That being said, a federated setting involving per-user personalization ( Chen et al., 2018 ;  Smith et al., 2017 ) is a natural meta-learning application. More specifically, our main contributions are: 1. We are the first to provide a taxonomy for the different notions of DP possible for meta-learning. In particular, we formalize on a variant we call task-global DP, showing and arguing that it adds a useful option to commonly studied settings in terms of trading privacy and accuracy. 2. We propose the first DP GBML algorithm, which we construct to satisfy this privacy setting. Further, we show a straightforward extension for obtaining a group DP version of our setting to protect multiple samples simultaneously. 3. While our privacy guarantees hold generally, we also prove learning-theoretic results in convex settings. Our learning guarantees scale with task-similarity, as measured by the closeness of the task-specific optimal parameters ( Denevi et al., 2019 ;  Khodak et al., 2019b ). 4. We show that our algorithm, along with its theoretical guarantees, naturally carries over to federated learning with personalization. Compared to previous notions of privacy considered in works for DP federated learning ( Agarwal et al., 2018 ; Bhowmick et al., 2019;  Geyer et al., 2018 ;  McMahan et al., 2018 ;  Truex et al., 2019 ), we are, to the best of our knowledge, the first to simultaneously provide both privacy and learning guarantees. 5. Empirically, we demonstrate that our proposed privacy setting allows for strong performance on federated language-modeling and few-shot image classification tasks. For the former, we achieve close to the performance of non-private models and significantly improve upon the performance of models trained with local-DP guarantees, a previously studied notion that also provides protections against the meta-learner. Our setting reasonably relaxes this latter notion but can achieve roughly 1.7-2.3 times the accuracy on a modified version of the Shakespeare dataset ( Caldas et al., 2018 ) and 1.6-1.7 times the accuracy on a modified version of Wiki-3029 ( Arora et al., 2019 ) across various privacy budgets. For image-classification, we show that we show that we can still retain significant benefits of meta-learning while applying task-global DP on Omniglot ( Lake et al., 2011 ) and Mini-ImageNet ( Ravi and Larochelle, 2017 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new class of video prediction models, VideoFlow, which can provide exact likelihoods, generate diverse stochastic futures, and accurately synthesize realistic and high-quality video frames. VideoFlow is based on flow-based generative models and is inspired by the Glow model for image generation. Results show that VideoFlow achieves competitive results with the state-of-the-art in stochastic video prediction on the action-free BAIR dataset, with quantitative results that rival the best VAE-based models. VideoFlow also produces excellent qualitative results, and avoids many of the common artifacts of models that use pixel-level mean-squared-error for training. Compared to models based on pixel-level autoregressive prediction, VideoFlow achieves substantially faster test-time image synthesis, making it much more practical for applications that require real-time prediction.",
        "Abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video.",
        "Introduction": "  INTRODUCTION Exponential progress in the capabilities of computational hardware, paired with a relentless effort towards greater insights and better methods, has pushed the field of machine learning from relative obscurity into the mainstream. Progress in the field has translated to improvements in various capabilities, such as classification of images (Krizhevsky et al., 2012), machine translation ( Vaswani et al., 2017 ) and super-human game-playing agents (Mnih et al., 2013;  Silver et al., 2017 ), among others. However, the application of machine learning technology has been largely constrained to situations where large amounts of supervision is available, such as in image classification or machine translation, or where highly accurate simulations of the environment are available to the learning agent, such as in game-playing agents. An appealing alternative to supervised learning is to utilize large unlabeled datasets, combined with predictive generative models. In order for a complex generative model to be able to effectively predict future events, it must build up an internal representation of the world. For example, a predictive generative model that can predict future frames in a video would need to model complex real-world phenomena, such as physical interactions. This provides an appealing mechanism for building models that have a rich understanding of the physical world, without any labeled examples. Videos of real-world interactions are plentiful and readily available, and a large generative model can be trained on large unlabeled datasets containing many video sequences, thereby learning about a wide range of real-world phenoma. Such a model could be useful for learning representations for further downstream tasks (Mathieu et al., 2016), or could even be used directly in applications where predicting the future enables effective decision making and control, such as robotics (Finn et al., 2016). A central challenge in video prediction is that the future is highly uncertain: a short sequence of observations of the present can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally (as in the case of pixel-level autoregressive models), or do not directly optimize the likelihood of the data. In this paper, we study the problem of stochastic prediction, focusing specifically on the case of conditional video prediction: synthesizing raw RGB video frames conditioned on a short context Published as a conference paper at ICLR 2020 of past observations (Ranzato et al., 2014;  Srivastava et al., 2015 ;  Vondrick et al., 2015 ;  Xingjian et al., 2015 ; Boots et al., 2014). Specifically, we propose a new class of video prediction models that can provide exact likelihoods, generate diverse stochastic futures, and accurately synthesize realistic and high-quality video frames. The main idea behind our approach is to extend flow-based generative models ( Dinh et al., 2014 ; 2016) into the setting of conditional video prediction. To our knowledge, flow-based models have been applied only to generation of non-temporal data, such as images (Kingma & Dhariwal, 2018), and to audio sequences (Prenger et al., 2018). Conditional generation of videos presents its own unique challenges: the high dimensionality of video sequences makes them difficult to model as individual datapoints. Instead, we learn a latent dynamical system model that predicts future values of the flow model's latent state. This induces Markovian dynamics on the latent state of the system, replacing the standard unconditional prior distribution. We further describe a practically applicable architecture for flow-based video prediction models, inspired by the Glow model for image generation (Kingma & Dhariwal, 2018), which we call VideoFlow. Our empirical results show that VideoFlow achieves results that are competitive with the state-of- the-art in stochastic video prediction on the action-free BAIR dataset, with quantitative results that rival the best VAE-based models. VideoFlow also produces excellent qualitative results, and avoids many of the common artifacts of models that use pixel-level mean-squared-error for training (e.g., blurry predictions), without the challenges associated with training adversarial models. Compared to models based on pixel-level autoregressive prediction, VideoFlow achieves substantially faster test-time image synthesis 1 , making it much more practical for applications that require real-time prediction, such as robotic control (Finn & Levine, 2017). Finally, since VideoFlow directly optimizes the likelihood of training videos, without relying on a variational lower bound, we can evaluate its performance directly in terms of likelihood values.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a study that uses Convolutional Neural Networks (CNNs) to classify swim bouts in zebrafish video recordings. The CNNs are trained on optical flow and achieve superior performance compared to the current state-of-the-art. The \"iNNvestigate\" toolbox is used to create heatmaps that highlight the areas that the CNN pays attention to when making a prediction. The results show that the CNN learns reasonable features which are different from those manually composed by Semmelhack et al. (2014).",
        "Abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.",
        "Introduction": "  INTRODUCTION In the study by  Semmelhack et al. (2014) , a well-performing classifier allowed to correlate neural interventions with behavioral changes. Support Vector Machines (SVMs) were commonly applied to such classification tasks, relying on feature engineering by domain experts. In recent years, Con- volutional Neural Networks (CNNs) have proven to reach high accuracies in classification tasks on images and videos reducing the need for manual feature engineering. After  Lecun & Bengio (1995)  introduced them in the 90s, CNNs had their break-through in the competition ILSVRC2012 with the architecture of  Krizhevsky et al. (2012) . Since then, more and more sophisticated architectures have been designed enabling them to identify increasingly abstract features. This development has become possible due to the availability of larger training sets, computing resources, GPU training implementations, and better regularization techniques, such as Dropout ( Hinton et al. (2012) ;  Zeiler & Fergus (2014) ). While these more complex deep neural network architectures achieved better results, they also kept their learnt features hidden if not further analyzed. This caused CNNs to come with significant drawbacks: a lack of trust in their classifications, missing interpretability of learned features in the application domain, and the absence of hints as to what data could enhance performance ( Molnar (2019) ). Explaining the decisions made by CNNs might even become a legal requirement in certain applications ( Alber et al. (2018) ). In order to overcome these drawbacks, subsequent research has developed approaches to shed light on the inner workings of CNNs. These approaches have been successfully used for uncovering how CNNs might learn unintended spurious correlations, termed \"Clever Hans\" predictions ( Lapuschkin et al. (2019) ). Such predictions could even become harmful if the predictions entailed decisions with severe consequences ( Leslie (2019) ). Also, since deep neural networks have become a popular Published as a conference paper at ICLR 2020 machine learning technique in applied domains, spurious correlations would undermine scientific discoveries. This paper focuses on zebrafish research as an applied domain of AI explainability, considering that the research community around this organism has grown immensely. The zebrafish is an ex- cellent model organism for vertebrates, including humans, due to the following four reasons: The genetic codes of humans and zebrafish are about 70% orthologue ( Howe et al. (2013) ). The fish are translucent which allows non-invasive observation of changes in the organism ( Bianco et al. (2011) ). Furthermore, zebrafish are relatively cheap to maintain, produce plenty of offspring, and develop rapidly. Finally, they are capable of recovering their brain structures within days after brain injury ( Kishimoto et al. (2011) ;  Kizil et al. (2012) ). In this paper, we adapt CNNs to work on highly controlled zebrafish video recordings and show the utility of a recently developed AI explainability technique on this task. We train the network on optical flow for binary classifying swim bouts and achieve superior performance when compared to the current state-of-the-art in bout classification ( Semmelhack et al. (2014) ). We then create heatmaps over the videos with the \"iNNvestigate\" toolbox ( Alber et al. (2018) ) which highlight the areas that our CNN pays attention to when making a prediction. The resulting heatmaps show that our CNN learns reasonable features which are very different from those manually composed by  Semmelhack et al. (2014) .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a semi-supervised latent variable model that combines state-of-the-art neural text-to-speech (TTS) systems with probabilistic latent variable models to discover aspects of speech that are rarely labelled or even difficult to describe. The model is trained using stochastic gradient variational Bayes (SGVB) and is able to reliably control high level attributes of speech, such as emotional expression (affect) or speaking rate, with as little as 30 minutes of supervision. The model is also able to transfer controllability to speakers for whom no labels are available. The results demonstrate that it is possible to imbue TTS models with control over affect, F0 and speaking rate whilst still maintaining prosodic variation when sampling.",
        "Abstract": "We present a novel generative model that combines state-of-the-art neural text- to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn’t been possible with purely unsupervised methods. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. We will release audio samples at https://google.github.io/tacotron/publications/semisupervised_generative_modeling_for_controllable_speech_synthesis/.",
        "Introduction": "  INTRODUCTION The ability to reliably control high level attributes of speech, such as emotional expression (affect) or speaking rate, is often desirable in speech synthesis applications. Achieving this control however is made difficult by the necessity of acquiring a large quantity of high quality labels. In this paper we show that semi-supervised latent variable models can take us a significant step closer towards solving this problem. Combining state-of-the-art neural text-to-speech (TTS) systems with probabilistic latent variable models provides a natural framework for discovering aspects of speech that are rarely labelled or even difficult to describe. Both inferring the latent prosody and generating samples with sufficient variety requires reasoning about uncertainty and is thus a natural fit for deep generative models. There has been recent progress in applying stochastic gradient variational Bayes (SGVB) ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ) to training probabilistic neural TTS models.  Battenberg et al. (2019)  and  Hsu et al. (2018)  have shown that it is possible to use latent variable models to discover features such as speaking style, speaking rate, arousal, gender and even the quality of the recording environment. However, these models are formally non-identifiable ( Hyvärinen & Pajunen, 1999 ;  Locatello et al., 2019 ) and this implies that repeated training runs will not reliably discover the same latent attributes. Even if they did, a lengthy human post-processing stage is necessary to identify what the model has learned on any given training run. In order to be of practical use for control, it is not enough for the models to discover latent attributes, they need to do so reliably and in a way that is robust to random initialization and to changes in the model. We demonstrate that the addition of even modest amounts of supervision can be sufficient to achieve this reliability. By augmenting state-of-the art neural TTS with semi-supervised deep generative models within the VAE framework ( Kingma et al., 2014 ;  Narayanaswamy et al., 2017 ), we show that it is possible to not only discover latent attributes of speech but to do so in a reliable and controllable manner. In particular we are able to achieve reliable control over affect, speaking rate and F0 variation (F0 is the Published as a conference paper at ICLR 2020 (a) CBHG block (b) Seq-to-seq network fundamental frequency of oscillation of the vocal folds). Further, we provide demonstrations that it is possible to transfer controllability to speakers for whom we have no labels. Our core contributions are: • To combine semi-supervised latent variable models with Neural TTS systems, producing a system that can reliably discover attributes of speech we wish to control. • To demonstrate that as little as 30 minutes supervision can be sufficient to improve prosody and allow control over speaking rate, fundamental frequency (F0) variation and affect, a problem of interest to the speech community for well over two decades ( Schröder, 2001 ). • To imbue TTS models with control over affect, F0 and speaking rate whilst still maintaining prosodic variation when sampling.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a compression method for convolutional networks, particularly adapted to ResNet-like architectures. The approach takes advantage of the high correlation in the convolutions by using a structured quantization algorithm, Product Quantization (PQ). This strategy reduces the memory footprint and produces compressed networks allowing efficient inference on CPU. The compression is guided by the activations of the uncompressed network on unlabelled data, allowing for both an efficient layer-by-layer compression procedure and a global fine-tuning of the codewords. Results show that applying the approach to the semi-supervised ResNet-50 leads to a 5 MB memory footprint and a 76.1% top-1 accuracy on ImageNet object classification, and a 6 MB memory footprint for a Mask R-CNN with a competitive performance.",
        "Abstract": "In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor.",
        "Introduction": "  INTRODUCTION There is a growing need for compressing the best convolutional networks (or ConvNets) to sup- port embedded devices for applications like robotics and virtual/augmented reality. Indeed, the performance of ConvNets on image classification has steadily improved since the introduction of AlexNet ( Krizhevsky et al., 2012 ). This progress has been fueled by deeper and richer ar- chitectures such as the ResNets ( He et al., 2015 ) and their variants ResNeXts ( Xie et al., 2017 ) or DenseNets ( Huang et al., 2017 ). Those models particularly benefit from the recent progress made with weak supervision (Mahajan et al., 2018;  Yalniz et al., 2019 ;  Berthelot et al., 2019 ). Compres- sion of ConvNets has been an active research topic in the recent years, leading to networks with a 71% top-1 accuracy on ImageNet object classification that fit in 1 MB ( Wang et al., 2018b ). In this work, we propose a compression method particularly adapted to ResNet-like architectures. Our approach takes advantage of the high correlation in the convolutions by the use of a structured quantization algorithm, Product Quantization (PQ) ( Jégou et al., 2011 ). More precisely, we exploit the spatial redundancy of information inherent to standard convolution filters ( Denton et al., 2014 ). Besides reducing the memory footprint, we also produce compressed networks allowing efficient inference on CPU by using byte-aligned indexes, as opposed to entropy decoders ( Han et al., 2016 ). Our approach departs from traditional scalar quantizers ( Han et al., 2016 ) and vector quantiz- ers ( Gong et al., 2014 ;  Carreira-Perpiñán & Idelbayev, 2017 ) by focusing on the accuracy of the activations rather than the weights. This is achieved by leveraging a weighted k-means technique. To our knowledge this strategy (see Section 3) is novel in this context. The closest work we are aware of is the one by  Choi et al. (2016) , but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization). Our method targets a better in-domain reconstruction, as depicted by  Figure 1 . Finally, we compress the network sequentially to account for the dependency of our method to the activations at each layer. To prevent the accumulation of errors across layers, we guide this compression with the activations of the uncompressed network on unlabelled data: training by dis- tillation ( Hinton et al., 2014 ) allows for both an efficient layer-by-layer compression procedure and a global fine-tuning of the codewords. Thus, we only need a set of unlabelled images to adjust the codewords. As opposed to recent works by  Mishra & Marr (2017)  or  Lopes et al. (2017) , our distillation scheme is sequential and the underlying compression method is different (PQ vs. scalar). We show that applying our approach to the semi-supervised ResNet-50 of Yalniz et al. ( Yalniz et al., 2019 ) leads to a 5 MB memory footprint and a 76.1% top-1 accuracy on ImageNet object classification (hence 20× compression vs. the original model). Moreover, our approach generalizes to other tasks such as image detection. As shown in Section 4.3, we compress a Mask R-CNN ( He et al., 2017 ) with a size budget around 6 MB (26× compression factor) while maintaining a competitive performance.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a data-adaptive pruning framework for graph scattering transforms (GSTs) to systematically retain important features. The proposed pruned graph scattering transform (pGST) is guided by a criterion promoting alignment of the input graph spectrum with that of the graph filters. Theoretical analysis is provided to show that the pGST is stable to perturbations of the input graph signals. Experiments demonstrate that the pGSTs perform similarly and in certain cases better than the baseline GSTs, while achieving significant computational savings. The extracted features from pGSTs can be utilized towards graph classification and 3D point cloud recognition, and the pruning patterns of the pGST can be used to deduce that graph signals in different domains call for different network architectures.",
        "Abstract": "Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures.",
        "Introduction": "  INTRODUCTION The abundance of graph-structured data calls for advanced learning techniques, and complements nicely standard machine learning tools that cannot be directly applied to irregular data domains. Permeating the benefits of deep learning to the graph domain, graph convolutional networks (GCNs) provide a versatile and powerful framework to learn from complex graph data ( Bronstein et al., 2017 ). GCNs and variants thereof have attained remarkable success in social network analysis, 3D point cloud processing, recommender systems and action recognition. However, researchers have recently reported inconsistent perspectives on the appropriate designs for GCN architectures. For example, experiments in social network analysis have argued that deeper GCNs marginally increase the learning performance ( Wu et al., 2019 ), whereas a method for 3D point cloud segmentation achieves state-of- the-art performance with a 56-layer GCN network ( Li et al., 2019 ). These 'controversial' empirical findings motivate theoretical analysis to understand the fundamental performance factors and the architecture design choices for GCNs. Aiming to bestow GCNs with theoretical guarantees, one promising research direction is to study graph scattering transforms (GSTs). GSTs are non-trainable GCNs comprising a cascade of graph filter banks followed by nonlinear activation functions. The graph filter banks are mathematically designed and are adopted to scatter an input graph signal into multiple channels. GSTs extract scattering features that can be utilized towards graph learning tasks ( Gao et al., 2019 ), with competitive performance especially when the number of training examples is small. Under certain conditions on the graph filter banks, GSTs are endowed with energy conservation properties ( Zou & Lerman, Published as a conference paper at ICLR 2020  (a) Academic collaboration (b) Protein-protein network (c) 3D point cloud 2019), as well as stability meaning robustness to graph topology deformations ( Gama et al., 2019a ). However, GSTs are associated with exponential complexity in space and time that increases with the number of layers. This discourages deployment of GSTs when a deep architecture is needed. Furthermore, stability should not come at odds with sensitivity. A filter's output should be sensitive to and \"detect\" perturbations of large magnitude. Lastly, graph data in different domains (social networks, 3D point clouds) have distinct properties, which encourages GSTs with domain-adaptive architectures. The present paper develops a data-adaptive pruning framework for the GST to systematically retain important features. Specifically, the contribution of this work is threefold. C1. We put forth a pruning approach to select informative GST features that we naturally term pruned graph scattering transform (pGST). The pruning decisions are guided by a criterion promoting alignment (matching) of the input graph spectrum with that of the graph filters. The optimal pruning decisions are provided on-the-fly, and alleviate the exponential complexity of GSTs. C2. We prove that the pGST is stable to perturbations of the input graph signals. Under certain conditions on the energy of the perturbations, the resulting pruning patterns before and after the perturbations are identical and the overall pGST is stable. C3. We showcase with extensive experiments that: i) the proposed pGSTs perform similarly and in certain cases better than the baseline GSTs that use all scattering features, while achieving significant computational savings; ii) The extracted features from pGSTs can be utilized towards graph classification and 3D point cloud recognition. Even without any training on the feature extraction step, the performance is comparable to state-of-the-art deep supervised learning approaches, particularly when training data are scarce; and iii) By analyzing the pruning patterns of the pGST, we deduce that graph signals in different domains call for different network architectures; see  Fig. 1 .",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces Deep Factorized INput token Embeddings (DeFINE) for neural sequence modeling. DeFINE approximates the complicated token embedding function with far fewer parameters compared to standard methods, allowing for lower-dimensional input and output mappings in sequence models, reducing their computational burden without reducing performance. Experiments show that DeFINE can be used with different token types and improves the performance of both LSTM- and Transformer-based sequence models. On the Wikitext-103 dataset, an LSTM-based language model with DeFINE provides a 9 point improvement over a full capacity model while using half as many parameters. For machine translation, DeFINE improves the efficiency of a Transformer model by 26% while maintaining translation quality.",
        "Abstract": "For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",
        "Introduction": "  INTRODUCTION Neural models for NLP tasks, such as language modeling and machine translation, require large vocabularies for generality (Chelba et al., 2013; Bahdanau et al., 2015; Luong et al., 2015; Merity et al., 2017). These models often employ a similar architecture: tokens (e.g., words, sub-words, or characters), represented as one-hot vectors, are mapped to a dense continuous space; they are then processed by a context model; finally, the contextualized representations are mapped back to a vocabulary-sized vector for computing next-token probabilities. A language modeling example is shown in Figure 1a. The mapping in the first and last steps often uses a shared learned look- up table, referred to as an embedding layer, which takes every token in the vocabulary to a fixed m-dimensional vector. One drawback of this approach is that the number of parameters in the em- bedding layer increases as the vocabulary size grows, limiting us to small values of m over large vocabularies. Researchers have sought to improve the efficiency of the embedding layer by assign- ing lower frequency tokens smaller dimensional vectors, however, significant parameter reductions come at the cost of performance (Morin & Bengio, 2005; Grave et al., 2017a; Baevski & Auli, 2019). In all these approaches, token embedding is approximated with a linear function from tokens to vectors. In this work, we introduce Deep Factorized INput token Embeddings (DeFINE) for neural se- quence modeling. DeFINE approximates the complicated token embedding function with far fewer parameters compared to standard methods. DeFINE allows for lower-dimensional input and output mappings in sequence models, reducing their computational burden without reducing performance. The representations produced by DeFINE are more powerful than those of other factorization tech- niques and even standard embedding layers. To accomplish this, DeFINE leverages a hierarchical group transformation (HGT) that learns deep representations efficiently and effectively. HGT con- nects different subsets of the input using sparse and dense connections. To improve the flow of information, DeFINE introduces a new skip-connection that establishes a direct link with the input layer at every level of its hierarchy, allowing gradients to flow back directly to the input via multiple paths. DeFINE replaces standard embedding layers, leaving the rest of the model untouched, and Published as a conference paper at ICLR 2020 so it can be used with a wide variety of sequence modeling architectures and token-types, includ- ing words and sub-words.  Figure 1  shows how we incorporate DeFINE with Transformer-XL (Dai et al., 2019), a state-of-the-art Transformer-based language model, and the resulting reduction in total parameters. Our experiments show that both LSTM- and Transformer-based sequence models benefit from the use of DeFINE. Furthermore, our experiments with word-level language modeling and sub-word level machine translation tasks show that DeFINE can be used with different token types. On the Wikitext-103 dataset, an LSTM-based language model with DeFINE provides a 9 point improve- ment over a full capacity model while using half as many parameters. When combined with adaptive input (Baevski & Auli, 2019) and output (Grave et al., 2017a) representations, DeFINE improves the performance by about 3 points across LSTM-based (see Table 1a) and Transformer-XL-based (see  Table 2 ) language models with a minimal increase in training parameters. Computation time at infer- ence is unaffected. 1 Incorporating DeFINE into the popular AWD-LSTM language model (Merity et al., 2018b) without finetuning results in a test perplexity of 54.2 on the Penn Treebank dataset, outperforming both the original and fine-tuned AWD-LSTM models as well as Transformer-XL and MoS (Yang et al., 2018). For machine translation, DeFINE improves the efficiency of a Transformer model (Vaswani et al., 2017) by 26% while maintaining translation quality. We provide substantive experiments which detail the impact of our architecture decisions and demonstrate the effectiveness of DeFINE across models of varying capacities.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper addresses the problem of estimating the Lipschitz constant of a neural network f d , which is of major importance in many successful applications of deep learning. We propose a method to provide tighter upper bounds on L(f d ) than the trivial upper bound given by the product of layer-wise Lipschitz constants, even at the expense of increased complexity. We compare our method to existing methods based on semidefinite programming (SDP) and show that our method provides tighter upper bounds.",
        "Abstract": "We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bound on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the $\\ell_\\infty$-Lipschitz constant, our approach yields superior estimates as compared to other baselines available in the literature.\n",
        "Introduction": "  INTRODUCTION We consider a neural network f d defined by the recursion: for an integer d larger than 1, matrices {W i } d i=1 of appropriate dimensions and an activation function σ, understood to be applied element-wise. We refer to d as the depth, and we focus on the case where f d has a single real value as output. In this work, we address the problem of estimating the Lipschitz constant of the network f d . A function f is Lipschitz continuous with respect to a norm · if there exists a constant L such that for all x, y we have |f (x) − f (y)| ≤ L x − y . The minimum over all such values satisfying this condition is called the Lipschitz constant of f and is denoted by L(f ). The Lipschitz constant of a neural network is of major importance in many successful applications of deep learning. In the context of supervised learning, Bartlett et al. (2017) show how it directly correlates with the generalization ability of neural network classifiers, suggesting it as model com- plexity measure. It also provides a measure of robustness against adversarial perturbations (Szegedy et al., 2014) and can be used to improve such metric (Cisse et al., 2017). Moreover, an upper bound on L(f d ) provides a certificate of robust classification around data points (Weng et al., 2018). Another example is the discriminator network of the Wasserstein GAN (Arjovsky et al., 2017), whose Lipschitz constant is constrained to be at most 1. To handle this constraint, researchers have proposed different methods like heuristic penalties (Gulrajani et al., 2017), upper bounds (Miyato et al., 2018), choice of activation function (Anil et al., 2019), among many others. This line of work has shown that accurate estimation of such constant is key to generating high quality images. Lower bounds or heuristic estimates of L(f d ) can be used to provide a general sense of how robust a network is, but fail to provide true certificates of robustness to input perturbations. Such certificates require true upper bounds, and are paramount when deploying safety-critical deep reinforcement learning applications (Berkenkamp et al., 2017; Jin & Lavaei, 2018). The trivial upper bound given by the product of layer-wise Lipschitz constants is easy to compute but rather loose and overly pessimistic, providing poor insight into the true robustness of a network (Huster et al., 2018). Indeed, there is a growing need for methods that provide tighter upper bounds on L(f d ), even at the expense of increased complexity. For example Raghunathan et al. (2018a); Jin & Lavaei (2018); Fa- zlyab et al. (2019) derive upper bounds based on semidefinite programming (SDP). While expensive Published as a conference paper at ICLR 2020 to compute, these type of certificates are in practice surprisingly tight. Our work belongs in this vein of research, and aims to overcome some limitations in the current state-of-the-art.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a practical instantiation of a robotic learning system that is able to autonomously learn from raw sensory inputs, assign rewards to its own trials without hand-designed perception systems or instrumentation, and learn continuously in non-episodic settings without requiring human intervention to manually reset the environment. We provide an empirical analysis of these issues, both in simulation and on a real-world robotic system, and propose a number of simple but effective solutions that together produce a complete robotic learning system. We show that this system can learn dexterous robotic manipulation tasks in the real world, substantially outperforming ablations and prior work.",
        "Abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) can in principle enable autonomous systems, such as robots, to acquire a large repertoire of skills automatically. Perhaps even more importantly, reinforcement learning can enable such systems to continuously improve the proficiency of their skills from experience. However, realizing this in reality has proven challenging: even with reinforcement learning methods that can acquire complex behaviors from high-dimensional low-level observations, such as images, the assumptions of the reinforcement learning problem setting do not fit cleanly into the constraints of the real world. For this reason, most successful robotic learning experiments have employed various kinds of environmental instrumentation in order to define reward functions, reset between trials, and obtain ground truth state (Levine et al., 2016; Haarnoja et al., 2018a; Kumar et al., 2016; Andrychowicz et al., 2018; Zhu et al., 2019; Chebotar et al., 2016; Nagabandi et al., 2019; Gupta et al., 2016). In order to practically and scalably deploy autonomous learning systems that improve continuously through real-world operation, we must lift these limitations and design algorithms that can learn under the constraints of real-world environments, as illustrated in  Figure 2 . We propose that overcoming these challenges in a scalable way requires designing robotic systems that possess three capabilities: they are able to (1) learn from their own raw sensory inputs, (2) assign rewards to their own trials without hand-designed perception systems or instrumentation, and (3) learn continuously in non-episodic settings without requiring human intervention to manually reset the environment. A system with these capabilities can autonomously collect large amounts of real world data - typically crucial for effective generalization - without significant instrumentation in each training environment, an example of which is shown in  Figure 1 . If successful, this would lift a major constraint that stands between current reinforcement learning algorithms and the ability to learn scalable, generalizable, and robust real-world behaviors. Such a system would also bring us significantly closer to the goal of embodied learning-based systems that improve continuously through their own real-world experience. Human intervention is only required in the goal collection phase (1). The robot is left to train unattended (2) during the learning phase and can be evaluated from arbitrary initial states at the end of training (3). We show sample goal and intermediate images from the training process of a real hardware system Having laid out these requirements, we propose a practical instantiation of such a learning system. While prior works have studied many of these issues in isolation, combining them into a complete real-world learning system presents a number of challenges, as we discuss in Section 3. We pro- vide an empirical analysis of these issues, both in simulation and on a real-world robotic system, and propose a number of simple but effective solutions that together produce a complete robotic learning system. This system can autonomously learn from raw sensory inputs, learn reward func- tions from easily available supervision, and learn without manually designed reset mechanisms. We show that this system can learn dexterous robotic manipulation tasks in the real world, substantially outperforming ablations and prior work.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes ELECTRA, a pre-training task in which a model learns to distinguish real input tokens from synthetically generated replacements. This discriminative task is more computationally efficient than existing generative approaches, such as masked language modeling (MLM). Through a series of ablations, the paper shows that ELECTRA trains faster than BERT and achieves higher accuracy on downstream tasks when fully trained. Experiments on the GLUE natural language understanding benchmark and SQuAD question answering benchmark demonstrate that ELECTRA substantially outperforms MLM-based methods such as BERT and XLNet given the same model size, data, and compute. ELECTRA-Large sets a new state-of-the-art for SQuAD 2.0.",
        "Abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.\n",
        "Introduction": "  INTRODUCTION Current state-of-the-art representation learning methods for language can be viewed as learning denoising autoencoders ( Vincent et al., 2008 ). They select a small subset of the unlabeled input sequence (typically 15%), mask the identities of those tokens (e.g., BERT;  Devlin et al. (2019) ) or attention to those tokens (e.g., XLNet;  Yang et al. (2019) ), and then train the network to recover the original input. While more effective than conventional language-model pre-training due to learning bidirectional representations, these masked language modeling (MLM) approaches incur a substan- tial compute cost because the network only learns from 15% of the tokens per example. As an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. Instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language model. This corruption proce- dure solves a mismatch in BERT (although not in XLNet) where the network sees artificial [MASK] tokens during pre-training but not when being fine-tuned on downstream tasks. We then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement. In contrast, MLM trains the network as a generator that predicts the original identities of the corrupted tokens. A key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient. Although our Published as a conference paper at ICLR 2020 approach is reminiscent of training the discriminator of a GAN, our method is not adversarial in that the generator producing corrupted tokens is trained with maximum likelihood due to the difficulty of applying GANs to text ( Caccia et al., 2018 ). We call our approach ELECTRA 1 for \"Efficiently Learning an Encoder that Classifies Token Re- placements Accurately.\" As in prior work, we apply it to pre-train Transformer text encoders ( Vaswani et al., 2017 ) that can be fine-tuned on downstream tasks. Through a series of ablations, we show that learning from all input positions causes ELECTRA to train much faster than BERT. We also show ELECTRA achieves higher accuracy on downstream tasks when fully trained. Most current pre-training methods require large amounts of compute to be effective, raising con- cerns about their cost and accessibility. Since pre-training with more compute almost always re- sults in better downstream accuracies, we argue an important consideration for pre-training methods should be compute efficiency as well as absolute downstream performance. From this viewpoint, we train ELECTRA models of various sizes and evaluate their downstream performance vs. their compute requirement. In particular, we run experiments on the GLUE natural language understand- ing benchmark ( Wang et al., 2019 ) and SQuAD question answering benchmark ( Rajpurkar et al., 2016 ). ELECTRA substantially outperforms MLM-based methods such as BERT and XLNet given the same model size, data, and compute (see  Figure 1 ). For example, we build an ELECTRA-Small model that can be trained on 1 GPU in 4 days. 2 ELECTRA-Small outperforms a comparably small BERT model by 5 points on GLUE, and even outperforms the much larger GPT model ( Radford et al., 2018 ). Our approach also works well at large scale, where we train an ELECTRA-Large model that performs comparably to RoBERTa ( Liu et al., 2019 ) and XLNet ( Yang et al., 2019 ), de- spite having fewer parameters and using 1/4 of the compute for training. Training ELECTRA-Large further results in an even stronger model that outperforms ALBERT ( Lan et al., 2019 ) on GLUE and sets a new state-of-the-art for SQuAD 2.0. Taken together, our results indicate that the discrim- inative task of distinguishing real data from challenging negative samples is more compute-efficient and parameter-efficient than existing generative approaches for language representation learning.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: Artificial Neural Networks (ANNs) have recently achieved human-level performance on various tasks, but have been shown to underperform when tested on data that differs from the training data. This lack of generalization presents two issues when ANNs are used in the real world: they are often tested on disturbed or noisy inputs, and are susceptible to adversarial attacks. These issues limit the applicability of ANNs and present potential security risks.",
        "Abstract": "Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform well on similar testing data, inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are noisy, blurred, or otherwise distorted.  It has been hypothesized that sleep promotes generalization of knowledge and improves robustness against noise in animals and humans. In this work, we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as in increasing ANN classification robustness. We compare the sleep algorithm's performance on various robustness tasks with two previously proposed adversarial defenses - defensive distillation and fine-tuning. We report an increase in robustness after sleep phase to adversarial attacks as well as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve existing problems in ANNs and guide the development of more robust, human-like ANNs.",
        "Introduction": "  INTRODUCTION Although artificial neural networks (ANNs) have recently begun to rival human performance on var- ious tasks, ranging from complex games ( Silver et al. (2016) ) to image classification ( Krizhevsky et al. (2012) ), ANNs have been shown to underperform when the testing data differs in specific ways even by a small amount from the training data ( Geirhos et al. (2018) ). This lack of generalization presents two issues when ANNs are utilized in the real world. First, ANNs are often trained on cu- rated datasets of images designed to best capture the image content, whereas in real-world scenarios, they may be tested on disturbed or noisy inputs, not observed during training. Second, ANNs are susceptible to adversarial attacks, or the deliberate creation of inputs designed to fool ANNs that may be imperceptibly different from correctly classified inputs ( Szegedy et al. (2013) ). These two issues limit ANNs applicability in the real world and present potential security risks when deployed.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on how to devise model-free reinforcement learning algorithms for efficient exploration that scale to large complex state spaces and have strong theoretical underpinnings. Current model-free approaches to exploration in deep RL do not employ optimistic initialisation, which is crucial to provably efficient exploration in all model-free tabular algorithms. To benefit exploration, the Q-values for novel state-action pairs must remain high until they are explored. This paper explores methods to achieve this goal.",
        "Abstract": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a simple count-based augmentation to pessimistically initialised Q-values that separates the source of optimism from the neural network. We show that this scheme is provably efficient in the tabular setting and extend it to the deep RL setting. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure optimism during both action selection and bootstrapping. We show that OPIQ outperforms non-optimistic DQN variants that utilise a pseudocount-based intrinsic motivation in hard exploration tasks, and that it predicts optimistic estimates for novel state-action pairs.",
        "Introduction": "  INTRODUCTION In reinforcement learning (RL), exploration is crucial for gathering sufficient data to infer a good control policy. As environment complexity grows, exploration becomes more challenging and simple randomisation strategies become inefficient. While most provably efficient methods for tabular RL are model-based (Brafman and Tennenholtz, 2002; Strehl and Littman, 2008; Azar et al., 2017), in deep RL, learning models that are useful for planning is notoriously difficult and often more complex (Hafner et al., 2019) than model- free methods. Consequently, model-free approaches have shown the best final performance on large complex tasks (Mnih et al., 2015; 2016; Hessel et al., 2018), especially those requiring hard exploration (Bellemare et al., 2016; Ostrovski et al., 2017). Therefore, in this paper, we focus on how to devise model-free RL algorithms for efficient exploration that scale to large complex state spaces and have strong theoretical underpinnings. Despite taking inspiration from tabular algorithms, current model-free approaches to exploration in deep RL do not employ optimistic initialisation, which is crucial to provably efficient exploration in all model-free tabular algorithms. This is because deep RL algorithms do not pay special attention to the initialisation of the neural networks and instead use common initialisation schemes that yield initial Q-values around zero. In the common case of non-negative rewards, this means Q-values are initialised to their lowest possible values, i.e., a pessimistic initialisation. While initialising a neural network optimistically would be trivial, e.g., by setting the bias of the final layer of the network, the uncontrolled generalisation in neural networks changes this initialisation quickly. Instead, to benefit exploration, we require the Q-values for novel state-action pairs must remain high until they are explored.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes contextual embedding alignment as a useful concept for understanding and improving multilingual BERT's cross-lingual transfer. Through experiments on the contextual word retrieval task, the paper shows that multilingual BERT achieves zero-shot transfer because its embeddings are partially aligned, and that a fine-tuning-based alignment procedure significantly improves BERT as a multilingual model. The paper also provides insight into the strengths and shortcomings of multilingual BERT, and suggests that contextual alignment is an important task that provides useful insight into large multilingual pre-trained models.",
        "Abstract": "We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models.",
        "Introduction": "  INTRODUCTION Embedding alignment was originally studied for word vectors with the goal of enabling cross-lingual transfer, where the embeddings for two languages are in alignment if word translations, e.g. cat and Katze, have similar representations ( Mikolov et al., 2013a ; Smith et al., 2017). Recently, large pre- trained models have largely subsumed word vectors based on their accuracy on downstream tasks, partly due to the fact that their word representations are context-dependent, allowing them to more richly capture the meaning of a word ( Peters et al., 2018 ;  Howard & Ruder, 2018 ;  Radford et al., 2018 ;  Devlin et al., 2018 ). Therefore, with the same goal of cross-lingual transfer but for these more complex models, we might consider contextual embedding alignment, where we observe whether word pairs within parallel sentences, e.g. cat in \"The cat sits\" and Katze in \"Die Katze sitzt,\" have similar representations. One model relevant to these questions is multilingual BERT, a version of BERT pre-trained on 104 languages that achieves remarkable transfer on downstream tasks. For example, after the model is fine-tuned on the English MultiNLI training set, it achieves 74.3% accuracy on the test set in Span- ish, which is only 7.1% lower than the English accuracy ( Devlin et al., 2018 ;  Conneau et al., 2018b ). Furthermore, while the model transfers better to languages similar to English, it still achieves rea- sonable accuracies even on languages with different scripts. However, given the way that multilingual BERT was pre-trained, it is unclear why we should expect such high zero-shot performance. Compared to monolingual BERT which exhibits no zero-shot transfer, multilingual BERT differs only in that (1) during pre-training (i.e. masked word prediction), each batch contains sentences from all of the languages, and (2) it uses a single shared vocabulary, formed by WordPiece on the concatenated monolingual corpora ( Devlin et al., 2019 ). Therefore, we might wonder: (1) How can we better understand BERT's multilingualism? (2) Can we further improve BERT's cross-lingual transfer? In this paper, we show that contextual embedding alignment is a useful concept for addressing these questions. First, we propose a contextual version of word retrieval to evaluate the degree of alignment, where a model is presented with two parallel corpora, and given a word within a sentence in one corpus, it must find the correct word and sentence in the other. Using this metric of alignment, we show that multilingual BERT achieves zero-shot transfer because its embeddings are partially aligned, as depicted in  Figure 1 , with the degree of alignment predicting the degree of downstream transfer. Next, using between 10K and 250K sentences per language from the Europarl corpus as parallel data ( Koehn, 2005 ), we propose a fine-tuning-based alignment procedure and show that it signifi- cantly improves BERT as a multilingual model. Specifically, on zero-shot XNLI, where the model is trained on English MultiNLI and tested on other languages ( Conneau et al., 2018b ), the aligned model improves accuracies by 2.78% on average over the base model, and it remarkably matches translate-train models for Bulgarian and Greek, which approximate the fully-supervised setting. To put our results in the context of past work, we also use word retrieval to compare our fine- tuning procedure to two alternatives: (1) fastText augmented with sentence and aligned using rota- tions ( Bojanowski et al., 2017 ;  Rücklé et al., 2018 ;  Artetxe et al., 2018 ), and (2) BERT aligned using rotations ( Aldarmaki & Diab, 2019 ;  Schuster et al., 2019 ;  Wang et al., 2019 ). We find that when there are multiple occurences per word, fine-tuned BERT outperforms fastText, which outperforms rotation-aligned BERT. This result supports the intuition that contextual alignment is more difficult than its non-contextual counterpart, given that a rotation, at least when applied naively, is no longer sufficient to produce strong alignments. In addition, when there is only one occurrence per word, fine-tuned BERT matches the performance of fastText. Given that context disambiguation is no longer necessary, this result suggests that our fine-tuning procedure is able to align BERT at the type level to a degree that matches non-contextual approaches. Finally, we use the contextual word retrieval task to conduct finer-grained analysis of multilingual BERT, with the goal of better understanding its strengths and shortcomings. Specifically, we find that base BERT has trouble aligning open-class compared to closed-class parts-of-speech, as well as word pairs that have large differences in usage frequency, suggesting insight into the pre-training procedure that we explore in Section 5. Together, these experiments support contextual alignment as an important task that provides useful insight into large multilingual pre-trained models.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a memory layer for joint graph representation learning and graph coarsening that consists of a multi-head array of memory keys and a convolution operator to aggregate the soft cluster assignments from different heads. This layer does not explicitly require connectivity information and does not struggle with over-smoothing problem. Two networks based on the proposed layer are introduced: memory-based GNN (MemGNN) and graph memory network (GMN). MemGNN consists of a GNN that learns the initial node representations, and a stack of memory layers that learn hierarchical representations up to the global graph representation. GMN, on the other hand, learns the hierarchical representations purely based on memory layers and hence does not require message passing.",
        "Abstract": "Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data.\n",
        "Introduction": "  INTRODUCTION Graph neural networks (GNNs) ( Wu et al., 2019 ;  Zhou et al., 2018 ;  Zhang et al., 2018 ) are a class of deep models that operate on data with arbitrary topology represented as graphs such as social networks ( Kipf & Welling, 2016 ), knowledge graphs ( Vivona & Hassani, 2019 ), molecules ( Duve- naud et al., 2015 ), point clouds ( Hassani & Haley, 2019 ), and robots ( Wang et al., 2019 ). Unlike regular-structured inputs such as grids (e.g., images and volumetric data) and sequences (e.g., speech and text), GNN inputs are permutation-invariant variable-size graphs consisting of nodes and their interactions. GNNs such as gated GNN (GGNN) ( Li et al., 2015 ), message passing neural net- work (MPNN) ( Gilmer et al., 2017 ), graph convolutional network (GCN) ( Kipf & Welling, 2016 ), and graph attention network (GAT) ( Velikovi et al., 2018 ) learn node representations through an iterative process of transferring, transforming, and aggregating the node representations from topo- logical neighbors. Each iteration expands the receptive field by one hop and after k iterations the nodes within k hops influence the node representations of one another. GNNs are shown to learn better representations compared to random walks ( Grover & Leskovec, 2016 ;  Perozzi et al., 2014 ), matrix factorization ( Belkin & Niyogi, 2002 ;  Ou et al., 2016 ), kernel methods ( Shervashidze et al., 2011 ;  Kriege et al., 2016 ), and probabilistic graphical models ( Dai et al., 2016 ). These models, however, cannot learn hierarchical representations as they do not exploit the compo- sitional nature of graphs. Recent work such as differentiable pooling (DiffPool) ( Ying et al., 2018 ), TopKPool ( Gao & Ji, 2019 ), and self-attention graph pooling (SAGPool) ( Lee et al., 2019 ) intro- duce parametric graph pooling layers that allow GNNs to learn hierarchical graph representations by stacking interleaved layers of GNN and pooling layers. These layers cluster nodes in the latent space. The clusters may correspond to communities in a social network or potent functional groups within a chemical dataset. Nevertheless, these models are not efficient as they require an iterative process of message passing after each pooling layer. In this paper, we introduce a memory layer for joint graph representation learning and graph coars- ening that consists of a multi-head array of memory keys and a convolution operator to aggregate the soft cluster assignments from different heads. The queries to a memory layer are node repre- sentations from the previous layer and the outputs are the node representations of the coarsened graph. The memory layer does not explicitly require connectivity information and unlike GNNs Published as a conference paper at ICLR 2020 relies on the global information rather than local topology. Hence, it does not struggle with over- smoothing problem ( Xu et al., 2018 ;  Li et al., 2018 ). These properties make memory layers more efficient and improves their performance. We also introduce two networks based on the proposed layer: memory-based GNN (MemGNN) and graph memory network (GMN). MemGNN consists of a GNN that learns the initial node representations, and a stack of memory layers that learn hi- erarchical representations up to the global graph representation. GMN, on the other hand, learns the hierarchical representations purely based on memory layers and hence does not require message passing.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents HiLLoC, a hierarchical latent variable model for lossless compression. HiLLoC combines Bits Back with Asymmetric Numeral Systems (BB-ANS) with novel techniques such as direct coding of arbitrary sized images using a fully convolutional model, vectorized ANS implementation supporting dynamic shape, dynamic discretization, and initializing the bits back chain using a different codec. Experiments demonstrate that HiLLoC can be used to compress color images from the ImageNet test set at rates close to the ELBO, outperforming other codecs, and resulting in a speedup of nearly three orders of magnitude. An open source implementation based on 'Craystack' is released.",
        "Abstract": "We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.",
        "Introduction": "  INTRODUCTION Bits back coding ( Wallace, 1990 ;  Hinton & van Camp, 1993 ) is a method for performing lossless compression using a latent variable model. In an ideal implementation, the method can achieve an expected message length equal to the variational free energy, often referred to as the negative evidence lower bound (ELBO) of the model. Bits back was first introduced to form a theoretical argument for using the ELBO as an objective function for machine learning ( Hinton & van Camp, 1993 ). The first implementation of bits back coding ( Frey, 1997 ;  Frey & Hinton, 1996 ) made use of first-in- first-out (FIFO) arithmetic coding (AC) ( Witten et al., 1987 ). However, the implementation did not achieve optimal compression, due to an incompatibility between a FIFO coder and bits back coding, and its use was only demonstrated on a small dataset of 8×8 binary images. Recently, zero-overhead bits back compression with a significantly simpler implementation has been developed by  Townsend et al. (2019) . This implementation makes use of asymmetric numeral systems (ANS), a last-in-first-out (LIFO) entropy coding scheme ( Duda, 2009 ). The method, known as 'Bits Back with Asymmetric Numeral Systems' (BB-ANS) was demonstrated by compressing the MNIST test set using a variational auto-encoder (VAE) model ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ), achieving a compression rate within 1% of the model ELBO. More recently,  Hoogeboom et al. (2019)  and  Ho et al. (2019)  have proposed flow-based methods for lossless compression, and  Kingma et al. (2019)  have presented 'Bit-Swap', extending BB-ANS to hierarchical models. In this work we present an alternative method for extending to hierarchical VAEs. This entails the following novel techniques: 1. Direct coding of arbitrary sized images using a fully convolutional model. 2. A vectorized ANS implementation supporting dynamic shape. 3. Dynamic discretization to avoid having to calibrate a static discretization. 4. Initializing the bits back chain using a different codec. We discuss each of these contributions in detail in Section 3. We call the combination of BB-ANS using a hierarchical latent variable model and the above techniques: 'Hierarchical Latent Lossless Published as a conference paper at ICLR 2020 PNG WebP FLIF Bit-Swap HiLLoC 0.0 0.2 0.4 0.6 0.8 1.0 Compression ratio Raw data Compression' (HiLLoC). In our experiments (Section 4), we demonstrate that HiLLoC can be used to compress color images from the ImageNet test set at rates close to the ELBO, outperforming all of the other codecs which we benchmark. We also demonstrate the speedup, of nearly three orders of magnitude, resulting from vectorization. We release an open source implementation based on 'Craystack', a Python package which we have written for general prototyping of lossless compression with ANS.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the theoretical properties of graph neural networks (GNNs) and establishes connections between GNNs and well-known logical formalisms. We focus on aggregate-combine GNNs (AC-GNNs) and the two variable fragment of first order predicate logic extended with counting quantifiers (FOC2). We characterize the fragment of FOC2 formulas that can be expressed as AC-GNNs, which corresponds to graded modal logic or ALCQ. We then extend the AC-GNN architecture with global readouts, allowing us to prove that each FOC2 formula can be captured by an aggregate-combine-readout GNN (ACR-GNN). We experimentally validate our findings, showing that ACR-GNNs can generalize to unseen graph sizes.",
        "Abstract": "The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or  false) can be expressed by GNNs.  We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors.  We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of  the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.",
        "Introduction": "  INTRODUCTION Graph neural networks (GNNs) ( Merkwirth & Lengauer, 2005 ;  Scarselli et al., 2009 ) are a class of neural network architectures that has recently become popular for a wide range of applications dealing with structured data, e.g., molecule classification, knowledge graph completion, and Web page ranking ( Battaglia et al., 2018 ;  Gilmer et al., 2017 ;  Kipf & Welling, 2017 ;  Schlichtkrull et al., 2018 ). The main idea behind GNNs is that the connections between neurons are not arbitrary but reflect the structure of the input data. This approach is motivated by convolutional and recurrent neural networks and generalize both of them ( Battaglia et al., 2018 ). Despite the fact that GNNs have recently been proven very efficient in many applications, their theoretical properties are not yet well-understood. In this paper we make a step towards understanding their expressive power by establishing connections between GNNs and well-known logical formalisms. We believe these connections to be conceptually important, as they permit us to understand the inherently procedural behavior of some fragments of GNNs in terms of the more declarative flavor of logical languages. Two recent papers ( Morris et al., 2019 ;  Xu et al., 2019 ) have started exploring the theoretical prop- erties of GNNs by establishing a close connection between GNNs and the Weisfeiler-Lehman (WL) test for checking graph isomorphism. The WL test works by constructing a labeling of the nodes of the graph, in an incremental fashion, and then decides whether two graphs are isomorphic by com- paring the labeling of each graph. To state the connection between GNNs and this test, consider the simple GNN architecture that updates the feature vector of each graph node by combining it with the aggregation of the feature vectors of its neighbors. We call such GNNs aggregate-combine GNNs, Published as a conference paper at ICLR 2020 or AC-GNNs. The authors of these papers independently observe that the node labeling produced by the WL test always refines the labeling produced by any GNN. More precisely, if two nodes are labeled the same by the algorithm underlying the WL test, then the feature vectors of these nodes produced by any AC-GNN will always be the same. Moreover, there are AC-GNNs that can repro- duce the WL labeling, and hence AC-GNNs can be as powerful as the WL test for distinguishing nodes. This does not imply, however, that AC-GNNs can capture every node classifier-that is, a function assigning true or false to every node-that is refined by the WL test. In fact, it is not difficult to see that there are many such classifiers that cannot be captured by AC-GNNs; one simple example is a classifier assigning true to every node if and only if the graph has an isolated node. Our work aims to answer the question of what are the node classifiers that can be captured by GNN architectures such as AC-GNNs. To start answering this question, we propose to focus on logical classifiers-that is, on unary formu- las expressible in first order predicate logic (FO): such a formula classifies each node v according to whether the formula holds for v or not. This focus gives us an opportunity to link GNNs with declarative and well understood formalisms, and to establish conclusions about GNNs drawing upon the vast amount of work on logic. For example, if one proves that two GNN architectures are cap- tured with two logics, then one can immediately transfer all the knowledge about the relationships between those logics, such as equivalence or incomparability of expressiveness, to the GNN setting. For AC-GNNs, a meaningful starting point to measure their expressive power is the logic FOC 2 , the two variable fragment of first order predicate logic extended with counting quantifiers of the form ∃ ≥N ϕ, which state that there are at least N nodes satisfying formula ϕ ( Cai et al., 1992 ). Indeed, this choice of FOC 2 is justified by a classical result due to  Cai et al. (1992)  establishing a tight connection between FOC 2 and WL: two nodes in a graph are classified the same by the WL test if and only if they satisfy exactly the same unary FOC 2 formulas. Moreover, the counting capabilities of FOC 2 can be mimicked in FO (albeit with more than just two variables), hence FOC 2 classifiers are in fact logical classifiers according to our definition. Given the connection between AC-GNNs and WL on the one hand, and that between WL and FOC 2 on the other hand, one may be tempted to think that the expressivity of AC-GNNs coincides with that of FOC 2 . However, the reality is not as simple, and there are many FOC 2 node classifiers (e.g., the trivial one above) that cannot be expressed by AC-GNNs. This leaves us with the following natural questions. First, what is the largest fragment of FOC 2 classifiers that can be captured by AC-GNNs? Second, is there an extension of AC-GNNs that allows to express all FOC 2 classifiers? In this paper we provide answers to these two questions. The following are our main contributions. • We characterize exactly the fragment of FOC 2 formulas that can be expressed as AC- GNNs. This fragment corresponds to graded modal logic ( de Rijke, 2000 ), or, equivalently, to the description logic ALCQ, which has received considerable attention in the knowledge representation community ( Baader et al., 2003 ;  Baader & Lutz, 2007 ). • Next we extend the AC-GNN architecture in a very simple way by allowing global read- outs, where in each layer we also compute a feature vector for the whole graph and combine it with local aggregations; we call these aggregate-combine-readout GNNs (ACR-GNNs). These networks are a special case of the ones proposed by  Battaglia et al. (2018)  for re- lational reasoning over graph representations. In this setting, we prove that each FOC 2 formula can be captured by an ACR-GNN. We experimentally validate our findings showing that the theoretical expressiveness of ACR-GNNs, as well as the differences between AC-GNNs and ACR-GNNs, can be observed when we learn from examples. In particular, we show that on synthetic graph data conforming to FOC 2 formulas, AC- GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper studies the properties of conditional generative models and assesses them on a spectrum of robustness tasks, including detection of worst-case outliers in the form of adversarial examples, detection of average-case outliers in the form of ambiguous inputs, and detection of incorrectly labeled in-distribution inputs. We analyze the theoretical guarantees of a strong conditional generative model and show that even a near-perfect model cannot be guaranteed to reject adversarially perturbed inputs with high probability. We also discuss the potential of class-conditional likelihood for outlier detection in the general case.",
        "Abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. \n\nOur theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",
        "Introduction": "  INTRODUCTION Conditional generative models have recently shown promise to overcome many limitations of their discriminative counterparts. They have been shown to be robust against adversarial attacks ( Schott et al., 2019 ;  Ghosh et al., 2019 ;  Song et al., 2018 ;  Li et al., 2018 ;  Frosst et al., 2018 ), to enable robust classification in the presence of outliers ( Nalisnick et al., 2019b ) and to achieve promising results in semi-supervised learning ( Kingma et al., 2014 ; Salimans et al., 2016). Motivated by these success stories, we study the properties of conditional generative models in more detail. Unlike discriminative models, which can ignore class-irrelevant information, conditional generative models cannot discard any information in the input, potentially making it harder to fool them. Further, Published as a conference paper at ICLR 2020 jointly modeling the input and target distribution should make it easy to detect out-of-distribution inputs. These traits lend hope to the belief that good class-conditional generative models can overcome important problems faced by discriminative models. In this work, we analyze conditional generative models by assessing them on a spectrum of robustness tasks. (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in- distribution inputs. If a generative classifier is able to perform well on all of these, it will naturally be robust to noisy, ambiguous or adversarially perturbed inputs. Outlier detection in the above settings is substantially different from general out-of-distribution (OOD) detection, where the goal is to use unconditional generative models to detect any OOD input. For the general case, likelihood has been shown to be a poor detector of OOD samples. In fact, often higher likelihood is assigned to OOD data than to the training data itself ( Nalisnick et al., 2019a ). However, class-conditional likelihood necessarily needs to decrease towards the decision-boundary for the classifier to work well. Thus, if the class-conditional generative model has high accuracy, rejection of outliers from the wrong class via likelihood may be possible. Our contributions are: Provable Robustness We answer: Can we theoretically guarantee that a strong conditional gen- erative model can robustly detect adversarially attacked inputs? In section 2 we show that even a near-perfect conditional generative model cannot be guaranteed to reject adversarially perturbed inputs with high probability.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces GAN-TTS, a Generative Adversarial Network for text-conditional high-fidelity speech synthesis. It is composed of a convolutional neural network generator coupled with an ensemble of multiple discriminators which evaluate the generated (and real) audio based on multi-frequency random windows. We also propose a family of quantitative metrics for speech generation based on Fréchet Inception Distance and Kernel Inception Distance, and present quantitative and subjective evaluation of TTS-GAN and its ablations. Our best-performing model achieves a MOS of 4.2, which is comparable to the state-of-the-art WaveNet MOS of 4.4, and establishes GANs as a viable option for efficient TTS.",
        "Abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\nOur architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fréchet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav",
        "Introduction": "  INTRODUCTION The Text-to-Speech (TTS) task consists in the conversion of text into speech audio. In recent years, the TTS field has seen remarkable progress, sparked by the development of neural autoregressive models for raw audio waveforms such as WaveNet ( van den Oord et al., 2016 ), SampleRNN ( Mehri et al., 2017 ) and WaveRNN ( Kalchbrenner et al., 2018 ). A notable limitation of these models is that they are difficult to parallelise over time: they predict each time step of an audio signal in sequence, which is computationally expensive and often impractical. A lot of recent research on neural models for TTS has focused on improving parallelism by predicting multiple time steps in parallel, e.g. using flow-based models ( van den Oord et al., 2018 ;  Ping et al., 2019 ;  Prenger et al., 2019 ;  Kim et al., 2019 ). Such highly parallelisable models are more suitable to run efficiently on modern hardware. An alternative approach for parallel waveform generation would be to use Generative Adversar- ial Networks (GANs,  Goodfellow et al., 2014 ). GANs currently constitute one of the dominant paradigms for generative modelling of images, and they are able to produce high-fidelity samples Published as a conference paper at ICLR 2020 that are almost indistinguishable from real data. However, their application to audio generation tasks has seen relatively limited success so far. In this paper, we explore raw waveform generation with GANs, and demonstrate that adversarially trained feed-forward generators are indeed able to synthesise high-fidelity speech audio. Our contributions are as follows: • We introduce GAN-TTS, a Generative Adversarial Network for text-conditional high- fidelity speech synthesis. Its feed-forward generator is a convolutional neural network, coupled with an ensemble of multiple discriminators which evaluate the generated (and real) audio based on multi-frequency random windows. Notably, some discriminators take the linguistic conditioning into account (so they can measure how well the generated au- dio corresponds to the input utterance), while others ignore the conditioning, and can only assess the general realism of the audio. • We propose a family of quantitative metrics for speech generation based on Fréchet Incep- tion Distance ( FID, Heusel et al., 2017 ) and Kernel Inception Distance ( KID, Bińkowski et al., 2018 ), where we replace the Inception image recognition network with the Deep- Speech audio recognition network. The code for our metrics is publicly available online 1 . • We present quantitative and subjective evaluation of TTS-GAN and its ablations, demon- strating the importance of our architectural choices. Our best-performing model achieves a MOS of 4.2, which is comparable to the state-of-the-art WaveNet MOS of 4.4, and estab- lishes GANs as a viable option for efficient TTS.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the connection between optimization and generalization of deep neural networks (DNNs). We propose a simplified model of the early part of the training trajectory of DNNs and present empirical evidence for two conjectures about the dependence of the entire optimization trajectory on the early phase of training. Specifically, we conjecture that the hyperparameters of stochastic gradient descent (SGD) used before reaching the break-even point control the spectral norms of the covariance of gradients and the local curvature of the loss surface, as well as the conditioning of these matrices. We also apply our analysis to a network with batch normalization layers and show that using a large learning rate is necessary to reach better-conditioned regions of the loss surface.",
        "Abstract": "The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the \"``break-even\" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction.",
        "Introduction": "  INTRODUCTION The connection between optimization and generalization of deep neural networks (DNNs) is not fully understood. For instance, using a large initial learning rate often improves generalization, which can come at the expense of the initial training loss reduction (Goodfellow et al., 2016; Li et al., 2019; Jiang et al., 2020). In contrast, using batch normalization layers typically improves both generalization and convergence speed of deep neural networks (Luo et al., 2019; Bjorck et al., 2018). These simple examples illustrate limitations of our understanding of DNNs. Understanding the early phase of training has recently emerged as a promising avenue for study- ing the link between optimization and generalization of DNNs. It has been observed that applying regularization in the early phase of training is necessary to arrive at a well generalizing final solu- tion (Keskar et al., 2017; Sagun et al., 2017; Achille et al., 2017). Another observed phenomenon is that the local shape of the loss surface changes rapidly in the beginning of training (LeCun et al., 2012; Keskar et al., 2017; Achille et al., 2017; Jastrzebski et al., 2018; Fort & Ganguli, 2019). The- oretical approaches to understanding deep networks also increasingly focus on the early part of the optimization trajectory (Li et al., 2019; Arora et al., 2019). In this work, we study the dependence of the entire optimization trajectory on the early phase of training. We investigate noise in the mini-batch gradients using the covariance of gradients, 1 and the local curvature of the loss surface using the Hessian. These two matrices capture important and 1 We define it as K = 1 N N i=1 (gi − g) T (gi − g), where gi = g(x i , yi; θ) is the gradient of the training loss L with respect to θ on xi, N is the number of training examples, and g is the full-batch gradient. Our first contribution is a simplified model of the early part of the training trajectory of DNNs. Based on prior empirical work (Sagun et al., 2017), we assume that the local curvature of the loss surface (the spectral norm of the Hessian) increases or decreases monotonically along the optimization tra- jectory. Under this model, gradient descent reaches a point in the early phase of training at which it oscillates along the most curved direction of the loss surface. We call this point the break-even point and show empirical evidence of its existence in the training of actual DNNs. Our main contribution is to state and present empirical evidence for two conjectures about the depen- dence of the entire optimization trajectory on the early phase of training. Specifically, we conjecture that the hyperparameters of stochastic gradient descent (SGD) used before reaching the break-even point control: (1) the spectral norms of K and H, and (2) the conditioning of K and H. In partic- ular, using a larger learning rate prior to reaching the break-even point reduces the spectral norm of K along the optimization trajectory (see  Fig. 1  for an illustration of this phenomenon). Reducing the spectral norm of K decreases the variance of the mini-batch gradient, which has been linked to improved convergence speed (Johnson & Zhang, 2013). Finally, we apply our analysis to a network with batch normalization (BN) layers and find that our predictions are valid in this case as well. Delving deeper in this line of investigation, we show that using a large learning rate is necessary to reach better-conditioned (relatively to a network without BN layers) regions of the loss surface, which was previously attributed to BN alone (Bjorck et al., 2018; Ghorbani et al., 2019; Page, 2019).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents the Co-occurrence Envelope Hypothesis, which states that by allowing equivariant feature mappings to detect transformations that co-occur in the data and focus learning on the set formed by these co-occurrent transformations, one is able to induce learning of more representative feature representations of the data, and, resultantly, enhance the descriptive power of neural networks utilizing them. This hypothesis is motivated by observations from neuroscience and psychology that suggest the human visual system does not react equally to all transformations encountered in visual data, and that it encodes orientation atypicality relative to the context rather than on an absolute manner.",
        "Abstract": "Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. an upright face with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment and utilizes this information to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to attend to the set of co-occurring transformations in data and generalize this notion to act on groups consisting of multiple symmetries. We show that our proposed co-attentive equivariant neural networks consistently outperform conventional rotation equivariant and rotation & reflection equivariant neural networks on rotated MNIST and CIFAR-10.",
        "Introduction": "  INTRODUCTION Thorough experimentation in the fields of psychology and neuroscience has provided support to the intuition that our visual perception and cognition systems are able to identify familiar objects despite modifications in size, location, background, viewpoint and lighting ( Bruce & Humphreys, 1994 ). Interestingly, we are not just able to recognize such modified objects, but are able to characterize which modifications have been applied to them as well. As an example, when we see a picture of a cat, we are not just able to tell that there is a cat in it, but also its position, its size, facts about the lighting conditions of the picture, and so forth. Such observations suggest that the human visual system is equivariant to a large transformation group containing translation, rotation, scaling, among others. In other words, the mental representation obtained by seeing a transformed version of an object, is equivalent to that of seeing the original object and transforming it mentally next. These fascinating abilities exhibited by biological visual systems have inspired a large field of re- search towards the development of neural architectures able to replicate them. Among these, the most popular and successful approach is the Convolutional Neural Network (CNN) ( LeCun et al., 1989 ), which incorporates equivariance to translation via convolution. Unfortunately, in counterpart to the human visual system, CNNs do not exhibit equivariance to other transformations encountered in visual data (e.g., rotations). Interestingly, however, if an ordinary CNN happens to learn rotated copies of the same filter, the stack of feature maps becomes equivariant to rotations even though individual feature maps are not ( Cohen & Welling, 2016 ). Since ordinary CNNs must learn such rotated copies independently, they effectively utilize an important number of network parameters suboptimally to this end (see  Fig. 3  in  Krizhevsky et al. (2012) ). Based on the idea that equivari- ance in CNNs can be extended to larger transformation groups by stacking convolutional feature maps, several approaches have emerged to extend equivariance to, e.g., planar rotations ( Dieleman et al., 2016 ;  Marcos et al., 2017 ;  Weiler et al., 2018 ;  Li et al., 2018 ), spherical rotations ( Cohen et al., 2018 ;  Worrall & Brostow, 2018 ;  Cohen et al., 2019 ), scaling ( Marcos et al., 2018 ;  Worrall & Welling, 2019 ) and general transformation groups ( Cohen & Welling, 2016 ), such that transformed copies of a single entity are not required to be learned independently. Although incorporating equivariance to arbitrary transformation groups is conceptually and theo- retically similar 1 , evidence from real-world experiences motivating their integration might strongly differ. Several studies in neuroscience and psychology have shown that our visual system does not react equally to all transformations we encounter in visual data. Take, for instance, translation and rotation. Although we easily recognize objects independently of their position of appearance, a large corpus of experimental research has shown that this is not always the case for in-plane rotations.  Yin (1969)  showed that mono-oriented objects, i.e., complex objects such as faces which are customarily seen in one orientation, are much more difficult to be accurately recognized when presented upside- down. This behaviour has been reproduced, among others, for magazine covers ( Dallett et al., 1968 ), symbols ( Henle, 1942 ) and even familiar faces (e.g., from classmates) ( Brooks & Goldstein, 1963 ).  Intriguingly, Schwarzer (2000)  found that this effect exacerbates with age (adults suffer from this effect much more than children), but, adults are much faster and accurate in detecting mono-oriented objects in usual orientations. Based on these studies, we draw the following conclusions: • The human visual system does not perform (fully) equivariant feature transformations to visual data. Consequently, it does not react equally to all possible input transformations encountered in visual data, even if they belong to the same transformation group (e.g., in-plane rotations). • The human visual system does not just encode familiarity to objects but seems to learn through experience the poses in which these objects customarily appear in the environment to assist and improve object recognition ( Freire et al., 2000 ;  Riesenhuber et al., 2004 ;  Sinha et al., 2006 ). Complementary studies ( Tarr & Pinker, 1989 ;  Oliva & Torralba, 2007 ) suggest that our visual system encodes orientation atypicality relative to the context rather than on an absolute manner ( Fig. 1 ). Motivated by the aforementioned observations we state the co-occurrence envelope hypothesis: The Co-occurrence Envelope Hypothesis. By allowing equivariant feature mappings to detect transformations that co-occur in the data and focus learning on the set formed by these co-occurrent transformations (i.e., the co-occurrence envelope of the data), one is able to induce learning of more representative feature representations of the data, and, resultantly, enhance the descriptive power of neural networks utilizing them. We refer to one such feature mapping as co-attentive equivariant.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the impact of the various mechanisms used in deep reinforcement learning algorithms on agent behavior. It is found that existing deep RL methods are brittle, hard to reproduce, unreliable across runs, and sometimes outperformed by simple baselines. This suggests that a re-evaluation of the inner workings of these algorithms is necessary in order to better understand how the parts of the algorithms impact agent training.",
        "Abstract": "We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains in deep reinforcement learning.\n",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (RL) algorithms have fueled many of the most publicized achievements in modern machine learning ( Silver et al., 2017 ;  OpenAI, 2018 ;  Abbeel & Schulman, 2016 ;  Mnih et al., 2013 ). However, despite these accomplishments, deep RL methods still are not nearly as reliable as their (deep) supervised learning counterparts. Indeed, recent research found the existing deep RL methods to be brittle ( Henderson et al., 2017 ;  Zhang et al., 2018 ), hard to reproduce ( Hen- derson et al., 2017 ;  Tucker et al., 2018 ), unreliable across runs ( Henderson et al., 2017 ; 2018), and sometimes outperformed by simple baselines ( Mania et al., 2018 ). The prevalence of these issues points to a broader problem: we do not understand how the parts comprising deep RL algorithms impact agent training, either separately or as a whole. This unsat- isfactory understanding suggests that we should re-evaluate the inner workings of our algorithms. Indeed, the overall question motivating our work is: how do the multitude of mechanisms used in deep RL training algorithms impact agent behavior?",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to deep learning for tabular data problems. We propose a universal DNN architecture that is shown to consistently outperform the state-of-the-art shallow models by a notable margin. We also provide evidence that our approach is applicable to a wide range of tabular data problems, including those from Kaggle ML competitions.",
        "Abstract": "Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",
        "Introduction": "  INTRODUCTION The recent rise of deep neural networks (DNN) resulted in a substantial breakthrough for a large number of machine learning tasks in computer vision, natural language processing, speech recogni- tion, reinforcement learning ( Goodfellow et al., 2016 ). Both gradient-based optimization via back- propagation ( Rumelhart et al., 1985 ) and hierarchical representation learning appear to be crucial in increasing the performance of machine learning for these problems by a large margin. While the superiority of deep architectures in these domains is undoubtful, machine learning for tabular data still did not fully benefit from the DNN power. Namely, the state-of-the-art perfor- mance in problems with tabular heterogeneous data is often achieved by \"shallow\" models, such as gradient boosted decision trees (GBDT) ( Friedman, 2001 ;  Chen & Guestrin, 2016 ;  Ke et al., 2017 ;  Prokhorenkova et al., 2018 ). While the importance of deep learning on tabular data is recognized by the ML community, and many works address this problem ( Zhou & Feng, 2017 ;  Yang et al., 2018 ;  Miller et al., 2017 ;  Lay et al., 2018 ;  Feng et al., 2018 ;  Ke et al., 2018 ), the proposed DNN approaches do not consistently outperform the state-of-the-art shallow models by a notable margin. In particular, to the best of our knowledge, there is still no universal DNN approach that was shown to systematically outperform the leading GBDT packages (e.g., XGBoost ( Chen & Guestrin, 2016 )). As additional evidence, a large number of Kaggle ML competitions with tabular data are still won by the shallow GBDT methods ( Harasymiv, 2015 ). Overall, at the moment, there is no dominant deep learning solution for tabular data problems, and we aim to reduce this gap by our paper.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores the potential of pure deep learning (DL) architectures for time series (TS) forecasting, and investigates the possibility of designing interpretable DL architectures to extract explainable driving factors that combine to produce a given forecast. It challenges the conclusion that hybrid approaches and combinations of methods are the way forward for improving forecasting accuracy and making forecasting more valuable.",
        "Abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.",
        "Introduction": "  INTRODUCTION Time series (TS) forecasting is an important business problem and a fruitful application area for machine learning (ML). It underlies most aspects of modern business, including such critical areas as inventory control and customer management, as well as business planning going from production and distribution to finance and marketing. As such, it has a considerable financial impact, often ranging in the millions of dollars for every point of forecasting accuracy gained ( Jain, 2017 ;  Kahn, 2003 ). And yet, unlike areas such as computer vision or natural language processing where deep learning (DL) techniques are now well entrenched, there still exists evidence that ML and DL struggle to outperform classical statistical TS forecasting approaches ( Makridakis et al., 2018a ; b ). For instance, the rankings of the six \"pure\" ML methods submitted to M4 competition were 23, 37, 38, 48, 54, and 57 out of a total of 60 entries, and most of the best-ranking methods were ensembles of classical statistical techniques ( Makridakis et al., 2018b ). On the other hand, the M4 competition winner ( Smyl, 2020 ), was based on a hybrid between neural residual/attention dilated LSTM stack with a classical Holt-Winters statistical model ( Holt, 1957 ; 2004;  Winters, 1960 ) with learnable parameters. Since Smyl's approach heavily depends on this Holt-Winters component,  Makridakis et al. (2018b)  further argue that \"hybrid approaches and combinations of method are the way forward for improving the forecasting accuracy and making forecasting more valuable\". In this work we aspire to challenge this conclusion by exploring the potential of pure DL architectures in the context of the TS forecasting. Moreover, in the context of interpretable DL architecture design, we are interested in answering the following question: can we Published as a conference paper at ICLR 2020 inject a suitable inductive bias in the model to make its internal operations more interpretable, in the sense of extracting some explainable driving factors combining to produce a given forecast?",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper explores the theoretical analysis of the generalization of deep learning models, with a focus on networks with convolutional layers. It is shown that small initial weights may promote vanishing gradients, and that accurate models can be found by traveling a short distance in parameter space. Furthermore, it is argued that the weight-tying employed in the convolutional layer constrains the set of functions computed by the layer, which should aid generalization.",
        "Abstract": "We prove bounds on the generalization error of convolutional networks.\nThe bounds are in terms of the training loss, the number of\nparameters, the Lipschitz constant of the loss and the distance from\nthe weights to the initial weights.  They are independent of the\nnumber of pixels in the input, and the height and width of hidden\nfeature maps.\nWe present experiments using CIFAR-10 with varying\nhyperparameters of a deep convolutional network, comparing our bounds\nwith practical generalization gaps.",
        "Introduction": "  INTRODUCTION Recently, substantial progress has been made regarding theoretical analysis of the generalization of deep learning models (see  Neyshabur et al., 2015 ;  Zhang et al., 2016 ;  Dziugaite and Roy, 2017 ;  Bartlett et al., 2017 ;  Neyshabur et al., 2017 ; 2018;  Arora et al., 2018 ;  Golowich et al., 2018 ;  Neyshabur et al., 2019 ;  Wei and Ma, 2019a ;  Cao and Gu, 2019 ;  Daniely and Granot, 2019 ). One interesting point that has been explored, with roots in ( Bartlett, 1998 ), is that even if there are many parameters, the set of models that can be represented using weights with small magnitude is limited enough to provide leverage for induction ( Neyshabur et al., 2015 ;  Bartlett et al., 2017 ;  Neyshabur et al., 2018 ). Intuitively, if the weights start small, since the most popular training algorithms make small, incremental updates that get smaller as the training accuracy improves, there is a tendency for these algorithms to produce small weights. (For some deeper theoretical exploration of implicit bias in deep learning and related settings, see ( Gunasekar et al., 2017 ;  2018a ; b ;  Ma et al., 2018 ).) Even more recently, authors have proved generalization bounds in terms of the distance from the initial setting of the weights instead of the size of the weights ( Dziugaite and Roy, 2017 ;  Bartlett et al., 2017 ;  Neyshabur et al., 2019 ; Nagarajan and Kolter, 2019). This is important because small initial weights may promote vanishing gradients; it is advisable instead to choose initial weights that maintain a strong but non-exploding signal as computation flows through the network (see  LeCun et al., 2012 ;  Glorot and Bengio, 2010 ;  Saxe et al., 2013 ;  He et al., 2015 ). A number of recent theoretical analyses have shown that, for a large network initialized in this way, accurate models can be found by traveling a short distance in parameter space (see  Du et al., 2019b ;a; Allen-Zhu et al., 2019;  Zou et al., 2018 ;  Lee et al., 2019 ). Thus, the distance from initialization may be expected to be significantly smaller than the magnitude of the weights. Furthermore, there is theoretical reason to expect that, as the number of parameters increases, the distance from initialization decreases. This motivates generalization bounds in terms of distance from initialization ( Dziugaite and Roy, 2017 ;  Bartlett et al., 2017 ). Convolutional layers are used in all competitive deep neural network architectures applied to image processing tasks. The most influential generalization analyses in terms of distance from initialization have thus far concentrated on networks with fully connected layers. Since a convolutional layer has an alternative representation as a fully connected layer, these analyses apply in the case of convolutional networks, but, intuitively, the weight-tying employed in the convolutional layer constrains the set of functions computed by the layer. This additional restriction should be expected to aid generalization.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper explores the neural underpinnings of flexible behavior in mammals and how it can be applied to artificial motor control systems. Recent efforts at the interface of neuroscience and machine learning have sparked renewed interest in constructive approaches in which artificial models that solve tasks similar to those solved by animals serve as normative models of biological intelligence. This paper examines how neural control approaches have been applied to the study of reaching movements, and how they can be used to model the interactions between animals and their environments.",
        "Abstract": "Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience.",
        "Introduction": "  INTRODUCTION Animals have nervous systems that allow them to coordinate their movement and perform a diverse set of complex behaviors. Mammals, in particular, are generalists in that they use the same general neural network to solve a wide variety of tasks. This flexibility in adapting behaviors towards many different goals far surpasses that of robots or artificial motor control systems. Hence, studies of the neural underpinnings of flexible behavior in mammals could yield important insights into the classes of algorithms capable of complex control across contexts and inspire algorithms for flexible control in artificial systems (Merel et al., 2019b). Recent efforts at the interface of neuroscience and machine learning have sparked renewed interest in constructive approaches in which artificial models that solve tasks similar to those solved by animals serve as normative models of biological intelligence. Researchers have attempted to leverage these models to gain insights into the functional transformations implemented by neurobiological circuits, prominently in vision (Khaligh-Razavi & Kriegeskorte, 2014; Yamins et al., 2014; Kar et al., 2019), but also increasingly in other areas, including audition (Kell et al., 2018) and navigation (Banino et al., 2018; Cueva & Wei, 2018). Efforts to construct models of biological locomotion systems have informed our understanding of the mechanisms and evolutionary history of bodies and behavior (Grillner et al., 2007; Ijspeert et al., 2007; Ramdya et al., 2017; Nyakatura et al., 2019). Neural control approaches have also been applied to the study of reaching movements, though often in constrained behavioral paradigms (Lillicrap & Scott, 2013), where supervised training is possible (Sussillo et al., 2015; Michaels et al., 2019). While these approaches model parts of the interactions between animals and their environments (Chiel & Beer, 1997), none attempt to capture the full complexity of embodied control, involving how an animal uses its senses, body and behaviors to solve challenges in a physical environment. Equal contribution.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a method for detecting under-sensitivity in natural language processing models, which is a vulnerability that allows models to make confident predictions without understanding the task-relevant textual comprehension skills. The method is designed to formally verify the under-sensitivity specification that a model should not become more confident as arbitrary subsets of input words are deleted. The paper also discusses prior work in this area and how it can be used to identify reduced inputs.",
        "Abstract": "Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently prove, given a model, whether a particular sample is free from the under-sensitivity problem. We compare different training methods to address under-sensitivity, and compare metrics to measure it. In our experiments on the SNLI and MNLI datasets, we observe that IBP training leads to a significantly improved verified accuracy. On the SNLI test set, we can verify 18.4% of samples, a substantial improvement over only 2.8% using standard training.",
        "Introduction": "  INTRODUCTION Natural language processing (NLP) widely relies on neural networks, a model class known to be vulnerable to adversarial input perturbations ( Szegedy et al., 2013 ;  Kurakin et al., 2016 ). Adversarial samples typically expose over-sensitivity to semantically invariant text transformations ( Belinkov & Bisk, 2017 ;  Ettinger et al., 2017 ), e.g. character flips ( Ebrahimi et al., 2018 ) or paraphrases ( Ribeiro et al., 2018b ;  Iyyer et al., 2018 ).  Feng et al. (2018)  exposed another type of problematic behaviour: deleting large parts of input text can cause a model's confidence to increase;  Figure 1  shows an example. That is, reduced sets of input words can suffice to trigger more confident predictions. Such under-sensitivity is problematic: neural models can 'solve' NLP tasks without task-relevant textual comprehension skills, but instead fit spurious cues in the data that suffice to form correct predictions. Models might then achieve strong nominal test accuracy on data of the same (biased) distribution as the training set, by exploiting predictive shortcuts that are not representative of the given NLP task at hand. Consequently, they fail drastically when evaluated on samples without these spurious cues ( Jia & Liang, 2017 ;  Poliak et al., 2018 ;  Gururangan et al., 2018 ;  Niven & Kao, 2019 ). A major issue with identifying reduced inputs is the combinatorially large space of arbitrary text deletions; this can only be searched exhaustively for short sequences. Prior work has considered heuristics like beam search ( Feng et al., 2018 ) or bandits ( Ribeiro et al., 2018a ), but these are gener- ally not guaranteed to find the worst-case reductions. In this work, we address the under-sensitivity issue by designing and formally verifying the under- sensitivity specification that a model should not become more confident as arbitrary subsets of input words are deleted. 1 Under-sensitivity behaviour is not reflected in nominal accuracy, but one can Published as a conference paper at ICLR 2020",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to inferring pairwise Granger causal relationships between a set of stochastic processes from their time series measurements. The approach uses recurrent neural networks to model the time series measurements, allowing for the detection of nonlinearities in the data. This method is compared to existing linear model-based Granger causality tests, which can fail catastrophically in the presence of even mild nonlinearities.",
        "Abstract": "Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes’ time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality. ",
        "Introduction": "  INTRODUCTION The physical mechanisms behind the functioning of any large-scale system can be understood in terms of the networked interactions between the underlying system processes. Granger causality is one widely-accepted criterion used in building network models of interactions between large en- sembles of stochastic processes. While Granger causality may not necessarily imply true causality, it has proven effective in qualifying pairwise interactions between stochastic processes in a variety of system identification problems, e.g., gene regulatory network mapping ( Fujita et al. (2007) ), and the mapping of human brain connectome ( Seth et al. (2015) ). This perspective has given rise to the canonical problem of inferring pairwise Granger causal relationships between a set of stochastic processes from their time series measurements. At present, the vast majority of Granger causal in- ference methods adopt a model-based inference approach whereby the measured time series data is modeled using with a suitable parameterized data generative model whose inferred parameters ul- timately reveal the true topology of pairwise Granger causal relationships. Such methods typically rely on using linear regression models for inference. However, as illustrated in the classical bivariate example by  Baek & Brock (1992) , linear model-based Granger causality tests can fail catastrophi- cally in the presence of even mild nonlinearities in the measurements, thus making a strong case for our work which tackles the nonlinearities in the measurements by exploring new generative models of the time series measurements based on recurrent neural networks.",
        "label": 1
    },
    {
        "Summary": "\nAbstract: This paper proposes a simple yet effective method for unsupervised model selection for the class of current state-of-the-art VAE-based unsupervised disentangled representation learning methods. The proposed Unsupervised Disentanglement Ranking (UDR) leverages theoretical results to quantify the quality of disentanglement by performing pairwise comparisons between trained model representations. UDR is evaluated against four existing supervised alternatives on two datasets with fully known generative processes and one dataset with partial ground truth attribute labels. Results show that UDR is able to match the supervised baselines in terms of guiding a hyperparameter search and picking the most disentangled trained models both quantitatively and qualitatively. UDR is also validated to correlate well with the final task performance on two reported tasks.",
        "Abstract": "Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",
        "Introduction": "  INTRODUCTION structure of the world. For example, to describe an object we often use words pertaining to its colour, position, shape and size. We can use different words to describe these properties because they relate to independent factors of variation in our world, i.e. properties which can be compositionally recombined. Hence a disentangled representation of objects should reflect this by factorising into dimensions which correspond to those properties ( Bengio et al., 2013 ;  Higgins et al., 2018a ). The ability to automatically discover the compositional factors of complex real datasets can be of great importance in many practical applications of machine learning and data science. However, it is important to be able to learn such representations in an unsupervised manner, since most interesting datasets do not have their generative factors fully labelled. For a long time scalable unsupervised disentangled representation learning was impossible, until recently a new class of models based on Variational Autoencoders (VAEs) ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ) was developed. These approaches ( Higgins et al., 2017a ; Burgess et al., 2017;  Chen et al., 2018 ;  Kumar et al., 2017 ;  Kim & Mnih, 2018 ) scale reasonably well and are the current state of the art in unsupervised disentangled representation learning. However, so far the benefits of these techniques have not been widely exploited because of two major shortcomings: First, the quality of the achieved disentangling is sensitive to the choice of hyperparameters, however, model selection is currently impossible without having access to the ground truth generative process and/or attribute labels, which are required by all the currently existing disentanglement metrics ( Higgins et al., 2017a ;  Kim & Mnih, 2018 ;  Chen et al., 2018 ;  Eastwood & Williams, 2018 ;  Ridgeway & Mozer, 2018 ). Second, even if one could apply any of the existing disentanglement metrics for model selection, the scores produced by these metrics can vary a lot even for models with the same hyperparameters and trained on the same data ( Locatello et al., 2018 ). While a lot of this variance is explained by the actual quality of the learnt representations, some of it is introduced by the metrics themselves. In particular, all of the existing supervised disentanglement metrics assume a single \"canonical\" factorisation of the generative factors, any deviation from which is penalised. Such a \"canonical\" factorisation, however, is not chosen in a principled manner. Indeed, for the majority of datasets, apart from the simplest ones, multiple equally valid disentangled representa- tions may be possible (see  Higgins et al. (2018a)  for a discussion). For example, the intuitive way that humans reason about colour is in terms of hue and saturation. However, colour may also be represented in RGB, YUV, HSV, HSL, CIELAB. Any of the above representations are as valid as each other, yet only one of them is allowed to be \"canonical\" by the supervised metrics. Hence, a model that learns to represent colour in a subspace aligned with HSV will be penalised by a supervised metric which assumes that the canonical disentangled representation of colour should be in RGB. This is despite the fact that both representations are equal in terms of preserving the compositional property at the core of what makes disentangled representations useful ( Higgins et al., 2018a ). Hence, the field finds itself in a predicament. From one point of view, there exists a set of approaches capable of reasonably scalable unsupervised disentangled representation learning. On the other hand, these models are hard to use in practice, because there is no easy way to do a hyperparameter search and model selection. This paper attempts to bridge this gap. We propose a simple yet effective method for unsupervised model selection for the class of current state-of-the-art VAE-based unsupervised disentangled representation learning methods. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages the recent Published as a conference paper at ICLR 2020 theoretical results that explain why variational autoencoders disentangle (Rolinek et al., 2019), to quan- tify the quality of disentanglement by performing pairwise comparisons between trained model repre- sentations. We evaluate the validity of our unsupervised model selection metric against the four best ex- isting supervised alternatives reported in the large scale study by  Locatello et al. (2018) : the β-VAE met- ric ( Higgins et al., 2017a ), the FactorVAE metric ( Kim & Mnih, 2018 ), Mutual Information Gap (MIG) ( Chen et al., 2018 ) and DCI Disentanglement scores ( Eastwood & Williams, 2018 ). We do so for all existing state of the art disentangled representation learning approaches: β-VAE ( Higgins et al., 2017a ), CCI-VAE (Burgess et al., 2017), FactorVAE ( Kim & Mnih, 2018 ), TC-VAE ( Chen et al., 2018 ) and two versions of DIP-VAE ( Kumar et al., 2017 ). We validate our proposed method on two datasets with fully known generative processes commonly used to evaluate the quality of disentangled representations: dSprites ( Matthey et al., 2017 ) and 3D Shapes ( Burgess & Kim, 2018 ), and show that our unsupervised model selection method is able to match the supervised baselines in terms of guiding a hyperparameter search and picking the most disentangled trained models both quantitatively and qualitatively. We also apply our approach to the 3D Cars dataset (Reed et al., 2014), where the full set of ground truth attribute labels is not available, and confirm through visual inspection that the ranking produced by our method is meaningful ( Fig. 1 ). Overall we evaluate 6 different model classes, with 6 separate hyperparameter settings and 50 seeds on 3 separate datasets, totalling 5400 models and show that our method is both ac- curate and consistent across models and datasets. Finally, we validate that the model ranking produced by our approach correlates well with the final task performance on two recently reported tasks: a classifi- cation fairness task (Locatello et al., 2019) and a model-based reinforcement learning (RL) task ( Watters et al., 2019 ). Indeed, on the former our approach outperformed the reported supervised baseline scores.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the phenomenon of out-of-distribution (OOD) detection using generative models, and proposes a widely-applicable OOD score for individual inputs that leverages estimates of complexity. Experiments are conducted on an extensive collection of alternatives, including a pool of 12 data sets, two conceptually-different generative models, increasing model sizes, and three variants of complexity estimates. Results show that the proposed score turns likelihood-based generative models into practical and effective OOD detectors, with performances comparable to, or even better than the state-of-the-art.",
        "Abstract": "Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to perform comparably to, or even better than, existing OOD detection approaches under a wide range of data sets, models, model sizes, and complexity estimates.",
        "Introduction": "  INTRODUCTION Assessing whether input data is novel or significantly different than the one used in training is critical for real-world machine learning applications. Such data are known as out-of-distribution (OOD) inputs, and detecting them should facilitate safe and reliable model operation. This is particularly necessary for deep neural network classifiers, which can be easily fooled by OOD data ( Nguyen et al., 2015 ). Several approaches have been proposed for OOD detection on top of or within a neural network classifier (Hendrycks & Gimpel, 2017;  Lakshminarayanan et al., 2017 ;  Liang et al., 2018 ;  Lee et al., 2018 ). Nonetheless, OOD detection is not limited to classification tasks nor to labeled data sets. Two examples of that are novelty detection from an unlabeled data set and next-frame prediction from video sequences. A rather obvious strategy to perform OOD detection in the absence of labels (and even in the pres- ence of them) is to learn a density model M that approximates the true distribution p * (X ) of training inputs x ∈ X ( Bishop, 1994 ). Then, if such approximation is good enough, that is, p(x|M) ≈ p * (x), OOD inputs should yield a low likelihood under model M. With complex data like audio or images, this strategy was long thought to be unattainable due to the difficulty of learning a sufficiently good model. However, with current approaches, we start having generative models that are able to learn good approximations of the density conveyed by those complex data. Autoregressive and invertible models such as PixelCNN++ ( Salimans et al., 2017 ) and Glow ( Kingma & Dhariwal, 2018 ) perform well in this regard and, in addition, can approximate p(x|M) with arbitrary accuracy. Recent works, however, have shown that likelihoods derived from generative models fail to distin- guish between training data and some OOD input types ( Choi et al., 2018 ;  Nalisnick et al., 2019a ;  Hendrycks et al., 2019 ). This occurs for different likelihood-based generative models, and even when inputs are unrelated to training data or have totally different semantics. For instance, when Published as a conference paper at ICLR 2020 trained on CIFAR10, generative models report higher likelihoods for SVHN than for CIFAR10 itself ( Fig. 1 ; data descriptions are available in Appendix A). Intriguingly, this behavior is not consistent across data sets, as other ones correctly tend to produce likelihoods lower than the ones of the train- ing data (see the example of TrafficSign in  Fig. 1 ). A number of explanations have been suggested for the root cause of this behavior ( Choi et al., 2018 ;  Nalisnick et al., 2019a ;  Ren et al., 2019 ) but, to date, a full understanding of the phenomenon remains elusive. In this paper, we shed light to the above phenomenon, showing that likelihoods computed from generative models exhibit a strong bias towards the complexity of the corresponding inputs. We find that qualitatively complex images tend to produce the lowest likelihoods, and that simple images always yield the highest ones. In fact, we show a clear negative correlation between quantitative estimates of complexity and the likelihood of generative models. In the second part of the paper, we propose to leverage such estimates of complexity to detect OOD inputs. To do so, we introduce a widely-applicable OOD score for individual inputs that corresponds, conceptually, to a likelihood- ratio test statistic. We show that such score turns likelihood-based generative models into practical and effective OOD detectors, with performances comparable to, or even better than the state-of-the- art. We base our experiments on an extensive collection of alternatives, including a pool of 12 data sets, two conceptually-different generative models, increasing model sizes, and three variants of complexity estimates.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new optimization algorithm that uses gradient sparsity to reduce the query complexity of variance reduction methods. The algorithm, which is based on the random-top-k operator, is a composition of the randomized coordinate descent operator and the top-k operator. Theoretical analyses are provided to show that the algorithm is never worse than existing methods and can strictly outperform them when the random-top-k operator captures gradient sparsity. Experiments on various tasks including image classification, natural language processing, and sparse matrix factorization demonstrate the improvements in computation.",
        "Abstract": "Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.",
        "Introduction": "  INTRODUCTION Optimization tools for machine learning applications seek to minimize the finite sum objective min x∈R d f (x) 1 n n i=1 f i (x), (1) where x is a vector of parameters, and f i : R d → R is the loss associated with sample i. Batch SGD serves as the prototype for modern stochastic gradient methods. It updates the iterate x with x − η∇f I (x), where η is the learning rate and ∇f I (x) is the batch stochastic gradient, i.e. The batch size |I| in batch SGD directly impacts the stochastic variance and gradient query com- plexity of each iteration of the update rule. In recent years, variance reduction techniques have been proposed by carefully blending large and small batch gradients (e.g.  Roux et al., 2012 ;  Johnson & Zhang, 2013 ;  Defazio et al., 2014 ;  Xiao & Zhang, 2014 ;  Allen-Zhu & Yuan, 2016 ;  Allen-Zhu & Hazan, 2016 ;  Reddi et al., 2016a ;b;  Allen-Zhu, 2017 ;  Lei & Jordan, 2017 ;  Lei et al., 2017 ;  Allen-Zhu, 2018b ;  Fang et al., 2018 ;  Zhou et al., 2018 ;  Wang et al., 2018 ;  Pham et al., 2019 ;  Nguyen et al., 2019 ;  Lei & Jordan, 2019 ). They are alterna- tives to batch SGD and are provably better than SGD in various settings. While these methods allow for greater learning rates than batch SGD and have appealing theoretical guarantees, they require a per-iteration query complexity which is more than double than that of batch SGD.  Defazio (2019)  questions the utility of variance reduction techniques in modern machine learning problems, empir- ically identifying query complexity as one issue. In this paper, we show that gradient sparsity ( Aji & Heafield, 2017 ) can be used to significantly reduce the query complexity of variance reduction methods. Our work is motivated by the observation that gradients tend to be \"sparse,\" having only Published as a conference paper at ICLR 2020 a small fraction of large coordinates. Specifically, if the indices of large gradient coordinates (mea- sured in absolute value) are known before updating model parameters, we compute the derivative of only those coordinates while setting the remaining gradient coordinates to zero. In principle, if spar- sity is exhibited, using large gradient coordinates will not effect performance and will significantly reduce the number of operations required to update model parameters. Nevertheless, this heuristic alone has three issues: (1) bias is introduced by setting other entries to zero; (2) the locations of large coordinates are typically unknown; (3) accessing a subset of coordinates may not be easily implemented for some problems like deep neural networks. We provide solutions for all three issues. First, we introduce a new sparse gradient operator: The random-top-k operator. The random-top-k operator is a composition of the randomized coordinate descent operator and the top-k operator. In prior work, the top-k operator has been used to reduce the communication complexity of distributed optimization ( Stich et al., 2018 ;  Aji & Heafield, 2017 ) applications. The random-top-k operator has two phases: Given a stochastic gradient and a pair of integers (k 1 , k 2 ) that sum to k, the operator retains k 1 coordinates which are most \"promising\" in terms of their \"likelihood\" to be large on average, then randomly selects k 2 of the remaining coordinates with appropriate rescaling. The first phase captures sparsity patterns while the second phase eliminates bias. Second, we make use of large batch gradients in variance reduction methods to estimate sparsity patterns. Inspired by the use of a memory vector in  Aji & Heafield (2017) , the algorithm maintains a memory vector initialized with the absolute value of the large batch gra- dient at the beginning of each outer loop and updated by taking an exponential moving average over subsequent stochastic gradients. Coordinates with large values in the memory vector are more \"promising,\" and the random-top-k operator will pick the top k 1 coordinate indices based on the memory vector. Since larger batch gradients have lower variance, the initial estimate is quite accu- rate. Finally, for software that supports dynamic computation graphs, we provide a cost-effective way (sparse back-propagation) to implement the random-top-k operator. In this work we apply the random-top-k operator to SpiderBoost ( Wang et al., 2018 ), a recent vari- ance reduction method that achieves optimal query complexity, with a slight modification based on the \"geometrization\" technique introduced by  Lei & Jordan (2019) . Theoretically, we show that our algorithm is never worse than SpiderBoost and can strictly outperform it when the random-top-k operator captures gradient sparsity. Empirically, we demonstrate the improvements in computa- tion for various tasks including image classification, natural language processing, and sparse matrix factorization. The rest of the paper is organized as follows. In Section 2, we define the random-top-k operator, our optimization algorithm, and a description of sparse backpropagation. The theoretical analyses are presented in Section 3, followed by experimental results in Section 4. All technical proofs are relegated to Appendix A, and additional experimental details can be found in Appendix B.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a deep network-based approach to associative memory, which addresses the problem of storing and retrieving a set of patterns based on a partially known or distorted version. The proposed model uses a deep network's weights to store and retrieve data, and is capable of modelling higher-order dependencies that exist in real-world data. The paper discusses the limitations of existing Hopfield memory models, and provides a more efficient recipe for processing high-dimensional inputs by modelling a hierarchy of signals with restricted or local dependencies.",
        "Abstract": "We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. \nIn such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. \nIn general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast.\nThus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing.\nWe present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. \nWe demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.",
        "Introduction": "  INTRODUCTION Associative memory has long been of interest to neuroscience and machine learning communities ( Willshaw et al., 1969 ;  Hopfield, 1982 ;  Kanerva, 1988 ). This interest has generated many proposals for associative memory models, both biological and synthetic. These models address the problem of storing a set of patterns in such a way that a stored pattern can be retrieved based on a partially known or distorted version. This kind of retrieval from memory is known as auto-association. Due to the generality of associative retrieval, successful implementations of associative memory models have the potential to impact many applications. Attractor networks provide one well-grounded foundation for associative memory models ( Amit & Amit, 1992 ). Patterns are stored in such a way that they become attractors of the update dynamics defined by the network. Then, if a query pattern that preserves sufficient information for association lies in the basin of attraction for the original stored pattern, a trajectory initialized by the query will converge to the stored pattern. A variety of implementations of the general attractor principle have been proposed. The classical Hopfield network ( Hopfield, 1982 ), for example, defines a simple quadratic energy function whose parameters serve as a memory. The update dynamics in Hopfield networks iteratively minimize the energy by changing elements of the pattern until it converges to a minimum, typically corresponding to one of the stored patterns. The goal of the writing process is to find parameter values such that the stored patterns become attractors for the optimization process and such that, ideally, no spurious attractors are created. Many different learning rules have been proposed for Hopfield energy models, and the simplicity of the model affords compelling closed-form analysis ( Storkey & Valabregue, 1999 ). At the same time, Hopfield memory models have fundamental limitations: (1) It is not possible to add capacity for more stored patterns by increasing the number of parameters since the number of parameters in a Hopfield network is quadratic in the dimensionality of the patterns. (2) The model lacks a means of modelling the higher-order dependencies that exist in real-world data. In domains such as natural images, the potentially large dimensionality of an input makes it both ineffective and often unnecessary to model global dependencies among raw input measurements. In fact, many auto-correlations that exist in real-world perceptual data can be efficiently compressed without significant sacrifice of fidelity using either algorithmic ( Wallace, 1992 ;  Candes & Tao, 2004 ) or machine learning tools ( Gregor et al., 2016 ;  Toderici et al., 2017 ). The success of existing deep learning techniques suggests a more efficient recipe for processing high-dimensional inputs by modelling a hierarchy of signals with restricted or local dependencies ( LeCun et al., 1995 ). In this paper we use a similar idea for building an associative memory: use a deep network's weights to store and retrieve data.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel hypothesis that language compositionality can be understood as a form of group-equivariance. To support this hypothesis, the authors provide tools to construct sequence-to-sequence models equivariant when the group symmetries are known, and demonstrate that these models solve all Simplified version of the CommAI Navigation (SCAN) tasks, except length generalization.",
        "Abstract": "Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of existing models under the lens of equivariance, and demonstrate that our equivariant architecture is able to achieve the type compositional generalization required in human language understanding.",
        "Introduction": "  INTRODUCTION When using language, humans recombine known concepts to understand novel sentences. For instance, if one understands the meaning of \"run\", \"jump\", and \"jump twice\", then one understands the meaning of \"run twice\", even if such sentence was never heard before. This relies on the notion of language compositionality, which states that the meaning of a sentence (\"jump twice\") is to be obtained by the meaning of its constituents (e.g. the verb \"jump\" and the quantifying adverb \"twice\") and the use of algebraic computation (a verb combined with a quantifying adverb m results in doing that verb m times) ( Kratzer & Heim, 1998 ). In the realm of machines, deep learning has achieved unprecedented results in language modeling tasks ( Bahdanau et al., 2015 ;  Vaswani et al., 2017 ). However, these models are sample inefficient, and do not generalize to examples that require the use of language compositionality ( Lake & Baroni, 2018 ;  Loula et al., 2018 ;  Dessì & Baroni, 2019 ). This result suggests that deep language models fail to leverage compositionality; a failure remaining to this day a roadblock towards true natural language understanding. Focusing on this issue,  Lake & Baroni (2018)  proposed the Simplified version of the CommAI Navigation (SCAN), a dataset to benchmark the compositional generalization capabilities of state-of- the-art sequence-to-sequence (seq2seq) translation models ( Sutskever et al., 2014 ;  Bahdanau et al., 2015 ). In a nutshell, the SCAN dataset contains compositional navigation commands such as JUMP TWICE AFTER RUN LEFT, to be translated into the sequence of actions LTURN RUN JUMP JUMP. Using SCAN,  Lake & Baroni (2018)  demonstrated that seq2seq models fail spectacularly at tasks requiring the use of language compositionality. Following our introductory example, models trained on the three commands JUMP, RUN and JUMP TWICE fail to generalize to RUN TWICE. Most recently,  Dessì & Baroni (2019)  showed that architectures based on temporal convolutions meet the same fate. SCAN did not only reveal the lack of compositionality in language models, but it also became the blueprint to build novel language models able to handle language compositionality. On the one hand,  Russin et al. (2019)  proposed a seq2seq model where semantic and syntactic information are represented separately, in a hope that such disentanglement would elicit compositional rules. However, their model was not able to solve all of the compositional tasks comprising SCAN. On the other hand,  Lake (2019)  introduced a meta-learning approach with excellent performance in multiple Published as a conference paper at ICLR 2020 SCAN tasks. However, their method requires substantial amounts of additional supervision, and a complex meta-learning procedure hand-engineered for each task. In this paper, we take a holistic look at the problem and connect language compositionality in SCAN to the disparate literature in models equivariant to certain group symmetries ( Kondor, 2008 ;  Cohen & Welling, 2016 ;  Ravanbakhsh et al., 2017 ;  Kondor & Trivedi, 2018 ). Interesting links have recently been proposed between group symmetries and the areas of causality ( Arjovsky et al., 2019 ) and disentangled representation learning ( Higgins et al., 2018 ), and this work proceeds in a similar fashion. In particular, the main contribution of this work is not to chase performance numbers, but to put forward the novel hypothesis that language compositionality can be understood as a form of group-equivariance (Section 3). To sustain our hypothesis, we provide tools to construct seq2seq models equivariant when the group symmetries are known (Section 4), and demonstrate that these models solve all SCAN tasks, except length generalization (Section 6).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces V-MPO, an approximate policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) to the on-policy setting. V-MPO relies on a learned state-value function V(s) instead of the state-action value function used in MPO, and constructs a target distribution for the policy update subject to a sample-based KL constraint, then calculates the gradient that partially moves the parameters toward that target, again subject to a KL constraint. This algorithm is compared to policy gradient-based methods such as Proximal Policy Optimization (PPO) and the Importance-Weighted Actor-Learner Architecture (IMPALA) in order to evaluate its performance in challenging environments.",
        "Abstract": "Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (RL) with neural network function approximators has achieved superhu- man performance in several challenging domains ( Mnih et al., 2015 ;  Silver et al., 2016 ; 2018). Some of the most successful recent applications of deep RL to difficult environments such as Dota 2 ( Ope- nAI, 2018a ), Capture the Flag ( Jaderberg et al., 2019 ), Starcraft II ( Vinyals et al., 2019 ), and dexterous object manipulation ( OpenAI, 2018b ) have used policy gradient-based methods such as Proximal Policy Optimization (PPO) ( Schulman et al., 2017 ) and the Importance-Weighted Actor-Learner Architecture (IMPALA) ( Espeholt et al., 2018 ), both in the approximately on-policy setting. Policy gradients, however, can suffer from large variance that may limit performance, especially for high-dimensional action spaces ( Wu et al., 2018 ). In practice, moreover, policy gradient methods typically employ carefully tuned entropy regularization in order to prevent policy collapse. As an alternative to policy gradient-based algorithms, in this work we introduce an approximate policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) ( Abdolmaleki et al., 2018a ;b) to the on-policy setting. The modified algorithm, V-MPO, relies on a learned state-value function V (s) instead of the state-action value function used in MPO. Like MPO, rather than directly updating the parameters in the direction of the policy gradient, V-MPO first constructs a target distribution for the policy update subject to a sample-based KL constraint, then calculates the gradient that partially moves the parameters toward that target, again subject to a KL constraint.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel approach to extract sub-networks from over-parameterized Transformer architectures without a post-hoc pruning process. The method involves randomly dropping model weights during training, making the network robust to subsequent pruning. Experiments on a variety of competitive benchmarks show that this approach can regularize and reduce the training time of very deep networks, while achieving state-of-the-art performance. Additionally, small and efficient models of any depth can be extracted automatically at test time from a single large pre-trained model, without the need for finetuning.",
        "Abstract": "Overparametrized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and  question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation\tand making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our\tapproach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality than when training from scratch or using distillation.",
        "Introduction": "  INTRODUCTION Transformer architectures ( Vaswani et al., 2017 ) have become the dominant architecture in natural language processing, with state-of-the-art performance across a variety of tasks, including machine translation ( Vaswani et al., 2017 ;  Ott et al., 2018 ), language modeling ( Dai et al., 2019 ;  Baevski & Auli, 2018 ) and sentence representation ( Devlin et al., 2018 ;  Yang et al., 2019 ). Each of its lay- ers contains millions of parameters accessed during the forward pass, making it computationally demanding in terms of memory and latency during both training and inference. In an ideal situ- ation, we would be able to extract sub-networks - automatically and without finetuning - from this over-parameterized network, for any given memory or latency constraint, while maintaining good performance. In contrast, standard pruning or distillation methods follow a strategy that often includes a finetuning or retraining step, and the process must be repeated for each desired depth. In this work, we propose a novel approach to extract any sub-network without a post-hoc pruning process from over-parameterized networks. The core of our method is to sample small sub-networks from the larger model during training by randomly dropping model weights as in Dropout ( Hinton et al., 2012 ) or DropConnect ( Wan et al., 2013 ). This has the advantage of making the network robust to subsequent pruning. If well-chosen groups of weights are dropped simultaneously, the resulting small sub-networks can be very efficient. In particular, we drop entire layers to extract shallow models at inference time. Previous work ( Huang et al., 2016 ) has shown that dropping layers during training can regularize and reduce the training time of very deep convolutional networks. In contrast, we focus on pruning. As illustrated in  Figure 1 , an advantage of our layer dropping technique, or LayerDrop, is that from one single deep model, we can extract shallow sub-networks of any desired depth on demand at inference time. We validate our findings on a variety of competitive benchmarks, namely WMT14 English- German for machine translation, WikiText-103 ( Merity et al., 2016 ) for language modeling, CNN- Dailymail ( Hermann et al., 2015 ) for abstractive summarization, ELI5 (Fan et al., 2017) for long form question answering, and several natural language understanding tasks ( Wang et al., 2019a ) for sentence representation. Our approach achieves state of the art on most of these benchmarks as a re- sult of the regularization effect, which stabilizes the training of larger and deeper networks. We also show that we can prune Transformer architectures to much smaller models while maintaining com- petitive performance, outperforming specific model reduction strategies dedicated to BERT ( Devlin et al., 2018 ;  Sanh, 2019 ) as well as training smaller models from scratch. Overall, applying Layer- Drop to Transformer networks provides the following key advantages: • LayerDrop regularizes very deep Transformers and stabilizes their training, leading to state- of-the-art performance across a variety of benchmarks. • Small and efficient models of any depth can be extracted automatically at test time from a single large pre-trained model, without the need for finetuning. • LayerDrop is as simple to implement as dropout.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents the Compressive Transformer, a simple extension to the Transformer which maps past hidden activations to a smaller set of compressed representations. The Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, learning to query both its short-term granular memory and longer-term coarse memory. Results show that the Compressive Transformer improves the modelling of text, achieving state-of-the-art results in character-based language modelling and word-level language modelling. Additionally, the Compressive Transformer can be used as a memory component within an RL agent, and a new book-level language-modelling benchmark is presented.",
        "Abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
        "Introduction": "  INTRODUCTION Humans have a remarkable ability to remember information over long time horizons. When reading a book, we build up a compressed representation of the past narrative, such as the characters and events that have built up the story so far. We can do this even if they are separated by thousands of words from the current text, or long stretches of time between readings. During daily life, we make use of memories at varying time-scales: from locating the car keys, placed in the morning, to recalling the name of an old friend from decades ago. These feats of memorisation are not achieved by storing every sensory glimpse throughout one's lifetime, but via lossy compression. We aggressively select, filter, or integrate input stimuli based on factors of surprise, perceived danger, or repetition - amongst other signals (Richards and Frankland, 2017). Memory systems in artificial neural networks began with very compact representations of the past. Recurrent neural networks (RNNs, Rumelhart et al. (1986)) learn to represent the history of obser- vations in a compressed state vector. The state is compressed because it uses far less space than the history of observations - the model only preserving information that is pertinent to the optimization of the loss. The LSTM (Hochreiter and Schmidhuber, 1997) is perhaps the most ubiquitous RNN variant; it uses learned gates on its state vector to determine what information is stored or forgotten from memory. However since the LSTM, there has been great benefit discovered in not bottlenecking all histori- cal information in the state, but instead in keeping past activations around in an external memory and attending to them. The Transformer (Vaswani et al., 2017) is a sequence model which stores the hidden activation of every time-step, and integrates this information using an attention operator (Bahdanau et al., 2014). The Transformer will thus represent the past with a tensor (depth × mem- ory size × dimension) of past observations that is, in practice, an order of magnitude larger than an LSTM's hidden state. With this granular memory, the Transformer has brought about a step-change in state-of-the-art performance, within machine translation (Vaswani et al., 2017), language mod- elling (Dai et al., 2019; Shoeybi et al., 2019), video captioning (Zhou et al., 2018), and a multitude of language understanding benchmarks (Devlin et al., 2018; Yang et al., 2019) amongst others. One drawback in storing everything is the computational cost of attending to every time-step and the storage cost of preserving this large memory. Several works have focused on reducing the computational cost of attention with sparse access mechanisms (Rae et al., 2016; Child et al., 2019; Sukhbaatar et al., 2019; Lample et al., 2019). However sparse attention does not solve the storage problem, and often requires custom sparse kernels for efficient implementation. Instead we look back to the notion of compactly representing the past. We show this can be built with simple dense linear-algebra components, such as convolutions, and can reduce both the space and compute cost of our models. We propose the Compressive Transformer, a simple extension to the Transformer which maps past hidden activations (memories) to a smaller set of compressed representations (compressed memo- ries). The Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, learning to query both its short-term granular memory and longer-term coarse memory. We observe this improves the modelling of text, achieving state-of-the-art results in character-based language modelling - 0.97 bpc on Enwik8 from the Hutter Prize (Hutter, 2012) - and word-level language modelling - 17.1 perplexity on WikiText-103 (Merity et al., 2016). Specifically, we see the Compressive Transformer improves the modelling of rare words. We show the Compressive Transformer works not only for language, but can also model the waveform of high-frequency speech with a trend of lower likelihood than the TransformerXL and Wavenet (Oord et al., 2016) when trained over 400,000 steps. We also show the Compressive Trans- former can be used as a memory component within an RL agent, IMPALA (Espeholt et al., 2018), and can successfully compress and make use of past observations. Furthermore we present a new book-level language-modelling benchmark PG-19, extracted from texts in Project Gutenberg 1 , to further promote the direction of long-context sequence modelling. This is over double the size of existing LM benchmarks and contains text with much longer contexts.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a novel method, Distribution-based Compositionality Assessment (DBCA), for quantitatively assessing the adequacy of a dataset split for measuring compositional generalization. The paper also presents the Compositional Freebase Questions (CFQ) dataset, which is specifically designed to measure compositional generalization using the DBCA method. The paper then uses the DBCA method to construct a series of experiments for measuring compositionality on CFQ and SCAN and to compare these experiments to other compositionality experiments. Finally, the paper analyzes the performance of three baseline ML architectures on these experiments and shows that these architectures fail to generalize compositionally, and that compound divergence between train and test sets is a good predictor of the test accuracy.",
        "Abstract": "State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings.\n",
        "Introduction": "  INTRODUCTION Human intelligence exhibits systematic compositionality (Fodor & Pylyshyn, 1988), the capacity to understand and produce a potentially infinite number of novel combinations of known components, i.e., to make \"infinite use of finite means\" (Chomsky, 1965). In the context of learning from a set of training examples, we can observe compositionality as compositional generalization, which we take to mean the ability to systematically generalize to composed test examples of a certain distribution after being exposed to the necessary components during training on a different distribution. Humans demonstrate this ability in many different domains, such as natural language understanding (NLU) and visual scene understanding. For example, we can learn the meaning of a new word and then apply it to other language contexts. As Lake & Baroni (2018) put it: \"Once a person learns the meaning of a new verb 'dax', he or she can immediately understand the meaning of 'dax twice' and 'sing and dax'.\" Similarly, we can learn a new object shape and then understand its compositions with previously learned colors or materials (Johnson et al., 2017; Higgins et al., 2018). In contrast, state-of-the-art machine learning (ML) methods often fail to capture the compositional structure that is underlying the problem domain and thus fail to generalize compositionally (Lake & Baroni, 2018; Bastings et al., 2018; Loula et al., 2018; Russin et al., 2019; Johnson et al., 2017). We believe that part of the reason for this shortcoming is a lack of realistic benchmarks that comprehen- sively measure this aspect of learning in realistic scenarios. As others have proposed, compositional generalization can be assessed using a train-test split based on observable properties of the examples that intuitively correlate with their underlying composi- tional structure. Finegan-Dollak et al. (2018), for example, propose to test on different output pat- terns than are in the train set, while Lake & Baroni (2018) propose, among others, to split examples by output length or to test on examples containing primitives that are rarely shown during training. In this paper, we formalize and generalize this intuition and make these contributions: • We introduce distribution-based compositionality assessment (DBCA), which is a novel method to quantitatively assess the adequacy of a particular dataset split for measuring compositional generalization and to construct splits that are ideally suited for this purpose (Section 2). Published as a conference paper at ICLR 2020 • We present the Compositional Freebase Questions (CFQ) 1 , a simple yet realistic and large NLU dataset that is specifically designed to measure compositional generalization using the DBCA method, and we describe how to construct such a dataset (Section 3). • We use the DBCA method to construct a series of experiments for measuring composi- tionality on CFQ and SCAN (Lake & Baroni, 2018) and to quantitatively compare these experiments to other compositionality experiments (Section 4). • We analyze the performance of three baseline ML architectures on these experiments and show that these architectures fail to generalize compositionally, and perhaps more surpris- ingly, that compound divergence between train and test sets is a good predictor of the test accuracy (Section 5).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the use of limited supervision to learn disentangled representations in machine learning. We perform a large-scale experimental study to test the benefits and trade-offs of incorporating limited labeled data for training compared to unsupervised training with supervised model validation. We find that a very small amount of supervision is enough to reliably learn disentangled representations, and that adding a simple supervised loss, using as little as 100 labeled examples, outperforms unsupervised training with supervised model validation both in terms of disentanglement scores and downstream performance. We also discover that both unsupervised training with supervised validation and semi-supervised training are surprisingly robust to label noise and tolerate coarse and partial annotations. Based on our findings, we provide guidelines helpful for practitioners to leverage disentangled representations in practical scenarios.",
        "Abstract": "Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions.  We observe that a small number of labeled examples (0.01--0.5% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.",
        "Introduction": "  INTRODUCTION In machine learning, it is commonly assumed that high-dimensional observations x (such as images) are the manifestation of a low-dimensional latent variable z of ground-truth factors of variation ( Ben- gio et al., 2013 ;  Kulkarni et al., 2015 ;  Chen et al., 2016 ;  Tschannen et al., 2018 ). More specifically, one often assumes that there is a distribution p(z) over these latent variables and that observations in this ground-truth model are generated by sampling z from p(z) first. Then, the observations x are sampled from a conditional distribution p(x|z). The goal of disentanglement learning is to find a representation of the data r(x) which captures all the ground-truth factors of variation in z independently. The hope is that such representations will be interpretable, maximally compact, allow for counterfactual reasoning and be useful for a large variety of downstream tasks ( Bengio et al., 2013 ;  Peters et al., 2017 ;  LeCun et al., 2015 ;  Bengio et al., 2007 ;  Schmidhuber, 1992 ;  Lake et al., 2017 ;  Goodfellow et al., 2009 ;  Lenc & Vedaldi, 2015 ;  Tschannen et al., 2018 ;  Higgins et al., 2018 ;  Suter et al., 2019 ;  Adel et al., 2018 ;  van Steenkiste et al., 2019 ;  Locatello et al., 2019a ;  Gondal et al., 2019 ). As all these applications rely on the assumption that disentangled representations can be reliably learned in practice, we hope to learn them with as little supervision as possible ( Bengio et al., 2013 ;  Schölkopf et al., 2012 ;  Peters et al., 2017 ;  Pearl, 2009 ;  Spirtes et al., 2000 ). Current state-of-the-art unsupervised disentanglement approaches enrich the Variational Autoencoder (VAE) ( Kingma & Welling, 2014 ) objective with different unsupervised regularizers that aim to encourage disentangled representations ( Higgins et al., 2017a ;  Burgess et al., 2018 ;  Kim & Mnih, 2018 ;  Chen et al., 2018 ;  Kumar et al., 2018 ;  Rubenstein et al., 2018 ;  Mathieu et al., 2018 ;  Rolinek et al., 2019 ). The disentanglement of the representation of such methods exhibit a large variance and while some models turn out to be well disentangled it appears hard to identify them without supervi- sion ( Locatello et al., 2019b ). This is consistent with the theoretical result of  Locatello et al. (2019b)  that the unsupervised learning of disentangled representations is impossible without inductive biases. While human inspection can be used to select good model runs and hyperparameters (e.g.  Higgins et al. (2017b , Appendix 5.1)), we argue that such supervision should be made explicit. We hence consider the setting where one has access to annotations (which we call labels in the following) of the latent variables z for a very limited number of observations x, for example through human annotation. Even though this setting is not universally applicable (e.g. when the observations are not human interpretable) and a completely unsupervised approach would be elegant, collecting a small number of human annotations is simple and cheap via crowd-sourcing platforms such as Amazon Mechanical Turk, and is common practice in the development of real-world machine learning systems. As a consequence, the considered setup allows us to explicitly encode prior knowledge and biases into the learned representation via annotation, rather than relying solely on implicit biases such as the choice of network architecture with possibly hard-to-control effects. Other forms of inductive biases such as relying on temporal information (video data) ( Denton & Birodkar, 2017 ;  Yingzhen & Mandt, 2018 ), allowing for interaction with the environment ( Thomas et al., 2017 ), or incorporating grouping information ( Kulkarni et al., 2015 ;  Bouchacourt et al., 2018 ) were discussed in the literature as candidates to circumvent the impossibility result by  Locatello et al. (2019b) . However, there currently seems to be no quantitative evidence that such approaches will lead to improved disentanglement on the metrics and data sets considered in this paper. Furthermore, these approaches come with additional challenges: Incorporating time can significantly increase computational costs (processing video data) and interaction often results in slow training (e.g. a robot arm interacting with the real world). By contrast, collecting a few labels is cheap and fast. We first investigate whether disentanglement scores are sample efficient and robust to imprecise labels. Second, we explore whether it is more beneficial to incorporate the limited amount of labels available into training and thoroughly test the benefits and trade-offs of this approach compared to supervised validation. For this purpose, we perform a reproducible large scale experimental study 1 , training over 52 000 models on four different data sets. We found that unsupervised training with supervised vali- dation enables reliable learning of disentangled representations. On the other hand, using some of the labeled data for training is beneficial both in terms of disentanglement and downstream performance. Overall, we show that a very small amount of supervision is enough to reliably learn disentangled representations as illustrated in  Figure 1 . Our key contributions can be summarized as follows: • We observe that some of the existing disentanglement metrics (which require observations of z) can be used to tune the hyperparameters of unsupervised methods even when only very few labeled examples are available (Section 3). Therefore, training a variety of models and introducing supervision to select the good runs is a viable solution to overcome the impossibility result of  Locatello et al. (2019b) . • We find that adding a simple supervised loss, using as little as 100 labeled examples, outperforms unsupervised training with supervised model validation both in terms of disentanglement scores and downstream performance (Section 4.2). • We discover that both unsupervised training with supervised validation and semi-supervised training are surprisingly robust to label noise (Sections 3.2 and 4.3) and tolerate coarse and partial annotations, the latter being particularly important if not all factors of variation are known. • Based on our findings we provide guidelines helpful for practitioners to leverage disentangled representations in practical scenarios.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to answering compositional questions against a paragraph of text as context, using neural module networks (NMNs). We introduce neural modules to perform reasoning over text using distributed representations, and perform symbolic reasoning, such as arithmetic, sorting, comparisons, and counting. We also introduce an unsupervised objective to provide an inductive bias to perform accurate information extraction from the context, and heuristically-obtained supervision for question programs and outputs for intermediate modules in a program. Experiments on a subset of the DROP dataset show that our model significantly outperforms state-of-the-art black box models.",
        "Abstract": "Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.",
        "Introduction": "  INTRODUCTION Being formalism-free and close to an end-user task, QA is increasingly becoming a proxy for gauging a model's natural language understanding capability ( He et al., 2015 ;  Talmor et al., 2018 ). Recent models have performed well on certain QA datasets, sometimes rivaling humans ( Zhang et al., 2019 ), but it has become increasingly clear that they primarily exploit surface level lexical cues ( Jia & Liang, 2017 ;  Feng et al., 2018 ) and compositional QA still remains a challenge. Answering complex compositional questions against text is challenging since it requires a comprehensive understanding of both the question semantics and the text against which the question needs to be answered. Consider the question in  Figure 1 ; a model needs to understand the compositional reasoning structure of the questions, perform accurate information extraction from the passage (eg. extract lengths, kickers, etc. for the field goals and touchdowns), and perform symbolic reasoning (eg. counting, sorting, etc.). Semantic parsing techniques, which map natural language utterances to executable programs, have been used for compositional question understanding for a long time ( Zelle & Mooney, 1996 ;  Zettle- moyer & Collins, 2005 ;  Liang et al., 2011 ), but have been limited to answering questions against structured and semi-structured knowledge sources. Neural module networks (NMNs;  Andreas et al., 2016 ) extend semantic parsers by making the program executor a learned function composed of neural network modules. These modules are designed to perform basic reasoning tasks and can be composed to perform complex reasoning over unstructured knowledge. NMNs perform well on synthetic visual question answering (VQA) domains such as CLEVR ( Johnson et al., 2017 ) and it is appealing to apply them to answer questions over text due to their interpretable, modular, and inherently compositional nature. We find, however, that it is non-trivial to extend NMNs for answering non-synthetic questions against open-domain text, where a model needs to deal with Published as a conference paper at ICLR 2020 the ambiguity and variability of real-world text while performing a diverse range of reasoning. Jointly learning the parser and executor using only QA supervision is also extremely challenging (§2.2). Our contributions are two-fold: Firstly, we extend NMNs to answer compositional questions against a paragraph of text as context. We introduce neural modules to perform reasoning over text using distributed representations, and perform symbolic reasoning, such as arithmetic, sorting, comparisons, and counting (§3). The modules we define are probabilistic and differentiable, which lets us maintain uncertainty about intermediate decisions and train the entire model via end-to-end differentiability. Secondly, we show that the challenges arising in learning from end-task QA supervision can be alleviated with an auxiliary loss over the intermediate latent decisions of the model. Specifically, we introduce an unsupervised objective that provides an inductive bias to perform accurate information extraction from the context (§4.1). Additionally, we show that providing heuristically-obtained supervision for question programs and outputs for intermediate modules in a program (§4.2) for a small subset of the training data (5-10%) is sufficient for accurate learning. We experiment on 21,800 questions from the recently proposed DROP dataset ( Dua et al., 2019 ) that are heuristically chosen based on their first n-gram such that they are covered by our designed modules. This is a significantly-sized subset that poses a wide variety of reasoning challenges and allows for controlled development and testing of models. We show that our model, which has interpretable intermediate outputs by design, significantly outperforms state-of-the-art black box models on this dataset. We conclude with a discussion of the challenges of pushing NMNs to the entire DROP dataset, where some questions require reasoning that is hard to design modules for.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents an approach to reinforcement learning from demonstrations in hard exploration tasks in partially observable environments with highly variable initial conditions. Our approach combines demonstrations with off-policy, recurrent Q-learning to make efficient use of the available data and vastly outperform behavioral cloning.",
        "Abstract": "This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.",
        "Introduction": "  INTRODUCTION Reinforcement learning from demonstrations has proven to be an effective strategy for attacking problems that require sample efficiency and involve hard exploration. For example,  Aytar et al. (2018) ,  Pohlen et al. (2018)  and  Salimans and Chen (2018b)  have shown that RL with demonstrations can address the hard exploration problem in Montezuma's Revenge.  Večerík et al. (2017) ,  Merel et al. (2017)  and  Paine et al. (2018)  have demonstrated similar results in robotics. Many other works have shown that demonstrations can accelerate learning and address hard-exploration tasks (e.g. see  Hester et al., 2018 ;  Kim et al., 2013 ;  Nair et al., 2018 ;  Kang et al., 2018 ). In this paper, we attack the problem of learning from demonstrations in hard exploration tasks in partially observable environments with highly variable initial conditions. These three aspects together conspire to make learning challenging: 1. Sparse rewards induce a difficult exploration problem, which is a challenge for many state of the art RL methods. An environment has sparse reward when a non-zero reward is only seen after taking a long sequence of correct actions. Our approach is able to solve tasks where standard methods run for billions of steps without seeing a single non-zero reward. 2. Partial observability forces the use of memory, and also reduces the generality of informa- tion provided by a single demonstration, since trajectories cannot be broken into isolated transitions using the Markov property. An environment has partial observability if the agent can only observe a part of the environment at each timestep. 3. Highly variable initial conditions (i.e. changes in the starting configuration of the envi- ronment in each episode) are a big challenge for learning from demonstrations, because the demonstrations can not account for all possible configurations. When the initial conditions are fixed it is possible to be extremely efficient through tracking ( Aytar et al., 2018 ;  Peng et al., 2018 ); however, with a large variety of initial conditions the agent is forced to general- ize over environment configurations not present in demonstrations. Generalizing between different initial conditions is known to be difficult ( Ghosh et al., 2017 ;  Langlois et al., 2019 ;  Zolna et al., 2019 ). Our approach to these problems combines demonstrations with off-policy, recurrent Q-learning in a way that allows us to make very efficient use of the available data. In particular, we vastly outperform behavioral cloning using the same set of demonstrations in all of our experiments.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a defense strategy against DNN model stealing attacks, which involves introducing controlled perturbations to predictions to poison the training objective of the attacker. The defense is evaluated against four recent and effective DNN stealing attack strategies, and is shown to consistently mitigate all stealing attacks with minimal impact to the defender's accuracy. The defense is also shown to reduce the attacker's performance by up to 53% on MNIST and 28% on CUB200, while introducing significantly lesser perturbation than baseline defenses.",
        "Abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.",
        "Introduction": "  INTRODUCTION Effectiveness of state-of-the-art DNN models at a variety of predictive tasks has encouraged their usage in a variety of real-world applications e.g., home assistants, autonomous vehicles, commercial cloud APIs. Models in such applications are valuable intellectual property of their creators, as developing them for commercial use is a product of intense labour and monetary effort. Hence, it is vital to preemptively identify and control threats from an adversarial lens focused at such models. In this work we address model stealing, which involves an adversary attempting to counterfeit the functionality of a target victim ML model by exploiting black-box access (query inputs in, posterior predictions out). Stealing attacks dates back to  Lowd & Meek (2005) , who addressed reverse-engineering linear spam classification models. Recent literature predominantly focus on DNNs (specifically CNN image classifiers), and are shown to be highly effective ( Tramèr et al., 2016 ) on complex models ( Orekondy et al., 2019 ), even without knowledge of the victim's architecture ( Papernot et al., 2017b ) nor the training data distribution. The attacks have also been shown to be highly effective at replicating pay-per-query image prediction APIs, for as little as $30 ( Orekondy et al., 2019 ). Defending against stealing attacks however has received little attention and is lacking. Existing defense strategies aim to either detect stealing query patterns ( Juuti et al., 2019 ), or degrade qual- ity of predicted posterior via perturbation. Since detection makes strong assumptions on the at- tacker's query distribution (e.g., small L 2 distances between successive queries), our focus is on the more popular perturbation-based defenses. A common theme among such defenses is accuracy- preserving posterior perturbation: the posterior distribution is manipulated while retaining the top-1 label. For instance, rounding decimals ( Tramèr et al., 2016 ), revealing only high-confidence predic- tions ( Orekondy et al., 2019 ), and introducing ambiguity at the tail end of the posterior distribution ( Lee et al., 2018 ). Such strategies benefit from preserving the accuracy metric of the defender. How- ever, in line with previous works ( Tramèr et al., 2016 ;  Orekondy et al., 2019 ;  Lee et al., 2018 ), we find models can be effectively stolen using just the top-1 predicted label returned by the black-box. Specifically, in many cases we observe <1% difference between attacks that use the full range of Published as a conference paper at ICLR 2020 posteriors (blue line in  Fig. 1 ) to train stolen models and the top-1 label (orange line) alone. In this paper, we work towards effective defenses (red line in  Fig. 1 ) against DNN stealing attacks with minimal impact to defender's accuracy. The main insight to our approach is that unlike a benign user, a model stealing attacker additionally uses the predictions to train a replica model. By introducing controlled perturbations to predic- tions, our approach targets poisoning the training objective (see  Fig. 2 ). Our approach allows for a utility-preserving defense, as well as trading-off a marginal utility cost to significantly degrade attacker's performance. As a practical benefit, the defense involves a single hyperparameter (perturbation utility budget) and can be used with minimal overhead to any classification model without retraining or modifications. We rigorously evaluate our approach by defending six victim mod- els, against four recent and effective DNN stealing attack strategies ( Papernot et al., 2017b ;  Juuti et al., 2019 ;  Orekondy et al., 2019 ). Our defense consistently mitigates all stealing attacks and further shows improvements over multiple baselines. In particular, we find our defenses degrades the attacker's query sample efficiency by 1-2 orders of magnitude. Our approach significantly reduces the at- tacker's performance (e.g., 30-53% reduction on MNIST and 13- 28% on CUB200) at a marginal cost (1-2%) to defender's test accu- racy. Furthermore, our approach can achieve the same level of mit- igation as baseline defenses, but by introducing significantly lesser perturbation.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the problem of exploration in deep reinforcement learning, which requires the number of visits to each state-action pair to approach infinity in order to guarantee an optimal policy. The simplest approach for tackling this problem is to consider stochastic policies with a non-zero probability of selecting all actions in each state, such as -greedy or Boltzmann exploration. While these techniques can perform well in dense reward scenarios, they can fail to learn in sparse reward settings due to the need for temporally-extended exploration.",
        "Abstract": "We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.",
        "Introduction": "  INTRODUCTION The problem of exploration remains one of the major challenges in deep reinforcement learning. In general, methods that guarantee finding an optimal policy require the number of visits to each state-action pair to approach infinity. Strategies that become greedy after a finite number of steps may never learn to act optimally; they may converge prematurely to suboptimal policies, and never gather the data they need to learn. Ensuring that all state-action pairs are encountered infinitely often is the general problem of maintaining exploration (François-Lavet et al., 2018; Sutton & Barto, 2018). The simplest approach for tackling this problem is to consider stochastic policies with a non-zero probability of selecting all actions in each state, e.g. -greedy or Boltzmann exploration. While these techniques will eventually learn the optimal policy in the tabular setting, they are very inefficient and the steps they require grow exponentially with the size of the state space. Despite these shortcomings, they can perform remarkably well in dense reward scenarios (Mnih et al., 2015). In sparse reward settings, however, they can completely fail to learn, as temporally-extended exploration (also called deep exploration) is crucial to even find the very few rewarding states (Osband et al., 2016).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to modeling the temporal dependency between two spatio-temporal phenomena, where the latter one is caused by the former one with a non-stationary time delay. The proposed approach is motivated by the application domain of space weather, specifically the prediction of the solar wind speed series recorded at the Lagrangian point L1 from heliospheric observations. The paper introduces a novel neural network architecture that is able to capture the non-stationary time lag between the cause and effect series, and provides an empirical evaluation of the proposed approach on a real-world dataset.",
        "Abstract": "This paper tackles a new regression problem, called Dynamic Time-Lag Regression (DTLR), where a cause signal drives an effect signal with an unknown time delay.\nThe motivating application, pertaining to space weather modelling, aims to predict the near-Earth solar wind speed based on estimates of the Sun's  coronal magnetic field. \nDTLR differs from mainstream regression and from sequence-to-sequence learning in two respects: firstly, no ground truth (e.g., pairs of associated sub-sequences) is available; secondly, the cause signal contains much information irrelevant to the effect signal (the solar magnetic field governs the solar wind propagation in the heliosphere, of which the Earth's magnetosphere is but a minuscule region). \n\nA Bayesian approach is presented to tackle the specifics of the DTLR problem, with theoretical justifications based on linear stability analysis. A proof of concept on synthetic problems is presented. Finally, the empirical results on the solar wind modelling task improve on the state of the art in solar wind forecasting.",
        "Introduction": "  INTRODUCTION A significant body of work in machine learning concerns the modeling of spatio-temporal phenomena ( Shi and Yeung, 2018 ;  Rangapuram et al., 2018 ), including the causal analysis of time series  Peters et al. (2017) , with applications ranging from markets ( Pennacchioli et al., 2014 ) to bioinformatics ( Brouard et al., 2016 ) to climate ( Nooteboom et al., 2018 ). This paper focuses on the problem of modeling the temporal dependency between two spatio-temporal phenomena, where the latter one is caused by the former one ( Granger, 1969 ;  Runge, 2018 ) with a non-stationary time delay. The motivating application domain is that of space weather. The sun, a perennial source of charged energetic particles, is at the origin of geomagnetic phenomena within the sun-earth system. Specifi- cally, the sun ejects charged particles into the surrounding space in all directions and some of these particle clouds, a.k.a. solar wind, reach the Earth's vicinity. High speed solar wind is a major threat for the modern world, causing severe damages to e.g., satellites, telecommunication infrastructures, under sea pipelines, among others. A key prediction task thus is to forecast the speed of the solar wind in the vicinity of the Earth ( Munteanu et al., 2013 ;  Haaland et al., 2010 ;  Reiss et al., 2019 ), sufficiently early to emit an alarm and be able to prevent the damage to the best possible extent. Formally the goal is to model the dependency between heliospheric observations (available at light speed), referred to as cause series, and the solar wind speed series recorded at the Lagrangian point L 1 (a point on the Sun-Earth line 1.5 million kilometers away from the Earth), referred to as effect series. The key difficulty is that the Published as a conference paper at ICLR 2020 time lag between an input and its effect, the solar wind recorded at L 1 , varies from circa 2 to 5 days depending on, among many factors, the initial direction of emitted particles and their energy. Would the lag be constant, the solar wind prediction problem would boil down to a mainstream regression problem. The challenge here is to predict, from the solar image x(t) at time t the value y(t + ∆t) of the solar wind speed reaching the earth at time t + ∆t where both the value y(t + ∆t) and the time lag ∆t depend on x(t).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new mixed competitive and cooperative physics-based environment in which agents compete in a simple game of hide-and-seek. Through only a visibility-based reward function and competition, agents learn many emergent skills and strategies including collaborative tool use. The paper also proposes a suite of targeted intelligence tests to measure capabilities in the environment and presents evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy. The main contributions of this work are: 1) clear evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy, 2) evidence that when induced in a physically grounded environment, multi-agent autocurricula can lead to human-relevant skills such as tool use, 3) a proposal to use transfer as a framework for evaluating agents in open-ended environments as well as a suite of targeted intelligence tests for our domain, and 4) open-sourced environments and code for environment construction to encourage further research in physically grounded multi-agent autocurricula.",
        "Abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",
        "Introduction": "  INTRODUCTION Creating intelligent artificial agents that can solve a wide variety of complex human-relevant tasks has been a long-standing challenge in the artificial intelligence community. Of particular relevance to humans will be agents that can sense and interact with objects in a physical world. One approach to creating these agents is to explicitly specify desired tasks and train a reinforcement learning (RL) agent to solve them. On this front, there has been much recent progress in solving physically grounded tasks, e.g. dexterous in-hand manipulation (Rajeswaran et al., 2017; Andrychowicz et al., 2018) or locomotion of complex bodies (Schulman et al., 2015; Heess et al., 2017). However, specifying reward functions or collecting demonstrations in order to supervise these tasks can be * This was a large project and many people made significant contributions. Bowen, Bob, and Igor conceived the project and provided guidance through all stages of the work. Bowen created the initial environment, infrastructure and models, and obtained the first results of sequential skill progression. Ingmar obtained the first results of tool use, contributed to environment variants, created domain-specific statistics, and with Bowen created the final environment. Todor created the manipulation tasks in the transfer suite, helped Yi with the RND baseline, and prepared code for open-sourcing. Yi created the navigation tasks in the transfer suite, intrinsic motivation comparisons, and contributed to environment variants. Glenn contributed to designing the final environment and created final renderings and project video. Igor provided research supervision and team leadership. time consuming and costly. Furthermore, the learned skills in these single-agent RL settings are inherently bounded by the task description; once the agent has learned to solve the task, there is little room to improve. Due to the high likelihood that direct supervision will not scale to unboundedly complex tasks, many have worked on unsupervised exploration and skill acquisition methods such as intrinsic motivation. However, current undirected exploration methods scale poorly with environment complexity and are drastically different from the way organisms evolve on Earth. The vast amount of complexity and diversity on Earth evolved due to co-evolution and competition between organisms, directed by natural selection (Dawkins & Krebs, 1979). When a new successful strategy or mutation emerges, it changes the implicit task distribution neighboring agents need to solve and creates a new pressure Published as a conference paper at ICLR 2020 for adaptation. These evolutionary arms races create implicit autocurricula (Leibo et al., 2019a) whereby competing agents continually create new tasks for each other. There has been much success in leveraging multi-agent autocurricula to solve multi-player games, both in classic discrete games such as Backgammon (Tesauro, 1995) and Go (Silver et al., 2017), as well as in continuous real-time domains such as Dota (OpenAI, 2018) and Starcraft (Vinyals et al., 2019). Despite the impressive emergent complexity in these environments, the learned behavior is quite abstract and disembodied from the physical world. Our work sees itself in the tradition of previous studies that showcase emergent complexity in simple physically grounded environments (Sims, 1994a; Bansal et al., 2018; Jaderberg et al., 2019; Liu et al., 2019); the success in these settings inspires confidence that inducing autocurricula in physically grounded and open-ended environments could eventually enable agents to acquire an unbounded number of human-relevant skills. We introduce a new mixed competitive and cooperative physics-based environment in which agents compete in a simple game of hide-and-seek. Through only a visibility-based reward function and competition, agents learn many emergent skills and strategies including collaborative tool use, where agents intentionally change their environment to suit their needs. For example, hiders learn to create shelter from the seekers by barricading doors or constructing multi-object forts, and as a counter strategy seekers learn to use ramps to jump into hiders' shelter. Moreover, we observe signs of dy- namic and growing complexity resulting from multi-agent competition and standard reinforcement learning algorithms; we find that agents go through as many as six distinct adaptations of strategy and counter-strategy, which are depicted in  Figure 1 . We further present evidence that multi-agent co-adaptation may scale better with environment complexity and qualitatively centers around more human-interpretable behavior than intrinsically motivated agents. However, as environments increase in scale and multi-agent autocurricula become more open-ended, evaluating progress by qualitative observation will become intractable. We therefore propose a suite of targeted intelligence tests to measure capabilities in our environment that we believe our agents may eventually learn, e.g. object permanence (Baillargeon & Carey, 2012), navigation, and construction. We find that for a number of the tests, agents pretrained in hide-and-seek learn faster or achieve higher final performance than agents trained from scratch or pretrained with intrinsic motivation; however, we find that the performance differences are not drastic, indicating that much of the skill and feature representations learned in hide-and-seek are entangled and hard to fine-tune. The main contributions of this work are: 1) clear evidence that multi-agent self-play can lead to emergent autocurricula with many distinct and compounding phase shifts in agent strategy, 2) evi- dence that when induced in a physically grounded environment, multi-agent autocurricula can lead to human-relevant skills such as tool use, 3) a proposal to use transfer as a framework for evaluating agents in open-ended environments as well as a suite of targeted intelligence tests for our domain, and 4) open-sourced environments and code 1 for environment construction to encourage further research in physically grounded multi-agent autocurricula.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to improve the performance and speed of multi-sentence scoring tasks, such as retrieval and dialogue tasks. The proposed Poly-encoder architecture combines the advantages of Bi-encoders and Cross-encoders, resulting in higher accuracy and faster prediction times than the current state-of-the-art. Additionally, the paper shows that pre-training on data more similar to the downstream task can bring significant gains over BERT pre-training. Experiments are conducted on four existing datasets in the domains of dialogue and information retrieval, with pre-training strategies based on Reddit compared to BERT.",
        "Abstract": "The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.",
        "Introduction": "  Introduction Recently, substantial improvements to state-of-the-art benchmarks on a variety of language under- standing tasks have been achieved through the use of deep pre-trained language models followed by fine-tuning ( Devlin et al., 2019 ). In this work we explore improvements to this approach for the class of tasks that require multi-sentence scoring: given an input context, score a set of candidate labels, a setup common in retrieval and dialogue tasks, amongst others. Performance in such tasks has to be measured via two axes: prediction quality and prediction speed, as scoring many candidates can be prohibitively slow. The current state-of-the-art focuses on using BERT models for pre-training ( Devlin et al., 2019 ), which employ large text corpora on general subjects: Wikipedia and the Toronto Books Corpus ( Zhu et al., 2015 ). Two classes of fine-tuned architecture are typically built on top: Bi-encoders and Cross-encoders. Cross-encoders ( Wolf et al., 2019 ;  Vig & Ramea, 2019 ), which perform full (cross) self-attention over a given input and label candidate, tend to attain much higher accuracies than their counterparts, Bi-encoders ( Mazaré et al., 2018 ;  Dinan et al., 2019 ), which perform self-attention over the input and candidate label separately and combine them at the end for a final representa- tion. As the representations are separate, Bi-encoders are able to cache the encoded candidates, and reuse these representations for each input resulting in fast prediction times. Cross-encoders must recompute the encoding for each input and label; as a result, they are prohibitively slow at test time. In this work, we provide novel contributions that improve both the quality and speed axes over the current state-of-the-art. We introduce the Poly-encoder, an architecture with an additional learnt at- tention mechanism that represents more global features from which to perform self-attention, result- ing in performance gains over Bi-encoders and large speed gains over Cross-Encoders. To pre-train our architectures, we show that choosing abundant data more similar to our downstream task also brings significant gains over BERT pre-training. This is true across all different architecture choices and downstream tasks we try. We conduct experiments comparing the new approaches, in addition to analysis of what works best for various setups of existing methods, on four existing datasets in the domains of dialogue and in- formation retrieval (IR), with pre-training strategies based on Reddit ( Mazaré et al., 2018 ) compared",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new video generation tool that is able to extract a character from a video, reanimate it, and generate a novel video of the modified scene. The reanimation is controlled by a low-dimensional signal, such as the one provided by a joystick, and the model has to complete this signal to a high-dimensional full-body signal, in order to generate realistic motion sequences. The method employs two networks, applied in a sequential manner, to separate the character from the background, generate a realistic video frame, and incorporate a photo-realistic generated character into a desired environment. The paper also discusses applications of the learned Pose2Frame network, such as predicting the pose from an existing video.",
        "Abstract": "We extract a controllable model from a video of a person performing a certain activity. The model generates novel image sequences of that person, according to user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. \n\nThe method is based on two networks. The first  maps a current pose, and a single-instance control signal to the next pose. The second maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.",
        "Introduction": "  INTRODUCTION We propose a new video generation tool that is able to extract a character from a video, reanimate it, and generate a novel video of the modified scene, see  Fig. 1 . Unlike previous work, the reanimation is controlled by a low-dimensional signal, such as the one provided by a joystick, and the model has to complete this signal to a high-dimensional full-body signal, in order to generate realistic motion sequences. In addition, our method is general enough to position the extracted character in a new background, which is possibly also dynamic. A video containing a short explanation of our method, samples of output videos, and a comparison to previous work, is provided in https: //youtu.be/sNp6HskavBE. Our work provides a general and convenient way for human users to control the dynamic development of a given video. The input is a video, which contains one or more characters. The characters are extracted, and each is associated with a sequence of displacements. In the current implementation, the motion is taken as the trajectory of the center of mass of that character in the frame. This can be readily generalized to separate different motion elements. Given a user-defined trajectory, a realistic video of the character, placed in front of an arbitrary background, is generated. The method employs two networks, applied in a sequential manner. The first is the Pose2Pose (P2P) network, responsible for manipulating a given pose in an autoregressive manner, based on an input stream of control signals. The second is the Pose2Frame (P2F) network, accountable for generating a high-resolution realistic video frame, given an input pose and a background image. Each network addresses a computational problem not previously fully met, together paving the way for the generation of video games with realistic graphics. The Pose2Pose network enables guided human-pose generation for a specific trained domain (e.g., a tennis player, a dancer, etc.), where guiding takes the form of 2D motion controls, while the Pose2Frame network allows the incorporation of a photo-realistic generated character into a desired environment. In order to enable this, the following challenges are to be addressed: (1) replacing the background requires the system to separate the character from the surroundings, which is not handled by previous work, since they either embed the character into the same learned background, or paste the generated character into the background with noticeable artifacts, (2) the separation is not binary, and some effects, such as shadows, blend the character's motion effect with that background information, (3) the control signal is arbitrary, and can lead the character to poses that are not covered by the training set, and (4) generated sequences may easily drift, by accumulating small errors over time. Both the Pose2Pose and Pose2Frame networks adopt the pix2pixHD framework of  Wang et al. (2018b)  as the generator and discriminator backbones, yet add many contributions in order to address the aforementioned challenges. As a building block, we use the pose representation provided by the DensePose framework by  Rĩza Alp Güler (2018) , unmodified. Similarly, the hand-held object is extracted using the semantic segmentation method of  Zhou et al. (2019) , which incorporates elements from  Maninis et al. (2018) ;  Law & Deng (2018) . In addition to the main application of generating a realistic video from a 2D trajectory, the learned Pose2Frame network can be used for other applications. For example, instead of predicting the pose, it can be extracted from an existing video. This allows us to compare the Pose2Frame network directly with recent video-to-video solutions.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents BlockSwap, a method for quickly identifying a suitable mixed-blocktype version of a large network for a given parameter budget. BlockSwap randomly samples a collection of candidate mixed-blocktype architectures that satisfy the parameter budget, and evaluates them using the sum of the total (empirical) Fisher information for each of its blocks. The network with the highest potential is selected as a student and trained through the distillation method of attention transfer. Experiments on CIFAR-10, ImageNet, and COCO demonstrate the potency of BlockSwap, and ablation studies validate the methodology.",
        "Abstract": "The desire to map neural networks to varying-capacity devices has led to the development of a wealth of compression techniques, many of which involve replacing standard convolutional blocks in a large network with cheap alternative blocks. However, not all blocks are created equally; for a required compute budget there may exist a potent combination of many different cheap blocks, though exhaustively searching for such a combination is prohibitively expensive. In this work, we develop BlockSwap: a fast algorithm for choosing networks with interleaved block types by passing a single minibatch of training data through randomly initialised networks and gauging their Fisher potential. These networks can then be used as students and distilled with the original large network as a teacher. We demonstrate the effectiveness of the chosen networks across CIFAR-10 and ImageNet for classification, and COCO for detection, and provide a comprehensive ablation study of our approach. BlockSwap quickly explores possible block configurations using a simple architecture ranking system, yielding highly competitive networks in orders of magnitude less time than most architecture search techniques (e.g. under 5 minutes on a single GPU for CIFAR-10).",
        "Introduction": "  INTRODUCTION Deep Convolutional Neural Networks are extremely popular, and demonstrate strong performance on a variety of challenging tasks. Because of this, there exist a large range of scenarios in the wild in which practitioners wish to deploy these networks e.g. pedestrian detection in a vehicle's computer, human activity recognition with wearables ( Radu et al., 2018 ). These networks are largely over-parameterised; a fact that can be exploited when specialising networks for different resource budgets. For example, there is a wealth of work demonstrating that it is possible to replace expensive convolutional blocks in a large network with cheap alternatives e.g. those using grouped convolutions ( Chollet, 2017 ;  Xie et al., 2017 ;  Ioannou et al., 2017 ;  Huang et al., 2018 ) or bottleneck structures ( He et al., 2016 ;  Sandler et al., 2018 ;  Peng et al., 2018 ). This creates a smaller network which may be used as a student, and trained through distillation ( Ba & Caruana, 2014 ;  Hinton et al., 2015 ) with the original large network as a teacher to retain performance. Typically, each network block is replaced with a single cheap alternative, producing a single-blocktype network for a given budget. By cheapening each block equally, one relies on the assumption that each block is of equal importance. We posit instead that for each budget, there exist more powerful mixed-blocktype networks that assign non-uniform importance to each block by cheapening them to different extents. We now have a paradox of choice; for a given budget, it is not obvious which cheap alternatives to use for our student network, nor where to place them. Let's assume we have a large network that consists of B expensive convolutional blocks, and a candidate pool of C cheap blocks that we could substitute each of these out for, and we are given a limit on the number of parameters (and therefore memory) that the network can use. Which of Published as a conference paper at ICLR 2020 Our goal in this paper is, given a desired parameter budget, to quickly identify a suitable mixed- blocktype version of the original network that makes for a powerful student. We present a simple method-BlockSwap-to achieve this. First, we randomly sample a collection of candidate mixed- blocktype architectures that satisfy the parameter budget. A single minibatch is then pushed through each candidate network to calculate its Fisher potential: the sum of the total (empirical) Fisher information for each of its blocks. Finally, the network with the highest potential is selected as a student and trained through the distillation method of attention transfer ( Zagoruyko & Komodakis, 2017 ) with the original teacher. Our method is illustrated in  Figure 1 . In Section 3 we describe the block substitutions used in BlockSwap, distillation via attention transfer, and Fisher information. We elaborate on our method in Section 4 as well as providing a comprehensive ablation study. Finally, we experimentally verify the potency of BlockSwap on CIFAR-10 (Section 5) as well as ImageNet (Section 6) and COCO (Section 7). Our contributions are as follows: 1. We introduce BlockSwap, an algorithm for reducing large neural networks by performing block-wise substitution. We show that this outperforms other top-down approaches such as depth/width scaling, parameter pruning, and random substitution. 2. We outline a simple method for quickly evaluating candidate models via Fisher information, which matches the performance of bottom-up approaches while reducing search time from days to minutes. 3. We conduct ablation studies to validate our methodology, highlighting the benefits of block mixing, and confirming that our ranking metric is highly correlated to the final error.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a human-in-the-loop system for counterfactually manipulating documents in order to disentangle spurious and non-spurious associations. Two new datasets are created, one for sentiment analysis and one for natural language inference, by collecting thousands of counterfactually-manipulated examples. Results show that classifiers trained on original datasets fail on counterfactually-revised data and vice versa, and that spurious correlations in these datasets are even picked up by linear models. Augmenting the revised examples breaks up these correlations, and retraining the classifier on the combined dataset improves performance.",
        "Abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources  for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence;  and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their  counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets  perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone  are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.",
        "Introduction": "  INTRODUCTION What makes a document's sentiment positive? What makes a loan applicant creditworthy? What makes a job candidate qualified? When does a photograph truly depict a dolphin? Moreover, what does it mean for a feature to be relevant to such a determination? Statistical learning offers one framework for approaching these questions. First, we swap out the semantic question for a more readily answerable associative question. For example, instead of asking what conveys a document's sentiment, we recast the question as which documents are likely to be labeled as positive (or negative)? Then, in this associative framing, we interpret as relevant, those features that are most predictive of the label. However, despite the rapid adoption and undeniable commercial success of associative learning, this framing seems unsatisfying. Alongside deep learning's predictive wins, critical questions have piled up concerning spurious patterns, artifacts, robustness, and discrimination, that the purely associative perspective appears ill-equipped to answer. For example, in computer vision, researchers have found that deep neural networks rely on surface-level texture (Jo & Bengio, 2017; Geirhos et al., 2018) or clues in the image's background to recognize foreground objects even when that seems both unnecessary and somehow wrong: the beach is not what makes a seagull a seagull. And yet, researchers struggle to articulate precisely why models should not rely on such patterns. In natural language processing (NLP), these issues have emerged as central concerns in the literature on annotation artifacts and societal biases. Across myriad tasks, researchers have demonstrated that models tend to rely on spurious associations (Poliak et al., 2018; Gururangan et al., 2018; Kaushik & Lipton, 2018; Kiritchenko & Mohammad, 2018). Notably, some models for question-answering tasks may not actually be sensitive to the choice of the question (Kaushik & Lipton, 2018), while in Natural Language Inference (NLI), classifiers trained on hypotheses only (vs hypotheses and premises) perform surprisingly well (Poliak et al., 2018; Gururangan et al., 2018). However, papers Published as a conference paper at ICLR 2020 Causality, however, offers a coherent notion of spuriousness. Spurious associations owe to con- founding rather than to a (direct or indirect) causal path. We might consider a factor of variation to be spuriously correlated with a label of interest if intervening upon it would not impact the appli- cability of the label or vice versa. While our paper does not call upon the mathematical machinery of causality, we draw inspiration from the underlying philosophy to design a new dataset creation procedure in which humans counterfactually revise documents. Returning to NLP, although we lack automated tools for mapping between raw text and disentangled factors, we nevertheless describe documents in terms of these abstract representations. Moreover, it seems natural to speak of manipulating these factors directly (Hovy, 1987). Consider, for exam- ple, the following interventions: (i) Revise the letter to make it more positive; (ii) Edit the second sentence so that it appears to contradict the first. These edits might be thought of as intervening on only those aspects of the text that are necessary to make the counterfactual label applicable. In this exploratory paper, we design a human-in-the-loop system for counterfactually manipulating documents. Our hope is that by intervening only upon the factor of interest, we might disentangle the spurious and non-spurious associations, yielding classifiers that hold up better when spurious associations do not transport out of domain. We employ crowd workers not to label documents, but rather to edit them, manipulating the text to make a targeted (counterfactual) class applicable. For sentiment analysis, we direct the worker to revise this negative movie review to make it posi- tive, without making any gratuitous changes. We might regard the second part of this directive as a least action principle, ensuring that we perturb only those spans necessary to alter the applicability of the label. For NLI, a 3-class classification task (entailment, contradiction, neutral), we ask the workers to modify the premise while keeping the hypothesis intact, and vice versa, collecting edits corresponding to each of the (two) counterfactual classes. Using this platform, we collect thou- sands of counterfactually-manipulated examples for both sentiment analysis and NLI, extending the IMDb (Maas et al., 2011) and SNLI (Bowman et al., 2015) datasets, respectively. The result is two new datasets (each an extension of a standard resource) that enable us to both probe fundamental properties of language and train classifiers less reliant on spurious signal. We show that classifiers trained on original IMDb reviews fail on counterfactually-revised data and vice versa. We further show that spurious correlations in these datasets are even picked up by linear models. However, augmenting the revised examples breaks up these correlations (e.g., genre ceases to be predictive of sentiment). For a Bidirectional LSTM (Graves & Schmidhuber, 2005) trained on IMDb reviews, classification accuracy goes down from 79.3% to 55.7% when evaluated on original vs revised reviews. The same classifier trained on revised reviews achieves an accuracy of 89.1% on revised reviews compared to 62.5% on their original counterparts. These numbers go to 81.7% and 92.0% on original and revised data, respectively, when the classifier is retrained on the combined dataset. Similar patterns are observed for linear classifiers. We discovered that BERT (Devlin et al., 2019) is more resilient to such drops in performance on sentiment analysis.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper investigates the conditions under which neural networks can exhibit generalisation beyond their training experience in a systematic way. Through experiments conducted in a 3D simulated room, it is established that a conventional neural-network-based agent exposed to raw visual input and symbolic instructions can learn to exhibit generalisation that approaches systematic behaviour. Three factors are identified as critical for this generalisation: the number of words and objects experienced during training, a bounded frame of reference or perspective, and the diversity of perceptual input afforded by the temporal aspect of the agent's perspective. These results suggest that robust systematic generalisation may be an emergent property of an agent interacting with a rich, situated environment.",
        "Abstract": "The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. We first describe a comparatively generic agent architecture that exhibits strong performance on these tests. We then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame of reference; and (c) the variety of visual input inherent in the perceptual aspect of the agent's perception. Our findings indicate that the degree of generalisation that networks exhibit can depend critically on particulars of the environment in which a given task is instantiated. They further suggest that the propensity for neural networks to generalise in systematic ways may increase if, like human children, those networks have access to many frames of richly varying, multi-modal observations as they learn.",
        "Introduction": "  INTRODUCTION Since the earliest days of research on neural networks, a recurring point of debate is whether neural networks exhibit generalisation beyond their training experience, in a systematic way ( Smolensky, 1988 ;  Fodor & Pylyshyn, 1988 ;  Marcus, 1998 ; McClelland et al., 1987). This debate has been re-energized over the past few years, given a resurgence in neural network research overall ( Lake & Baroni, 2017 ;  Bahdanau et al., 2018 ; Lake, 2019). Generalisation in neural networks is not a binary question; since there are cases where networks generalise well and others where they do not, the pertinent research question is when and under what conditions neural networks are able to generalise. Here, we establish that a conventional neural-network-based agent exposed to raw visual input and symbolic (language-like) instructions readily learns to exhibit generalisation that approaches systematic behaviour, and we explore the conditions supporting its emergence. First, we show in a 3D simulated room that an agent trained to find all objects from a set and lift only some of them can lift withheld test objects never lifted during training. Second, we show that the same agent trained to lift all of the objects and put only some of them during training can put withheld test objects, zero-shot, in the correct location. That is, the model learns to re-compose known concepts (verbs and nouns) in novel combinations. In order to better understand this generalisation, we conduct several experiments to isolate its con- tributing factors. We find three to be critical: (a) the number of words and objects experienced during training; (b) a bounded frame of reference or perspective; and (c) the diversity of perceptual input afforded by the temporal aspect of the agent's perspective. Crucially, these factors can be enhanced by situating an agent in a realistic environment, rather than an abstract, simplified setting. These results serve to explain differences between our findings and studies showing poor generalisation, where networks were typically trained in a supervised fashion on abstract or idealised stimuli from a Published as a conference paper at ICLR 2020 single modality (e.g.  Lake & Baroni, 2017 ). They also suggest that the human capacity to exploit the compositionality of the world, when learning to generalise in systematic ways, might be replicated in artificial neural networks if those networks are afforded access to a rich, interactive, multimodal stream of stimuli that better matches the experience of an embodied human learner ( Clerkin et al., 2017 ;  Kellman & Arterberry, 2000 ;  James et al., 2014 ; Yurovsky et al., 2013;  Anderson, 2003 ). Our results suggest that robust systematic generalisation may be an emergent property of an agent interacting with a rich, situated environment (c.f. McClelland et al., 2010).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the success of deep neural networks (DNNs) and the importance of high-level, flexible, and efficient software libraries like Tensorflow, Keras, PyTorch.nn, Chainer, JAX, and others in enabling researchers to rapidly build complex models. It emphasizes the need for sophisticated software tools to support new machine learning approaches.",
        "Abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) owe their success in part to the broad availability of high-level, flexible, and efficient software libraries like Tensorflow ( Abadi et al., 2015 ), Keras ( Chollet et al., 2015 ), PyTorch.nn ( Paszke et al., 2017 ), Chainer ( Tokui et al., 2015 ;  Akiba et al., 2017 ), JAX ( Bradbury et al., 2018a ), and others. These libraries enable researchers to rapidly build complex models by constructing them out of smaller primitives. The success of new machine learning approaches will similarly depend on developing sophisticated software tools to support them.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel imitation learning algorithm that combines the advantages of model-based reinforcement learning (MBRL) and imitation learning (IL). The algorithm is trained to forecast expert trajectories with a density function, and then uses a probabilistic inference objective to create plans that incorporate both the model and arbitrary new tasks. The method is evaluated on a dynamic simulated autonomous driving task, and is shown to generate expert-like plans with minimal reward engineering, flexibly incorporate and achieve goals not seen during training, and be robust to noise in the goal specification. It is also shown to outperform MBRL, a custom IL method, and all five prior CARLA IL methods known to us.",
        "Abstract": "Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose \"Imitative Models\" to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection.  We also show our approach is robust to poorly-specified goals, such as goals on the wrong side of the road.",
        "Introduction": "  INTRODUCTION Imitation learning (IL) is a framework for learning a model to mimic behavior. At test-time, the model pursues its best-guess of desirable behavior. By letting the model choose its own behavior, we cannot direct it to achieve different goals. While work has augmented IL with goal conditioning ( Dosovitskiy & Koltun, 2016 ;  Codevilla et al., 2018 ), it requires goals to be specified during training, explicit goal labels, and are simple (e.g., turning). In contrast, we seek flexibility to achieve general goals for which we have no demonstrations. In contrast to IL, planning-based algorithms like model-based reinforcement learning (MBRL) methods do not require expert demonstrations. MBRL can adapt to new tasks specified through reward functions ( Kuvayev & Sutton, 1996 ;  Deisenroth & Rasmussen, 2011 ). The \"model\" is a dynamics model, used to plan under the user-supplied reward function. Planning enables these approaches to perform new tasks at test-time. The key drawback is that these models learn dynamics of possible behavior rather than dynamics of desirable behavior. This means that the responsibility of evoking desirable behavior is entirely deferred to engineering the input reward function. Designing reward functions that cause MBRL to evoke complex, desirable behavior is difficult when the space of possible undesirable behaviors is large. In order to succeed, the rewards cannot lead the model astray towards observations significantly different than those with which the model was trained. Our goal is to devise an algorithm that combines the advantages of MBRL and IL by offering MBRL's flexibility to achieve new tasks at test-time and IL's potential to learn desirable behavior entirely from offline data. To accomplish this, we first train a model to forecast expert trajectories with a density function, which can score trajectories and plans by how likely they are to come from the expert. A probabilistic model is necessary because expert behavior is stochastic: e.g. at an intersection, the expert could choose to turn left or right. Next, we derive a principled probabilistic inference objective to create plans that incorporate both (1) the model and (2) arbitrary new tasks. Finally, we derive families of tasks that we can provide to the inference framework. Our method can accomplish new tasks specified as complex goals without having seen an expert complete these tasks before. We investigate properties of our method on a dynamic simulated autonomous driving task (see  Fig. 1 ). Videos are available at https://sites.google.com/view/imitative-models. Our contributions are as follows: 1. Interpretable expert-like plans with minimal reward engineering. Our method outputs multi- step expert-like plans, offering superior interpretability to one-step imitation learning models. In contrast to MBRL, our method generates expert-like behaviors with minimal reward engineering. 2. Flexibility to new tasks: In contrast to IL, our method flexibly incorporates and achieves goals not seen during training, and performs complex tasks that were never demonstrated, such as navigating to goal regions and avoiding test-time only potholes, as depicted in  Fig. 1 . 3. Robustness to goal specification noise: We show that our method is robust to noise in the goal specification. In our application, we show that our agent can receive goals on the wrong side of the road, yet still navigate towards them while staying on the correct side of the road. 4. State-of-the-art CARLA performance: Our method substantially outperforms MBRL, a custom IL method, and all five prior CARLA IL methods known to us. It learned near-perfect driving through dynamic and static CARLA environments from expert observations alone.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the loss surface of deep neural networks (DNNs) and its Hessian. It compares the loss surface of DNNs to the energy landscape of different physical models and discusses the implications of the non-convexity of the loss function. It also examines the relation between the rank of saddle points and their loss, as well as the large number of flat directions in overparametrized DNNs. Finally, it explores the behavior of the Hessian of the loss of DNNs in the small region explored by the parameters during training.",
        "Abstract": "The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training. ",
        "Introduction": "  Introduction The advent of deep learning has sparked a lot of interest in the loss surface of deep neural networks (DNN), and in particular its Hessian. However to our knowledge, there is still no theoretical description of the spectrum of the Hessian. Nevertheless a number of phenomena have been observed numerically. The loss surface of neural networks has been compared to the energy landscape of different physical models (Choromanska et al., 2015; Geiger et al., 2018; Mei et al., 2018). It appears that the loss surface of DNNs may change significantly depending on the width of the network (the number of neurons in the hidden layer), motivating the distinction between the under- and over-parametrized regimes (Baity-Jesi et al., 2018; Geiger et al., 2018; 2019). The non-convexity of the loss function implies the existence of a very large number of saddle points, which could slow down training. In particular, in (Pascanu et al., 2014; Dauphin et al., 2014), a relation between the rank of saddle points (the number of negative eigenvalues of the Hessian) and their loss has been observed. For overparametrized DNNs, a possibly more important phenomenon is the large number of flat directions (Baity-Jesi et al., 2018). The existence of these flat minima is conjectured to be related to the generalization of DNNs and may depend on the training procedure (Hochreiter & Schmidhuber, 1997; Chaudhari et al., 2016; Wu et al., 2017). In (Jacot et al., 2018) it has been shown, using a functional approach, that in the infinite- width limit, DNNs behave like kernel methods with respect to the so-called Neural Tangent Kernel, which is determined by the architecture of the network. This leads to convergence guarantees for DNNs (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2018; Huang & Yau, 2019) and strengthens the connections between neural networks and kernel methods (Neal, 1996; Cho & Saul, 2009; Lee et al., 2018). Our approach also allows one to probe the so-called mean-field/active limit (studied in (Rotskoff & Vanden-Eijnden, 2018; Chizat & Bach, 2018a; Mei et al., 2018) for shallow networks), where the NTK varies during training. This raises the question: can we use these new results to gain insight into the behavior of the Hessian of the loss of DNNs, at least in the small region explored by the parameters during training?",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to graph-structured problems, called neural graph algorithm execution, which combines graph representation learning with supervised learning to learn multiple algorithms simultaneously. The approach provides a supervision signal driven by how a known classical algorithm would process such inputs, providing explicit guidance on how to tackle graph-structured problems. Results show that this approach is able to demonstrate positive knowledge transfer between learning different algorithms.",
        "Abstract": "Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.",
        "Introduction": "  INTRODUCTION A multitude of important real-world tasks can be formulated as tasks over graph-structured inputs, such as navigation, web search, protein folding, and game-playing. Theoretical computer science has successfully discovered effective and highly influential algorithms for many of these tasks. But many problems are still considered intractable from this perspective. Machine learning approaches have been applied to many of these classic tasks, from tasks with known polynomial time algorithms such as shortest paths ( Graves et al., 2016 ; Xu et al., 2019) and sorting (Reed & De Freitas, 2015), to intractable tasks such as travelling salesman ( Vinyals et al., 2015 ;  Bello et al., 2016 ;  Kool et al., 2018 ), boolean satisfiability ( Selsam et al., 2018 ;  Selsam & Bjørner, 2019 ), and even probabilistic inference ( Yoon et al., 2018 ). Recently, this work often relies on advancements in graph representation learning ( Bronstein et al., 2017 ;  Hamilton et al., 2017 ;  Battaglia et al., 2018 ) with graph neural networks (GNNs) ( Li et al., 2015 ;  Kipf & Welling, 2016 ;  Gilmer et al., 2017 ;  Veličković et al., 2018 ). In almost all cases so far, ground-truth solutions are used to drive learning, giving the model complete freedom to find a mapping from raw inputs to such solution 1 . Many classical algorithms share related subroutines: for example, shortest path computation (via the Bellman-Ford ( Bellman, 1958 ) algorithm) and breadth-first search both must enumerate sets of edges adjacent to a particular node. Inspired by previous work on the more general tasks of program synthesis and learning to execute ( Zaremba & Sutskever, 2014 ;  Kaiser & Sutskever, 2015 ;  Kurach et al., 2015 ; Reed & De Freitas, 2015;  Santoro et al., 2018 ), we show that by learning several algorithms simultaneously and providing a supervision signal, our neural network is able to demonstrate positive knowledge transfer between learning different algorithms. The supervision signal is driven by how a known classical algorithm would process such inputs (including any relevant intermediate outputs), providing explicit (and reusable) guidance on how to tackle graph-structured problems. We call this approach neural graph algorithm execution.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper presents a distributed machine learning algorithm, CHOCO-SGD, which enables computational scalability and data-locality. The algorithm is evaluated on two different scenarios: a peer-to-peer setting and a datacenter setting. Results show that CHOCO-SGD can improve time-to-accuracy on large tasks, such as ImageNet training. However, when scaling to larger numbers of nodes, decentralized schemes encounter difficulties and often do not reach the same performance as centralized schemes.",
        "Abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that Choco-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data.  We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter. ",
        "Introduction": "  INTRODUCTION Distributed machine learning-i.e. the training of machine learning models using distributed opti- mization algorithms-has recently enabled many successful applications in research and industry. Such methods offer two of the key success factors: 1) computational scalability by leveraging the simultaneous computational power of many devices, and 2) data-locality, the ability to perform joint training while keeping each part of the training data local to each participating device. Recent theoretical results indicate that decentralized schemes can be as efficient as the centralized approaches, at least when considering convergence of training loss vs. iterations ( Scaman et al., 2017 ; 2018;  Lian et al., 2017 ;  Tang et al., 2018 ;  Koloskova et al., 2019 ;  Assran et al., 2019 ). Gradient compression techniques have been proposed for the standard distributed training case ( Alis- tarh et al., 2017 ;  Wen et al., 2017 ;  Lin et al., 2018 ;  Wangni et al., 2018 ;  Stich et al., 2018 ), to reduce the amount of data that has to be sent over each communication link in the network. For decentralized training of deep neural networks,  Tang et al. (2018)  introduce two algorithms (DCD, ECD) which allow for communication compression. However, both these algorithms are restrictive with respect to the used compression operators, only allowing for unbiased compressors and-more significantly- so far not supporting arbitrarily high compression ratios. We here study CHOCO-SGD-recently introduced for convex problems only ( Koloskova et al., 2019 )-which overcomes these constraints. For the evaluation of our algorithm we in particular focus on the generalization performance (on the test-set) on standard machine learning benchmarks, hereby departing from previous work such as e.g. ( Tang et al., 2018 ;  Wang et al., 2019 ;  Tang et al., 2019 ;  Reisizadeh et al., 2019 ) that mostly con- sidered training performance (on the train-set). We study two different scenarios: firstly, (i) training on a challenging peer-to-peer setting, where the training data is distributed over the training devices (and not allowed to move), similar to the federated learning setting ( McMahan et al., 2017 ). We are again able to show speed-ups for CHOCO-SGD over the decentralized baseline ( Lian et al., 2017 ) with much less communication overhead. Secondly, (ii) training in a datacenter setting, where decen- tralized communication patterns allow better scalability than centralized approaches. For this setting Published as a conference paper at ICLR 2020 we show that communication efficient CHOCO-SGD can improve time-to-accuracy on large tasks, such as e.g. ImageNet training. However, when investigating the scaling of decentralized algorithms to larger number of nodes we observe that (all) decentralized schemes encounter difficulties and often do not reach the same (test and train) performance as centralized schemes. As these findings point out some deficiencies of current decentralized training schemes (and are not particular to our scheme) we think that reporting these results is a helpful contribution to the community to spur further research on decentralized training schemes that scale to large number of peers.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes an end-to-end method to learn an active learning strategy for semantic segmentation with reinforcement learning. The method is designed to select the most informative regions of images to label, so that a learning algorithm can perform better with less data than a non-selective approach. The method is tested on the CamVid and Cityscapes datasets, and results show that it outperforms a state-of-the-art technique known as BALD and uniform sampling baselines.",
        "Abstract": "Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.",
        "Introduction": "  INTRODUCTION Semantic segmentation, the task of labelling an image pixel-by-pixel with the category it belongs to, is critical for a variety of applications such as autonomous driving ( Müller et al., 2018 ;  Wang & Pan, 2018 ), robot manipulation ( Schwarz et al., 2018 ), embodied question answering ( Yu et al., 2019 ) and biomedical image analysis ( Ronneberger et al., 2015 ). Convolutional neural networks ( Lecun et al., 1998 )-based methods have achieved excellent results on large-scale supervised semantic segmentation, in which we assume pixel-level annotations are available ( Farabet et al., 2013 ;  Pinheiro & Collobert, 2014 ;  Long et al., 2015 ). For such models to work, however, they need a large amount of pixel-level annotations that may require costly human labor ( Cordts et al., 2016 ;  Bearman et al., 2016 ). Current semantic segmentation datasets have pixel-wise annotations for each image. This standard approach has two important issues: (i) pixel-level labelling is extremely time consuming. For example, annotation and quality control required more than 1.5h per image (on average) on Cityscapes ( Cordts et al., 2016 ), a popular dataset used for benchmarking semantic segmentation methods. (ii) Class imbalance in the data is typically extreme. Certain categories (such as 'building' or 'sky') can appear with two orders of magnitude more frequently than others (e.g. 'pedestrian' or 'bicycle'). This can lead to undesired biases and performance properties for learned models. This is specially relevant when we want to collect annotated data with a human in the loop to create a new dataset or to add more labeled data to an existing one. We can tackle the aforementioned problems by selecting, in an efficient and effective way, which regions of the images should be labeled next. Active learning (AL) is a well-established field that studies precisely this: selecting the most informative samples to label so that a learning algorithm will perform better with less data than a non-selective approach, such as labelling the entire collection of data. Active learning methods can be roughly divided in two groups: (i) methods that combine different manually-designed AL strategies ( Roy & McCallum, 2001 ;  Osugi et al., 2005 ;  Gal et al., 2017 ;  Baram et al., 2004 ;  Chu & Lin, 2016 ;  Hsu & Lin, 2015 ;  Ebert et al., 2012 ;  Long & Hua, 2015 ) and (ii) data-driven AL approaches ( Bachman et al., 2017 ;  Fang et al., 2017 ;  Konyushkova et al., 2017 ;  Woodward & Finn, 2016 ;  Ravi & Larochelle, 2018 ; Konyushkova et al., 2018), that learn which samples are most informative to train a model using information of the model itself. Although label acquisition for semantic segmentation is more costly and time consuming than image classification, there has been considerably less work in active learning for semantic segmentation (Dutt Jain & Grauman, 2016;  Mackowiak et al., 2018 ;  Vezhnevets et al., 2012 ;  Konyushkova et al., 2015 ;  Gorriz et al., 2017 ;  Yang et al., 2017 ), and they focus on hand-crafted strategies. Current AL techniques that use reinforcement learning (Konyushkova et al., 2018;  Fang et al., 2017 ;  Woodward & Finn, 2016 ;  Pang et al., 2018 ;  Padmakumar et al., 2018 ;  Bachman et al., 2017 ) focus on labelling one sample per step until a budget of labels is met. In semantic segmentation, this would translate into labelling a single region per step. This is highly inefficient, since each step involves updating the segmentation network and computing the rewards. In this work, we propose an end-to-end method to learn an active learning strategy for semantic segmentation with reinforcement learning by directly maximizing the performance metric we care about, Intersection over Union (IoU). We aim at learning a policy from the data that finds the most informative regions on a set of unlabeled images and asks for its labels, such that a segmentation network can achieve high-quality performance with a minimum number of labeled pixels. Selecting regions, instead of entire images, allows the algorithm to focus on the most relevant parts of the images, as shown in  Figure 1 . Although class imbalance in segmentation datasets has been previously addressed in ( Badrinarayanan et al., 2017 ;  Chan et al., 2019 ;  Sudre et al., 2017 ), among others, they try to solve a problem that arises from the data collection process. We show that our proposed method can help mitigate the problem at its source, i.e. in the data annotation itself. Because our method maximizes the mean IoU per class, it indirectly learns to ask for more labels of regions with under-represented classes, compared to the baselines. Moreover, we propose and explore a batch-mode active learning approach that uses an adapted DQN to efficiently chose batches of regions for labelling at each step. To the best of our knowledge, all current approaches for active learning in semantic segmentation rely on hand-crafted active learning heuristics. However, learning a labelling policy from the data could allow the query agent to ask for labeled data as a function of the data characteristics and class imbalances, that may vary between datasets. Our main contributions can be summarized as follows: (i) we learn a RL-based acquisition function for region-based active learning for segmentation, (ii) we formulate our active learning framework with a batch-mode DQN, which labels multiple regions in parallel at each active learning iteration (a more efficient strategy for large-scale datasets that is compatible with standard mini-batch gradient descent), and (iii) we test the proof of concept Published as a conference paper at ICLR 2020 in CamVid ( Brostow et al., 2008 ) dataset and provide results in Cityscapes ( Cordts et al., 2016 ) dataset, beating a recent state-of-the-art technique known as BALD ( Gal et al., 2017 ), a widely used entropy-based selection criterion and uniform sampling baselines.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents the Counterfactual Physics benchmark (CoPhy) and a framework for causal learning of dynamics in mechanical systems with multiple degrees of freedom. CoPhy is a large-scale dataset of 300k synthetic experiments including rendered sequences of frames, metadata (object positions, angles, sizes) and values of confounders (masses, frictions, gravity). The task is to predict the alternative outcome of a physical experiment given an intervention, by estimating the latent representation of the confounders. The paper also presents a counterfactual neural model that outperforms state-of-the-art solutions implementing feedforward video prediction, successfully generalizes to unseen initial states and does not require supervision on the confounders. Extensive ablations on the effects of key design choices and comparisons with human performance are also provided.",
        "Abstract": "Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input.  We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.",
        "Introduction": "  INTRODUCTION Reasoning is an essential ability of intelligent agents that enables them to understand complex rela- tionships between observations, detect affordances, interpret knowledge and beliefs, and to leverage this understanding to anticipate future events and act accordingly. The capacity for observational discovery of causal effects in physical reality and making sense of fundamental physical concepts, such as mass, velocity, friction, etc., may be one of differentiating properties of human intelligence that ensures our ability to robustly generalize to new scenarios (Martin-Ordas et al., 2008). One way to express causality is based on the concept of counterfactual reasoning, that deals with a problem containing an if statement, which is untrue or unrealized. Predicting the effect of the interventions based on the given observations without explicitly observing the effect of the intervention on data is a hard task and requires modeling of the causal relationships between the variable on which the intervention is performed and the variable whose alternative future should be predicted (Balke & Pearl, 1994). Using counterfactuals has been shown to be a way to perform reasoning over causal relationships between the variables of low dimensional spaces and has been an unexplored direction for high dimensional signals such as videos. In this work, we develop the Counterfactual Physics benchmark (CoPhy) and propose a framework for causal learning of dynamics in mechanical systems with multiple degrees of freedom, as illustrated in  Fig. 1 . For a number of scenarios, such as tower of blocks falling, balls bouncing against walls or objects colliding, we are given the starting frame A = X 0 and a sequence of following frames B = X 1:τ , where τ covers the range of 6 sec. The observed sequences B, conditioned on the initial state A, are direct effects of the physical principles (such as inertia, gravity or friction) applied to the closed system, that cause the objects change their positions and 3D poses over time. The task is formulated as follows: having observed the tuple (A, B), we wish to predict positions and poses of all objects in the scene at time t=τ , if we had changed the initial frame X 0 by performing an intervention. The intervention is formalized by the do-operator introduced by Pearl et al. (Pearl, 2009; Pearl & McKenzie, 2018) for dealing with causal inference (Spirtes, 2010). In our case, it implies modification of the variable A to C, defined as C = do(X 0 =X 0 ). Accordingly, for each experiment in the CoPhy benchmark, we provide pairs of original sequences X 0:τ and their modified counterpartsX 0:τ sharing the same values of all confounders. We note the fundamental difference between this problem of counterfactual future forecasting and the conventional setup of feedforward future forecasting, like video prediction (Mathieu et al., 2016). The latter involves learning spatio-temporal regularities and thereby predicting future frames X 1...τ from one or several past frame(s) X 0 (the causal chain of this problem is shown in Fig. 2a). On the other hand, counterfactual forecasting benefits from additional observations in the form of the original outcome X 1:τ before the do-operator. This adds a confounder variable U into the causal chain (Fig. 2b), which provides information not observable in frame X 0 . For instance, in the case of the CoPhy benchmark, observing the pair (A, B) might give us information on the masses, velocities or friction coefficients of the objects in the scene, which otherwise cannot be inferred from frameX 0 alone. Therefore, predicting the alternative outcome after performing counterfactual intervention then involves using the estimate of the confounder U together with the modified past do(X 0 =X 0 ). Overall, we employ the idea of counterfactual intervention in predictive models and argue that counterfactual reasoning is an important step towards human-like reasoning and general intelligence. More specifically, key contributions of this work include: • a new task of counterfactual prediction of physical dynamics from high-dimensional visual input, as a way to access capacity of intelligent agents for causal discovery; • a large-scale CoPhy benchmark with three physical scenarios and 300k synthetic experiments including rendered sequences of frames, metadata (object positions, angles, sizes) and values of confounders (masses, frictions, gravity). This benchmark was specifically designed in bias-free fashion to make the counter-factual reasoning task challenging by optimizing the impact of the confounders on the outcome of the experiment. The dataset is publicly available 1 . • a counterfactual neural model predicting an alternative outcome of a physical experiment given an intervention, by estimating the latent representation of the confounders. The model outperforms state-of-the-art solutions implementing feedforward video prediction, successfully generalizes to unseen initial states and does not require supervision on the confounders. We provide extensive ablations on the effects of key design choices and compare results with human performance, that show that the task is hard for humans to solve. The code will be made publicly available.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces the CONVCNP, a new member of the Neural Process (NP) family that accounts for translation equivariance. The CONVCNP is achieved by extending the theory of learning on sets to include functional representations, which in turn can be used to express any translation-equivariant NP model. The paper provides a representation theorem for translation-equivariant functions on sets, extending a key result of Zaheer et al. (2017) to functional embeddings, including sets of varying size. The paper also evaluates the CONVCNP and demonstrates that it exhibits excellent performance on several synthetic and real-world benchmarks.",
        "Abstract": "We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve state-of-the-art performance compared to existing NPs. We demonstrate that building in translation equivariance enables zero-shot generalization to challenging, out-of-domain tasks.",
        "Introduction": "  INTRODUCTION Neural Processes (NPs; Garnelo et al., 2018b;a) are a rich class of models that define a conditional distribution p(y|x, Z, θ) over output variables y given input variables x, parameters θ, and a set of observed data points in a context set Z = {x m , y m } M m=1 . A key component of NPs is the embedding of context sets Z into a representation space through an encoder Z → E(Z), which is achieved using a DEEPSETS function approximator (Zaheer et al., 2017). This simple model specification allows NPs to be used for (i) meta-learning (Thrun & Pratt, 2012; Schmidhuber, 1987), since predictions can be generated on the fly from new context sets at test time; and (ii) multi-task or transfer learning (Requeima et al., 2019), since they provide a natural way of sharing information between data sets. Moreover, conditional NPs (CNPs; Garnelo et al., 2018a), a deterministic variant of NPs, can be trained in a particularly simple way with maximum likelihood learning of the parameters θ, which mimics how the system is used at test time, leading to strong performance (Gordon et al., 2019). Natural application areas of NPs include time series, spatial data, and images with missing values. Consequently, such domains have been used extensively to benchmark current NPs (Garnelo et al., 2018a;b; Kim et al., 2019). Often, ideal solutions to prediction problems in such domains should be translation equivariant: if the data are translated in time or space, then the predictions should be translated correspondingly (Kondor & Trivedi, 2018; Cohen & Welling, 2016). This relates to the notion of stationarity. As such, NPs would ideally have translation equivariance built directly into the modelling assumptions as an inductive bias. Unfortunately, current NP models must learn this structure from the data set instead, which is sample and parameter inefficient as well as impacting the ability of the models to generalize. The goal of this paper is to build translation equivariance into NPs. Famously, convolutional neural networks (CNNs) added translation equivariance to standard multilayer perceptrons (LeCun et al., 1998; Cohen & Welling, 2016). However, it is not straightforward to generalize NPs in an analogous way: (i) CNNs require data to live \"on the grid\" (e.g. image pixels form a regularly spaced grid), while many of the above domains have data that live \"off the grid\" (e.g. time series data may be observed irregularly at any time t ∈ R). (ii) NPs operate on partially observed context sets whereas Published as a conference paper at ICLR 2020 CNNs typically do not. (iii) NPs rely on embedding sets into a finite-dimensional vector space for which the notion of equivariance with respect to input translations is not natural, as we detail in Section 3. In this work, we introduce the CONVCNP, a new member of the NP family that accounts for translation equivariance. 1 This is achieved by extending the theory of learning on sets to include functional representations, which in turn can be used to express any translation-equivariant NP model. Our key contributions can be summarized as follows. (i) We provide a representation theorem for translation-equivariant functions on sets, extending a key result of Zaheer et al. (2017) to functional embeddings, including sets of varying size. (ii) We extend the NP family of models to include translation equivariance. (iii) We evaluate the CONVCNP and demonstrate that it exhibits excellent performance on several synthetic and real-world benchmarks.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a unique blend of cheap coarse-grained supervision in the form of rules and expensive fine-grained supervision in the form of labeled instances. Instead of supervising rules and instance labels independently, each labeling rule is attached with exemplars of where the rule correctly 'fires'. This approach is demonstrated with two illustrative applications from the text domain, and the learning algorithm is agnostic to how rules are expressed.",
        "Abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.",
        "Introduction": "  INTRODUCTION With the ever-increasing reach of machine learning, a common hurdle to new adoptions is the lack of labeled data and the pain-staking process involved in collecting human supervision. Over the years, several strategies have evolved. On the one hand are methods like active learning and crowd- consensus learning that seek to reduce the cost of supervision in the form of per-instance labels. On the other hand is the rich history of rule-based methods ( Appelt et al., 1993 ;  Cunningham, 2002 ) where humans code-up their supervision as labeling rules. There is growing interest in learning from such efficient, albiet noisy, supervision ( Ratner et al., 2016 ;  Pal & Balasubramanian, 2018 ;  Bach et al., 2019 ; Sun et al., 2018; Kang et al., 2018). However, clean task-specific instance labels continue to be critical for reliable results ( Goh et al., 2018 ;  Bach et al., 2019 ) in spite of easy availability of pre-trained models (Sun et al., 2017; Devlin et al., 2018). In this paper we propose a unique blend of cheap coarse-grained supervision in the form of rules and expensive fine-grained supervision in the form of labeled instances. Instead of supervising rules and instance labels independently, we propose that each labeling rule be attached with exemplars of where the rule correctly 'fires'. Thus, the rule can be treated as a noisy generalization of those exem- plars. Often rules are coded up only after inspecting data. As a human inspects instances, he labels them, and then generalizes them to rules. Thus, humans provide paired supervision of rules and exemplars demonstrating correct deployment of that rule. We explain further with two illustrative applications. Our examples below are from the text domain because rules have been traditionally used in many NLP tasks, but our learning algorithm is agnostic to how rules are expressed.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper considers the problem of zeroth-order optimization, where the goal is to minimize an objective function with as few evaluations of the function as possible. We analyze a class of algorithms as gradient descent on a Gaussian smoothing of the objective and propose a two-point evaluation scheme that constructs gradient estimates from the difference between function values at two points. We discuss the application of this scheme to stochastic, nonconvex, non-smooth, and non-Euclidean norm settings, as well as the use of heuristics such as CMA-ES. We also consider the case of low-dimensional structure in the objective function, and show that our algorithm will inherently pick up any low-dimensional structure and achieve a convergence rate that depends on k log(n).",
        "Abstract": "Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \\ge n$, we present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.\n",
        "Introduction": "  INTRODUCTION We consider the problem of zeroth-order optimization (also known as gradient-free optimization, or bandit optimization), where our goal is to minimize an objective function f : R n → R with as few evaluations of f (x) as possible. For many practical and interesting objective functions, gradients are difficult to compute and there is still a need for zeroth-order optimization in applications such as reinforcement learning ( Mania et al., 2018 ;  Salimans et al., 2017 ;  Choromanski et al., 2018 ), attacking neural networks ( Chen et al., 2017 ;  Papernot et al., 2017 ), hyperparameter tuning of deep networks ( Snoek et al., 2012 ), and network control ( Liu et al., 2017 ). The standard approach to zeroth-order optimization is, ironically, to estimate the gradients from function values and apply a first-order optimization algorithm ( Flaxman et al., 2005 ).  Nesterov & Spokoiny (2011)  analyze this class of algorithms as gradient descent on a Gaussian smoothing of the objective and gives an accelerated O(n √ Q log((LR 2 + F )/ )) iteration complexity for an L- Lipschitz convex function with condition number Q and R = x 0 − x * and F = f (x 0 ) − f (x * ). They propose a two-point evaluation scheme that constructs gradient estimates from the difference between function values at two points that are close to each other. This scheme was extended by ( Duchi et al., 2015 ) for stochastic settings, by (Ghadimi & Lan, 2013) for nonconvex settings, and by ( Shamir, 2017 ) for non-smooth and non-Euclidean norm settings. Since then, first-order techniques such as variance reduction ( Liu et al., 2018 ), conditional gradients ( Balasubramanian & Ghadimi, 2018 ), and diagonal preconditioning ( Mania et al., 2018 ) have been successfully adopted in this setting. This class of algorithms are also known as stochastic search, random search, or (natural) evolutionary strategies and have been augmented with a variety of heuristics, such as the popular CMA-ES ( Auger & Hansen, 2005 ). These algorithms, however, suffer from high variance due to non-robust local minima or highly non-smooth objectives, which are common in the fields of deep learning and reinforcement learn- Published as a conference paper at ICLR 2020 ing.  Mania et al. (2018)  notes that gradient variance increases as training progresses due to higher variance in the objective functions, since often parameters must be tuned precisely to achieve rea- sonable models. Therefore, some attention has shifted into direct search algorithms that usually finds a descent direction u and moves to x + δu, where the step size is not scaled by the function difference. The first approaches for direct search were based on deterministic approaches with a positive span- ning set and date back to the 1950s ( Brooks, 1958 ). Only recently have theoretical bounds surfaced, with  Gratton et al. (2015)  giving an iteration complexity that is a large polynomial of n and  Dodan- geh & Vicente (2016)  giving an improved O(n 2 L 2 / ). Stochastic approaches tend to have better complexities:  Stich et al. (2013)  uses line search to give a O(nQ log(F/ )) iteration complexity for convex functions with condition number Q and most recently,  Gorbunov et al. (2019)  uses impor- tance sampling to give a O(nQ log(F/ )) complexity for convex functions with average condition numberQ, assuming access to sampling probabilities.  Stich et al. (2013)  notes that direct search algorithms are invariant under monotone transforms of the objective, a property that might explain their robustness in high-variance settings. In general, zeroth order optimization suffers an at least linear dependence on input dimension n and recent works have tried to address this limitation when n is large but f (x) admits a low-dimensional structure. Some papers assume that f (x) depends only on k coordinates and  Wang et al. (2017)  applies Lasso to find the important set of coordinates, whereas  Balasubramanian & Ghadimi (2018)  simply change the step size to achieve an O(k(log(n)/ ) 2 ) iteration complexity. Other papers as- sume more generally that f (x) = g(P A x) only depends on a k-dimensional subspace given by the range of P A and  Djolonga et al. (2013)  apply low-rank approximation to find the low-dimensional subspace while  Wang et al. (2013)  use random embeddings.  Hazan et al. (2017)  assume that f (x) is a sparse collection of k-degree monomials on the Boolean hypercube and apply sparse recov- ery to achieve a O(n k ) runtime bound. We will show that under the case that f (x) = g(P A x), our algorithm will inherently pick up any low-dimensional structure in f (x) and achieve a con- vergence rate that depends on k log(n). This initial convergence rate survives, even if we perturb f (x) = g(P A x) + h(x), so long as h(x) is sufficiently small. We will not cover the whole variety of black-box optimization methods, such as Bayesian opti- mization or genetic algorithms. In general, these methods attempt to solve a broader problem (e.g. multiple optima), have weaker theoretical guarantees and may require substantial computation at each step: e.g. Bayesian optimization generally has theoretical iteration complexities that grow ex- ponentially in dimension, and CMA-ES lacks provable complexity bounds beyond convex quadratic functions. In addition to the slow runtime and weaker guarantees, Bayesian optimization assumes the success of an inner optimization loop of the acquisition function. This inner optimization is often implemented with many iterations of a simpler zeroth-order methods, justifying the need to understand gradient-less descent algorithms within its own context.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new method called \"Search with Amortized Value Estimates\" (SAVE) which combines real experience with the results of past searches to improve overall performance and reduce planning cost. SAVE uses Monte-Carlo Tree Search (MCTS) to estimate the Q-values at encountered states, which are then used to fit a Q-function. This Q-function is used as a prior for subsequent searches, resulting in a symbiotic relationship between model-free learning and MCTS. At test time, SAVE uses MCTS guided by the learned prior to produce effective behavior, even with very small search budgets and in environments with tens of thousands of possible actions per state.",
        "Abstract": "We introduce \"Search with Amortized Value Estimates\" (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which we demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE consistently achieves higher rewards with fewer training steps, and---in contrast to typical model-based search approaches---yields strong performance with very small search budgets. By combining real experience with information computed during search, SAVE demonstrates that it is possible to improve on both the performance of model-free learning and the computational cost of planning.",
        "Introduction": "  INTRODUCTION Model-based methods have been at the heart of reinforcement learning (RL) since its inception ( Bellman, 1957 ), and have recently seen a resurgence in the era of deep learning, with powerful function approximators inspiring a variety of effective new approaches ( Silver et al., 2018 ;  Chua et al., 2018 ;  Hamrick, 2019 ; Wang et al., 2019). Despite the success of model-free RL in reaching state-of-the-art performance in challenging domains (e.g.  Kapturowski et al., 2018 ;  Haarnoja et al., 2018 ), model-based methods hold the promise of allowing agents to more flexibly adapt to new situations and efficiently reason about what will happen to avoid potentially bad outcomes. The two key components of any such system are the model, which captures the dynamics of the world, and the planning algorithm, which chooses what computations to perform with the model in order to produce a decision or action ( Sutton & Barto, 2018 ). Much recent work on model-based RL places an emphasis on model learning rather than plan- ning, typically using generic off-the-shelf planners like Monte-Carlo rollouts or search (see  Ham- rick (2019) ; Wang et al. (2019) for recent surveys). Yet, with most generic planners, even a perfect model of the world may require large amounts of computation to be effective in high-dimensional, sparse reward settings. For example, recent methods which use Monte-Carlo Tree Search (MCTS) require 100s or 1000s of model evaluations per action during training, and even upwards of a million simulations per time step at test time ( Anthony et al., 2017 ;  Silver et al., 2018 ). These large search budgets are required, in part, because much of the computation performed during planning-such Published as a conference paper at ICLR 2020 as the estimation of action values-is coarsely summarized in behavioral traces such as visit counts ( Anthony et al., 2017 ;  Silver et al., 2018 ), or discarded entirely after an action is selected ( Bapst et al., 2019 ;  Azizzadenesheli et al., 2018 ). However, large search budgets are a luxury that is not always available: many real-world simulators are expensive and may only be feasible to query a handful of times. In this paper, we explore preserving the value estimates that were computed by search by amortizing them via a neural network and then using this network to guide future search, resulting in an approach which works well even with very small search budgets. We propose a new method called \"Search with Amortized Value Estimates\" (SAVE) which uses a combination of real experience as well as the results of past searches to improve overall performance and reduce planning cost. During training, SAVE uses MCTS to estimate the Q-values at encoun- tered states. These Q-values are used along with real experience to fit a Q-function, thus amortizing the computation required to estimate values during search. The Q-function is then used as a prior for subsequent searches, resulting in a symbiotic relationship between model-free learning and MCTS. At test time, SAVE uses MCTS guided by the learned prior to produce effective behavior, even with very small search budgets and in environments with tens of thousands of possible actions per state-settings which are very challenging for traditional planners.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an experiment to investigate whether theorem proving can be performed entirely in a latent space, without relying on hard algorithms. We build on HOList, an environment and benchmark for automated theorem provers based on deep learning, and train a neural network to map mathematical formulas into a latent space of fixed dimension. We evaluate the feasibility of reasoning in latent space over two steps, and show that even after nine steps of reasoning purely in latent space, neural networks show non-trivial reasoning capabilities.",
        "Abstract": "We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.",
        "Introduction": "  INTRODUCTION Automated reasoning has long been considered to require development of logics and \"hard\" algo- rithms, such as backtracking search. Recently, approaches that employ deep learning have also been applied, but these have focused on predicting the next step of a proof, which is again executed with a hard algorithm ( Loos et al., 2017 ;  Gauthier et al., 2017 ;  Lederman et al., 2018 ;  Bansal et al., 2019b ). We raise the question of whether hard algorithms could be omitted from this process and mathe- matical reasoning performed entirely in the latent space. To this end, we investigate whether we can predict useful latent representations of the mathematical formulas that result from proof steps. Ideally, we could rely entirely on predicted latent representations to sketch out proofs and only go back to the concrete mathematical formulas to check if our intuitive reasoning was correct. This would allow for more flexible and robust system designs for automated reasoning. In this work, we present a first experiment indicating directly that theorem proving in the latent space might be possible. We build on HOList, an environment and benchmark for automated the- orem provers based on deep learning ( Bansal et al., 2019b ), which makes use of the interactive theorem prover HOL Light ( Harrison, 1996 ), an interactive proof assistant. The HOList theorem database comprises over 19 thousand theorems and lemmas from a variety of mathematical do- mains, including topology, multivariate calculus, real and complex analysis, geometric algebra, and measure theory. Concrete examples include basic properties of real and complex numbers such as (x −1 = y) ⇔ (x = y −1 ), and also well-known theorems, such as Pythagoras' theorem, the funda- mental theorem of calculus, Skolem's theorem, Abel's theorem for complex power series, and that the eigenvalues of a complex matrix are the roots of its characteristic polynomial. We focus on rewrite rules (or rewrites in short). Rewrites are only one of several proof tactics in HOL Light, but they enable powerful transformations on mathematical formulas, as they can be given arbitrary theorems as parameters. For example, the formula 3 2 = z can be rewritten to 3·3 = z by performing a rewrite with the parameter x 2 = x · x. Alternatively, a rewrite may diverge (as it operates recursively) or it may return the same formula - in both these cases we consider the rewrite Published as a conference paper at ICLR 2020 Formula 1 ℝ 1024 Param. 1 Formula 2 ℝ 1024 Param. 2 Formula 3 ℝ 1024 Param. 3 Formula 4 ℝ 1024 In our experiments, we first train a neural network to map mathematical formulas into a latent space of fixed dimension. This network is trained by predicting - based on the latent representation being trained - whether a given rewrite is going to succeed (i.e. returns with a new formula). For successful rewrites we also predict the latent representation of the resulting formula. To evaluate the feasibility of reasoning in latent space over two steps, we first predict the latent representation of the result of a rewrite, then we evaluate whether the predicted latent representation still allows for accurate predictions of the rewrite success of the resulting formula. For multi-step reasoning beyond two steps, we predict the future latent representations based on the previous latent representation only - without seeing the intermediate formula (cf.  Figure 1 ). Our experiments suggest that even after nine steps of reasoning purely in latent space, neural networks show non-trivial reasoning capabilities, despite not being trained on this task directly.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a unified theory connecting node embeddings and structural graph representations. We provide a visual example to illustrate the differences between positional node embeddings and structural representations over node classification and link prediction tasks. We discuss the implications of this unified theory for applications such as multi-ary relationships, clustering, natural language processing, knowledge acquisition, graph classification, and role discovery.",
        "Abstract": "This work provides the first unifying theoretical framework for node (positional) embeddings and structural graph representations, bridging methods like matrix factorization and graph neural networks. Using invariant theory, we show that relationship between structural representations and node embeddings is analogous to that of a distribution and its samples. We prove that all tasks that can be performed by node embeddings can also be performed by structural representations and vice-versa. We also show that the concept of transductive and inductive learning is unrelated to node embeddings and graph representations, clearing another source of confusion in the literature. Finally, we introduce new practical guidelines to generating  and  using  node  embeddings, which further augments standard operating procedures used today.",
        "Introduction": "  INTRODUCTION The theory of structural graph representations is a recently emerging field. It creates a link between relational learning and invariant theory. Interestingly, or rather unfortunately, there is no unified theory connecting node embeddings -low-rank matrix approximations, factor analysis, latent se- mantic analysis, etc.- with structural graph representations. Instead, conflicting interpretations have manifested over the last few years, that further confound practitioners and researchers alike. For instance, consider the direction, word embeddings → structural representations, where the struc- tural equivalence between men → king and women → queen is described as being obtained by just adding or subtracting their node embeddings (positions in the embedding space) (Arora et al., 2016; Mikolov et al., 2013). Hence, can all (positional) node embeddings provide structural relationships akin to word analogies? We provide a visual example in Appendix (Section 7) using the food web of  Figure 1 . In the opposite direction, structural representations → node embeddings, graph neural networks (GNNs) are often optimized to predict edges even though their structural node represen- tations are provably incapable of performing the task. For instance, the node representations of the lynx and the orca in  Figure 1  are indistinguishable due to an isomorphic equivalence between the nodes, making any edge prediction task that distinguishes the edges of lynx and orca a seemly futile exercise (see Appendix (Section 7) for more details). Hence, are structural representations in gen- eral -and GNNs in particular- fundamentally incapable of performing link (dyadic) and multi-ary (polyadic) predictions tasks? GNNs, however, can perform node classification tasks, which is a task not associated with positional node embeddings (see Appendix (Section 7) for a concrete visual interpretation of the differences between positional node embeddings and structural representations over node classification and link prediction tasks). Confirmation bias has seemingly appeared to thwart recent efforts to bring node embeddings and structural representations into a single overarching framework. Preconceived notions of the two being fundamentally different (see Appendix (Section 7)) have been reinforced in the existing lit- erature, arguing they belong in different applications: (Positional) Node embeddings would find applications in multi-ary relationships such as link prediction, clustering, and natural language pro- cessing and knowledge acquisition through word and entity embeddings. Structural representations would find applications in node classification, graph classification, and role discovery. A unified theory is required if we wish to eliminate these artificial boundaries, and better cross-pollinate, node embeddings and structural representations in novel techniques.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces DrKIT, an efficient, end-to-end differentiable framework for complex question answering over a large text corpus. DrKIT uses a query-independent encoding of the corpus and a maximum inner product search (MIPS) to retrieve the top-K spans from the index. The output is a sparse-vector representing the weighted set of entities, aggregated over entity mentions in the top-K spans. DrKIT is tested on the MetaQA benchmark for complex question answering, a new dataset of multi-hop slot-filling over Wikipedia articles, and multi-hop information retrieval on the HotpotQA dataset. Results show that DrKIT outperforms prior text-based systems and is up to 15x faster than existing state-of-the-art multi-hop and open-domain QA systems.",
        "Abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. On HotpotQA, DrKIT leads to a 10% improvement over a BERT-based re-ranking approach to retrieving the relevant passages required to answer a question. DrKIT is also very efficient, processing up to 10-100x more queries per second than existing multi-hop systems.",
        "Introduction": "  INTRODUCTION Large knowledge bases (KBs), such as Freebase and WikiData, organize information around entities, which makes it easy to reason over their contents. For example, given a query like \"When was the Grateful Dead's lead singer born?\", one can identify the entity Grateful Dead and the path of relations LeadSinger, BirthDate to efficiently extract the answer-provided that this information is present in the KB. Unfortunately, KBs are often incomplete (Min et al., 2013). While relation extraction methods can be used to populate KBs, this process is inherently error-prone, expensive and slow. Advances in open-domain QA (Moldovan et al., 2002; Yang et al., 2019) suggest an alternative- instead of performing relation extraction, one could treat a large corpus as a virtual KB by answering queries with spans from the corpus. This ensures facts are not lost in the relation extraction process, but also poses challenges. One challenge is that it is relatively expensive to answer questions using QA models which encode each document in a query-dependent fashion (Chen et al., 2017; Devlin et al., 2019)-even with modern hardware (Strubell et al., 2019; Schwartz et al., 2019). The cost of QA is especially problematic for certain complex questions, such as the example question above. If the passages stating that \"Jerry Garcia was the lead singer of the Grateful Dead\" and \"Jerry Garcia was born in 1942\" are far apart in the corpus, it is difficult for systems that retrieve and read a single passage to find an answer-even though in this example, it might be easy to answer the question after the relations were explicitly extracted into a KB. More generally, complex questions involving sets of entities or paths of relations may require aggregating information from multiple documents, which is expensive. Published as a conference paper at ICLR 2020 contextual representations and then indexed for fast retrieval. Natural language questions are then answered by converting them into vectors that are used to perform maximum inner product search (MIPS) against the index. This can be done efficiently using approximate algorithms (Shrivastava & Li, 2014). However, this approach cannot be directly used to answer complex queries, since by construction, the information stored in the index is about the local context around a span-it can only be used for questions where the answer can be derived by reading a single passage. This paper addresses this limitation of phrase-indexed question answering. We introduce an efficient, end-to-end differentiable framework for doing complex QA over a large text corpus that has been encoded in a query-independent manner. Specifically, we consider \"multi-hop\" complex queries which can be answered by repeatedly executing a \"soft\" version of the operation below, defined over a set of entities X and a relation R: In past work soft, differentiable versions of this operation were used to answer multi-hop questions against an explicit KB (Cohen et al., 2019). Here we propose a more powerful neural module which approximates this operation against an indexed corpus (a virtual KB). In our module, the input X is a sparse-vector representing a weighted set of entities, and the relation R is a dense feature vector, e.g. a vector derived from a neural network over a natural language query. X and R are used to construct a MIPS query used for retrieving the top-K spans from the index. The output Y is another sparse-vector representing the weighted set of entities, aggregated over entity mentions in the top-K spans. We discuss pretraining schemes for the index in §2.3. For multi-hop queries, the output entities Y can be recursively passed as input to the next iteration of the same module. The weights of the entities in Y are differentiable w.r.t the MIPS queries, which allows end-to-end learning without any intermediate supervision. We discuss an implementation based on sparse-matrix-vector products, whose runtime and memory depend only on the number of spans K retrieved from the index. This is crucial for scaling up to large corpora, providing up to 15x faster inference than existing state-of-the-art multi-hop and open-domain QA systems. The system we introduce is called DrKIT (for Differentiable Reasoning over a Knowledge base of Indexed Text). We test DrKIT on the MetaQA benchmark for complex question answering, and show that it improves on prior text-based systems by 5 points on 2-hop and 9 points on 3-hop questions, reducing the gap between text-based and KB-based systems by 30% and 70%, respectively. We also test DrKIT on a new dataset of multi-hop slot-filling over Wikipedia articles, and show that it outperforms DrQA (Chen et al., 2017) and PIQA (Seo et al., 2019) adapted to this task. Finally, we apply DrKIT to multi-hop information retrieval on the HotpotQA dataset (Yang et al., 2018), and show that it significantly improves over a BERT-based reranking approach, while being 10x faster.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a sparse scattering network architecture for image classification, which outperforms AlexNet on ImageNet 2012. The classification is performed over a sparse code computed with a single learned dictionary of scattering coefficients. A new dictionary learning algorithm with homotopy sparse coding is optimized by gradient descent in a deep convolutional network, and is proved to converge exponentially under appropriate assumptions. The architecture and mathematical properties of each element of the sparse scattering network are discussed.",
        "Abstract": "We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations.\nA sparse $\\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet.",
        "Introduction": "  INTRODUCTION Deep convolutional networks have spectacular applications to classification and regression ( LeCun et al., 2015 ), but they are black boxes that are hard to analyze mathematically because of their architecture complexity. Scattering transforms are simplified convolutional neural networks with wavelet filters which are not learned ( Bruna & Mallat, 2013 ). They provide state-of-the-art clas- sification results among predefined or unsupervised representations, and are nearly as efficient as learned deep networks on relatively simple image datasets, such as digits in MNIST, textures ( Bruna & Mallat, 2013 ) or small CIFAR images ( Oyallon & Mallat, 2014 ;  Mallat, 2016 ). However, over complex datasets such as ImageNet, the classification accuracy of a learned deep convolutional net- work is much higher than a scattering transform or any other predefined representation ( Oyallon et al., 2019 ). A fundamental issue is to understand the source of this improvement. This paper addresses this question by showing that one can reduce the learning to a single dictionary matrix, which is used to compute a positive sparse 1 code. The resulting algorithm is implemented with a simplified convolutional neural network architecture illustrated in  Figure 1 . The classifier input is a positive 1 sparse code of scattering coefficients cal- culated in a dictionary D. The matrix D is learned together with the classifier by minimizing a clas- sification loss over a training set. We show that learning D improves the performance of a scattering representation considerably and is sufficient to reach a higher accuracy than AlexNet ( Krizhevsky et al., 2012 ) over ImageNet 2012. This cascade of well understood mathematical operators provides a simplified mathematical model to analyze optimization and classification performances of deep neural networks. Dictionary learning for classification was introduced in  Mairal et al. (2009)  and implemented with deep convolutional neural network architectures by several authors ( Sulam et al., 2018 ;  Mahdizade- haghdam et al., 2019 ;  Sun et al., 2018 ). To reach good classification accuracies, these networks cascade several dictionary learning blocks. As a result, there is no indication that these operators compute optimal sparse 1 codes. These architectures are thus difficult to analyze mathematically and involve heavy calculations. They have only been applied to small image classification problems such as MNIST or CIFAR, as opposed to ImageNet. Our architecture reaches a high classification performance on ImageNet with only one dictionary D, because it is applied to scattering coefficients as opposed to raw images. Intra-class variabilities due to geometric image transformations such as translations or small deformations are linearized by a scattering transform ( Bruna & Mallat, 2013 ), which avoids unnecessary learning. Learning a dictionary in a deep neural network requires to implement a sparse 1 code. We show that homotopy iterative thresholding algorithms lead to more efficient sparse coding implementations with fewer layers. We prove their exponential convergence in a general framework that includes the ALISTA ( Liu et al., 2019 ) algorithm. The main contributions of the paper are summarized below: • A sparse scattering network architecture, illustrated in  Figure 1 , where the classification is performed over a sparse code computed with a single learned dictionary of scattering coefficients. It outperforms AlexNet over ImageNet 2012. • A new dictionary learning algorithm with homotopy sparse coding, optimized by gradient descent in a deep convolutional network. If the dictionary is sufficiently incoherent, the homotopy sparse coding error is proved to convergence exponentially. We explain the implementation and mathematical properties of each element of the sparse scattering network. Section 2 briefly reviews multiscale scattering transforms. Section 3 introduces homotopy dictionary learning for classification, with a proof of exponential convergence under appropriate assumptions. Section 4 analyzes image classification results of sparse scattering networks on Ima- geNet 2012.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a neural network-based reconstruction algorithm for cryo-electron microscopy (cryo-EM) that learns a continuous low-dimensional manifold over a protein's conformational states from unlabeled 2D images. The algorithm uses an image encoder-volume decoder neural network architecture and formulates the decoder as a function of 3D Cartesian coordinates and unconstrained latent variables. All inference is performed in Fourier space, and the unconstrained latent variables are trained in the standard variational autoencoder approach. Results are presented on both real and simulated cryo-EM data.",
        "Abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the 3D structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented 2D projection images. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes structures in Fourier space using coordinate-based deep neural networks, and trains these networks from unlabeled 2D cryo-EM images by combining exact inference over image orientation with variational inference for structural heterogeneity. We demonstrate that the proposed method, termed cryoDRGN, can perform ab-initio reconstruction of 3D protein complexes from simulated and real 2D cryo-EM image data. To our knowledge, cryoDRGN is the first neural network-based approach for cryo-EM reconstruction and the first end-to-end method for directly reconstructing continuous ensembles of protein structures from cryo-EM images.",
        "Introduction": "  INTRODUCTION Cryo-electron microscopy (cryo-EM) is a Nobel Prize-winning technique capable of determining the structure of proteins and macromolecular complexes at near-atomic resolution. In a single particle cryo-EM experiment, a purified solution of the target protein or biomolecular complex is frozen in a thin layer of vitreous ice and imaged at sub-nanometer resolution using an electron microscope. After initial preprocessing and segmentation of the raw data, the dataset typically comprises 10 4−7 noisy projection images. Each image contains a separate instance of the molecule, recorded as the molecule's electron density integrated along the imaging axis ( Figure 1 ). A major bottleneck in cryo-EM structure determination is the computational task of 3D reconstruction, where the goal is to solve the inverse problem of learning the structure, i.e. the 3D electron density volume, which gave rise to the projection images. Unlike classic tomographic reconstruction (e.g. MRI), cryo- EM reconstruction is complicated by the unknown orientation of each copy of the molecule in the ice. Furthermore, cryo-EM reconstruction algorithms must handle challenges such as an extremely low signal to noise ratio (SNR), unknown in-plane translations, imperfect signal transfer due to microscope optics, and discretization of the measurements. Despite these challenges, continuing advances in hardware and software have enabled structure determination at near-atomic resolution for rigid proteins ( Kühlbrandt (2014) ;  Scheres (2012b) ;  Renaud et al. (2018) ;  Li et al. (2013) ). Many proteins and other biomolecules are intrinsically flexible and undergo large conformational changes to perform their function. Since each cryo-EM image contains a unique instance of the molecule of interest, cryo-EM has the potential to resolve structural heterogeneity, which is experi- mentally infeasible with other structural biology techniques such as X-ray crystallography. However, this heterogeneity poses a substantial challenge for reconstruction as each image is no longer of the same structure. Traditional reconstruction algorithms address heterogeneity with discrete clustering Published as a conference paper at ICLR 2020 approaches, however, protein conformations are continuous and may be poorly approximated with discrete clusters ( Malhotra & Udgaonkar (2016) ;  Nakane et al. (2018) ). Here, we introduce a neural network-based re- construction algorithm that learns a continu- ous low-dimensional manifold over a protein's conformational states from unlabeled 2D cryo- EM images. We present an end-to-end learning framework for a generative model over 3D vol- umes using an image encoder-volume decoder neural network architecture. Extending spatial- VAE, we formulate our decoder as a function of 3D Cartesian coordinates and unconstrained la- tent variables representing factors of image vari- ation that we expect to result from protein struc- tural heterogeneity ( Bepler et al. (2019) ). All inference is performed in Fourier space, which allows us to efficiently relate 2D projections to 3D volumes via the Fourier slice theorem. By formulating our decoder as a function of Cartesian coordinates, we can explicitly model the imag- ing operation to disentangle the orientation of the molecule during imaging from intrinsic protein structural heterogeneity. Our learning framework avoids errant local minima in image orientation by optimizing with exact inference over a discretization of SO(3) × R 2 using a branch and bound algorithm. The unconstrained latent variables are trained in the standard variational autoencoder approach. We present results on both real and simulated cryo-EM data.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a new training method which combines ideas from adversarial training and provable defense methods to produce accurate and robust neural networks with provable guarantees. The proposed method works by producing a convex relaxation of all possible intermediate vector outputs in the neural network, then an adversary searches over this convex region to find a latent adversarial example which when propagated through the network causes a misclassification which prevents verification. The resulting latent adversarial examples are then incorporated into the training scheme using adversarial training. Experiments on the challenging CIFAR-10 dataset show that the proposed method results in a neural network with state-of-the-art 78.4% accuracy and 60.5% certified robustness with 2/255 L ∞ perturbation.",
        "Abstract": "We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses. The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail. We experimentally show that this training method, named convex layerwise adversarial training (COLT), is promising and achieves the best of both worlds -- it produces a state-of-the-art neural network with certified robustness of 60.5% and accuracy of 78.4% on the challenging CIFAR-10 dataset with a 2/255 L-infinity perturbation. This significantly improves over the best concurrent results of 54.0% certified robustness and 71.5% accuracy.\n   \n",
        "Introduction": "  INTRODUCTION The discovery of adversarial examples in deep learning (Szegedy et al., 2013; Biggio et al., 2013) has increased the importance of creating new training methods which produce accurate and robust neural networks with provable guarantees. Existing work: adversarial and provable defenses Adversarial training (Goodfellow et al., 2015; Kurakin et al., 2017) provides a framework to augment the training procedure with adversarial inputs produced by an adversarial attack. Madry et al. (2018) instantiated adversarial training using a strong iterative adversary and showed that their approach can train models which are highly ro- bust against the strongest known adversarial attacks such as Carlini & Wagner (2017). This method has also been able to train robust ImageNet models (Xie et al., 2019). While promising, the main drawback of the method is that when instantiated in practice, via an approximation of an otherwise intractable optimization problem, it provides no guarantees - it does not produce a certificate that there are no possible adversarial attacks which could potentially break the model. To address this lack of guarantees, recent line of work on provable defenses (Wong & Kolter, 2018; Raghunathan et al., 2018; Mirman et al., 2018) has proposed to train neural networks that are certifiably robust to a specific attacker threat model. However, these guarantees come at the cost of a significantly lower standard accuracy than models trained using adversarial training. This setting raises a natural question: can we leverage ideas from both, adversarial training techniques and provable defense methods, so to obtain models with high accuracy and certified robustness? This work: combining adversarial and provable defenses In this work, we take a step towards addressing this challenge. We show that it is possible to train more accurate and provably robust neu- ral networks using the same convex relaxations as those used in existing, state-of-the-art provable defense methods, but with a new, different optimization procedure inspired by adversarial training. Our optimization works as follows: (i) to certify a property (e.g., robustness) of the network, the ver- ifier produces a convex relaxation of all possible intermediate vector outputs in the neural network, then (ii) an adversary now searches over this (intermediate) convex region in order to find, what we refer to as a latent adversarial example - a concrete intermediate input contained in the relaxation that when propagated through the network causes a misclassification which prevents verification, and finally (iii) the resulting latent adversarial examples are now incorporated into our training scheme Published as a conference paper at ICLR 2020 using adversarial training. Overall, we can see this method as bridging the gap between adversarial training and provable defenses (it can conceptually be instantiated with any convex relaxation). We experimentally show that the method is promising and results in a neural network with state-of-the- art 78.4% accuracy and 60.5% certified robustness on the challenging CIFAR-10 dataset with 2/255 L ∞ perturbation (the best known existing results are 71.5% accuracy and 54.0% certified robustness from concurrent work of Zhang et al. (2020)).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel semi-supervised Latent Normalizing Flows for Many-to-Many Mappings (LNFMM) framework for joint image-text representation. The framework leverages normalizing flows to capture complex joint distributions in the latent space and encodes both shared cross-domain information as well as domain-specific information. Experiments on the COCO dataset show that the proposed model outperforms the current state of the art for image captioning and text-to-image synthesis tasks in terms of accuracy and diversity metrics.",
        "Abstract": "Learned joint representations of images and text form the backbone of several important cross-domain tasks such as image captioning. Prior work mostly maps both domains into a common latent representation in a purely supervised fashion. This is rather restrictive, however, as the two domains follow distinct generative processes. Therefore, we propose a novel semi-supervised framework, which models shared information between domains and domain-specific information separately. \nThe information shared between the domains is aligned with an invertible neural network. Our model integrates normalizing flow-based priors for the domain-specific information, which allows us to learn diverse many-to-many mappings between the two domains. We demonstrate the effectiveness of our model on diverse tasks, including image captioning and text-to-image synthesis.",
        "Introduction": "  INTRODUCTION Joint image-text representations find applica- tion in cross-domain tasks such as image- conditioned text generation (captioning;  Mao et al., 2015 ;  Karpathy & Fei-Fei, 2017 ;  Xu et al., 2018 ) and text-conditioned image synthe- sis ( Reed et al., 2016 ). Yet, image and text dis- tributions follow distinct generative processes, making joint generative modeling of the two distributions challenging. Current state-of-the-art models for learning joint image-text distributions encode the two domains in a common shared latent space in a fully supervised setup ( Gu et al., 2018 ;  Wang et al., 2019 ). While such approaches can model supervised information in the shared latent space, they do not preserve domain-specific information. However, as the domains under consideration, e.g. im- ages and texts, follow distinct generative processes, many-to-many mappings naturally emerge - there are many likely captions for a given image and vice versa. Therefore, it is crucial to also encode domain-specific variations in the latent space to enable many-to-many mappings. State-of-the-art models for cross-domain synthesis leverage conditional variational autoencoders (VAEs, cVAEs;  Kingma & Welling, 2014 ) or generative adversarial networks (GANs;  Goodfellow et al., 2014 ) for learning conditional distributions. However, such generative models (e.g.,  Wang et al., 2017 ;  Aneja et al., 2019 ) enforce a Gaussian prior in the latent space. Gaussian priors can result in strong regularization or posterior collapse as they impose stringent constraints while mod- eling complex distributions in the latent space ( Tomczak & Welling, 2018 ). This severely limits the accuracy and diversity of the cross-domain generative model. Recent work ( Ziegler & Rush, 2019 ;  Bhattacharyya et al., 2019 ) has found normalizing flows ( Dinh et al., 2015 ) advantageous for modeling complex distributions in the latent space. Normalizing flows can capture a high degree of multimodality in the latent space through a series of transformations from a simple distribution to a complex data-dependent prior.  Ziegler & Rush (2019)  apply nor- malizing flow-based priors in the latent space of unconditional variational autoencoders for discrete distributions and character-level modeling. We propose to leverage normalizing flows to overcome the limitations of existing cross-domain generative models in capturing heterogeneous distributions and introduce a novel semi-supervised Latent Normalizing Flows for Many-to-Many Mappings (LNFMM) framework. We exploit normal- izing flows ( Dinh et al., 2015 ) to capture complex joint distributions in the latent space of our model ( Fig. 1 ). Moreover, since the domains under consideration, e.g. images and texts, have different gen- erative processes, the latent representation for each distribution is modeled such that it contains both shared cross-domain information as well as domain-specific information. The latent dimensions constrained by supervised information from paired data model the common (semantic) information across images and texts. The diversity within the image and text distributions, e.g. different visual or textual styles, are encoded in the residual latent dimensions, thus preserving domain-specific vari- ation. We can hence synthesize diverse samples from a distribution given a reference point in the other domain in a many-to-many setup. We show the benefits of our learned many-to-many latent spaces for real-world image captioning and text-to-image synthesis tasks on the COCO dataset ( Lin et al., 2014 ). Our model outperforms the current state of the art for image captioning w.r.t. the Bleu and CIDEr metrics for accuracy as well as on various diversity metrics. Additionally, we also show improvements in diversity metrics over the state of the art in text-to-image generation.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a causal framework to explore modularity in deep generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). The framework is based on the causal principle of Independent Mechanisms, which states that the causal mechanisms contributing to the overall generating process do not influence nor inform each other. The paper uses counterfactuals to assess the role of specific internal variables in the overall functioning of trained deep generative models, along with a rigorous definition of disentanglement in a causal framework. Empirical results show that VAEs and GANs trained on image databases exhibit modularity of their hidden units, encoding different features and allowing counterfactual editing of generated images.",
        "Abstract": "Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to \\textit{disentangle} latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network, based on counterfactual manipulations. Our experiments support that modularity between groups of channels is achieved to a certain degree on a variety of generative models. This allowed the design of targeted interventions on complex image datasets, opening the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.",
        "Introduction": "  INTRODUCTION Deep generative models, by learning a non-linear function mapping a latent space to a space of observations, have proven successful at designing realistic images in a variety of complex domains (objects, animals, human faces, interior scenes). In particular, two kinds of approaches emerged as state-of-the-art (SOTA): Generative Adversarial Networks (GAN) ( Goodfellow et al., 2014 ), and Variational Autoencoders (VAE) ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ). Efforts have been made to have such models produce disentangled latent representations that can control interpretable properties of images ( Kulkarni et al., 2015 ;  Higgins et al., 2017 ). However, the resulting models are not necessarily mechanistic (or causal) in the sense that interpretable properties of an image cannot be ascribed to a particular part, a module, of the network architecture. Gaining access to a modular organization of generative models would benefit the interpretability and allow extrapolations, such as generating an object in a background that was not previously associated with this object, as illustrated in a preview of our experimental results in  Fig. 1 . Such extrapolations are an integral part of human representational capabilities (consider common expressions such as \"like an elephant in a china shop\") and consistent with the modular organization Published as a conference paper at ICLR 2020 of its visual system, comprising specialized regions encoding objects, faces and places (see e.g.  Grill- Spector & Malach (2004) ). Extrapolations moreover likely support adaptability to environmental changes and robust decision making ( Dvornik et al., 2018 ). How to leverage trained deep generative architectures to perform such extrapolations is an open problem, largely due to the non-linearities and high dimensionality that prevent interpretability of computations performed in successive layers. In this paper, we propose a causal framework to explore modularity, which relates to the causal principle of Independent Mechanisms, stating that the causal mechanisms contributing to the overall generating process do not influence nor inform each other ( Peters et al., 2017 ). 1 We study the effect of direct interventions in the network from the point of view that the mechanisms involved in generating data can be modified individually without affecting each other. This principle can be applied to generative models to assess how well they capture a causal mechanism ( Besserve et al., 2018a ). Causality allows to assay how an outcome would have changed, had some variables taken different values, referred to as a counterfactual ( Pearl, 2009 ;  Imbens & Rubin, 2015 ). We use counterfactuals to assess the role of specific internal variables in the overall functioning of trained deep generative models, along with a rigorous definition of disentanglement in a causal framework. Then, we analyze this disentanglement in implemented models based on unsupervised counterfactual manipulations. We show empirically how VAEs and GANs trained on image databases exhibit modularity of their hidden units, encoding different features and allowing counterfactual editing of generated images.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces NAS-Bench-1Shot1, a novel benchmarking framework for one-shot neural architecture search (NAS) methods. It allows for the reuse of the extreme amount of compute time that went into generating NAS-Bench-101 (Ying et al., 2019) to cheaply benchmark one-shot NAS methods. The paper also introduces a general framework for one-shot NAS methods that can be instantiated to many recent one-shot NAS variants, enabling fair head-to-head evaluations based on a single code base. Experiments are conducted to compare several state-of-the-art one-shot NAS methods, assess the correlation between their one-shot model performance and final test performance, examine how sensitive they are to their hyperparameters, and compare their performance to that of black-box optimizers used in NAS-Bench-101. An open-source implementation is provided.",
        "Abstract": "One-shot neural architecture search (NAS) has played a crucial role in making\nNAS methods computationally feasible in practice. Nevertheless, there is still a\nlack of understanding on how these weight-sharing algorithms exactly work due\nto the many factors controlling the dynamics of the process. In order to allow\na scientific study of these components, we introduce a general framework for\none-shot NAS that can be instantiated to many recently-introduced variants and\nintroduce a general benchmarking framework that draws on the recent large-scale\ntabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot\nNAS methods. To showcase the framework, we compare several state-of-the-art\none-shot NAS methods, examine how sensitive they are to their hyperparameters\nand how they can be improved by tuning their hyperparameters, and compare their\nperformance to that of blackbox optimizers for NAS-Bench-101.",
        "Introduction": "  INTRODUCTION While neural architecture search (NAS) has attracted a lot of attention due to the effectiveness in automatically designing state-of-the-art neural networks (Zoph & Le, 2017; Zoph et al., 2018; Real et al., 2017; 2019), the focus has recently shifted to making the search process more efficient (Pham et al., 2018; Elsken et al., 2019; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019; Casale et al., 2019). The most crucial concept which led to a reduction in search costs to the order of a single function evaluation is certainly the weight-sharing paradigm: Training only a single large architecture (the one-shot model) subsuming all the possible architectures in the search space (Brock et al., 2018; Pham et al., 2018). Despite the great advancements of these methods, the exact results of many NAS papers are often hard to reproduce (Li & Talwalkar, 2019; Yu et al., 2020; Yang et al., 2020). This is a result of several factors, such as unavailable original implementations, differences in the employed search spaces, training or evaluation pipelines, hyperparameter settings, and even pseudorandom number seeds (Lindauer & Hutter, 2019). One solution to guard against these problems would be a common library of NAS methods that provides primitives to construct different algorithm variants, similar to what as RLlib (Liang et al., 2017) offers for the field of reinforcement learning. Our paper makes a first step into this direction. Furthermore, experiments in NAS can be computationally extremely costly, making it virtually im- possible to perform proper scientific evaluations with many repeated runs to draw statistically robust conclusions. To address this issue, Ying et al. (2019) introduced NAS-Bench-101, a large tabular benchmark with 423k unique cell architectures, trained and fully evaluated using a one-time ex- treme amount of compute power (several months on thousands of TPUs), which now allows to cheaply simulate an arbitrary number of runs of NAS methods, even on a laptop. NAS-Bench-101 enabled a comprehensive benchmarking of many discrete NAS optimizers (Zoph & Le, 2017; Real et al., 2019), using the exact same settings. However, the discrete nature of this benchmark does not Published as a conference paper at ICLR 2020 allow to directly benchmark one-shot NAS optimizers (Pham et al., 2018; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019). In this paper, we introduce the first method for making this possible. Specifically, after providing some background (Section 2), we make the following contributions: 1. We introduce NAS-Bench-1Shot1, a novel benchmarking framework that allows us to reuse the extreme amount of compute time that went into generating NAS-Bench-101 (Ying et al., 2019) to cheaply benchmark one-shot NAS methods. Our mapping between search space representations is novel to the best of our knowledge and it allows querying the performance of found architec- tures from one-shot NAS methods, contrary to what is claimed by Ying et al. (2019). Specifically, it allows us to follow the full trajectory of architectures found by arbitrary one-shot NAS methods at each search epoch without the need for retraining them individually, allowing for a careful and statistically sound analysis (Section 3). 2. We introduce a general framework for one-shot NAS methods that can be instantiated to many recent one-shot NAS variants, enabling fair head-to-head evaluations based on a single code base (Section 4). 3. We use the above to compare several state-of-the-art one-shot NAS methods, assess the correla- tion between their one-shot model performance and final test performance, examine how sensitive they are to their hyperparameters, and compare their performance to that of black-box optimizers used in NAS-Bench-101 (Section 5). We provide our open-source implementation 1 , which we expect will also facilitate the reproducibil- ity and benchmarking of other one-shot NAS methods in the future.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents MetaPix, a novel approach to personalization for video retargeting. MetaPix is a meta-learning algorithm that adapts a generic generative model of human actions to a specific person given a few samples of their appearance. The proposed formulation is agnostic to the actual generative model used, and is optimized for efficient adaptation (personalization) given only a few samples and on a computational budget. Results show that MetaPix outperforms models not optimized in this form, and naturally enforces strong temporal coherence in the generated frames.",
        "Abstract": "We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt – or personalize – a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n",
        "Introduction": "  INTRODUCTION One of the hallmarks of human intelligence is the ability to imagine. For example, given an image of a never-before-seen person, one can easily imagine them performing different actions. To do so, we make use of years of experience watching humans act and interact with the world. We implicitly encode the rules of physical transformations of humans, objects, clothing and so on. Crucially, we effortlessly adapt or retarget those universal rules to a specific human and environment - a child on a playground will likely move differently than an adult walking into work. Our goal in this work is to develop models that similarly learn to generate human motions by specializing universal knowledge to a particular target human and target environment, given only a few samples of the target. It is attractive to tackle such video generation tasks using the framework of generative (adversarial) neural networks (GANs). Past work has cast the core computational problem as one of conditional image generation where input source poses (automatically extracted with an off-the-shelf pose estimator) are transcoded into image frames ( Balakrishnan et al., 2018 ;  Siarohin et al., 2018 ;  Ma et al., 2017 ). However, it is notoriously challenging to build generative models that are capable of synthesizing diverse, in-the-wild imagery. Notable exceptions make use of massively-large networks trained on large-scale compute infrastructure ( Brock et al., 2019 ). However, modestly-sized generative networks perform quite well at synthesis of targeted domains (such as faces ( Bansal et al., 2018 ) or facades ( Isola et al., 2017 )). A particularly successful approach to generating from pose-to-image is training of specialized - or personalized - models to particular scenes. These often require large-scale target datasets, such as 20 minutes of footage in a target lab setting ( Chan et al., 2018 ) The above approaches make use of personalization as an implicit but crucial ingredient, by on-the-fly training of a generative model tuned to the particular target domain of interest. Often, personalization is operationalized by fine-tuning a generic model on the specific target frames of interest. Our key insight is recasting personalization as an explicit component of a video-retargeting engine, allowing us to make use of meta-learning to learn how best to fine-tune (or personalize) a generic model to a particular target domain. We demonstrate that (meta)learning-to-fine-tune is particularly effective in the few-shot regime, where few target frames are available. From a technical perspective, one of our contributions is extending meta-learning to GANs, which is nontrivial because both a generator and discriminator need to be adversarially fine-tuned. To that end, we propose MetaPix, a novel approach to personalization for video retargeting. Our formulation treats personalization as a few-shot learning problem, where the task is to adapt a generic generative model of human actions to a specific person given a few samples of their appearance. Our formulation is agnostic to the actual generative model used, and is compatible with both pose- conditioned transfer ( Balakrishnan et al., 2018 ) or generative ( Chan et al., 2018 ) approaches. Taking inspiration from the recent successes of meta-learning approaches for few-shot tasks ( Nichol et al., 2018 ;  Finn et al., 2017 ), we propose a novel formulation by adapting the popular first-order meta- learning algorithm Reptile ( Nichol et al., 2018 ) for jointly learning initial weights for both the generator and discriminator. Hence, our model is optimized for efficient adaptation (personalization), given only a few samples and on a computational budget, and obtains stronger performance compared to a model not optimized in this form. Interestingly, we find this personalized model naturally enforces strong temporal coherence in the generated frames, even though it is not explicitly optimized for that task.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to the verification of Binarized Neural Networks (BNNs). It examines the influence of different architectural design choices on the performance of SAT solvers, and proposes a modified training procedure to make the resulting network easier for logic-based verification tools to reason about. The proposed methods demonstrate significant performance gains over previous work, with more than 10x-20x improvements on tested benchmarks for both verification and quantitative queries.",
        "Abstract": "Analyzing the behavior of neural networks is  one of the most pressing challenges in deep learning.  Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. We propose changes to the BNN architecture and the training procedure to get a simpler network for SAT solvers without sacrificing accuracy on the primary task. Our experimental results demonstrate that our approach scales to larger deep neural networks compared to existing work for existential and probabilistic queries, leading to significant speed ups on all tested datasets.\n",
        "Introduction": "  INTRODUCTION Deep neural networks are among the most successful AI technologies making impact in a variety of practical applications ranging from vision to speech recognition and natural language (Goodfellow et al., 2016). However, many concerns have been raised about the decision making process behind machine learning technology. For instance, can we trust decisions that neural networks make (EU Data Protection Regulation, 2016; Goodman & Flaxman, 2017; NIPS IML Symposium, 2017)? One way to address this problem is to define properties that we expect the network to satisfy. Verifying whether the network satisfies these properties sheds light on the properties of the function that it represents. Verification guarantees can reassure the user that the network behaves as expected. There are two main approaches to neural network analysis. The first approach, the certification of neural networks, trains a verified network that satisfies given properties, e.g. a network that is guaranteed to be robust to adversarial perturbations (Wong & Kolter, 2018; Dvijotham et al., 2018; Raghunathan et al., 2018; Mirman et al., 2018). However, a set of properties must be known in advance, which might not always be possible. Moreover, enforcing a set of properties during the training procedure can significantly affect the accuracy of the network on the primary task. Finally, certification techniques work with relaxation of the original problem and might not be able to certify robust inputs. The second approach, the verification of neural networks, takes a trained network as input and focuses only on the verification task (Katz et al., 2017; Weng et al., 2018; Singh et al., 2019; Xiao et al., 2019). A number of verification frameworks were proposed over the last few years that can be roughly divided into complete (Katz et al., 2017; 2019; Tjeng et al., 2019) and incomplete methods (Weng et al., 2018; Zhang et al., 2018; Singh et al., 2018). As the training and Published as a conference paper at ICLR 2020 verification tasks are separated, a wide set of properties can be checked. However, the scalability of this approach remains an issue, especially for complete methods. In this work we tackle the scalability problem of the complete verification approach, in the context of an important class of networks, Binarized Neural Networks(BNNs) (Hubara et al., 2016). A BNN is an extreme case of quantized neural networks where parameters are primarily binary. These net- works have a number of important features that are useful in resource constrained environments, like embedded devices or mobile phones (McDanel et al., 2017; Kung et al., 2017). They are memory efficient as only one bit per weight must be stored and are computationally efficient as all activations are binary, which enables the use of specialized algorithms for fast binary matrix multiplication. More importantly, these networks admit an exact representation in Boolean logic (Narodytska et al., 2018; Cheng et al., 2018). Such a representation enables us to apply powerful Boolean reasoning tools to the analysis of BNNs. For example, we can perform a rich set of queries, ranging from existential queries (e.g., is there a faulty input to the network?), to counting queries (e.g., how many faulty inputs exist?), using logic-based reasoners (Baluta et al., 2019; Shih et al., 2019). This paper makes two main contributions. • First, we analyze how different architectural design choices for BNNs affect the perfor- mance of SAT solvers. To identify influential parts of the design, we scrutinize BNNs at three levels of granularity: individual neurons, blocks of layers, and network as a whole. Our work continues work of (Narodytska et al., 2018; Cheng et al., 2018) where BNNs were analyzed on the network level. For the block and network levels, we only analyse bottlenecks, propose possible research directions, and position existing work on the net- work level. Our main contribution here is within the network level. • Second, we exploit our findings to train SAT-friendly BNNs. We propose a modified train- ing procedure that makes the resulting network easier for logic-based verification tools, like SAT solvers, to reason about. Modifications to training are crucially performed so that the accuracy of the network is unaffected. Overall, our approach (a) preserves the separation between training and verification, by not committing to a certain property during training and (b) boosts the performance of logic-based verification. We implemented the proposed methods and demonstrated significant performance gains over previous work (Narodytska et al., 2018; Khalil et al., 2019; Baluta et al., 2019). We get more than 10x-20x improve- ments on tested benchmarks for both verification and quantitative queries, e.g., finding the probability that a perturbation yields an adversarial example.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents an approximate Bayesian approach for training Bayesian neural networks (BNNs) incrementally with non-stationary streaming data. The approach consists of a diagonal Gaussian distribution and a running memory, and provides a novel sequential update method for both components. Two alternative adaptation methods are proposed to generalize online variational Bayes with BNNs to non-stationary data. The sequential update method is compared to variational continual learning (VCL) in the online-inference setting on several popular datasets, and the adaptation methods are validated on datasets with concept drift. Results demonstrate that the proposed approach is favorable and provides performance improvements compared to online variational Bayes without adaptation.",
        "Abstract": "This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to better predictive performance for non-stationary data. ",
        "Introduction": "  INTRODUCTION Continual learning (CL), also referred to as lifelong learning, is typically described informally by the following set of desiderata for computational systems: the system should (i) learn incrementally from a data stream, (ii) exhibit information transfer forward and backward in time, (iii) avoid catastrophic forgetting of previous data, and (iv) adapt to changes in the data distribution (Ring, 1997; Silver et al., 2013; Chen & Liu, 2016; Ruvolo & Eaton, 2013; Parisi et al., 2018). The necessity to adapt to non-stationary data is often not reconcilable with the goal of preventing forgetting. This problem is also known as the stability-plasticity dilemma (Grossberg, 1987). The majority of current CL research is conducted in the context of online multi-task learning (Nguyen et al., 2018; Kirkpatrick et al., 2017; Schwarz et al., 2018; Rusu et al., 2016; Fernando et al., 2017), where the main objective is to prevent catastrophic forgetting of previously learned tasks. This focus is reasonable since changes in the statistics of the data distribution are usually an artefact of learning different tasks sequentially. However, changes in the statistics of the data can also be real properties of the data-generating process. Examples include models of energy demand, climate analysis, financial market, or user-behavior analytics (Ditzler et al., 2015). In such applications, the statistics of the current data distribution are of particular interest. Old data may be outdated and can even deteriorate learning if the drift in the data distribution is neglected. Consequently, CL systems for non-stationary data require adaptation methods, which deliberately forget outdated information. In this work, we develop an approximate Bayesian approach for training Bayesian neural networks (BNN) (Hinton & van Camp, 1993; Graves, 2011; Blundell et al., 2015) incrementally with non- stationary streaming data. Similar to variational continual learning (VCL) (Nguyen et al., 2018) and the Virtual Vector Machine (VVM) (Minka et al., 2009), we approximate the posterior using a Gaussian distribution and a complementary memory of previous data. Both components are updated sequentially, while adapting to changes in the data distribution. Our main contributions are as follows: • We propose an online approximation consisting of a diagonal Gaussian distribution and a running memory, and we provide a novel sequential update method for both components. • We extend the online approximation by two alternative adaptation methods, thereby general- ising online variational Bayes with Bayesian neural networks to non-stationary data. We compare our sequential update method to VCL in the online-inference setting on several popular datasets, demonstrating that our approach is favorable. Furthermore, we validate our adaptation methods on several datasets with concept drift (Widmer & Kubat, 1996), showing performance improvements compared to online variational Bayes without adaptation.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a set of metrics to measure the reliability of reinforcement learning (RL) algorithms. These metrics are designed to measure different aspects of reliability, such as reproducibility, stability, dispersion, and risk. The paper also provides practical recommendations for statistical tests to compare metric results and how to report the results. The metrics are applied to a set of algorithms and environments, and the code used in this paper is released as an open-source Python package.",
        "Abstract": "Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) algorithms, especially Deep RL algorithms, tend to be highly variable in performance and considerably sensitive to a range of different factors, including implementation details, hyper-parameters, choice of environments, and even random seeds ( Henderson et al., 2017 ). This variability hinders reproducible research, and can be costly or even dangerous for real-world applications. Furthermore, it impedes scientific progress in the field when practitioners cannot reliably evaluate or predict the performance of any particular algorithm, compare different algorithms, or even compare different implementations of the same algorithm. Recently,  Henderson et al. (2017)  has performed a detailed analysis of reliability for several policy gradient algorithms, while  Duan et al. (2016)  has benchmarked average performance of different continuous-control algorithms. In other related work,  Colas et al. (2018)  have provided a detailed anal- ysis on power analyses for mean performance in RL, and  Colas et al. (2019)  provide a comprehensive primer on statistical testing for mean and median performance in RL. In this work, we aim to devise a set of metrics that measure reliability of RL algorithms. Our analysis distinguishes between several typical modes to evaluate RL performance: \"evaluation during training\", which is computed over the course of training, vs. \"evaluation after learning\", which is evaluated on a fixed policy after it has been trained. These metrics are also designed to measure different aspects of reliability, e.g. reproducibility (variability across training runs and variability across rollouts of a fixed policy) or stability (variability within training runs). Additionally, the metrics capture multiple aspects of variability - dispersion (the width of a distribution), and risk (the heaviness and extremity of the lower tail of a distribution). Standardized measures of reliability can benefit the field of RL by allowing RL practitioners to compare algorithms in a rigorous and consistent way. This in turn allows the field to measure Published as a conference paper at ICLR 2020 progress, and also informs the selection of algorithms for both research and production environments. By measuring various aspects of reliability, we can also identify particular strengths and weaknesses of algorithms, allowing users to pinpoint specific areas of improvement. In this paper, in addition to describing these reliability metrics, we also present practical recommen- dations for statistical tests to compare metric results and how to report the results more generally. As examples, we apply these metrics to a set of algorithms and environments (discrete and continuous, off-policy and on-policy). We have released the code used in this paper as an open-source Python package to ease the adoption of these metrics and their complementary statistics.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel formulation of intrinsic motivation for encouraging synergistic behavior in multi-agent tasks. The proposed formulation leverages the difference between the true effect of an action and the composition of individual-agent predicted effects. Experiments on six simulated robotic tasks demonstrate that this formulation yields more efficient learning than both training with only the sparse reward signal and shaping the reward via the more standard single-agent formulation of intrinsic motivation as \"surprise.\" This work is a step toward general-purpose synergistic multi-agent reinforcement learning.",
        "Abstract": "We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. We study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained concurrently with the policy. While the former is simpler, the latter has the benefit of being analytically differentiable with respect to the action taken. We validate our approach in robotic bimanual manipulation and multi-agent locomotion tasks with sparse rewards; we find that our approach yields more efficient learning than both 1) training with only the sparse reward and 2) using the typical surprise-based formulation of intrinsic motivation, which does not bias toward synergistic behavior. Videos are available on the project webpage: https://sites.google.com/view/iclr2020-synergistic.",
        "Introduction": "  INTRODUCTION Consider a multi-agent environment such as a team of robots working together to play soccer. It is critical for a joint policy within such an environment to produce synergistic behavior, allowing multiple agents to work together to achieve a goal which they could not achieve individually. How should agents learn such synergistic behavior efficiently? A naive strategy would be to learn policies jointly and hope that synergistic behavior emerges. However, learning policies from sparse, binary rewards is very challenging - exploration is a huge bottleneck when positive reinforcement is in- frequent and rare. In sparse-reward multi-agent environments where synergistic behavior is critical, exploration is an even bigger issue due to the much larger action space. A common approach for handling the exploration bottleneck in reinforcement learning is to shape the reward using intrinsic motivation, as was first proposed by  Schmidhuber (1991) . This has been shown to yield improved performance across a variety of domains, such as robotic control tasks ( Oudeyer et al., 2007 ) and Atari games ( Bellemare et al., 2016 ;  Pathak et al., 2017 ). Typically, intrinsic motivation is formulated as the agent's prediction error regarding some aspects of the world; shaping the reward with such an error term incentivizes the agent to take actions that \"surprise it,\" and is intuitively a useful heuristic for exploration. But is this a good strategy for encouraging syn- ergistic behavior in multi-agent settings? Although synergistic behavior may be difficult to predict, it could be equally difficult to predict the effects of certain single-agent behaviors; this formulation of intrinsic motivation as \"surprise\" does not specifically favor the emergence of synergy. In this paper, we study an alternative strategy for employing intrinsic motivation to encourage syn- ergistic behavior in multi-agent tasks. Our method is based on the simple insight that synergistic behavior leads to effects which would not be achieved if the individual agents were acting alone. So, we propose to reward agents for joint actions that lead to different results compared to if those same actions were done by the agents individually, in a sequential composition. For instance, consider the task of twisting open a water bottle, which requires two hands (agents): one to hold the base in place, and another to twist the cap. Only holding the base in place would not effect any change in the bottle's pose, while twisting the cap without holding the bottle in place would cause the entire bottle to twist, rather than just the cap. Here, holding with one hand and subsequently twisting with the other would not open the bottle, but holding and twisting concurrently would. Based on this intuition, we propose a formulation for intrinsic motivation that leverages the differ- ence between the true effect of an action and the composition of individual-agent predicted effects. We then present a second formulation that instead uses the discrepancy of predictions between a joint and a compositional prediction model. While the latter formulation requires training a forward model alongside learning the control strategy, it has the benefit of being analytically differentiable with respect to the action taken. We later show that this can be leveraged within the policy gradient framework, in order to obtain improved sample complexity over using the policy gradient as-is. As our experimental point of focus, we study six simulated robotic tasks: four bimanual manip- ulation (bottle opening, ball pickup, corkscrew rotating, and bar pickup) and two multi-agent lo- comotion (ant push and soccer). All tasks have sparse rewards: 1 if the goal is achieved and 0 otherwise. These tasks were chosen both because they require synergistic behavior, and because they represent challenging control problems for modern state-of-the-art deep reinforcement learning algorithms ( Levine et al., 2016 ;  Lillicrap et al., 2016 ; Gu et al., 2017;  Mnih et al., 2016 ;  Nagabandi et al., 2018 ). Across all tasks, we find that shaping the reward via our formulation of intrinsic moti- vation yields more efficient learning than both 1) training with only the sparse reward signal and 2) shaping the reward via the more standard single-agent formulation of intrinsic motivation as \"sur- prise,\" which does not explicitly encourage synergistic behavior. We view this work as a step toward general-purpose synergistic multi-agent reinforcement learning.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a general framework for black-box verification of the robustness of classifiers, which recovers prior work as special cases and improves upon previous results in various ways. The framework is based on the randomized smoothing strategy, which shows that robustness properties can be more easily verified for the smoothed version of a base classifier. This approach does not require access to the internals of the classifier and only requires estimating the distribution of outputs of the classifier under random perturbations of the input.",
        "Abstract": "Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far require knowledge of the architecture of the machine learning model and remain hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using f-divergences. Our methodology improves upon the state of the art in terms of computation time or certified robustness on several image classification tasks and an audio classification task, with respect to several classes of adversarial perturbations. ",
        "Introduction": "  INTRODUCTION Predictors obtained from machine learning algorithms have been shown to be vulnerable to making errors when the inputs are perturbed by carefully chosen small but imperceptible amounts ( Szegedy et al., 2014 ;  Biggio et al., 2013 ). This has motivated significant amount of research in improving adversarial robustness of a machine learning model (see, e.g.  Goodfellow et al., 2015 ;  Madry et al., 2018 ). While significant advances have been made, it has been shown that models that were estimated to be robust have later been broken by stronger attacks ( Athalye et al., 2018 ;  Uesato et al., 2018 ). This has led to the need for methods that offer provable guarantees that the predictor cannot be forced to misclassify an example by any attack algorithm restricted to produce perturbations within a certain set (for example, within an p norm ball). While progress has been made leading to methods that are able to compute provable guarantees for several image and text classification tasks ( Wong & Kolter, 2018 ;  Wong et al., 2018 ;  Raghunathan et al., 2018 ;  Dvijotham et al., 2018 ;  Katz et al., 2017 ;  Huang et al., 2019 ;  Jia et al., 2019 ), these methods require extensive knowledge of the architecture of the predictor and are not easy to extend to new models or architectures, requiring specialized algorithms for each new class of models. Furthermore, the computational complexity of these methods grows significantly with input dimension and model size. To deal with these obstacles, recent work has proposed the randomized smoothing strategy for verifying the robustness of classifiers. Specifically,  Lecuyer et al. (2019)  and  Cohen et al. (2019)  Published as a conference paper at ICLR 2020 have shown that robustness properties can be more easily verified for the smoothed version of a base classifier h producing labels in some set Y: h s (x) = arg max y∈Y P X∼µ(x) [h(X) = y] , (1) where the labels returned by the smoothed classifier h s are obtained by taking a \"majority vote\" over the predictions of the original classifier h on random inputs drawn from a probability distribution µ(x), called the smoothing measure.  Lecuyer et al. (2019)  showed that verifying the robustness of this smoothed classifier is significantly simpler than verifying the original classifier h and only requires estimating the distribution of outputs of the classifier under random perturbations of the input, but does not require access to the internals of the classifier h. We refer to this as black-box verification. In this work, we develop a general framework for black-box verification that recovers prior work as special cases, and improves upon previous results in various ways.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents SARFA, a perturbation-based approach for generating saliency maps to interpret and explain the strategies learned by reinforcement learning-based agents. SARFA combines two desired properties of action-focused saliency, specificity and relevance, to generate a saliency map that highlights features of the input state that are relevant for the action to be explained. We demonstrate that SARFA is more effective in identifying important pieces in board games (Chess and Go) and Atari games (Breakout, Pong and Space Invaders) when compared to existing approaches, and further, in aiding skilled chess players to solve chess puzzles.",
        "Abstract": "As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps by balancing two aspects (specificity and relevance) that capture different desiderata of saliency. The first captures the impact of perturbation on the relative expected reward of the action to be explained. The second downweighs irrelevant features that alter the relative expected rewards of actions other than the action to be explained. We compare SARFA with existing approaches on agents trained to play board games (Chess and Go) and Atari games (Breakout, Pong and Space Invaders). We show through illustrative examples (Chess, Atari, Go), human studies (Chess), and automated evaluation methods (Chess) that SARFA generates saliency maps that are more interpretable for humans than existing approaches. For the code release and demo videos, see: https://nikaashpuri.github.io/sarfa-saliency/.",
        "Introduction": "  INTRODUCTION Deep learning has achieved success in various domains such as image classification ( He et al., 2016 ;  Krizhevsky et al., 2012 ), machine translation ( Mikolov et al., 2010 ), image captioning ( Karpathy et al., 2015 ), and deep Reinforcement Learning (RL) ( Mnih et al., 2015 ;  Silver et al., 2017 ). To explain and interpret the predictions made by these complex, \"black-box\"-like systems, various gradient and perturbation techniques have been introduced for image classification ( Simonyan et al., 2013 ;  Zeiler & Fergus, 2014 ;  Fong & Vedaldi, 2017 ) and deep sequential models ( Karpathy et al., 2015 ). However, interpretability for RL-based agents has received significantly less attention. Interpreting the strategies learned by RL agents can help users better understand the problem that the agent is trained to solve. For instance, interpreting the actions of a chess-playing agent in a position could provide useful information about aspects of the position. Interpretation of RL agents is also an important step before deploying such models to solve real-world problems. Inspired by the popularity and use of saliency maps to interpret in computer vision, a number of existing approaches have proposed similar methods for reinforcement learning-based agents.  Greydanus et al. (2018)  derive saliency maps that explain RL agent behavior by applying a Gaussian blur to different parts of the input image. They generate saliency maps using differences in the value Published as a conference paper at ICLR 2020 (a) Original Position (b)  Iyer et al. (2018)  (c)  Greydanus et al. (2018)  (d) SARFA There are two primary limitations to these approaches. The first is that they highlight features whose perturbation affects actions apart from the one we are explaining. This is illustrated in  Figure 1 , which shows a chess position (it is white's turn). Stockfish 1 plays the move Bb6 in this position, which traps the black rook (a5) and queen (c7) 2 . The knight protects the white bishop on a4, and hence the move works. In this position, if we consider the saliency of the white queen (square d1), then it is apparent that the queen is not involved in the tactic and hence the saliency should be low. However, perturbing the state (by removing the queen) leads to a state with substantially different values for Q(s, a) and V (s). Therefore, existing approaches ( Greydanus et al., 2018 ;  Iyer et al., 2018 ) mark the queen as salient. The second limitation is that they highlight features that are not relevant to the action to be explained. In Figure 1c, perturbing the state by removing the black pawn on c6 alters the expected reward for actions other than the one to be explained. Therefore, it alters the policy vector and is marked salient. However, the pawn is not relevant to explain the move played in the position (Bb6). In this work we propose SARFA, Specific and Relevant Feature Attribution, a perturbation based approach for generating saliency maps for black-box agents that builds on two desired properties of action-focused saliency. The first, specificity, captures the impact of perturbation only on the Q-value of the action to be explained. In the above example, this term downweighs features such as the white queen that impact the expected reward of all actions equally. The second, relevance, downweighs irrelevant features that alter the expected rewards of actions other than the action to be explained. It removes features such as the black pawn on c6 that increase the expected reward of other actions (in this case, Bb4). By combining these aspects, we generate a saliency map that highlights features of the input state that are relevant for the action to be explained.  Figure 1  illustrates how the saliency map generated by SARFA only highlights pieces relevant to the move, unlike existing approaches. We use our approach, SARFA to explain the actions taken by agents for board games (Chess and Go), and for Atari games (Breakout, Pong and Space Invaders). Using a number of illustrative examples, we show that SARFA obtains more focused and accurate interpretations for all of these setups when compared to  Greydanus et al. (2018)  and  Iyer et al. (2018) . We also demonstrate that SARFA is more effective in identifying important pieces in chess puzzles, and further, in aiding skilled chess players to solve chess puzzles (improves accuracy of solving them by nearly 25% and reduces the time taken by 31% over existing approaches).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel approach to continual learning (CL) in which a task-conditioned hypernetwork is used to map a task embedding to weights, allowing a single point to be memorized per task. This approach is shown to avoid catastrophic forgetting on a set of standard CL benchmarks, and is capable of retaining memories with no decrease in performance when presented with long sequences of tasks. Additionally, the metamodelling framework is extended to generative replay methods, which are current state-of-the-art performers in many practical problems.",
        "Abstract": "Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task sequences reveal that task-conditioned hypernetworks display a very large capacity to retain previous memories. Notably, such long memory lifetimes are achieved in a compressive regime, when the number of trainable hypernetwork weights is comparable or smaller than target network size. We provide insight into the structure of low-dimensional task embedding spaces (the input space of the hypernetwork) and show that task-conditioned hypernetworks demonstrate transfer learning. Finally, forward information transfer is further supported by empirical results on a challenging CL benchmark based on the CIFAR-10/100 image datasets.",
        "Introduction": "  INTRODUCTION We assume that a neural network f (x, Θ) with trainable weights Θ is given data from a set of tasks {(X (1) , Y (1) ), . . . , (X (T ) , Y (T ) )}, with input samples X (t) = {x (t,i) } nt i=1 and output samples Y (t) = {y (t,i) } nt i=1 , where n t ≡ |X (t) |. A standard training approach learns the model using data from all tasks at once. However, this is not always possible in real-world problems, nor desirable in an online learning setting. Continual learning (CL) refers to an online learning setup in which tasks are presented sequentially (see  van de Ven & Tolias, 2019 , for a recent review on CL). In CL, when learning a new task t, starting with weights Θ (t−1) and observing only (X (t) , Y (t) ), the goal is to find a new set of parameters Θ (t) that (1) retains (no catastrophic forgetting) or (2) improves (positive backward transfer) performance on previous tasks compared to Θ (t−1) and (3) solves the new task t potentially utilizing previously acquired knowledge (positive forward transfer). Achieving these goals is non-trivial, and a longstanding issue in neural networks research. Here, we propose addressing catastrophic forgetting at the meta level: instead of directly attempting to retain f (x, Θ) for previous tasks, we fix the outputs of a metamodel f h (e, Θ h ) termed task-conditioned hypernetwork which maps a task embedding e to weights Θ. Now, a single point has to be memorized per task. To motivate such approach, we perform a thought experiment: we assume that we are allowed to store all inputs {X (1) , . . . , X (T ) } seen so far, and to use these data to compute model outputs corresponding to Θ (T −1) . In this idealized setting, one can avoid forgetting by simply mixing data from the current task with data from the past, {(X (1) ,Ŷ (1) ), . . . , (X (T −1) ,Ŷ (T −1) ), (X (T ) , Y (T ) )}, whereŶ (t) refers to a set of synthetic targets generated using the model itself f ( · , Θ (t−1) ). Hence, by training to retain previously acquired input-output mappings, one can obtain a sequential algorithm in principle as powerful as multi-task learning. Multi-task learning, where all tasks are learned Published as a conference paper at ICLR 2020 simultaneously, can be seen as a CL upper-bound. The strategy described above has been termed rehearsal ( Robins, 1995 ). However, storing previous task data violates our CL desiderata. Therefore, we introduce a change in perspective and move from the challenge of maintaining individual input-output data points to the problem of maintaining sets of parameters {Θ (t) }, without explicitly storing them. To achieve this, we train the metamodel parameters Θ h analogous to the above outlined learning scheme, where synthetic targets now correspond to weight configurations that are suitable for previous tasks. This exchanges the storage of an entire dataset by a single low-dimensional task descriptor, yielding a massive memory saving in all but the simplest of tasks. Despite relying on regularization, our approach is a conceptual departure from previous algorithms based on regularization in weight (e.g.,  Kirkpatrick et al., 2017 ;  Zenke et al., 2017 ) or activation space (e.g.,  He & Jaeger, 2018 ). Our experimental results show that task-conditioned hypernetworks do not suffer from catastrophic forgetting on a set of standard CL benchmarks. Remarkably, they are capable of retaining memories with practically no decrease in performance, when presented with very long sequences of tasks. Thanks to the expressive power of neural networks, task-conditioned hypernetworks exploit task-to- task similarities and transfer information forward in time to future tasks. Finally, the task-conditional metamodelling perspective that we put forth is generic, as it does not depend on the specifics of the target network architecture. We exploit this key principle and show that the very same metamodelling framework extends to, and can improve, an important class of CL methods known as generative replay methods, which are current state-of-the-art performers in many practical problems ( Shin et al., 2017 ;  Wu et al., 2018 ;  van de Ven & Tolias, 2018 ).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel language-conditioned policy learning problem, Read to Fight Monsters (RTFM), in which an agent must jointly reason over a language goal, a document that specifies environment dynamics, and environment observations. The agent must identify relevant information in the document to shape its policy and accomplish the goal. To necessitate reading comprehension, the agent is exposed to ever changing environment dynamics and corresponding language descriptions such that it cannot avoid reading by memorising any particular environment dynamics. We procedurally generate environment dynamics and natural language templated descriptions of dynamics and goals to produce a combinatorially large number of environment dynamics to train and evaluate RTFM.",
        "Abstract": "Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and corresponding language descriptions of the dynamics, such that agents must read to understand new environment dynamics instead of memorising any particular information. In addition, we propose txt2π, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2π generalises to new environments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2π produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) has been successful in a variety of areas such as continuous con- trol (Lillicrap et al., 2015), dialogue systems ( Li et al., 2016 ), and game-playing ( Mnih et al., 2013 ). However, RL adoption in real-world problems is limited due to poor sample efficiency and failure to generalise to environments even slightly different from those seen during training. We explore language-conditioned policy learning, where agents use machine reading to discover strategies re- quired to solve a task, thereby leveraging language as a means to generalise to new environments. Prior work on language grounding and language-based RL (see  Luketina et al. (2019)  for a recent survey) are limited to scenarios in which language specifies the goal for some fixed environment dynamics ( Branavan et al., 2011 ;  Hermann et al., 2017 ;  Bahdanau et al., 2019 ;  Fried et al., 2018 ;  Co-Reyes et al., 2019 ), or the dynamics of the environment vary and are presented in language for some fixed goal ( Branavan et al., 2012 ). In practice, changes to goals and to environment dynamics tend to occur simultaneously-given some goal, we need to find and interpret relevant information to understand how to achieve the goal. That is, the agent should account for variations in both by selectively reading, thereby generalising to environments with dynamics not seen during training. Our contributions are two-fold. First, we propose a grounded policy learning problem that we call Read to Fight Monsters (RTFM). In RTFM, the agent must jointly reason over a language goal, a document that specifies environment dynamics, and environment observations. In particular, it must identify relevant information in the document to shape its policy and accomplish the goal. To necessitate reading comprehension, we expose the agent to ever changing environment dynam- ics and corresponding language descriptions such that it cannot avoid reading by memorising any particular environment dynamics. We procedurally generate environment dynamics and natural lan- guage templated descriptions of dynamics and goals to produced a combinatorially large number of environment dynamics to train and evaluate RTFM.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper examines the effects of depth and width on the neural tangent kernel (NTK) of an overparameterized neural network. It is shown that, when the depth and width are both large, the NTK is not close to a delta function, and the standard deviation of the NTK is at least as large as its mean. It is also shown that, when the loss is the square loss, the NTK has the potential to evolve in a data-dependent way. These results suggest that, even in an overparameterized regime, feature learning can occur during training.",
        "Abstract": "We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and wide networks, deep and wide ReLU networks are capable of learning data-dependent features even in the so-called lazy training regime. ",
        "Introduction": "  INTRODUCTION Modern neural networks are typically overparameterized: they have many more parameters than the size of the datasets on which they are trained. That some setting of parameters in such networks can interpolate the data is therefore not surprising. But it is a priori unexpected that not only can such interpolating parameter values can be found by stochastic gradient descent (SGD) on the highly non-convex empirical risk but also that the resulting network function generalizes to unseen data. In an overparameterized neural network N (x) the individual parameters can be difficult to interpret, and one way to understand training is to rewrite the SGD updates ∆θ p = − λ ∂L ∂θ p , p = 1, . . . , P of trainable parameters θ = {θ p } P p=1 with a loss L and learning rate λ as kernel gradient descent updates for the values N (x) of the function computed by the network: Here B = {(x 1 , y 1 ), . . . , (x |B| , y |B| )} is the current batch, the inner product is the empirical 2 inner product over B, and K N is the neural tangent kernel (NTK): Relation (1) is valid to first order in λ. It translates between two ways of thinking about the difficulty of neural network optimization: (i) The parameter space view where the loss L, a complicated function of θ ∈ R #parameters , is minimized using gradient descent with respect to a simple (Euclidean) metric; Published as a conference paper at ICLR 2020 (ii) The function space view where the loss L, which is a simple function of the network map- ping x → N (x), is minimized over the manifold M N of all functions representable by the architecture of N using gradient descent with respect to a potentially complicated Rie- mannian metric K N on M N . A remarkable observation of  Jacot et al. (2018)  is that K N simplifies dramatically when the network depth d is fixed and its width n tends to infinity. In this setting, by the universal approximation theorem ( Cybenko, 1989 ;  Hornik et al., 1989 ), the manifold M N fills out any (reasonable) ambient linear space of functions. The results in  Jacot et al. (2018)  then show that the kernel K N in this limit is frozen throughout training to the infinite width limit of its average E[K N ] at initialization, which depends on the depth and non-linearity of N but not on the dataset. This mapping between parameter space SGD and kernel gradient descent for a fixed kernel can be viewed as two separate statements. First, at initialization, the distribution of K N converges in the infinite width limit to the delta function on the infinite width limit of its mean E[K N ]. Second, the infinite width limit of SGD dynamics in function space is kernel gradient descent for this limiting mean kernel for any fixed number of SGD iterations. As long as the loss L is well-behaved with respect to the network outputs N (x) and E[K N ] is non-degenerate in the subspace of function space given by values on inputs from the dataset, SGD for infinitely wide networks will converge with probability 1 to a minimum of the loss. Further, kernel method-based theorems show that even in this infinitely overparameterized regime neural networks will have non-vacuous guarantees on generalization ( Wei et al., 2018 ). However, as ( Wei et al., 2018 ) shows, the regularized neural networks at finite width can have better sample complexity the corresponding infinite width kernel method. But replacing neural network training by gradient descent for a fixed kernel in function space is also not completely satisfactory for several reasons. First, it suggests that no feature learning occurs during training for infinitely wide networks in the sense that the kernel E[K N ] (and hence its asso- ciated feature map) is data-independent. In fact, empirically, networks with finite but large width trained with initially large learning rates often outperform NTK predictions at infinite width ( Arora et al., 2019 ). One interpretation is that, at finite width, K N evolves through training, learning data- dependent features not captured by the infinite width limit of its mean at initialization. In part for such reasons, it is important to study both empirically and theoretically finite width corrections to K N . Another interpretation is that the specific NTK scaling of weights at initialization ( Chizat & Bach, 2018b ;a;  Mei et al., 2019 ; 2018;  Rotskoff & Vanden-Eijnden, 2018a ;b) and the implicit small learning rate limit ( Li et al., 2019 ) obscure important aspects of SGD dynamics. Second, even in the infinite width limit, although K N is deterministic, it has no simple analytical formula for deep networks, since it is defined via a layer by layer recursion. In particular, the exact dependence, even in the infinite width limit, of K N on network depth is not well understood. Moreover, the joint statistical effects of depth and width on K N in finite size networks remain unclear, and the purpose of this article is to shed light on the simultaneous effects of depth and width on K N for finite but large widths n and any depth d. Our results apply to fully connected ReLU networks at initialization for which our main contributions are: 1. In contrast to the regime in which the depth d is fixed but the width n is large, K N is not approximately deterministic at initialization so long as d/n is bounded away from 0. Specifically, for a fixed input x the normalized on-diagonal second moment of K N satisfies Thus, when d/n is bounded away from 0, even when both n, d are large, the standard deviation of K N (x, x) is at least as large as its mean, showing that its distribution at initialization is not close to a delta function. See Theorem 1. 2. Moreover, when L is the square loss, the average of the SGD update ∆K N (x, x) to K N (x, x) from a batch of size one containing x satisfies Published as a conference paper at ICLR 2020 where n 0 is the input dimension. Therefore, if d 2 /nn 0 > 0, the NTK will have the potential to evolve in a data-dependent way. Moreover, if n 0 is comparable to n and d/n > 0 then it is possible that this evolution will have a well-defined expansion in d/n. See Theorem 2. In both statements above, means is bounded above and below by universal constants. We em- phasize that our results hold at finite d, n and the implicit constants in both and in the error terms O(d/n 2 ) are independent of d, n. Moreover, our precise results, stated in §2 below, hold for networks with variable layer widths. We have denoted network width by n only for the sake of exposition. The appropriate generalization of d/n to networks with varying layer widths is the parameter β := d i=1 1 n j , which in light of the estimates in (1) and (2) plays the role of an inverse temperature.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the use of Recurrent Neural Networks (RNNs) for online prediction problems, which are partially observable and require a history to make accurate predictions. RNNs contain recurrent connections to their hidden layers which allow past information to propagate through time. RNNs have been used in various applications, but there are stability and computational issues in training them online. This paper examines two common approaches for training RNNs online: Backpropagation-through-time (BPTT) and Real-Time Recurrent Learning (RTRL). It discusses the advantages and disadvantages of each approach and how they can be used to address long-term dependencies.",
        "Abstract": "Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm's performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically computationally expensive and focus computation on computing gradients back in time. In this work, we reformulate the RNN training objective to explicitly learn state vectors; this breaks the dependence across time and so avoids the need to estimate gradients far back in time. We show that for a fixed buffer of data, our algorithm---called Fixed Point Propagation (FPP)---is sound: it converges to a stationary point of the new objective. We investigate the empirical performance of our online FPP algorithm, particularly in terms of computation compared to truncated BPTT with varying truncation levels. ",
        "Introduction": "  INTRODUCTION Many online prediction problems are partially observable: the most recent observation is typically insufficient to make accurate predictions about the future. Augmenting the inputs with a history can improve accuracy, but can require a long history when there are long-term dependencies back in time. Recurrent Neural Networks (RNNs) ( Elman, 1990 ;  Hopfield, 1982 ) learn a state which summarizes this history. Specifically, RNNs contain recurrent connections to their hidden layers which allow past information to propagate through time. This state need not correspond to a true underlying state; rather, the state is subjective and constructed to facilitate prediction. RNNs have been widely used, in speech recognition ( Hinton et al., 2012 ;  Graves et al., 2013 ;  Miao et al., 2015 ;  Chan et al., 2016 ), image captioning ( Mao et al., 2014 ;  Lu et al., 2016 ;  Vinyals et al., 2014 ), speech synthesis ( Mehri et al., 2016 ) and reinforcement learning ( Hochreiter and Schmidhuber, 1997 ;  Düll et al., 2012 ). Despite these success, there are significant stability and computational issues in training RNNs online ( Pascanu et al., 2013 ;  Tallec and Ollivier, 2017 ). In the online setting, the agent faces an unending stream of data and on each step the agent must update its parameters to make a new prediction. RNNs are typically trained either using Backpropagation-through-time (BPTT) ( Werbos, 1990 ) or approximations to an algorithm called Real-Time Recurrent Learning (RTRL) ( Williams and Zipser, 1989a ;  Pearlmutter, 1995 ), although there are methods that appeal to other principles (see  Murray (2019)  for instance). The update for BPTT is a variant of standard backpropagation, computing gradients all the way back in time. This approach is problematic because the computational cost scales linearly with the number of time-steps. A more common alternative is truncated BPTT (T-BPTT) ( Williams and Peng, 1990 ) which only computes the gradient up to some maximum number of steps: we truncate how far back in time we unroll the network to update the parameters. This approximation, though, is not robust to long-term dependencies ( Tallec and Ollivier, 2017 ). Approximate gradients can also be computed online by RTRL ( Williams and Zipser, 1989b ). This online algorithm, however, has high computational complexity per step and therefore is not commonly used in practice.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel application of auxiliary models, namely privacy-preserving generative models, to stand in for direct data examination during the process of debugging data errors during inference or training. By combining ideas from deep generative models, federated learning, and user-level differential privacy, the paper demonstrates how some needs traditionally met with data inspection can instead be met by generating synthetic examples from a privacy-preserving federated generative model. The paper also identifies key challenges in implementing end-to-end workflows with non-inspectable data, proposes a methodology to resolve these challenges, and demonstrates how privacy preserving federated generative models can be trained to high enough fidelity to discover introduced data errors.",
        "Abstract": "To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data—of representative samples, of outliers, of misclassifications—is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses,\nand c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access aggregated outputs such as metrics or model parameters. This paper demonstrates that generative models—trained using federated methods and with formal differential privacy guarantees—can be used effectively to debug data issues even\nwhen the data cannot be directly inspected. We explore these methods in applications to text with differentially private federated RNNs and to images using a novel algorithm for differentially private federated GANs.",
        "Introduction": "  INTRODUCTION Real-world systems increasingly depend on machine learning (ML) to make decisions, detect anomalies, and power products. Applications ranging from fraud detection to mobile phone keyboards use models trained on data that may be privacy sensitive. The data may contain financial, medical, or behavioral informa- tion about individuals. Institutions responsible for these applications of ML must balance data stewardship obligations-including minimizing the risks of data loss, theft, or abuse-with the practical needs of the \"modeler\" whose job is to develop and improve the machine learned models. Modelers working with privacy-sensitive data face significant challenges in model development and debug- ging. Often, a modeler's first step would be to inspect individual examples in order to discover bugs, generate hypotheses, improve labeling, or similar. But direct inspection of data may be audited or disallowed by pol- icy in some settings. In other settings-including federated learning (FL) (McMahan & Ramage, 2017; McMahan et al., 2017), which is the motivation and focus of this work-the data cannot be inspected. In FL, raw data remains distributed across a fleet of devices, such as mobile phones, while an orchestrating server coordinates training of a shared global model. Only the final model parameters and statistics are gathered and made available to the modeler. How can a modeler effectively debug when training data is privacy sensitive or decentralized? This paper demonstrates that the novel application of auxiliary models, namely privacy-preserving generative models, can stand in for direct data examination during the process of debugging data errors during inference or training. By combining ideas from deep generative models, FL, and user-level differential privacy (DP), we show how some needs traditionally met with data inspection can instead be met by generating synthetic examples from a privacy-preserving federated generative model. These examples could be representative of all or a subset of the non-inspectable data, while at the same time preserving the privacy of individuals. Our contributions include: • Identifying key challenges in implementing end-to-end workflows with non-inspectable data, e.g., for debugging a 'primary' ML model used in a mobile application. • Proposing a methodology that allows (sufficiently powerful) 'auxiliary' generative models to re- solve these challenges. • Demonstrating how privacy preserving federated generative models-RNNs for text and GANs for images-can be trained to high enough fidelity to discover introduced data errors matching those encountered in real world scenarios. This requires a novel adaption of generative adversarial networks (GANs) to the federated setting with user-level DP guarantees.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces a recurrent graph-based retrieval method for open-domain Question Answering (QA) that learns to retrieve evidence documents as reasoning paths for answering complex questions. The method sequentially retrieves each evidence document, given the history of previously retrieved documents to form several reasoning paths in a graph of entities. The retriever model is a recurrent neural network that scores reasoning paths in the graph by maximizing the likelihood of selecting a correct evidence paragraph at each step and fine-tuning paragraph BERT encodings. The reader model is a multi-task learner to score each reasoning path according to its likelihood of containing and extracting the correct answer phrase. Experimental results show that the method achieves state-of-the-art results on HotpotQA full wiki and HotpotQA distractor settings, outperforming the previous state-of-the-art methods by more than 14 points absolute gain on the full wiki setting. Additionally, the framework provides interpretable insights into the underlying entity relationships used for multi-hop reasoning.",
        "Abstract": "Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. \nOur reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path.\nExperimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.",
        "Introduction": "  INTRODUCTION Open-domain Question Answering (QA) is the task of answering a question given a large collection of text documents (e.g., Wikipedia). Most state-of-the-art approaches for open-domain QA (Chen et al., 2017; Wang et al., 2018a; Lee et al., 2018; Yang et al., 2019) leverage non-parameterized mod- els (e.g., TF-IDF or BM25) to retrieve a fixed set of documents, where an answer span is extracted by a neural reading comprehension model. Despite the success of these pipeline methods in single- hop QA, whose questions can be answered based on a single paragraph, they often fail to retrieve the required evidence for answering multi-hop questions, e.g., the question in  Figure 1 . Multi-hop QA (Yang et al., 2018) usually requires finding more than one evidence document, one of which often consists of little lexical overlap or semantic relationship to the original question. However, retrieving a fixed list of documents independently does not capture relationships between evidence documents through bridge entities that are required for multi-hop reasoning. Recent open-domain QA methods learn end-to-end models to jointly retrieve and read docu- ments (Seo et al., 2019; Lee et al., 2019). These methods, however, face challenges for entity-centric questions since compressing the necessary information into an embedding space does not capture lexical information in entities. Cognitive Graph (Ding et al., 2019) incorporates entity links between documents for multi-hop QA to extend the list of retrieved documents. This method, however, com- piles a fixed list of documents independently and expects the reader to find the reasoning paths. In this paper, we introduce a new recurrent graph-based retrieval method that learns to retrieve evi- dence documents as reasoning paths for answering complex questions. Our method sequentially re- trieves each evidence document, given the history of previously retrieved documents to form several reasoning paths in a graph of entities. Our method then leverages an existing reading comprehension model to answer questions by ranking the retrieved reasoning paths. The strong interplay between the retriever model and reader model enables our entire method to answer complex questions by exploring more accurate reasoning paths compared to other methods. To be more specific, our method (sketched in  Figure 2 ) constructs the Wikipedia paragraph graph using Wikipedia hyperlinks and document structures to model the relationships between paragraphs. Our retriever trains a recurrent neural network to score reasoning paths in this graph by maximizing the likelihood of selecting a correct evidence paragraph at each step and fine-tuning paragraph BERT encodings. Our reader model is a multi-task learner to score each reasoning path according to its likelihood of containing and extracting the correct answer phrase. We leverage data augmentation and negative example mining for robust training of both models. Our experimental results show that our method achieves the state-of-the-art results on HotpotQA full wiki and HotpotQA distractor settings (Yang et al., 2018), outperforming the previous state- of-the-art methods by more than 14 points absolute gain on the full wiki setting. We also evaluate our approach on SQuAD Open (Chen et al., 2017) and Natural Questions Open (Lee et al., 2019) without changing any architectural designs, achieving better or comparable to the state of the art, which suggests that our method is robust across different datasets. Additionally, our framework provides interpretable insights into the underlying entity relationships used for multi-hop reasoning.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates how to learn feature representations across spatial and motion visual clues by proposing a new multi-stream neural architecture search algorithm with connection learning guided evolution. This algorithm focuses on finding higher-level connectivity between network blocks taking multiple input streams at different temporal resolutions. The proposed algorithm for learning video architectures is very effective, outperforming all prior work and baselines on two challenging benchmark datasets, and establishing a new state-of-the-art.",
        "Abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.",
        "Introduction": "  INTRODUCTION Learning to represent videos is a challenging problem. Because a video contains spatio-temporal data, its representation is required to abstract both appearance and motion information. This is particularly important for tasks such as activity recognition, as understanding detailed semantic contents of the video is needed. Previously, researchers approached this challenge by designing a two-stream model for appearance and motion information respectively, combining them by late or intermediate fusion to obtain successful results:  Simonyan & Zisserman (2014) ;  Feichtenhofer et al. (2016b ;a; 2017; 2018). However, combining appearance and motion information is an open problem and the study on how and where different modalities should interchange representations and what temporal aspect/resolution each stream (or module) should focus on has been very limited. In this paper, we investigate how to learn feature representations across spatial and motion visual clues. We propose a new multi-stream neural architecture search algorithm with connection learning guided evolution, which focuses on finding higher-level connectivity between network blocks taking multiple input streams at different temporal resolutions. Each block itself is composed of multiple residual modules with space-time convolutional layers, learning spatio-temporal representations. Our architecture learning not only considers the connectivity between such multi-stream, multi-resolution blocks, but also merges and splits network blocks to find better multi-stream video CNN architectures. Our objective is to address two main questions in video representation learning: (1) what feature representations are needed at each intermediate stage of the network and at which resolution and (2) how to combine or exchange such intermediate representations (i.e., connectivity learning). Unlike previous neural architecture search methods for images that focus on finding a good 'module' of convolutional layers to be repeated in a single-stream networks ( Zoph et al., 2018 ;  Real et al., 2019 ), our objective is to search for higher-level connections between multiple sequential or concurrent blocks to form multi-stream architectures. We propose the concept of AssembleNet, a new method of fusing different sub-networks with different input modalities and temporal resolutions. AssembleNet is a general formulation that Published as a conference paper at ICLR 2020 2D conv. allows representing various forms of multi-stream CNNs as directed graphs, coupled with an efficient evolutionary algorithm to explore the network connectivity. Specifically, this is done by utilizing the learned connection weights to guide evolution, in addition to randomly combining, splitting, or connecting sub-network blocks. AssembleNet is a 'family' of learnable architectures; they provide a generic approach to learn connectivity among feature representations across input modalities, while being optimized for the target task. We believe this is the first work to (i) conduct research on automated architecture search with multi-stream connections for video understanding, and (ii) introduce the new connection-learning-guided evolutionary algorithm for neural architecture search.  Figure 1  shows an example learned AssembleNet. The proposed algorithm for learning video architectures is very effective: it outperforms all prior work and baselines on two very challenging benchmark datasets, and establishes a new state-of-the-art. AssembleNet models use equivalent number of parameters to standard two-stream (2+1)D ResNet models.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents a novel generative model, generative ratio matching (GRAM) networks, which uses a critic to map samples from the generator and the data into a lower-dimensional representation, and MMD is applied in this transformed space. GRAM-nets are trained to preserve density ratios, namely, the ratio of the true density to the model density, and are shown to generate high quality images while avoiding a zero-sum game (in critic and generator) and being more stable to train and robust to the choice of hyperparameters.",
        "Abstract": "Deep generative models can learn to generate realistic-looking images, but many of the most effective methods are adversarial and involve a saddlepoint optimization, which requires a careful balancing of training between a generator network and a critic network. Maximum mean discrepancy networks (MMD-nets) avoid this issue by using kernel as a fixed adversary, but unfortunately, they have not on their own been able to match the generative quality of adversarial training. In this work, we take their insight of using kernels as fixed adversaries further and present a novel method for training deep generative models that does not involve saddlepoint optimization. We call our method generative ratio matching or GRAM for short. In GRAM, the generator and the critic networks do not play a zero-sum game against each other, instead, they do so against a fixed kernel. Thus GRAM networks are not only stable to train like MMD-nets but they also match and beat the generative quality of adversarially trained generative networks.",
        "Introduction": "  INTRODUCTION Deep generative models (Kingma & Welling, 2013;  Goodfellow et al., 2014 ;  Kingma & Dhariwal, 2018 ) have been shown to learn to generate realistic-looking images. These methods train a deep neural network, called a generator, to transform samples from a noise distribution to samples from the data distribution. Most methods use adversarial learning ( Goodfellow et al., 2014 ), in which the generator is pitted against a critic function, also called a discriminator, which is trained to distinguish between the samples from the data distribution and from the generator. Upon successful training the two sets of samples become indistinguishable with respect to the critic. Maximum mean discrepancy networks (MMD-nets) ( Li et al., 2015 ;  Dziugaite et al., 2015 ) are a class of generative models that are trained to minimize the MMD (Gretton et al., 2012) between the true data distribution and the model distribution. MMD-nets are similar in spirit to generative adversarial networks (GANs) ( Goodfellow et al., 2014 ;  Nowozin et al., 2016 ), in the sense that the MMD is defined by maximizing over a class of critic functions. However, in contrast to GANs, where finding the right balance between generator and critic is difficult, training is simpler for MMD-nets because using the kernel trick the MMD can be estimated without the need to numerically optimize over the critic function. This avoids the need in GANs to numerically solve a saddlepoint problem. Unfortunately, although MMD-nets work well on low dimensional data, these networks have not on their own matched the generative performance of adversarial methods on higher dimensional datasets, such as natural images ( Dziugaite et al., 2015 ). Several authors ( Li et al., 2017 ;  Bińkowski et al., 2018a ) suggest that a reason is that MMD is sensitive to the choice of kernel.  Li et al. (2017)  propose a method called MMD-GAN, in which the critic maps the samples from the generator and the data into a lower-dimensional representation, and MMD is applied in this transformed space. This Published as a conference paper at ICLR 2020 can be interpreted as a method for learning the kernel in MMD. The critic is learned adversarially by maximizing the MMD at the same time as it is minimized with respect to the generator. This is much more effective than MMD-nets, but training MMD-GANs can be challenging, because in this saddlepoint optimization problem, the need to balance training of the learned kernel and the generator can create a sensitivity to hyperparameter settings. In practice, it is necessary to introduce several additional penalties to the loss function in order for training to be effective. In this work, we present a novel training method that builds on MMD-nets' insight to use kernels as fixed adversaries in order to avoid saddlepoint optimization based training for the critic and the generator. Our goal is for the critic to map the samples into a lower-dimensional space in which the MMD-net estimator will be more effective. Our proposal is that the critic should be trained to preserve density ratios, namely, the ratio of the true density to the model density. If the critic is successful in this, then matching the generator to the true data in the lower dimensional space will also match the distributions in the original space. We call networks that have been trained using this criterion generative ratio matching (GRAM) networks, or GRAM-nets 2 . We show empirically that our method is not only able to generate high quality images but by virtue of avoiding a zero-sum game (in critic and generator) it avoids saddlepoint optimization and hence is more stable to train and robust to the choice of hyperparameters.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes Transformers which adapt the number of layers to each input in order to achieve a good speed-accuracy trade off at inference time. We extend Graves (2016; ACT) who introduced dynamic computation to recurrent neural networks in several ways, including applying different layers at each stage, investigating a range of designs and training targets for the halting module, and explicitly supervising through simple oracles. Experiments on IWSLT14 German-English translation and WMT'14 English-French translation show that we can match the performance of well tuned baseline models at up to 76% less computation.",
        "Abstract": "State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less than a quarter of the decoder layers.",
        "Introduction": "  INTRODUCTION The size of modern neural sequence models ( Gehring et al., 2017 ;  Vaswani et al., 2017 ;  Devlin et al., 2019 ) can amount to billions of parameters ( Radford et al., 2019 ). For example, the winning entry of the WMT'19 news machine translation task in English-German used an ensemble totaling two billion parameters ( Ng et al., 2019 ). While large models are required to do better on hard examples, small models are likely to perform as well on easy ones, e.g., the aforementioned ensemble is prob- ably not required to translate a short phrase such as \"Thank you\". However, current models apply the same amount of computation regardless of whether the input is easy or hard. In this paper, we propose Transformers which adapt the number of layers to each input in order to achieve a good speed-accuracy trade off at inference time. We extend  Graves (2016;  ACT) who introduced dynamic computation to recurrent neural networks in several ways: we apply different layers at each stage, we investigate a range of designs and training targets for the halting module and we explicitly supervise through simple oracles to achieve good performance on large-scale tasks. Universal Transformers (UT) rely on ACT for dynamic computation and repeatedly apply the same layer ( Dehghani et al., 2018 ). Our work considers a variety of mechanisms to estimate the network depth and applies a different layer at each step. Moreover,  Dehghani et al. (2018)  fix the number of steps for large-scale machine translation whereas we vary the number of steps to demonstrate substantial improvements in speed at no loss in accuracy. UT uses a layer which contains as many weights as an entire standard Transformer and this layer is applied several times which impacts speed. Our approach does not increase the size of individual layers. We also extend the resource efficient object classification work of  Huang et al. (2017)  and  Bolukbasi et al. (2017)  to structured prediction where dynamic computation decisions impact future computation. Related work from computer vision includes  Teerapittayanon et al. (2016) ;  Figurnov et al. (2017)  and  Wang et al. (2018)  who explored the idea of dynamic routing either by exiting early or by skipping layers. We encode the input sequence using a standard Transformer encoder to generate the output sequence with a varying amount of computation in the decoder network. Dynamic computation poses a chal- lenge for self-attention because omitted layers in prior time-steps may be required in the future. We experiment with two approaches to address this and show that a simple approach works well (§2). Next, we investigate different mechanisms to control the amount of computation in the de- coder network, either for the entire sequence or on a per-token basis. This includes multinomial and binomial classifiers supervised by the model likelihood or whether the argmax is already correct as well as simply thresholding the model score (§3). Experiments on IWSLT14 German-English Published as a conference paper at ICLR 2020 translation ( Cettolo et al., 2014 ) as well as WMT'14 English-French translation show that we can match the performance of well tuned baseline models at up to 76% less computation (§4).",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper examines the behavior of temporal difference (TD) learning under generic function approximation. We prove that the set of smooth homogeneous functions, including ReLU networks, is amenable to the expected dynamics of TD, and that the ODE is attracted to a compact set containing the true value function. We also prove global convergence to the true value function when the environment is \"more reversible\" than the function approximator is \"poorly conditioned\". Finally, we generalize a divergent TD example to a broad class of non-reversible environments. These results begin to explain how the geometry of nonlinear function approximators and the structure of the environment interact with TD learning.",
        "Abstract": "While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here we take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, we consider the expected learning dynamics of the TD(0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear ODE which depends on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. We find a set of function approximators that includes ReLU networks and has geometry amenable to TD learning regardless of environment, so that the solution performs about as well as linear TD in the worst case. Then, we show how environments that are more reversible induce dynamics that are better for TD learning and prove global convergence to the true value function for well-conditioned function approximators. Finally, we generalize a divergent counterexample to a family of divergent problems to demonstrate how the interaction between approximator and environment can go wrong and to motivate the assumptions needed to prove convergence. ",
        "Introduction": "  INTRODUCTION The instability of reinforcement learning (RL) algorithms is well known, but not well characterized theoretically. Notably, there is no guarantee that value estimation by temporal difference (TD) learn- ing converges when using nonlinear function approximators, even in the on-policy case. The use of a function approximator introduces a projection of the tabular TD update into the class of repre- sentable functions. Since the dynamics of TD do not follow the gradient of any objective function, the interaction of the geometry of the function class with that of the TD algorithm in the space of all functions potentially eliminates any convergence guarantees. This lack of convergence has motivated many authors to seek variants of TD learning that re- establish convergence guarantees, such as two timescale algorithms. In contrast, in this work we focus on TD learning directly and examine its behavior under generic function approximation. We consider the simplest case: on-policy discounted value estimation. To further simplify the analy- sis, we only consider the expected learning dynamics in continuous time as opposed to the online algorithm with sampling. This means that we are eschewing discussions of off-policy data, explo- ration, sampling variance, and step size. In this continuous limit, the dynamics of TD learning are modeled as a (nonlinear) ODE. Stability of this ODE is a pre-requisite for convergence of the algo- rithm. However, for general approximators and MDPs it can diverge as demonstrated by  Tsitsiklis & Van Roy (1997) . Today, the convergence of this ODE is known in two regimes: under linear function approximation for general environments ( Tsitsiklis & Van Roy, 1997 ) and under reversible environments for general function approximation ( Ollivier, 2018 ). We significantly close this gap through the following contributions: 1. We prove that the set of smooth homogeneous functions, including ReLU networks, is amenable to the expected dynamics of TD. In this case, the ODE is attracted to a compact Published as a conference paper at ICLR 2020 set containing the true value function. Moreover, when we use a parametrization inspired by ResNets, nonlinear TD will have error comparable to linear TD in the worst case. 2. We prove global convergence to the true value function when the environment is \"more reversible\" than the function approximator is \"poorly conditioned\". 3. We generalize a divergent TD example to a broad class of non-reversible environments. These results begin to explain how the geometry of nonlinear function approximators and the struc- ture of the environment interact with TD learning.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper introduces an algorithmic framework for Deep Reinforcement Learning (DRL) that can handle concurrent environments, where the environment state evolves substantially as the agent processes observations and plans its next actions. We derive a modified Bellman operator for concurrent MDPs and present the minimal set of information that must be augmented to state observations in order to recover blocking performance with Q-learning. Experiments are conducted on different simulated environments, ranging from common simple control domains to vision-based robotic grasping tasks. Results show that an agent that acts concurrently in a real-world robotic grasping task is able to achieve comparable task success to a blocking baseline while acting 49% faster.",
        "Abstract": "We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to existing value-based deep reinforcement learning algorithms. We evaluate our methods on simulated benchmark tasks and a large-scale robotic grasping task where the robot must \"think while moving.\"",
        "Introduction": "  INTRODUCTION In recent years, Deep Reinforcement Learning (DRL) methods have achieved tremendous success on a variety of diverse environments, including video games ( Mnih et al., 2015 ), zero-sum games ( Sil- ver et al., 2016 ), robotic grasping ( Kalashnikov et al., 2018 ), and in-hand manipulation tasks ( Ope- nAI et al., 2018 ). While impressive, all of these examples use a blocking observe-think-act paradigm: the agent assumes that the environment will remain static while it thinks, so that its actions will be executed on the same states from which they were computed. This assumption breaks in the con- current real world, where the environment state evolves substantially as the agent processes obser- vations and plans its next actions. As an example, consider a dynamic task such as catching a ball: it is not possible to pause the ball mid-air while waiting for the agent to decide on the next control to command. In addition to solving dynamic tasks where blocking models would fail, thinking and acting concurrently can provide benefits such as smoother, human-like motions and the ability to seamlessly plan for next actions while executing the current one. Despite these potential benefits, most DRL approaches are mainly evaluated in blocking simulation environments. Blocking environments make the assumption that the environment state will not change between when the environment state is observed and when the action is executed. This assumption holds true in most simulated environments, which encompass popular domains such as Atari ( Mnih et al., 2013 ) and Gym control benchmarks ( Brockman et al., 2016 ). The system is treated in a sequential manner: the agent observes a state, freezes time while computing an action, and finally applies the action and unfreezes time. However, in dynamic real-time environments such as real-world robotics, the synchronous environment assumption is no longer valid. After observing the state of the environment and computing an action, the agent often finds that when it executes an action, the environment state has evolved from what it had initially observed; we consider this environment a concurrent environment. In this paper, we introduce an algorithmic framework that can handle concurrent environments in the context of DRL. In particular, we derive a modified Bellman operator for concurrent MDPs and Published as a conference paper at ICLR 2020 present the minimal set of information that we must augment state observations with in order to recover blocking performance with Q-learning. We introduce experiments on different simulated environments that incorporate concurrent actions, ranging from common simple control domains to vision-based robotic grasping tasks. Finally, we show an agent that acts concurrently in a real-world robotic grasping task is able to achieve comparable task success to a blocking baseline while acting 49% faster.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel probabilistic deep learning framework, DPS, for task-adaptive subsampling using a sub-Nyquist sampling scheme. DPS enables joint optimization of a subsampling pattern with a predictive downstream model, without the need for explicit knowledge on a sparsifying basis. Experiments demonstrate improved performance over strong subsampling baselines in image classification and reconstruction, while sampling both in Fourier and pixel space.",
        "Abstract": "The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal samples with a subsequent model that performs a required task. We demonstrate strong performance on reconstruction and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent subsampling rates in both the pixel and the spatial frequency domain. Due to the task-agnostic nature of the framework, DPS is directly applicable to all real-world domains that benefit from sample rate reduction.",
        "Introduction": "  INTRODUCTION In many real-world prediction problems, acquiring data is expensive and often bandwidth- constrained. Such is the case in regimes as medical imaging ( Lustig et al., 2007 ;  Choi et al., 2010 ;  Chernyakova & Eldar, 2014 ), radar ( Baraniuk, 2007 ), and seismic surveying ( Herrmann et al., 2012 ). By carefully reducing the number of samples acquired over time, in pixel-coordinate space or in k- space, efficient subsampling schemes lead to meaningful reductions in acquisition time, radiation exposure, battery drain, and data transfer. Subsampling is traditionally approached by exploiting expert knowledge on the signal of interest. Famously, the Nyquist theorem states that when the maximum frequency of a continuous signal is known, perfect reconstruction is possible when sampled at twice this frequency. More recently, it has been shown that if the signal is sparse in a certain domain, sub-Nyquist rate sampling can be achieved through compressive measurements and subsequent optimization of a linear system under said sparsity prior; a framework known as compressed sensing (CS) ( Donoho et al., 2006 ;  Eldar & Kutyniok, 2012 ;  Baraniuk, 2007 ). CS methods however lack in the sense that they do not (under a given data distribution) focus solely on the information required to solve the downstream task of interest, such as disease prediction or semantic segmentation. Formalizing such knowledge is challenging in its own right and would require careful analysis for each modality and downstream task. In this work, we propose to explore Published as a conference paper at ICLR 2020 the deep learning hypothesis as a promising alternative: reducing the need for expert knowledge in lieu of large datasets and end-to-end optimization of neural networks. As subsampling is non-differentiable, its integration into an end-to-end optimized deep learning model is non-trivial. Here we take a probabilistic approach: rather than learning a subsampling scheme directly, we pose a probability distribution that expresses belief over effective subsampling patterns and optimize the distribution's parameters instead. To enable differentiable sampling from this distribution, we leverage recent advancements in a continuous relaxation of this sampling pro- cess, known as Gumbel-softmax sampling or sampling from a concrete distribution ( Jang et al., 2017 ;  Maddison et al., 2016 ). This enables end-to-end training of both the subsampling scheme and the downstream model. Naively, the number of parameters of a distribution over an n-choose-k problem scales factorially, which is intractable for all practical purposes. We propose a novel, expressive yet tractable, parame- terization for the subsampling distribution that conditions on the output sample index. We hypothe- size that such conditioning prevents redundant sampling: it enables modeling the scenario in which multiple candidate samples can be equally good, yet redundant in combination. Furthermore, we investigate a parameter-restricted approach with a single parameter per candidate sample, balancing tractability and exploration ( Kool et al., 2019 ). In this case, we adopt a continuous relaxation of top-K sampling to guarantee differentiability ( Plötz & Roth, 2018 ). Our main contributions are as follows: • DPS: A new regime for task-adaptive subsampling using a novel probabilistic deep learning framework for incorporating a sub-Nyquist sampling scheme into an end-to-end network. • DPS enables joint optimization of a subsampling pattern with a predictive downstream model, without the need for explicit knowledge on a sparsifying basis. • We demonstrate improved performance over strong subsampling baselines in image classi- fication and reconstruction, while sampling both in Fourier and pixel space.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Unlikelihood Training, an approach to neural text generation that addresses the flaws of the standard approach of training a sequence to sequence model to maximize log-likelihood. Unlikelihood Training combines two types of updates: a likelihood update on the true target tokens so that they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. This approach is shown to improve metrics that measure dullness and repetition of the model, while maintaining performance in other metrics such as perplexity or token accuracy compared to the maximum likelihood baseline. Human evaluations show that Unlikelihood Training vastly improves the quality of generated text compared to likelihood trained models when both models use beam search decoding.",
        "Abstract": "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",
        "Introduction": "  INTRODUCTION Neural text generation is a vital tool in a wide range of natural language applications. However, the standard approach - training a sequence to sequence model, e.g. Transformer ( Vaswani et al., 2017 ), to maximize log-likelihood and approximately decoding the most likely sequence - is known to be flawed. Generated text in open-ended applications such as language modeling or dialogue has been observed to be dull, with high frequency tokens used too often and interesting content words used too rarely ( Holtzman et al., 2019 ;  Dinan et al., 2019 ). Moreover, the models repeat themselves at the token, phrase, and sentence levels, and statistics comparing a set of human-generated utterances and model-generated responses indicate a discrepancy between the human and model word distributions. This does not appear to be rectified by training on more data ( Radford et al., 2019 ). Recent fixes involve modifying the decoding strategy using sampling or more sophisticated beam search variants. However, these decoding strategies do not address the core issue: the model's underlying sequence probabilities are clearly not correct. Several reasons for exactly why neural text is degenerate have been posited, with the cause currently unknown. Possible candidates include the problem being (i) a by-product of the model architecture, e.g. the Transformer architecture preferring repeats ( Holtzman et al., 2019 ;  Vig, 2018 ), (ii) an intrin- sic property of human language ( Holtzman et al., 2019 ) rather than a modeling deficiency, or that (iii) a training objective relying on fixed corpora cannot take into account the real goal of using the language ( Choi, 2018 ). Our work shows that, while the above may be factors, a primary factor is the use of the likelihood objective itself, as we demonstrate that degeneration is alleviated if we replace the likelihood objective with our proposal. While low perplexity in the limit should lead to predicting the correct next target word, there are two major flaws of the likelihood objective: (i) it pays relatively little attention to the argmax or the top of the ranked list of next token probabilities, instead optimizing the likelihood of the entire distribution; (ii) it is not focused on optimizing sequence generation, only on producing the next token. The first issue means that greedy or beam search decoding, which rely on the top of the list to generate, are not optimized - there is a discrepancy between maximizing the log-probability of a ground-truth token and ensuring the rank of the ground-truth token to be one. The second issue means that during sequence generation, any imperfection in next token prediction leads to error accumulation that is not addressed by likelihood training. In this work, we introduce unlikelihood training, an approach that addresses the two aforementioned issues. It combines two types of updates: a likelihood update on the true target tokens so that they are assigned high probability, and an unlikelihood update on tokens that are otherwise assigned too high a probability. We can collect these unlikely token candidates either during next-token prediction or from generated sequences, allowing us to train at both the token and sequence levels. Both token and sequence level unlikelihood training are shown to improve metrics that measure dullness and repetition of the model, while maintaining performance in other metrics such as perplexity or token accuracy compared to the maximum likelihood baseline. Finally, we assess our models using human evaluations. We find that our generations have vastly improved quality compared to likelihood trained models when both models use beam search decoding. Moreover, our approach when using beam search also significantly improves over likelihood trained models using either beam blocking or nucleus sampling, thus outperforming the current state-of-the-art.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes an extension to the Long Short-Term Memory (LSTM) architecture to improve the generalization ability of language models. The proposed model introduces additional gating operations to the LSTM, where the input is gated conditioned on the output of the previous step and the output of the previous time step is gated. Results demonstrate the utility of the proposed approach, which consistently improves on the LSTM and establishes a new state of the art on all but the largest dataset.",
        "Abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3–4 perplexity points on Penn Treebank and Wikitext-2, and 0.01–0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n",
        "Introduction": "  INTRODUCTION The domination of Natural Language Processing by neural models is hampered only by their limited ability to generalize and questionable sample complexity ( Belinkov and Bisk 2017 ;  Jia and Liang 2017 ;  Iyyer et al. 2018 ;  Moosavi and Strube 2017 ;  Agrawal et al. 2016 ), their poor grasp of grammar ( Linzen et al. 2016 ;  Kuncoro et al. 2018 ), and their inability to chunk input sequences into meaningful units ( Wang et al. 2017 ). While direct attacks on the latter are possible, in this paper, we take a language-agnostic approach to improving Recurrent Neural Networks (RNN,  Rumelhart et al. (1988) ), which brought about many advances in tasks such as language modelling, semantic parsing, machine translation, with no shortage of non-NLP applications either ( Bakker 2002 ;  Mayer et al. 2008 ). Many neural models are built from RNNs including the sequence-to-sequence family ( Sutskever et al. 2014 ) and its attention-based branch ( Bahdanau et al. 2014 ). Thus, innovations in RNN architecture tend to have a trickle-down effect from language modelling, where evaluation is often the easiest and data the most readily available, to many other tasks, a trend greatly strengthened by ULMFiT ( Howard and Ruder 2018 ), ELMo (Peters et al. 2018) and BERT (Devlin et al. 2018), which promote language models from architectural blueprints to pretrained building blocks. To improve the generalization ability of language models, we propose an extension to the LSTM ( Hochreiter and Schmidhuber 1997 ), where the LSTM's input x is gated conditioned on the output of the previous step h prev . Next, the gated input is used in a similar manner to gate the output of the previous time step. After a couple of rounds of this mutual gating, the last updated x and h prev are fed to an LSTM. By introducing these additional of gating operations, in one sense, our model joins the long list of recurrent architectures with gating structures of varying complexity which followed the invention of Elman Networks ( Elman 1990 ). Examples include the LSTM, the GRU ( Chung et al. 2015 ), and even designs by Neural Architecture Search (Zoph and Le 2016). Intuitively, in the lowermost layer, the first gating step scales the input embedding (itself a representa- tion of the average context in which the token occurs) depending on the actual context, resulting in a contextualized representation of the input. While intuitive, as Section 4 shows, this interpretation cannot account for all the observed phenomena. In a more encompassing view, our model can be seen as enriching the mostly additive dynamics of recurrent transitions placing it in the company of the Input Switched Affine Network ( Foerster et al. Published as a conference paper at ICLR 2020 2017) with a separate transition matrix for each possible input, and the Multiplicative RNN ( Sutskever et al. 2011 ), which factorizes the three-way tensor of stacked transition matrices. Also following this line of research are the Multiplicative Integration LSTM ( Wu et al. 2016 ) and - closest to our model in the literature - the Multiplicative LSTM ( Krause et al. 2016 ). The results in Section 3.4 demonstrate the utility of our approach, which consistently improves on the LSTM and establishes a new state of the art on all but the largest dataset, Enwik8, where we match similarly sized transformer models.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper outlines the relationship between probabilistic inference and optimal control, and how reinforcement learning (RL) combines the two into a general framework for decision making under uncertainty. It reviews the RL problem and presents a simple and coherent framing of RL as probabilistic inference. Three approximations to the intractable Bayes-optimal policy are presented, including Thompson sampling, the popular 'RL as inference' framing, and K-learning. Computational studies are presented to support the claims.",
        "Abstract": "Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference’ and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular ‘RL as inference’ approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.\n",
        "Introduction": "  INTRODUCTION Probabilistic inference is a procedure of making sense of uncertain data using Bayes' rule. The optimal control problem is to take actions in a known system in order to maximize the cumulative rewards through time. Probabilistic graphical models (PGMs) offer a coherent and flexible language to specify causal relationships, for which a rich literature of learning and inference techniques have developed ( Koller & Friedman, 2009 ). Although control dynamics might also be encoded as a PGM, the relationship between action planning and probabilistic inference is not immediately clear. For inference, it is typically enough to specify the system and pose the question, and the objectives for learning emerge automatically. In control, the system and objectives are known, but the question of how to approach a solution may remain extremely complex ( Bertsekas, 2005 ). Perhaps surprisingly, there is a deep sense in which inference and control can represent a dual view of the same problem. This relationship is most clearly stated in the case of linear quadratic systems, where the Ricatti equations relate the optimal control policy in terms of the system dynamics ( Welch et al., 1995 ). In fact, this connection extends to a wide range of systems, where control tasks can be related to a dual inference problem through rewards as exponentiated probabilities in a distinct, but coupled, PGM ( Todorov, 2007 ; 2008). A great benefit of this connection is that it can allow the tools of inference to make progress in control problems, and vice-versa. In both cases the connections provide new insights, inspire new algorithms and enrich our understanding ( Toussaint & Storkey, 2006 ;  Ziebart et al., 2008 ;  Kappen et al., 2012 ). Reinforcement learning (RL) is the problem of learning to control an unknown system ( Sutton & Barto, 2018 ). Like the control setting, an RL agent should take actions to maximize its cumulative rewards through time. Like the inference problem, the agent is initially uncertain of the system dynamics, but can learn through the transitions it observes. This leads to a fundamental tradeoff: The agent may be able to improve its understanding through exploring poorly-understood states and actions, but it may be able to attain higher immediate reward through exploiting its existing knowledge ( Kearns & Singh, 2002 ). In many ways, RL combines control and inference into a general framework for decision making under uncertainty. Although there has been ongoing research Published as a conference paper at ICLR 2020 in this area for many decades, there has been a recent explosion of interest as RL techniques have made high-profile breakthroughs in grand challenges of artificial intelligence research ( Mnih et al., 2013 ;  Silver et al., 2016 ). A popular line of research has sought to cast 'RL as inference', mirroring the dual relationship for control in known systems. This approach is most clearly stated in the tutorial and review of  Levine (2018) , and provides a key reference for research in this field. It suggests that a generalization of the RL problem can be cast as probabilistic inference through inference over exponentiated rewards, in a continuation of previous work in optimal control ( Todorov, 2009 ). This perspective promises several benefits: A probabilistic perspective on rewards, the ability to apply powerful inference algorithms to solve RL problems and a natural exploration strategy. In this paper we will outline an important way in which this perspective is incomplete. This shortcoming ultimately results in algorithms that can perform poorly in even very simple decision problems. Importantly, these are not simply technical issues that show up in some edge cases, but fundamental failures of this approach that arise in even the most simple decision problems. In this paper we revisit an alternative framing of 'RL as inference'. In fact, we show that the orig- inal RL problem was already an inference problem all along. 1 Importantly, this inference problem includes inference over the agent's future actions and observations. Of course, this perspective is not new, and has long been known as simply the Bayes-optimal solution, see, e.g.,  Ghavamzadeh et al. (2015) . The problem is that, due to the exponential lookahead, this inference problem is fundamentally intractable for all but the simplest problems ( Gittins, 1979 ). For this reason, RL re- search focuses on computationally efficient approaches that maintain a level of statistical efficiency ( Furmston & Barber, 2010 ;  Osband et al., 2017 ). We provide a review of the RL problem in Section 2, together with a simple and coherent framing of RL as probabilistic inference. In Section 3 we present three approximations to the intractable Bayes-optimal policy. We begin with the celebrated Thompson sampling algorithm, then we review the popular 'RL as inference' framing, as presented by  Levine (2018) , and highlight a clear and simple shortcoming in this approach. Finally, we review K-learning ( O'Donoghue, 2018 ), which we re-interpret as a modification to the RL as inference framework that provides a principled approach to the statistical inference problem, as well as a presenting a relationship with Thompson sampling. In Section 4 we present computational studies that support our claims.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper proposes a novel attribution method, IBA (Information Bottleneck Attribution), which uses a variational approximation to upper-bound the amount of information an image region provides to the network's prediction. This method improves model interpretability and increases trust in attribution results. Two approaches are proposed to learn the parameters of the information bottleneck - either using a single sample (Per-Sample Bottleneck), or the entire dataset (Readout Bottleneck). The method is evaluated against ten different baselines and outperforms them consistently. An easy-to-use implementation of the method is provided, along with a novel evaluation method for attribution based on bounding boxes.",
        "Abstract": "Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work, we adopt the information bottleneck concept for attribution. By adding noise to intermediate feature maps, we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for attribution values (bits) and a guarantee that regions scored close to zero are not necessary for the network's decision. ",
        "Introduction": "  INTRODUCTION Deep neural networks have become state of the art in many real- world applications. However, their increasing complexity makes it difficult to explain the model's output. For some applications such as in medical decision making or autonomous driving, model interpretability is an important requirement with legal implications. Attribution methods (Selvaraju et al., 2017; Zeiler & Fergus, 2014; Smilkov et al., 2017) aim to explain the model behavior by assigning a relevance score to each input variable. When applied to images, the relevance scores can be visualized as heatmaps over the input pixel space, thus highlighting salient areas relevant for the network's decision. For attribution, no ground truth exists. If an attribution heatmap highlights subjectively irrelevant areas, this might correctly reflect the network's unexpected way of processing the data, or the heatmap might be inaccurate (Nie et al., 2018; Viering et al., 2019; Sixt et al., 2019). Given an image of a railway locomotive, the attribution map might highlight the train tracks instead of the train itself. Current attribution methods cannot guarantee that the network is ignroing the low-scored locomotive for the prediction. We propose a novel attribution method that estimates the amount of information an image region provides to the network's prediction. We use a variational approximation to upper-bound this estimate and therefore, can guarantee that areas with zero bits of information are not necessary for the prediction.  Figure 1  shows an exemplary heatmap of our method. Up to 3 bits per pixel are available for regions corresponding to the monkeys' faces, whereas the tree is scored with close to zero bits per pixel. We can thus guarantee that the tree is not necessary for predicting the correct class. To estimate the amount of information, we adapt the information bottleneck concept (Tishby et al., 2000; Alemi et al., 2017). The bottleneck is inserted into an existing neural network and restricts the information flow by adding noise to the activation maps. Unimportant activations are replaced almost entirely by noise, removing all information for subsequent network layers. We developed two approaches to learn the parameters of the bottleneck - either using a single sample (Per-Sample Bottleneck), or the entire dataset (Readout Bottleneck). We evaluate against ten different baselines. First, we calculated the Sensitivity-n metric proposed by Ancona et al. (2018). Secondly, we quantified how well the object of interest was localized using bounding boxes and extend the degradation task proposed by Ancona et al. (2017). In all these metrics, our method outperforms the baselines consistently. Additionally, we test the impact of cascading layer-wise weight randomizations on the attribution heatmaps (Adebayo et al., 2018). For reproducibility, we share our source code and provide an easy-to-use * implementation. We name our method IBA which stands for Information Bottleneck Attribution. It provides a theoretic upper-bound on the used information while demonstrating strong empirical performance. Our work improves model interpretablility and increases trust in attribution results. To summarize our contributions: • We adapt the information bottleneck concept for attribution to estimate the information used by the network. Information theory provides a guarantee that areas scored irrelevant are indeed not necessary for the network's prediction. • We propose two ways - Per-Sample and Readout Bottleneck - to learn the parameters of the information bottleneck. • We contribute a novel evaluation method for attribution based on bounding boxes and we also extend the metric proposed by Ancona et al. (2017) to provide a single scalar value and improve the metric's comparability between different network architectures.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents soft Q imitation learning (SQIL), a simple and general imitation learning algorithm that is effective in Markov Decision Processes (MDPs) with high-dimensional, continuous observations and unknown dynamics. SQIL is a regularized variant of behavioral cloning (BC) that learns long-horizon imitation by imposing a sparsity prior on the reward function implied by the imitation policy and incorporating information about the state transition dynamics into the imitation policy. Experiments in four image-based environments and three low-dimensional environments show that SQIL outperforms BC and achieves competitive results compared to generative adversarial imitation learning (GAIL). SQIL is advantageous because it can overcome the state distribution shift problem of BC without adversarial training or learning a reward function, and is simple to implement using existing Q-learning or off-policy actor-critic algorithms.",
        "Abstract": "Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards.",
        "Introduction": "  INTRODUCTION Many sequential decision-making problems can be tackled by imitation learning: an expert demon- strates near-optimal behavior to an agent, and the agent attempts to replicate that behavior in novel situations ( Argall et al., 2009 ). This paper considers the problem of training an agent to imitate an expert, given expert action demonstrations and the ability to interact with the environment. The agent does not observe a reward signal or query the expert, and does not know the state transition dynamics. Standard approaches based on behavioral cloning (BC) use supervised learning to greedily imitate demonstrated actions, without reasoning about the consequences of actions (Pomerleau, 1991). As a result, compounding errors cause the agent to drift away from the demonstrated states (Ross et al., 2011). The problem with BC is that, when the agent drifts and encounters out-of-distribution states, the agent does not know how to return to the demonstrated states. Recent methods based on in- verse reinforcement learning (IRL) overcome this issue by training an RL agent not only to imitate demonstrated actions, but also to visit demonstrated states ( Ng et al., 2000 ;  Wulfmeier et al., 2015 ;  Finn et al., 2016b ;  Fu et al., 2017 ). This is also the core idea behind generative adversarial imi- tation learning (GAIL) ( Ho & Ermon, 2016 ), which implements IRL using generative adversarial Published as a conference paper at ICLR 2020 networks ( Goodfellow et al., 2014 ;  Finn et al., 2016a ). Since the true reward function for the task is unknown, these methods construct a reward signal from the demonstrations through adversarial training, making them difficult to implement and use in practice ( Kurach et al., 2018 ). The main idea in this paper is that the effectiveness of adversarial imitation methods can be achieved by a much simpler approach that does not require adversarial training, or indeed learning a reward function at all. Intuitively, adversarial methods encourage long-horizon imitation by providing the agent with (1) an incentive to imitate the demonstrated actions in demonstrated states, and (2) an incentive to take actions that lead it back to demonstrated states when it encounters new, out-of- distribution states. One of the reasons why adversarial methods outperform greedy methods, such as BC, is that greedy methods only do (1), while adversarial methods do both (1) and (2). Our approach is intended to do both (1) and (2) without adversarial training, by using constant rewards instead of learned rewards. The key idea is that, instead of using a learned reward function to provide a reward signal to the agent, we can simply give the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior. We motivate this approach theoretically, by showing that it implements a regularized variant of BC that learns long-horizon imitation by (a) imposing a sparsity prior on the reward function implied by the imitation policy, and (b) incorporating information about the state transition dynamics into the imitation policy. Intuitively, our method accomplishes (a) by training the agent using an ex- tremely sparse reward function - +1 for demonstrations, 0 everywhere else - and accomplishes (b) by training the agent with RL instead of supervised learning. We instantiate our approach with soft Q-learning ( Haarnoja et al., 2017 ) by initializing the agent's experience replay buffer with expert demonstrations, setting the rewards to a constant r = +1 in the demonstration experiences, and setting rewards to a constant r = 0 in all of the new experiences the agent collects while interacting with the environment. Since soft Q-learning is an off-policy algorithm, the agent does not necessarily have to visit the demonstrated states in order to experience positive rewards. Instead, the agent replays the demonstrations that were initially added to its buffer. Thus, our method can be applied in environments with stochastic dynamics and continuous states, where the demonstrated states are not necessarily reachable by the agent. We call this method soft Q imitation learning (SQIL). The main contribution of this paper is SQIL: a simple and general imitation learning algorithm that is effective in MDPs with high-dimensional, continuous observations and unknown dynamics. We run experiments in four image-based environments - Car Racing, Pong, Breakout, and Space Invaders - and three low-dimensional environments - Humanoid, HalfCheetah, and Lunar Lander - to compare SQIL to two prior methods: BC and GAIL. We find that SQIL outperforms BC and achieves com- petitive results compared to GAIL. Our experiments illustrate two key benefits of SQIL: (1) that it can overcome the state distribution shift problem of BC without adversarial training or learning a re- ward function, which makes it easier to use, e.g., with images, and (2) that it is simple to implement using existing Q-learning or off-policy actor-critic algorithms.",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper explores how learned video models can enable learning in the Atari Learning Environment (ALE) benchmark with a budget restricted to 100K time steps. We introduce a system, called Simulated Policy Learning (SimPLe), that utilizes stochastic video prediction techniques and trains a policy to play the game within the learned model. Our empirical evaluation finds that SimPLe is significantly more sample-efficient than a highly tuned version of the state-of-the-art Rainbow algorithm on almost all games. This work advances the state-of-the-art in model-based reinforcement learning by demonstrating successful planning with a learned model in the ALE.",
        "Abstract": "Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.",
        "Introduction": "  INTRODUCTION Human players can learn to play Atari games in minutes ( Tsividis et al., 2017 ). However, some of the best model-free reinforcement learning algorithms require tens or hundreds of millions of time steps - the equivalent of several weeks of training in real time. How is it that humans can learn these games so much faster? Perhaps part of the puzzle is that humans possess an intuitive understanding of the physical processes that are represented in the game: we know that planes can fly, balls can roll, and bullets can destroy aliens. We can therefore predict the outcomes of our actions. In this paper, we explore how learned video models can enable learning in the Atari Learning Environment (ALE) benchmark  Bellemare et al. (2015) ;  Machado et al. (2018)  with a budget restricted to 100K time steps - roughly to two hours of a play time. Although prior works have proposed training predictive models for next-frame, future-frame, as well as combined future-frame and reward predictions in Atari games ( Oh et al. (2015) ;  Chiappa et al. (2017) ;  Leibfried et al. (2016) ), no prior work has successfully demonstrated model-based control via predictive models that achieve competitive results with model-free RL. Indeed, in a recent survey (Section 7.2 in  Machado et al. (2018) ) this was formulated as the following challenge: \"So far, there has been no clear demonstration of successful planning with a learned model in the ALE\". Using models of environments, or informally giving the agent ability to predict its future, has a fundamental appeal for reinforcement learning. The spectrum of possible applications is vast, including learning policies from the model ( Watter et al., 2015 ;  Finn et al., 2016 ;  Finn & Levine, 2017 ;  Ebert et al., 2017 ;  Hafner et al., 2019 ;  Piergiovanni et al., 2018 ;  Rybkin et al., 2018 ;  Sutton & Barto, Published as a conference paper at ICLR 2020  2017, Chapter 8), capturing important details of the scene ( Ha & Schmidhuber, 2018 ), encouraging exploration ( Oh et al., 2015 ), creating intrinsic motivation ( Schmidhuber, 2010 ) or counterfactual reasoning ( Buesing et al., 2019 ). One of the exciting benefits of model-based learning is the promise to substantially improve sample efficiency of deep reinforcement learning (see Chapter 8 in  Sutton & Barto (2017) ). Our work advances the state-of-the-art in model-based reinforcement learning by introducing a system that, to our knowledge, is the first to successfully handle a variety of challenging games in the ALE benchmark. To that end, we experiment with several stochastic video prediction techniques, including a novel model based on discrete latent variables. We present an approach, called Simulated Policy Learning (SimPLe), that utilizes these video prediction techniques and trains a policy to play the game within the learned model. With several iterations of dataset aggregation, where the policy is deployed to collect more data in the original game, we learn a policy that, for many games, successfully plays the game in the real environment (see videos on the project webpage https://goo.gl/itykP8). In our empirical evaluation, we find that SimPLe is significantly more sample-efficient than a highly tuned version of the state-of-the-art Rainbow algorithm ( Hessel et al., 2018 ) on almost all games. In particular, in low data regime of 100k samples, on more than half of the games, our method achieves a score which Rainbow requires at least twice as many samples. In the best case of Freeway, our method is more than 10x more sample-efficient, see  Figure 3 . Since the publication of the first preprint of this work, it has been shown in  van Hasselt et al. (2019) ;  Kielak (2020)  that Rainbow can be tuned to have better results in low data regime. The results are on a par with SimPLe - both of the model-free methods are better in 13 games, while SimPLe is better in the other 13 out of the total 26 games tested (note that in Section 4.2 van  Hasselt et al. (2019)  compares with the results of our first preprint, later improved).",
        "label": 1
    },
    {
        "Summary": "\n\nThis paper presents Dreamer, an agent that learns long-horizon behaviors from images purely by latent imagination. A novel actor critic algorithm accounts for rewards beyond the imagination horizon while making efficient use of the neural network dynamics. The agent is evaluated on the DeepMind Control Suite with image inputs, and is shown to exceed previous model-based and model-free agents in terms of data-efficiency, computation time, and final performance.",
        "Abstract": "Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.",
        "Introduction": "  INTRODUCTION Intelligent agents can achieve goals in complex environments even though they never encounter the exact same situation twice. This ability requires building representations of the world from past experience that enable generalization to novel situations. World models offer an explicit way to represent an agent's knowledge about the world in a parametric model that can make predictions about the future. When the sensory inputs are high-dimensional images, latent dynamics models can abstract observations to predict forward in compact state spaces (Watter et al., 2015; Oh et al., 2017;  Gregor et al., 2019 ). Compared to predictions in image space, latent states have a small memory footprint that enables imagining thousands of trajectories in parallel. Learning effective latent dynamics models is becoming feasible through advances in deep learning and latent variable models (Krishnan et al., 2015;  Karl et al., 2016 ;  Doerr et al., 2018 ;  Buesing et al., 2018 ). Behaviors can be derived from dynamics models in many ways. Often, imagined rewards are maximized with a parametric policy ( Sutton, 1991 ;  Ha and Schmidhuber, 2018 ; Zhang et al., 2019) or by online planning ( Chua et al., 2018 ;  Hafner et al., 2018 ). However, considering only rewards within a fixed imagination horizon results in shortsighted behaviors (Wang et al., 2019). Moreover, prior work commonly resorts to derivative-free optimization for robustness to model errors ( Ebert et al., 2017 ;  Chua et al., 2018 ; Parmas et al., 2019), rather than leveraging analytic gradients offered by neural network dynamics (Henaff et al., 2019;  Srinivas et al., 2018 ). We present Dreamer, an agent that learns long-horizon behaviors from images purely by latent imagination. A novel actor critic algorithm accounts for rewards beyond the imagination horizon while making efficient use of the neural network dynamics. For this, we predict state values and actions in the learned latent space as summarized in  Figure 1 . The values optimize Bellman consistency for imagined rewards and the policy maximizes the values by propagating their analytic gradients back through the dynamics. In comparison to actor critic algorithms that learn online or by experience replay (Lillicrap et al., 2015;  Mnih et al., 2016 ; Schulman et al., 2017;  Haarnoja et al., 2018 ;  Lee et al., 2019 ), world models can interpolate past experience and offer analytic gradients of multi-step returns for efficient policy optimization. The key contributions of this paper are summarized as follows: • Learning long-horizon behaviors by latent imagination Model-based agents can be short- sighted if they use a finite imagination horizon. We approach this limitation by predicting both actions and state values. Training purely by imagination in a latent space lets us efficiently learn the policy by propagating analytic value gradients back through the latent dynamics. • Empirical performance for visual control We pair Dreamer with existing representation learning methods and evaluate it on the DeepMind Control Suite with image inputs, illustrated in  Figure 2 . Using the same hyper parameters for all tasks, Dreamer exceeds previous model-based and model-free agents in terms of data-efficiency, computation time, and final performance.",
        "label": 1
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the use of long short-term memory (LSTM) networks to learn quantum optical setups and predict the characteristics of the resulting quantum states. The neural networks are trained using millions of setups generated by MELVIN, and cluster cross validation is used to evaluate the models. This approach has the potential to automate the design of complex quantum experiments, which is important for multiparticle and multisetting violations of local realist models and for applications in emerging quantum technologies.",
        "Abstract": "We demonstrate how machine learning is able to model experiments in quantum physics. Quantum entanglement is a cornerstone for upcoming quantum technologies such as quantum computation and quantum cryptography. Of particular interest are complex quantum states with more than two particles and a large number of entangled quantum levels. Given such a multiparticle high-dimensional quantum state, it is usually impossible to reconstruct an experimental setup that produces it. To search for interesting experiments, one thus has to randomly create millions of setups on a computer and calculate the respective output states. In this work, we show that machine learning models can provide significant improvement over random search. We demonstrate that a long short-term memory (LSTM) neural network can successfully learn to model quantum experiments by correctly predicting output state characteristics for given setups without the necessity of computing the states themselves. This approach not only allows for faster search but is also an essential step towards automated design of multiparticle high-dimensional quantum experiments using generative machine learning models.",
        "Introduction": "  INTRODUCTION In the past decade, artificial neural networks have been applied to a plethora of scientific disciplines, commercial applications, and every-day tasks with outstanding performance in, e.g., medical diagno- sis, self-driving, and board games ( Esteva et al., 2017 ;  Silver et al., 2017 ). In contrast to standard feedforward neural networks, long short-term memory (LSTM) ( Hochreiter, 1991 ;  Hochreiter & Schmidhuber, 1997 ) architectures have recurrent connections, which allow them to process sequential data such as text and speech ( Sutskever et al., 2014 ). Such sequence-processing capabilities can be particularly useful for designing complex quantum experiments, since the final state of quantum particles depends on the sequence of elements, i.e. the experimental setup, these particles pass through. For instance, in quantum optical experiments, photons may traverse a sequence of wave plates, beam splitters, and holographic plates. High- dimensional quantum states are important for multiparticle and multisetting violations of local realist models as well as for applications in emerging quantum technologies such as quantum communication and error correction in quantum computers ( Shor, 2000 ;  Kaszlikowski et al., 2000 ). Already for three photons and only a few quantum levels, it becomes in general infeasible for humans to determine the required setup for a desired final quantum state, which makes automated design procedures for this inverse problem necessary. One example of such an automated procedure is the algorithm MELVIN ( Krenn et al., 2016 ), which uses a toolbox of optical elements, randomly generates sequences of these elements, calculates the resulting quantum state, and then checks whether the state is interesting, i.e. maximally entangled and involving many quantum levels. The setups proposed by MELVIN have been realized in laboratory experiments ( Malik et al., 2016 ;  Erhard et al., 2018b ). Recently, also a reinforcement learning approach has been applied to design new experiments ( Melnikov et al., 2018 ). Inspired by these advances, we investigate how LSTM networks can learn quantum optical setups and predict the characteristics of the resulting quantum states. We train the neural networks using millions of setups generated by MELVIN. The huge amount of data makes deep learning approaches the first choice. We use cluster cross validation ( Mayr et al., 2016 ) to evaluate the models.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of hidden features of regression neural networks for out-of-distribution (OOD) detection. We argue empirically and theoretically that OOD inputs cannot be identified simply using the networks' predictions. We demonstrate that the features of in-distribution inputs lie on an intrinsically low-dimensional portion of the feature space, and that it is possible to model the in-distribution features with simple generative models. We develop new OOD detection tasks based on large-scale computer vision regression datasets and evaluate the OOD detection performance of several generative models. We find that a simple mixture of Gaussians is better at OOD detection than more complex models such as variational autoencoders, and that GMM models of in-distribution features are able to outperform other regression OOD methods across several benchmarks.",
        "Abstract": "Neural network out-of-distribution (OOD) detection aims to identify when a model is unable to generalize to new inputs, either due to covariate shift or anomalous data. Most existing OOD methods only apply to classification tasks, as they assume a discrete set of possible predictions. In this paper, we propose a method for neural network OOD detection that can be applied to regression problems. We demonstrate that the hidden features for in-distribution data can be described by a highly concentrated, low dimensional distribution. Therefore, we can model these in-distribution features with an extremely simple generative model, such as a Gaussian mixture model (GMM) with 4 or fewer components. We demonstrate on several real-world benchmark data sets that GMM-based feature detection achieves state-of-the-art OOD detection results on several regression tasks. Moreover, this approach is simple to implement and computationally efficient.",
        "Introduction": "  INTRODUCTION The success of deep neural networks in many domains ( Krizhevsky et al., 2012 ;  Lample et al., 2016 ;  Mnih et al., 2016 ) is due to their ability to learn complex functions that generalize to new samples. However, this observed generalization only extends to data that are sufficiently similar to the training data. If the neural network encounters data that deviates from the distribution of training data, its predictions are likely to be erroneous or nonsensical ( Guo et al., 2017 ;  Jiang et al., 2012 ;  Begoli et al., 2019 ). This may occur if the model is used in scenarios that experience covariate shift ( Sugiyama et al., 2007 ) or if the model encounters previously-unseen categories of data ( Yu et al., 2017 ;  Hassen & Chan, 2018 ). Such scenarios are examples of out-of-distribution (OOD) inputs. Ideally we would like for neural networks to adapt to such shifts in the data distribution ( Amodei et al., 2016 ). In the absence of such adaptation, out-of-distribution detection should be used to identify when a model is unable to generalize to a previously-unseen input. While there are sev- eral proposed methods for neural network OOD detection ( Hendrycks & Gimpel, 2017 ;  Liang et al., 2018 ;  Lee et al., 2018b ), many of these methods rely on architectural components specific to classifi- cation neural networks. Consequentially, they cannot be applied to regression problems. Regression neural networks typically output only a point prediction rather than a predictive distribution, and thus the output does not indicate its uncertainty or reliability for a given input. This is illustrated in  Figure 1 , which displays predictions from a network trained to predict prices of middle-class houses in Kentucky. The network outputs a house price prediction for any possible input - even for out- of-distribution images like a California mansion or chair. These predictions fall within the normal range of possible prices, and therefore do not convey that the inputs are not valid for this model. Because the predictions cannot identify OOD inputs, we must look for alternative signals. Previous approaches perform regression OOD detection through ensembles (Gal & Ghahramani, 2016;  Lak- shminarayanan et al., 2017 ) or through an additional uncertainty prediction layer ( Kendall & Gal, 2017 ;  Malinin et al., 2017 ). In this paper, we instead turn to the space of hidden features. During training a neural network learns to extract relevant features about the training data and discards ir- relevant information. Whether or not a network generalizes to a given test sample depends on the extracted features from that sample. Networks do not generalize to out-of-distribution data because the distributional shift causes the network to extract the wrong information. For example, when the housing neural network is applied to the California mansion in  Figure 1 , the network's features do not extract the relevant information that would indicate the true price of the house. (e.g. presence of Under review as a conference paper at ICLR 2020 palm trees). In other words, the extracted features of the California mansion will differ significantly from training data features and therefore the network does not generalize. Based on this intuition, we investigate how to utilize the hidden features of regression neural net- works for OOD detection. In particular, we make several contributions. First, we argue empirically and theoretically that we cannot identify OOD inputs simply using only the networks' predictions. However, we then demonstrate that the features of in-distribution inputs lie on an intrinsically low- dimensional portion of the feature space. It is unlikely that OOD inputs map to similar locations in feature space because of this low-dimensionality. We additionally show that, because of this low- dimensionality, it is possible to model the in-distribution features with simple generative models. To evaluate our proposed approach, we develop new OOD detection tasks based on large-scale com- puter vision regression datasets. We evaluate the OOD detection performance of several generative models trained on in-distribution features. Surprisingly, we find that a simple mixture of Gaussians - often with no more than 2 components - is better at OOD detection than more complex models such as variational autoencoders. Finally, we demonstrate that GMM models of in-distribution features are able to outperform other regression OOD methods across several benchmarks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a new set of contextualized embeddings for source code and explores its usefulness on the task of name-based bug detection. We demonstrate that our method significantly outperforms existing static representations methods on both the DeepBugs dataset as well as a new previously unused test set of JavaScript projects. We release our implementation and representations as they could lead to improvements in a great variety of software engineering tasks.",
        "Abstract": "Continuous embeddings of tokens in computer programs have been used to support a variety of software development tools, including readability, code search, and program repair. \nContextual embeddings are common in natural language processing but have not been previously applied in software engineering.\nWe introduce a new set of deep contextualized word representations for computer programs based on language models.\nWe train a set of embeddings using the ELMo (embeddings from language models) framework of Peters et al (2018).\nWe investigate whether these embeddings are effective when fine-tuned for the downstream task of bug detection.\nWe show that even a low-dimensional embedding trained on a relatively small corpus of programs can improve a state-of-the-art machine learning system for bug detection.",
        "Introduction": "  INTRODUCTION Learning rich representations for source code is an open problem that has the potential to enable software engineering and development tools. Some work on machine learning for source code has used hand engineered features (Long & Rinard, 2016, e.g.), but designing and implementing such features can be tedious and error-prone. For this reason, other work considers the task of learning a representation of source code from data ( Allamanis et al., 2018a ). Many models of source code are based on learned representations called embeddings, which transform words into a continuous vector space ( Mikolov et al., 2013 ). Currently in software engineering (SE) researchers have used static embeddings ( Harer et al., 2018 ;  White et al., 2019 ;  Pradel & Sen, 2018 ), which map a word to the same vector regardless of its context. However, recent work in natural language processing (NLP) has found that contextual embeddings can lead to better performance ( Peters et al., 2018 ;  Devlin et al., 2018 ;  Yang et al., 2019 ;  Liu et al., 2019 ). Contextualized embeddings assign a different vector to a word based on the context it is used. For NLP this has the advantage that it can model phenomena like polysemy. A natural question to ask is if these methods would also be beneficial for learning better SE representations. In this paper, we introduce a new set of contextual embeddings for source code. Contextual embeddings have several potential modelling advantages that are specifically suited to modelling source code: • Surrounding names contain important information about an identifier. For example, for a variable name, surrounding tokens might include functions that take that variable as an argument or assignments to the variable. These tokens provide indirect information about possible values the variable could take, and so should affect its representation. Even keywords can have very different meanings based on their context. For instance, a private function is not the same as a private variable or a private class (in the case of Java / C++). • Contextual embeddings assign a different representation to a variable each time it is used in the program. By doing this, they can potentially capture how a variable's value evolves through the program execution. • Contextual embeddings enable the use of transfer learning. Pre-training a large neural language model and querying it for contextualized representations while simultaneously fine-tuning for the specific task is a very effective technique for supervised tasks for which there is a small amount of supervised data available. As a result only a small model needs to be fine-tuned atop the pre-trained model, without the need for task-specific architectures nor the need of training a large model for each task separately. In this paper, we highlight the potential of contextual code embeddings for program repair. Automatically finding bugs in code is an important open problem in SE. Even simple bugs can be hard to spot and repair. A promising approach to this end is name-based bug detection, introduced by DeepBugs ( Pradel & Sen, 2018 ). The current state-of-the-art in name-based bug detection relies on static representations from Word2Vec ( Mikolov et al., 2013 ) to learn a classifier that distinguishes correct from incorrect code for a specific bug pattern. We introduce a new set of contextualized Under review as a conference paper at ICLR 2020 embeddings for code and explore its usefulness on the task of name-based bug detection. Our method significantly outperforms DeepBugs as well as other static representations methods on both the DeepBugs dataset as well as a new previously unused test set of JavaScript projects. We release our implementation and representations as they could lead to improvements in a great variety of SE tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new field of Explainable AI (XAI) and the XAI saliency map tool, which is used to show what parts of an image or video frame are most important to a network's decisions. It discusses several methods of deriving a gradient saliency map, such as back-propagating a gradient from the end of the network, iteratively augmenting the image or a mask, and training a saliency map encoder within the network itself. Additionally, it discusses Class Activation Map (CAM) methods, which efficiently map a specified class to a region in an image, but the saliency map is very coarse.",
        "Abstract": "We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale. This is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence.  Finally, we visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods.  Our method is generally straight forward and should be applicable to the most commonly used CNNs. (Full source code is available at http://www.anonymous.submission.com).",
        "Introduction": "  INTRODUCTION Deep neural networks (DNN) have provided a new burst of research in the machine learning com- munity. However, their complexity obfuscates the underlying processes that drive their inferences. This has lead to a new field of explainable AI (XAI). A variety of tools are being developed to enable researchers to peer into the inner workings of DNNs. One such tool is the XAI saliency map. It is generally used with image or video processing applications and is supposed to show what parts of an image or video frame are most important to a network's decisions. The seemingly most popular methods derive a gradient saliency map by back-propagating a gradient from the end of the network and project it onto an image plane (Simonyan et al., 2014; Zeiler & Fergus, 2014; Springenberg et al., 2015; Sundararajan et al., 2017; Patro et al., 2019). The gradient can typically be from a loss function, layer activation or class activation. Thus, it requires storage of the data necessary to compute a full backward pass on the input image. Several newer methods attempt to iteratively augment the image or a mask in ways that affect the precision of the results (Fong & Vedaldi, 2017; Chang et al., 2018). Additionally, saliency map encoders can be trained within the network itself (Dabkowski & Gal, 2017). Both of these methods have a distinct advantage of being more self-evidently empirical when compared with gradient tech- niques. Class Activation Map (CAM) methods (Selvaraju et al., 2017; Chattopadhyay et al., 2018; Omeiza et al., 2019) efficiently map a specified class to a region in an image, but the saliency map is very coarse. They generally use a method like Guided Backprop (Springenberg et al., 2015) to add finer pixel level details. This requires a full backwards pass through the network, and it adds signif- icant memory and computational overhead to CAM solutions relative to just computing the CAM alone. Several of the CAM methods compute gradients aside from the use of Guided Backprop, but we will differentiate them by referring to them as CAM methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces two embedding-based models, HSimplE and HypE, for link prediction in knowledge hypergraphs. These models are evaluated on two new datasets obtained from subsets of FREEBASE, and outperform existing models and baselines. The results demonstrate the advantage of disentangling position representation from entity embeddings. The contributions of this paper are two new models for knowledge hypergraph completion, a set of baselines for knowledge hypergraph completion, and two new knowledge hypergraphs obtained from subsets of FREEBASE.",
        "Abstract": "A Knowledge Hypergraph is a knowledge base where relations are defined on two or more entities. In this work, we introduce two embedding-based models that perform link prediction in knowledge hypergraphs:\n(1) HSimplE is a shift-based method that is inspired by an existing model operating on knowledge graphs, in which the representation of an entity is a function of its position in the relation, and (2) HypE is a convolution-based method which disentangles the representation of an entity from its position in the relation. We test our models on two new knowledge hypergraph datasets that we obtain from Freebase, and show that both HSimplE and HypE are more effective in predicting links in knowledge hypergraphs than the proposed baselines and existing methods.\nOur experiments show that HypE outperforms HSimplE when trained with fewer parameters and when tested on samples that contain at least one entity in a position never encountered during training.",
        "Introduction": "  INTRODUCTION Knowledge Hypergraphs are graph structured knowledge bases that store facts about the world in the form of relations between two or more entities. They can be seen as one generalization of Knowl- edge Graphs, in which relations are defined on exactly two entities. Since accessing and storing all the facts in the world is difficult, knowledge bases are incomplete; the goal of link prediction (or knowledge completion) in knowledge (hyper)graphs is to predict unknown links or relationships between entities based on the existing ones. In this work we are interested in the problem of link prediction in knowledge hypergraphs. Our motivation for studying link prediction in these more sophisticated knowledge structures is based on the fact that most knowledge in the world has in- herently complex composition, and that not all data can be represented as a relation between two entities without either losing a portion of the information or creating incorrect data points. Link prediction in knowledge graphs is a problem that is studied extensively, and has applications in several tasks such as searching ( Singhal, 2012 ) and automatic question answering ( Ferrucci et al., 2010 ). In these studies, knowledge graphs are defined as directed graphs having nodes as entities and labeled edges as relations; edges are directed from the head entity to the tail entity. The com- mon data structure for representing knowledge graphs is a set of triples relation(head, tail) that represent information as a collection of binary relations. There exist a large number of knowledge graphs that are publicly available, such as NELL ( Carlson et al., 2010 ) and FREEBASE ( Bollacker et al., 2008 ). It is noteworthy to mention that FREEBASE is a complex knowledge base where 61% of the relations are beyond binary (defined on more than two nodes). However, current methods use a simplified version of FREEBASE where the non-binary relations are converted to binary ones (defined on exactly two entities). Embedding-based models ( Nguyen, 2017 ) have proved to be effective for knowledge graph comple- tion. These approaches learn embeddings for entities and relations. To find out if r(h, t) is a fact (i.e. is true), such models define a function that embeds relation r and entities h and t, and produces the probability that r(h, t) is a fact. While successful, such embedding-based methods make the strong assumption that all relations are binary. In this work, we introduce two embedding-based models that perform link prediction in knowledge hypergraphs. The first is HSimplE, which is inspired from SimplE ( Kazemi & Poole, 2018 ), origi- Under review as a conference paper at ICLR 2020 nally designed to perform link prediction in knowledge graphs. For a given entity, HSimplE shifts the entity embedding by a value that depends on the position of the entity in the given relation. Our second model is HypE, which in addition to learning entity embeddings, learns positional (convo- lutional) filters; these filters are disentangled from entity representations and are used to transform the representation of an entity based on its position in a relation. We show that both HSimplE and HypE are fully expressive. To evaluate our models, we introduce two new datasets from subsets of FREEBASE, and develop baselines by extending existing models on knowledge graphs to work with hypergraphs. We evaluate the proposed methods on standard binary and non-binary datasets. While both HSimplE and HypE outperform our baselines and the state-of-the-art, HypE is more effective with fewer parameters. It also produces much better results when predicting relations that contain at least one entity in a position never encountered during training, demonstrating the clear advantage of disentangling position representation from entity embeddings. The contributions of this paper are: (1) HypE and HSimplE, two embedding-based methods for knowledge hypergraph completion that outperform the baselines for knowledge hypergraphs, (2) a set of baselines for knowledge hypergraph completion, and (3) two new knowledge hypergraphs obtained from subsets of FREEBASE, which can serve as new evaluation benchmarks for knowledge hypergraph completion methods.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel technique to perform amortized approximate posterior inference over discrete latent variables in mixture models. The technique uses neural networks to express posteriors in the form of multinomial distributions in terms of fixed-dimensional, distributed representations that respect the permutation symmetries imposed by the discrete variables. This method is a natural choice for nonparametric Bayesian models, such as Dirichlet process mixture models, and can be applied to both conjugate and non-conjugate models.",
        "Abstract": "Mixture models, a basic building block in countless statistical models, involve latent random variables over discrete spaces, and existing posterior inference methods can be inaccurate and/or very slow.  In this work we introduce a novel deep learning architecture for efficient amortized Bayesian inference over mixture models. While previous approaches to amortized clustering assumed a fixed or maximum number of mixture components and only amortized over the continuous parameters of each mixture component, our method amortizes over the local discrete labels of all the data points, and performs inference over an unbounded number of mixture components. The latter property makes our method natural for the challenging case of nonparametric Bayesian models, where the number of mixture components grows with the dataset. Our approach exploits the exchangeability of the generative models and is based on mapping distributed, permutation-invariant representations of discrete  arrangements into varying-size multinomial conditional probabilities. The resulting algorithm parallelizes easily, yields iid samples from the approximate posteriors along with a normalized probability estimate of each sample (a quantity generally unavailable using Markov Chain Monte Carlo) and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model. We also present an extension of the method to models of random communities (such as infinite relational or stochastic block models). As a scientific application, we present a novel approach to neural spike sorting for high-density multielectrode arrays. \n",
        "Introduction": "  INTRODUCTION Mixture models (or equivalently, probabilistic clustering models) are a staple of statistical modelling in which a discrete latent variable is introduced for each observation, indicating its mixture component identity. Popular inference methods in these models fall into two main classes. When exploring the full posterior is crucial (e.g. there is irreducible uncertainty about the latent structure or many separate local optima exist), the method of choice is Markov Chain Monte Carlo (MCMC) (Neal, 2000; Jain & Neal, 2004). This method is asymptotically accurate but time-consuming, with convergence that is difficult to assess. Models whose likelihood and prior are non-conjugate are particularly challenging, since in general in these cases the model parameters cannot be marginalized and must be kept as part of the state of the Markov chain. Alternatively, variational methods (Blei & Jordan, 2004; Kurihara et al., 2007; Hughes et al., 2015) are typically much faster but do not come with accuracy guarantees. As an alternative to MCMC and variational approaches, in recent years there has been steady progress on amortized inference methods, and such is the spirit of this work. Concretely, we propose a novel technique to perform amortized approximate posterior inference over discrete latent variables in mixture models. The basic idea is to use neural networks to express posteriors in the form of multinomial distributions (with varying support) in terms of fixed-dimensional, distributed representations that respect the permutation symmetries imposed by the discrete variables. A major advantage of our architecture, compared to previous approaches to amortized clustering, is its ability to handle an arbitrary number of clusters. This makes the method a natural choice for nonparametric Bayesian models, such as Dirichlet process mixture models (DPMM), and their extensions, where the number of components, a measure of the model complexity, is inferred as a posterior random variable; see (Rodriguez & Mueller, 2013) for a recent overview. Moreover, the method can be applied to both conjugate and non-conjugate models.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a semi-parametric Gaussian Copula approach to transfer learning for hyperparameter optimization (HPO). We propose two HPO strategies, a Copula Thompson Sampling and a Gaussian Copula Process, which can jointly model several objectives with potentially different scales, such as validation error and compute time, without requiring processing. We demonstrate significant speed-ups over a number of baselines in extensive experiments.",
        "Abstract": "Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Despite its success, standard BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance metrics of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different metrics. The main idea is to regress the mapping from hyperparameter to metric quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple metrics such as runtime and accuracy, steering the optimization toward cheaper hyperparameters for the same level of accuracy. Experiments on an extensive set of hyperparameter tuning tasks demonstrate significant improvements over state-of-the-art methods.",
        "Introduction": "  INTRODUCTION Tuning complex machine learning models such as deep neural networks can be a daunting task. Object detection or language understanding models often rely on deep neural networks with many tunable hyperparameters, and automatic hyperparameter optimization (HPO) techniques such as Bayesian optimization (BO) are critical to find the good hyperparameters in short time. BO ad- dresses the black-box optimization problem by placing a probabilistic model on the function to minimize (e.g., the mapping of neural network hyperparameters to a validation loss), and determine which hyperparameters to evaluate next by trading off exploration and exploitation through an ac- quisition function. While traditional BO focuses on each problem in isolation, recent years have seen a surge of interest in transfer learning for HPO. The key idea is to exploit evaluations from previous, related tasks (e.g., the same neural network tuned on multiple datasets) to further speed up the hyperparameter search. A central challenge of hyperparameter transfer learning is that different tasks typically have different scales, varying noise levels, and possibly contain outliers, making it hard to learn a joint model. In this work, we show how a semi-parametric Gaussian Copula can be leveraged to learn a joint prior across datasets in such a way that scale issues vanish. We then demonstrate how such prior estimate can be used to transfer information across tasks and objectives. We propose two HPO strategies: a Copula Thompson Sampling and a Gaussian Copula Process. We show that these approaches can jointly model several objectives with potentially different scales, such as validation error and compute time, without requiring processing. We demonstrate significant speed-ups over a number of baselines in extensive experiments. The paper is organized as follows. Section 2 reviews related work on transfer learning for HPO. Section 3 introduces Copula regression, the building block for the HPO strategies we propose in Section 4. Specifically, we show how Copula regression can be applied to design two HPO strategies, one based on Thompson sampling and an alternative GP-based approach. Experimental results are given in Section 5 where we evaluate both approaches against state-of-the-art methods on three algorithms. Finally, Section 6 outlines conclusions and further developments. A variety of methods have been developed to induce transfer learning in HPO. The most com- mon approach is to model tasks jointly or via a conditional independence structure, which has been been explored through multi-output GPs ( Swersky et al., 2013 ), weighted combination of GPs ( Schilling et al., 2016 ; Wistuba et al., 2018;  Feurer et al., 2018 ), and neural networks, either fully Bayesian ( Springenberg et al., 2016 ) or hybrid ( Snoek et al., 2015 ;  Perrone et al., 2018 ;  Law et al., 2018 ). A different line of research has focused on the setting where tasks come over time as a sequence and models need to be updated online as new problems accrue. A way to achieve this is to fit a sequence of surrogate models to the residuals relative to predictions of the previously fitted model ( Golovin et al., 2017 ;  Poloczek et al., 2016 ). Specifically, the GP over the new task is centered on the predictive mean of the previously learned GP. Finally, rather than fitting a surrogate model to all past data, some transfer can be achieved by warm-starting BO with the solutions to the previous BO problems ( Feurer et al., 2015 ; Wistuba et al., 2015b). A key challenge for joint models is that different black-boxes can exhibit heterogeneous scale and noise levels ( Bardenet et al., 2013 ;  Feurer et al., 2018 ). To address this, some methods have instead focused on search-space level, aiming to prune it to focus on regions of the hyperparameter space where good configurations are likely to lie. An example is  Wistuba et al. (2015a) , where related tasks are used to learn a promising search space during HPO, defining task similarity in terms of the distance of the respective data set meta-features. A more recent alternative that does not require meta-features was introduced in  Perrone et al. (2019) , where a restricted search space in the form of a low-volume hyper-rectangle or hyper-ellipsoid is learned from the optimal hyperparameters of related tasks. Rank estimation can be used to alleviate scales issues however the difficulty of feeding back rank information to GP leads to restricting assumptions, for instance ( Bardenet et al., 2013 ) does not model the rank estimation uncertainty while ( Feurer et al., 2018 ) uses independent GPs removing the adaptivity of the GP to the current task. Gaussian Copula Process (GCP) ( Wilson & Ghahramani, 2010 ) can also be used to alleviate scale issues on a single task at the extra cost of estimating the CDF of the data. Using GCP for HPO was proposed in  Anderson et al. (2017)  to handle potentially non-Gaussian data, albeit only considering non-parametric homoskedastic priors for the single-task and single objective case.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a meta-algorithm to improve the quality of translation tasks, such as molecular optimization, which is a key step in drug development. The approach builds on the idea that it is easier to evaluate candidate objects than to generate them, and uses a learned predictor of target object quality (a filter) to guide the generation process. The generative model is viewed as an adaptively tuned prior distribution over complex objects, with the filter as the likelihood. The approach is similar to self-training or reranking approaches employed for parsing, but in this case, the filter is relatively simple and remains fixed during the iterative process.",
        "Abstract": "Many challenging prediction problems, from molecular optimization to program synthesis, involve creating complex structured objects as outputs. However, available training data may not be sufficient for a generative model to learn all possible complex transformations. By leveraging the idea that evaluation is easier than generation, we show how a simple, broadly applicable, iterative target augmentation scheme can be surprisingly effective in guiding the training and use of such models. Our scheme views the generative model as a prior distribution, and employs a separately trained filter as the likelihood. In each augmentation step, we filter the model's outputs to obtain additional prediction targets for the next training epoch. Our method is applicable in the supervised as well as semi-supervised settings. We demonstrate that our approach yields significant gains over strong baselines both in molecular optimization and program synthesis. In particular, our augmented model outperforms the previous state-of-the-art in molecular optimization by over 10% in absolute gain. ",
        "Introduction": "  INTRODUCTION Deep architectures are becoming increasingly adept at generating complex objects such as images, text, molecules, or programs. Many useful generation problems can be seen as translation tasks, where the goal is to take a source (precursor) object such as a molecule and turn it into a target satisfying given design characteristics. Indeed, molecular optimization of this kind is a key step in drug development, though the adoption of automated tools remains limited due to accuracy concerns. We propose here a simple, broadly applicable meta-algorithm to improve translation quality. Translation is a challenging task for many reasons. Objects are complex and the available training data pairs do not fully exemplify the intricate ways in which valid targets can be created from the precursors. Moreover, precursors provided at test time may differ substantially from those available during training - a scenario common in drug development. While data augmentation and semi- supervised methods have been used to address some of these challenges, the focus has been on either simple prediction tasks (e.g., classification) or augmenting data primarily on the source side. We show, in contrast, that iteratively augmenting translation targets significantly improves performance on complex generation tasks in which each precursor corresponds to multiple possible outputs. Our iterative target augmentation approach builds on the idea that it is easier to evaluate candidate objects than to generate them. Thus a learned predictor of target object quality (a filter) can be used to effectively guide the generation process. To this end, we construct an external filter and apply it to the complex generative model's sampled translations of training set precursors. Candidate translations that pass the filter criteria become part of the training data for the next training epoch. The translation model is therefore iteratively guided to generate candidates that pass the filter. The generative model can be viewed as an adaptively tuned prior distribution over complex objects, with the filter as the likelihood. For this reason, it is helpful to apply the filter at test time as well, or to use the approach transductively 1 to adapt the generation process to novel test cases. The approach is reminiscent of self-training or reranking approaches employed with some success for parsing ( McClosky et al., 2006 ;  Charniak et al., 2016 ). However, in our case, it is the candidate generator that is complex while the filter is relatively simple and remains fixed during the iterative process.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new metric called Fréchet Joint Distance (FJD) for evaluating the performance of generative models, such as Variational Auto-Encoders (VAEs), auto-regressive models, and Generative Adversarial Networks (GANs). FJD is able to implicitly assess image quality, conditional consistency, and intra-conditioning diversity, and introduces only small computational overhead over existing metrics such as Inception Score (IS) and Fréchet Inception Distance (FID). Experiments on a synthetic dataset and real-world datasets demonstrate that FJD captures the desired properties of conditional generation, can be applied to any kind of conditioning, and has potential to be used as a unified metric for hyper-parameter selection and cGAN benchmarking.",
        "Abstract": "Conditional Generative Adversarial Networks (cGANs) are finding increasingly widespread use in many application domains. Despite outstanding progress, quantitative evaluation of such models often involves multiple distinct metrics to assess different desirable properties, such as image quality, conditional consistency, and intra-conditioning diversity. In this setting, model benchmarking becomes a challenge, as each metric may indicate a different \"best\" model. In this paper, we propose the Frechet Joint Distance (FJD), which is defined as the Frechet distance between joint distributions of images and conditioning, allowing it to implicitly capture the aforementioned properties in a single metric. We conduct proof-of-concept experiments on a controllable synthetic dataset, which consistently highlight the benefits of FJD when compared to currently established metrics. Moreover, we use the newly introduced metric to compare existing cGAN-based models for a variety of conditioning modalities (e.g. class labels, object masks, bounding boxes, images, and text captions). We show that FJD can be used as a promising single metric for model benchmarking.",
        "Introduction": "  INTRODUCTION The use of generative models is growing across many domains ( van den Oord et al., 2016c ;  Vondrick et al., 2016 ;  Serban et al., 2017 ;  Karras et al., 2018 ;  Brock et al., 2019 ). Among the most promising approaches, Variational Auto-Encoders (VAEs) ( Kingma & Welling, 2014 ), auto-regressive models ( van den Oord et al., 2016a ;b), and Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ) have been driving significant progress, with the latter at the forefront of a wide-range of applications ( Mirza & Osindero, 2014 ;  Reed et al., 2016 ;  Zhang et al., 2018a ;  Vondrick et al., 2016 ;  Almahairi et al., 2018 ;  Subramanian et al., 2018 ;  Salvador et al., 2019 ). In particular, significant research has emerged from practical applications, which require generation to be based on existing context. For example, tasks such as image inpainting, super-resolution, or text-to-image synthesis have been successfully addressed within the framework of conditional generation, with conditional GANs (cGANs) among the most competitive approaches. Despite these outstanding advances, quantitative evaluation of GANs remains a challenge ( Theis et al., 2016 ;  Borji, 2018 ). In the last few years, a significant number of evaluation metrics for GANs have been introduced in the literature ( Salimans et al., 2016 ;  Heusel et al., 2017 ;  Bińkowski et al., 2018 ;  Shmelkov et al., 2018 ;  Zhou et al., 2019 ;  Kynkäänniemi et al., 2019 ;  Ravuri & Vinyals, 2019 ). Although there is no clear consensus on which quantitative metric is most appropriate to benchmark GAN-based models, Inception Score (IS) ( Salimans et al., 2016 ) and Fréchet Inception Distance (FID) ( Heusel et al., 2017 ) have been extensively used. However, both IS and FID were introduced in the context of unconditional image generation and, hence, focus on capturing certain desirable properties such as visual quality and sample diversity, which do not fully encapsulate all the different phenomena that arise during conditional image generation. In conditional generation, we care about visual quality, conditional consistency - i.e., verifying that the generation respects its conditioning, and intra-conditioning diversity - i.e., sample diversity per conditioning. Although visual quality is captured by both metrics, IS is agnostic to intra-conditioning diversity and FID only captures it indirectly. 1 Moreover, neither of them can capture conditional con- Under review as a conference paper at ICLR 2020 sistency. In order to overcome these shortcomings, researchers have resorted to reporting conditional consistency and diversity metrics in conjunction with FID ( Zhao et al., 2019 ;  Park et al., 2019 ). Consistency metrics often use some form of concept detector to ensure that the requested conditioning appears in the generated image as expected. Although intuitive to use, these metrics require pre- trained models that cover the same target concepts in the same format as the conditioning (i.e., classifiers for image-level class conditioning, semantic segmentation for mask conditioning, etc.), which may or may not be available off-the-shelf. Moreover, using different metrics to evaluate different desirable properties may hinder the process of model selection, as there may not be a single model that surpasses the rest in all measures. In fact, it has recently been demonstrated that there is a natural trade-off between image quality and sample diversity ( Yang et al., 2019 ), which calls into question how we might select the correct balance of these properties. In this paper we introduce a new metric called Fréchet Joint Distance (FJD), which is able to implicitly assess image quality, conditional consistency, and intra-conditioning diversity. FJD computes the Fréchet distance on an embedding of the joint image-conditioning distribution, and introduces only small computational overhead over FID compared to alternative methods. We evaluate the properties of FJD on a variant of the synthetic dSprite dataset ( Matthey et al., 2017 ) and verify that it successfully captures the desired properties. We provide an analysis on the behavior of both FID and FJD under different types of conditioning such as class labels, bounding boxes, and object masks, and evaluate a variety of existing cGAN models for real-world datasets with the newly introduced metric. Our experiments show that (1) FJD captures the three highlighted properties of conditional generation; (2) it can be applied to any kind of conditioning (e.g., class, bounding box, mask, image, text, etc.); and (3) when applied to existing cGAN-based models, FJD demonstrates its potential to be used as a promising unified metric for hyper-parameter selection and cGAN benchmarking. To our knowledge, there are no existing metrics for conditional generation that capture all of these key properties.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes three easy-to-use benchmark datasets to assess distortion robustness in object detection. Each dataset contains versions of the original object detection dataset which are corrupted with 15 distortions, each spanning five levels of severity. After evaluating standard object detection algorithms on these benchmark datasets, the paper shows how a simple data augmentation technique-stylizing the training images-can strongly improve robustness across corruption type, severity and dataset.",
        "Abstract": "The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed PASCAL-C, COCO-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30-60% of the original performance). However, a simple data augmentation trick - stylizing the training images - leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are available at: (hidden for double blind review)",
        "Introduction": "  INTRODUCTION A day in the near future: Autonomous vehicles are swarming the streets all over the world, tirelessly collecting data. But on this cold November afternoon traffic comes to an abrupt halt as it suddenly begins to snow: winter is coming. Huge snowflakes are falling from the sky and the cameras of autonomous vehicles are no longer able to make sense of their surroundings, triggering immediate emergency brakes. A day later, an investigation of this traffic disaster reveals that the unexpectedly large size of the snowflakes was the cause of the chaos: While state-of-the-art vision systems had been trained on a variety of common weather types, their training data contained hardly any snowflakes of this size... This fictional example highlights the problems that arise when Convolutional Neural Networks (CNNs) encounter settings that were not explicitly part of their training regime. For example, state- of-the-art object detection algorithms such as Faster R-CNN (Ren et al., 2015) fail to recognize objects when snow is added to an image (as shown in Figure 1), even though the objects are still clearly visible to a human eye. At the same time, augmenting the training data with several types of distortions is not a sufficient solution to achieve general robustness against previously unknown corruptions: It has recently been demonstrated that CNNs generalize poorly to novel distortion types, despite being trained on a variety of other distortions (Geirhos et al., 2018). On a more general level, CNNs often fail to generalize outside of the training domain or training data distribution. Examples include the failure to generalize to images with uncommon poses of objects (Alcorn et al., 2019) or to cope with small distributional changes (e.g. Zech et al., 2018; Touvron et al., 2019). One of the most extreme cases are adversarial examples (Szegedy et al., 2013): images with a domain shift so small that it is imperceptible for humans yet sufficient to fool a DNN. We here focus on the less extreme but far more common problem of perceptible image distortions like blurry images, noise or natural distortions like snow. As an example, autonomous vehicles need to be able to cope with wildly varying outdoor conditions such as fog, frost, snow, sand storms, or falling leaves, just to name a few (as visualized in  Figure 2 ). One of the major reasons why autonomous cars have not yet gone mainstream is the inability of their recognition models to function well in adverse weather conditions (Dai & Van Gool, 2018). Getting data for unusual weather conditions is hard and while many common environmental conditions can (and have been) modelled, including fog (Sakaridis et al., 2018b), rain (Hospach et al., 2016), snow (Bernuth et al., 2019) and daytime to nighttime transitions (Dai & Van Gool, 2018), it is impossible to foresee all potential conditions that might occur \"in the wild\". If we could build models that are robust to every possible image corruption, is is to be expected that weather changes would not be an issue. However, in order to assess the robustness of models one first needs to define a measure. While testing models on the set of all possible corruption types is impossible. We therefore propose to evaluate models on a diverse range of corruption types that were not part of the training data and demonstrate that this is a useful approximation for predicting performance under natural distortions like rain, snow, fog or the transition between day and night. More specifically we propose three easy-to-use benchmark datasets termed PASCAL-C, COCO-C and Cityscapes-C to assess distortion robustness in object detection. Each dataset contains versions of the original object detection dataset which are corrupted with 15 distortions, each spanning five levels of severity. This approach follows Hendrycks & Dietterich (2019), who introduced corrupted versions of commonly used classification datasets (ImageNet-C, CIFAR10-C) as standardized benchmarks. After evaluating standard object detection algorithms on these benchmark datasets, we show how a simple data augmentation technique-stylizing the training images-can strongly improve robustness across corruption type, severity and dataset.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new approach to retrieving generating dynamical equations from observed time series data. The approach uses a piecewise-linear recurrent neural network (PLRNN) with special regularization terms for a subset of RNN units to push the network toward line attractor configurations along some directions in state space. The approach is demonstrated to outperform or be on par with LSTM and other initialization-based methods on a number of machine learning benchmarks. Additionally, the approach is shown to efficiently capture all relevant time scales in a dynamical system that exhibits widely different time scales.",
        "Abstract": "Vanilla RNN with ReLU activation have a simple structure that is amenable to systematic dynamical systems analysis and interpretation, but they suffer from the exploding vs. vanishing gradients problem. Recent attempts to retain this simplicity while alleviating the gradient problem are based on proper initialization schemes or orthogonality/unitary constraints on the RNN’s recurrency matrix, which, however, comes with limitations to its expressive power with regards to dynamical systems phenomena like chaos or multi-stability. Here, we instead suggest a regularization scheme that pushes part of the RNN’s latent subspace toward a line attractor configuration that enables long short-term memory and arbitrarily slow time scales. We show that our approach excels on a number of benchmarks like the sequential MNIST or multiplication problems, and enables reconstruction of dynamical systems which harbor widely different time scales.",
        "Introduction": "  INTRODUCTION Theories of complex systems in biology and physics are often formulated in terms of sets of stochas- tic differential or difference equations, i.e. as stochastic dynamical systems (DS). A long-standing desire is to retrieve these generating dynamical equations directly from observed time series data ( Kantz & Schreiber, 2004 ). A variety of machine and deep learning methodologies toward this goal have been introduced in recent years ( Chen et al., 2017 ;  Champion et al., 2019 ;  Jordan et al., 2019 ;  Duncker et al., 2019 ;  Ayed et al., 2019 ;  Durstewitz, 2017 ;  Koppe et al., 2019 ), many of them based on recurrent neural networks (RNN) which can universally approximate any DS (i.e., its flow field) under some mild conditions ( Funahashi & Nakamura, 1993 ;  Kimura & Nakano, 1998 ). However, vanilla RNN as often used in this context are well known for their problems in capturing long-term dependencies and slow time scales in the data ( Hochreiter & Schmidhuber, 1997 ;  Bengio et al., 1994 ). In DS terms, this is generally due to the fact that flexible information maintenance over long periods requires precise fine-tuning of model parameters toward 'line attractor' configurations ( Fig. 1 ), a concept first propagated in computational neuroscience for addressing animal perfor- mance in parametric working memory tasks ( Seung, 1996 ;  Seung et al., 2000 ;  Durstewitz, 2003 ). Line attractors introduce directions of zero-flow into the model's state space that enable long-term maintenance of arbitrary values ( Fig. 1 ). Specially designed RNN architectures equipped with gat- ing mechanisms and (linear) memory cells have been suggested for solving this issue ( Hochreiter & Schmidhuber, 1997 ;  Cho et al., 2014 ). However, from a DS perspective, simpler models that can more easily be analyzed and interpreted in DS terms, and for which more efficient inference algorithms exist that emphasize approximation of the true underlying DS would be preferable. Recent solutions to the vanishing vs. exploding gradient problem attempt to retain the simplicity of vanilla RNN by initializing or constraining the recurrent weight matrix to be the identity ( Le et al., 2015 ), orthogonal ( Henaff et al., 2016 ;  Helfrich et al., 2018 ) or unitary (Arjovsky et al., 2016). In this way, in a system including piecewise linear (PL) components like rectified-linear units (ReLU), line attractor dimensions are established from the start by construction or ensured throughout training by a specifically parameterized matrix decomposition. However, for many DS problems, line attractors instantiated by mere initialization procedures may be unstable and quickly dissolve during training. On the other hand, orthogonal or unitary constraints are too restrictive for reconstructing DS, and more generally from a computational perspective as well ( Kerg et al., 2019 ): For instance, neither Under review as a conference paper at ICLR 2020 chaotic behavior (that requires diverging directions) nor settings with multiple isolated fixed point or limit cycle attractors are possible. Here we therefore suggest a different solution to the problem, by pushing (but not strictly enforcing) ReLU-based, piecewise-linear RNN (PLRNN) toward line attractor configurations along some (but not all) directions in state space. We achieve this by adding special regularization terms for a subset of RNN units to the loss function that promote such a configuration. We demonstrate that our approach outperforms, or is en par with, LSTM and other, initialization-based, methods on a number of 'classical' machine learning benchmarks ( Hochreiter & Schmidhuber, 1997 ). More importantly, we demonstrate that while with previous methods it was difficult to capture slow behavior in a DS that exhibits widely different time scales, our new regularization-supported inference efficiently captures all relevant time scales.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel tensor variant of the popular graph convolutional network (GCN) architecture, called TensorGCN, for learning representations of dynamic graphs. TensorGCN captures correlation over time by leveraging the tensor M-product framework. The flexibility and matrix mimeticability of the framework help adapt the GCN architecture to tensor space. Experiments on real datasets illustrate the performance of TensorGCN for the edge classification task on dynamic graphs. Elements of the method can also be used as a preprocessing step for other dynamic graph methods.",
        "Abstract": "Many irregular domains such as social networks, financial transactions, neuron connections, and natural language structures are represented as graphs. In recent years, a variety of  graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. However, in many of the applications, the underlying graph changes over time and existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs based on a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results that establish the connection between the proposed tensor approach and spectral convolution of tensors are developed. Numerical experiments on real datasets demonstrate the usefulness of the proposed method for an edge classification task on dynamic graphs.",
        "Introduction": "  INTRODUCTION Graphs are popular data structures used to effectively represent interactions and structural relation- ships between entities in structured data domains. Inspired by the success of deep neural networks for learning representations in the image and language domains, recently, application of neural net- works for graph representation learning has attracted much interest. A number of graph neural net- work (GNN) architectures have been explored in the contemporary literature for a variety of graph related tasks and applications ( Hamilton et al., 2017 ;  Seo et al., 2018 ;  Chen et al., 2018 ;  Zhou et al., 2018 ;  Wu et al., 2019 ). Methods based on graph convolution filters which extend convolutional neu- ral networks (CNNs) to irregular graph domains are popular ( Bruna et al., 2013 ;  Defferrard et al., 2016 ;  Kipf and Welling, 2016 ). Most of these GNN models operate on a given, static graph. In many real-world applications, the underlining graph changes over time, and learning representa- tions of such dynamic graphs is essential. Examples include analyzing social networks ( Berger-Wolf and Saia, 2006 ), predicting collaboration in citation networks ( Leskovec et al., 2005 ), detecting fraud and crime in financial networks ( Weber et al., 2018 ;  Pareja et al., 2019 ), traffic control ( Zhao et al., 2019 ), and understanding neuronal activities in the brain ( De Vico Fallani et al., 2014 ). In such dynamic settings, the temporal interdependence in the graph connections and features also play a substantial role. However, efficient GNN methods that handle time varying graphs and that capture the temporal correlations are lacking. By dynamic graph, we mean a sequence of graphs (V, A (t) , X (t) ), t ∈ {1, 2, . . . , T }, with a fixed set V of N nodes, adjacency matrices A (t) ∈ R N ×N , and graph feature matrices X (t) ∈ R N ×F where X (t) n: ∈ R F is the feature vector consisting of F features associated with node n at time t. The graphs can be weighted, and directed or undirected. They can also have additional properties like (time varying) node and edge classes, which would be stored in a separate structure. Suppose we only observe the first T < T graphs in the sequence. The goal of our method is to use these observations to predict some property of the remaining T − T graphs. In this paper, we use it for edge classification. Other potential applications are node classification and edge/link prediction. In recent years, tensor constructs have been explored to effectively process high-dimensional data, in order to better leverage the multidimensional structure of such data ( Kolda and Bader, 2009 ). Tensor based approaches have been shown to perform well in many image and video processing ap- Under review as a conference paper at ICLR 2020 TensorGCN T 2 Embedding 1 Time A (1) Graph tasks Link prediction Edge classification Node classification Dynamic graph Adjacency tensor Feature Tensor X (1) In this paper, we propose a novel tensor variant of the popular graph convolutional network (GCN) architecture ( Kipf and Welling, 2016 ), which we call TensorGCN. It captures correlation over time by leveraging the tensor M-product framework. The flexibility and matrix mimeticability of the framework, help us adapt the GCN architecture to tensor space.  Figure 1  illustrates our method at a high level: First, the time varying adjacency matrices A (t) and feature matrices X (t) of the dynamic graph are aggregated into an adjacency tensor and a feature tensor, respectively. These tensors are then fed into our TensorGCN, which computes an embedding that can be used for a variety of tasks, such as link prediction, and edge and node classification. GCN architectures are motivated by graph convolution filtering, i.e., applying filters/functions to the graph Laplacian (in turn its eigenvalues) ( Bruna et al., 2013 ), and we establish a similar connection between TensorGCN and spectral filtering of tensors. Experimental results on real datasets illustrate the performance of our method for the edge classification task on dynamic graphs. Elements of our method can also be used as a preprocessing step for other dynamic graph methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a two-step process for inferring the implementation of user interface components from an image. The first step involves training a neural model to predict the most likely initial attribute values, and the second step uses imitation learning to iteratively refine the attribute values to achieve pixel-level accuracy. The approach is instantiated to the task of inferring the implementation of Android Button attributes and is evaluated on both synthetic and real-world datasets. Results show that the approach successfully infers the correct attribute values in 94.8% and 92.5% of the cases for the synthetic and the real-world datasets, respectively.",
        "Abstract": "We present a new approach that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer (e.g, using a vector graphics editor), we learn to infer its implementation which when rendered (e.g., on the Android platform), looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we also use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values.\n",
        "Introduction": "  INTRODUCTION With over 5 million applications in Google Play Store and Apple App Store and over a billion webpages, a significant amount of time can be saved by automating even small parts of their de- velopment. To achieve this, several tools have been recently developed that help user interface designers explore and quickly prototype different ideas, including Sketch2Code ( Microsoft, 2018 ) and InkToCode ( Corrado et al., 2018 ), which generate user interface sketches from hand-drawn images, Swire ( Huang et al., 2019 ) and Rico ( Deka et al., 2017 ), which allow retrieving designs similar to the one supplied by the user and Rewire ( Swearngin et al., 2018 ), which transforms im- ages into vector representations consisting of rectangles, circles and lines. At the same time, to help developers implement the design, a number of approaches have been proposed that generate layout code that places the user interface components at the desired position (e.g., when resizing the application). These include both symbolic synthesis approaches such as InferUI ( Bielik et al., 2018 ), which encodes the problem as a satisfiability query of a first-order logic formula, as well as statistical approaches ( Beltramelli, 2018 ;  Chen et al., 2018 ), which use encoder-decoder neural networks to process the input image and output the corresponding implementation. In this work, we explore a new domain of inferring an implementation of an user interface compo- nent from an image which when rendered, looks visually the same as the input image. Going from an image to a concrete implementation is a time consuming, yet necessary task, which is often out- sourced to a company for a high fee ( replia, 2019 ;  psd2android, 2019 ;  psd2mobi, 2019 ). Compared to prior work, we focus on the pixel-accurate implementation, rather than on producing sketches or the complementary task of synthesizing layouts that place the components at the desired positions. Concretely, given a black box rendering engine that defines a set of categorical and numerical at- tributes of a component, we design a two step process which predicts the attribute values from an input image - (i) first, we train a neural model to predict the most likely initial attribute values, and then (ii) we use imitation learning to iteratively refine the attribute values to achieve pixel-level accu- racy. Crucially, all our models are trained using synthetic datasets that are obtained by sampling the black box rendering engine, which makes it easy to train models for other attributes in the future. We instantiate our approach to the task of inferring the implementation of Android Button attributes and show that it generalizes well to a real-world dataset consisting of buttons found in existing Google Play Store applications. In particular, our approach successfully infers the correct attribute values in 94.8% and 92.5% of the cases for the synthetic and the real-world datasets, respectively.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the formal properties of counter machines as grammars, in order to understand the computational properties of long short-term memory networks (LSTMs) and their capacity for representing the structure of natural language. We prove that general counter machines, incremental counter machines, and stateless counter machines have equivalent expressive capacity, and demonstrate that counter languages are closed under complement, union, intersection, and other common operations. We also show that counter machines are incapable of representing the deep syntactic structure or semantics of boolean expressions, and prove that a certain subclass of the counter languages are semilinear. Our results provide insight into the capacity of LSTMs for representing natural language structure.",
        "Abstract": "While counter machines have received little attention in theoretical computer science since the 1960s, they have recently achieved a newfound relevance to the field of natural language processing (NLP). Recent work has suggested that some strong-performing recurrent neural networks utilize their memory as counters. Thus, one potential way to understand the sucess of these networks is to revisit the theory of counter computation. Therefore, we choose to study the abilities of real-time counter machines as formal grammars. We first show that several variants of the counter machine converge to express the same class of formal languages. We also prove that counter languages are closed under complement, union, intersection, and many other common set operations. Next, we show that counter machines cannot evaluate boolean expressions, even though they can weakly validate their syntax. This has implications for the interpretability and evaluation of neural network systems: successfully matching syntactic patterns does not guarantee that a counter-like model accurately represents underlying semantic structures. Finally, we consider the question of whether counter languages are semilinear. This work makes general contributions to the theory of formal languages that are of particular interest for the interpretability of recurrent neural networks.",
        "Introduction": "  INTRODUCTION It is often taken for granted that modeling natural language syntax well requires a hierarchically structured grammar formalism. Early work in linguistics established that finite-state models are insufficient for describing the dependencies in natural language data ( Chomsky, 1956 ). Instead, a formalism capable of expressing the relations in terms of hierarchical constituents ought to be necessary. Recent advances in deep learning and NLP, however, challenge this long-held belief. Neural network formalisms like the long short-term memory network (LSTM) ( Hochreiter & Schmidhuber, 1997 ) have been shown to perform well on tasks requiring structure sensitivity (Linzen et al., 2016), even though it is not obvious that such models have the capacity to represent hierarchical structure. This mismatch raises interesting questions for both linguists and practitioners of NLP. It is unclear what about the LSTM's structure lends itself towards good linguistic representations, and under what conditions these representations might fall short of grasping the structure and meaning of language. Recent work has suggested that the expressive capacity of LSTMs resembles that of counter ma- chines ( Merrill, 2019 ; Suzgun et al., 2019;  Weiss et al., 2018 ).  Weiss et al. (2018)  studied LSTMs with fully saturated weights (i.e. the activation functions evaluate to their asymptotic values instead of intermediate rational values) and showed that such models can express simplified counter lan- guages.  Merrill (2019) , on the other hand, showed that the general counter languages are an upper bound on the expressive capacity of saturated LSTMs. Thus, there seems to be a strong theoretical connection between LSTMs and the counter automata.  Merrill (2019) ;  Suzgun et al. (2019) ;  Weiss et al. (2018)  also all report experimental results suggesting that some class of counter languages matches the learnable capacity of LSTMs trained by gradient descent. Taking the counter machine as a simplified formal model of the LSTM, we study the formal prop- erties of counter machines as grammars. We do this with the hope of understanding to what degree counter machines, and LSTMs by extension, have computational properties well-suited for repre- senting the structure of natural language. The contributions of this paper are as follows: Under review as a conference paper at ICLR 2020 • We prove that general counter machines, incremental counter machines, and stateless counter machines have equivalent expressive capacity, whereas simplified counter ma- chines ( Weiss et al., 2018 ) are strictly weaker than the general class. • We demonstrate that counter languages are closed under complement, union, intersection, and many other common operations. • We show that counter machines are incapable of representing the deep syntactic structure or semantics of boolean expressions, even though they can validate whether a boolean expression is well-formed. • We prove that a certain subclass of the counter languages are semilinear, and conjecture that this result holds for all counter languages.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel variational deep learning approach for hashing-based collaborative filtering, called Variational Hashing-based collaborative filtering with Self-Masking (VaHSM-CF). VaHSM-CF is optimized for a novel self-masking technique, which modifies item hash codes by applying an AND operation between an item and user hash code, before computing the standard Hamming distance between the user and self-masked item hash codes. This provides a user-level bitwise binary weighting. Experiments show that VaHSM-CF outperforms state-of-the-art baselines by up to 12% in NDCG across 4 different datasets, while yielding less than 4% runtime overhead compared to the standard Hamming distance.",
        "Abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \nTo this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.",
        "Introduction": "  INTRODUCTION Collaborative filtering ( Herlocker et al., 1999 ) is an integral part of personalized recommender sys- tems and works by modelling user preference on past item interactions to predict new items the user may like ( Sarwar et al., 2001 ). Early work is based on matrix factorization approaches ( Koren et al., 2009 ) that learn a mapping to a shared m-dimensional real-valued space between users and items, such that user-item similarity can be estimated by the inner product. The purpose of hashing-based collaborative filtering ( Liu et al., 2014 ) is the same as traditional collaborative filtering, but allows for fast similarity searches to massively increase efficiency (e.g., realtime brute-force search in a bil- lion items ( Shan et al., 2018 )). This is done by learning semantic hash functions that map users and items into binary vector representations (hash codes) and then using the Hamming distance (the sum of differing bits between two hash codes) to compute user-item similarity. This leads to both large storage reduction (floating point versus binary representations) and massively faster computation through the use of the Hamming distance. One problem with hashing-based collaborative filtering is that each bit is weighted equally when computing the Hamming distance. This is a problem because the importance of each bit in an item hash code might differ between users. The only step towards addressing this problem has been to associate a weight with k-bit blocks of each hash code ( Liu et al., 2019 ). However, smaller values of k lead to increased storage cost, but also significantly slower computation due to the need of com- puting multiple weighted Hamming distances. To solve this problem, without using any additional storage and only a marginal increase in computation time, we present Variational Hashing-based collaborative filtering with Self-Masking (VaHSM-CF). VaHSM-CF is our novel variational deep learning approach for hashing-based collaborative filtering that learns hash codes optimized for self- masking. Self-masking is a novel technique that we propose in this paper for user-level bit-weighting on all items. Self-masking modifies item hash codes by applying an AND operation between an item and user hash code, before computing the standard Hamming distance between the user and self- masked item hash codes. Hash codes optimized with self-masking represent which bit-dimensions encode properties that are important for the user (rather than a bitwise -1/1 preference towards each Under review as a conference paper at ICLR 2020 property). In practice, when ranking a set of items for a specific user, self-masking ensures that only bit differences on bit-dimensions that are equal to 1 for the user hash code are considered, while ignoring the ones with a -1 value, thus providing a user-level bitwise binary weigthing. Since self- masking is applied while having the user and item hash codes in the lowest levels of memory (i.e., register), it only leads to a very marginal efficiency decrease. We contribute (i) a new variational hashing-based collaborative filtering approach, which is opti- mized for (ii) a novel self-masking technique, that outperforms state-of-the-art baselines by up to 12% in NDCG across 4 different datasets, while experimentally yielding less than 4% runtime over- head compared to the standard Hamming distance. We publicly release the code for our model, as well as an efficient implementation of the Hamming distance with self-masking 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper reviews the current research on how reinforcement learning rules inspired by artificial neural networks can be implemented in the brain. It discusses the challenges of implementing the error-backpropagation rule in the brain, and reviews recent research on how this challenge can be addressed. It also discusses the potential implications of this research for deep learning and the brain.",
        "Abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ",
        "Introduction": "  INTRODUCTION Among the learning rules for neural networks, reinforcement learning (RL) has the important virtue of occurring in animals and humans. Hence, RL by artificial neural networks can be used as a model for learning in the brain ( Bishop et al., 1995 ). Indeed, previous theories have suggested how powerful RL rules inspired by artificial neural networks could be implemented in the brain ( Roelfsema & Holtmaat, 2018 ) and the methodology for shaping neural networks with rewards and punishments is an active area of research ( Schmidhuber et al., 2011 ;  Friedrich et al., 2010 ;  Vasilaki et al., 2009 ;  O'Reilly & Frank, 2006 ;  Huang et al., 2013 ). In current deep learning, deep artificial neural networks are typically trained using supervised learning, with variants of the error-backpropagation (EBP) rule. EBP is a method that adjusts synaptic weights in multilayer networks to reduce the errors in the mapping of inputs into the lower layer to outputs in the top layer. It does so by first computing the output error, which is the difference between the actual and desired activity levels of output units, and then determines how the strength of connections between successively lower layers should change to decrease this error using gradient descent ( Rumelhart et al., 1986 ). Can the brain, with its many layers between input and output indeed solve this credit-assignment problem in a manner that is as powerful as deep learning? Similarly to deep neural networks, the brain of humans and animals are composed of many layers between the sensory neurons that register the stimuli and the motor neurons that control the muscles. Hence it is tempting to speculate that the methods for deep learning that work so well for artificial neural networks also play a role in the brain ( Marblestone et al., 2016 ;  Scholte et al., 2017 ). A number of important challenges need to be solved, however, and some of them were elegantly expressed by Francis Crick who argued that the error-backpropagation rule is neurobiologically unrealistic ( Crick, 1989 ). The main question is: how can the synapses compute the error derivative based on information available locally? In more recent years, researchers have started to address this challenge by proposing ways in which learning rules that are equivalent to error-backpropagation might be implemented in the brain (Urbanczik & Senn, 2014;  Schiess et al., 2016 ;  Roelfsema & Ooyen, 2005 ;  Rombouts et al., 2015 ;  Brosch et al., 2015 ;  Richards & Lillicrap, 2019 ;  Scellier & Bengio, 2019 ; Amit, 2018;  Sacramento et al., 2018 ), most of which were reviewed in ( Marblestone et al., 2016 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the implications of the relation between iterated elimination of dominated strategies and reinforcement learning in multiagent systems. We examine applications in mechanism design, specifically principal-agent problems, to incentivize agents to maximally exert costly effort. We analyze the implications of independent MARL in games that are solvable by iterated elimination of dominated strategies, and discuss how this can be used to guarantee convergence to a Nash equilibrium.",
        "Abstract": "Multiagent reinforcement learning (MARL) attempts to optimize policies of intelligent agents interacting in the same environment. However, it may fail to converge to a Nash equilibrium in some games.  We study independent MARL under the more demanding solution concept of iterated elimination of strictly dominated strategies.  In dominance solvable games, if players iteratively eliminate strictly dominated strategies until no further strategies can be eliminated, we obtain a single strategy profile. We show that convergence to the iterated dominance solution is guaranteed for several reinforcement learning algorithms (for multiple independent learners). We illustrate an application of our results by studying mechanism design for principal-agent problems, where a principal wishes to incentivize agents to exert costly effort in a joint project when it can only observe whether the project succeeded, but not whether agents actually exerted effort. We show that MARL converges to the desired outcome if the rewards are designed so that exerting effort is the iterated dominance solution, but fails if it is merely a Nash equilibrium.",
        "Introduction": "  INTRODUCTION Intelligent agents sharing a common environment are affected by the actions taken by their peers. Using reinforcement learning (RL) to derive agent policies becomes challenging since the environment becomes non-stationary for each agent when its peers adapt their behaviour through their learning process. One simple form of multiagent reinforcement learning (MARL) is independent learning, where each agent simply treats its experience as part of the non-stationary environment. Unfortunately, independent MARL fails to converge to a Nash equilibrium in many settings (Bowling, 2000;  Shoham et al., 2003 ). To guarantee convergence to a Nash equilibrium, one must either examine restricted classes of games such as fully cooperative games ( Claus & Boutilier, 1998 ;  Bu et al., 2008 ;  Panait et al., 2006 ;  Matignon et al., 2007 ), or devise specialized algorithms that guarantee convergence ( Hu & Wellman, 2003 ;  Wang & Sandholm, 2003 ). We investigate independent MARL in games that are solvable by iterated elimination of dominated strategies (Moulin, 1979). We say that an action by an agent is dominated by another if the first action offers the agent a strictly lower reward than taking the second action, no matter which actions are taken by the other agents. In iterated elimination of dominated strategy we iteratively examine the actions of every agent, and remove strictly dominated actions, until no further actions can be removed. A game is dominance solvable if only one action profile survives the process of iteratively eliminating strictly dominated strategies. We examine implications of the relation between iterated dominance and RL through applications in mechanism design, a field in economics that studies how to set incentives for rational agents, so as to achieve desired objectives. One key line of work in mechanism design deals with principal-agent problems ( Holmstrom et al., 1979 ) holmstrom1982moral,grossman1992analysis,laffont2009theory, relating to a principal in charge of a joint project, whose success depends on the exertion of effort by multiple agents; the principal wishes to incentivize agents to maximally exert costly effort, but cannot observe how much effort any individual agent exerted.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents BADGE, a batch active learning algorithm for deep neural networks that creates diverse batches of examples about which the current model is uncertain. BADGE is robust to architecture choice, batch size, and dataset, and generally performs as well as or better than the best baseline across experiments. The algorithm measures uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, and captures diversity by collecting a batch of examples where these gradients span a diverse set of directions.",
        "Abstract": "In many real-world deployments of machine learning systems, data arrive piecemeal. These learning scenarios may be passive, where data arrive incrementally due to structural properties of the problem (e.g., daily financial data) or active, where samples are selected according to a measure of their quality (e.g., experimental design). In both of these cases, we are building a sequence of models that incorporate an increasing amount of data. We would like each of these models in the sequence to be performant and take advantage of all the data that are available to that point. Conventional intuition suggests that when solving a sequence of related optimization problems of this form, it should be possible to initialize using the solution of the previous iterate---to \"warm start'' the optimization rather than initialize from scratch---and see reductions in wall-clock time. However, in practice this warm-starting seems to yield poorer generalization performance than models that have fresh random initializations, even though the final training losses are similar. While it appears that some hyperparameter settings allow a practitioner to close this generalization gap, they seem to only do so in regimes that damage the wall-clock gains of the warm start. Nevertheless, it is highly desirable to be able to warm-start neural network training, as it would dramatically reduce the resource usage associated with the construction of performant deep learning systems. In this work, we take a closer look at this empirical phenomenon and try to understand when and how it occurs. Although the present investigation did not lead to a solution, we hope that a thorough articulation of the problem will spur new research that may lead to improved methods that consume fewer resources during training.",
        "Introduction": "  INTRODUCTION In recent years, deep neural networks have produced state-of-the-art results on a variety of important supervised learning tasks. However, many of these successes have been in domains where large amounts of data are available. A promising approach for minimizing labeling effort is active learning, a supervised learning protocol where labels can be requested by the algorithm in a sequential feedback-driven fashion. Active learning algorithms aim to identify and label only maximally informative samples, so that a high- performing classifier can be trained with minimal labeling effort. As such, a robust active learning algorithm for deep neural networks may considerably expand the domains where these models are applicable. How should a practical, general-purpose, label-efficient active learning algorithm for deep neural networks be designed? Theory for active learning suggests a version-space-based approach ( Balcan et al., 2006 ), which explicitly or implicitly maintains a set of plausible models, and queries examples for which two models that make different predictions. But when using highly-expressive models, these algorithms degenerate to querying every example. Further, the computational overhead of training deep neural networks preclude algorithms that update the model to best fit the data after each label query, as is often done (exactly or approximately) for linear methods ( Beygelzimer et al., 2010 ;  Cesa-Bianchi et al., 2009 ). Unfortunately, the theory provides little guidance for these models. One option is to use the network's uncertainty to inform a query strategy, for example by labeling samples for which the model is least confident. However, in a batch setting this creates a pathological scenario where data in the batch are nearly identical, a clear inefficiency. Remedying this issue, we could select samples to maximize batch diversity, but this might choose points that provide little useful new information to the model. For these reasons, methods that exploit just uncertainty or diversity do not consistently work well across model architectures, batch sizes, or datasets. An algorithm that performs well when using a ResNet, for example, might perform poorly when using a multilayer-perceptron. A diversity-based approach might work well when the batch size is very large, but poorly when the batch size is small. Even what in practice constitutes a \"big\" or \"small\" batch size is largely a function of the statistical properties of the data in question. These Under review as a conference paper at ICLR 2020 weaknesses pose a major problem for real, practical batch active learning situations, where data are unfamiliar and potentially unstructured. There is no way to know which active learning algorithm is best to use. Further, in a real active learning scenario, every change of hyperparameters typically causes the algorithm to label examples not chosen under other hyperparameters, provoking substantial labeling inefficiency. That is, hyperparameter sweeps in active learning can be label expensive. Because of this, active learning algorithms need to \"just work\", given fixed hyperparameters, to a greater extent than is typical for supervised learning. Based on these observations, we design an approach which creates diverse batches of examples about which the current model is uncertain. We measure uncertainty as the gradient magnitude with respect to parameters in the final (output) layer, which is computed using the most likely label according to the model. To capture diversity, we collect a batch of examples where these gradients span a diverse set of directions. More specifically, we build up the batch of query points based on these hallucinated gradients using the k-MEANS++ initialization ( Arthur and Vassilvitskii, 2007 ), which simultaneously captures both the magnitude of a candidate gradient and its distance from previously included points in the batch. We name the resulting approach Batch Active learning by Diverse Gradient Embeddings (BADGE). We show that BADGE is robust to architecture choice, batch size, and dataset, generally performing as well as or better than the best baseline across our experiments, which vary all of the aforementioned environmental conditions. We begin by introducing our notation and setting, followed by a description of the BADGE algorithm in Section 3, and experiments in Section 4. We defer discussion of related work to Section 5.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a Jacobian regularization scheme to ensure robustness of nonlinear models against perturbations in the input space. The regularization scheme is illustrated by visualizing the classification margins of a simple MNIST digit classifier. The effectiveness of the regularizer is empirically studied and shown to improve robustness against both random and adversarial perturbations. The paper also contrasts the proposed regularization scheme with existing approaches in the literature.",
        "Abstract": "Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.",
        "Introduction": "  INTRODUCTION Stability analysis lies at the heart of many scientific and engineering disciplines. In an unstable system, infinitesimal perturbations amplify and have substantial impacts on the performance of the system. It is especially critical to perform a thorough stability analysis on complex engineered systems deployed in practice, or else what may seem like innocuous perturbations can lead to catastrophic consequences such as the Tacoma Narrows Bridge collapse ( Amman et al., 1941 ) and the Space Shuttle Challenger disaster ( Feynman and Leighton, 2001 ). As a rule of thumb, well-engineered systems should be robust against any input shifts - expected or unexpected. Most models in machine learning are complex nonlinear systems and thus no exception to this rule. For instance, a reliable model must withstand shifts from training data to unseen test data, bridging the so-called generalization gap. This problem is severe especially when training data are strongly biased with respect to test data, as in domain-adaptation tasks, or when only sparse sampling of a true underlying distribution is available, as in few-shot learning. Any instability in the system can further be exploited by adversaries to render trained models utterly useless ( Szegedy et al., 2013 ;  Goodfellow et al., 2014 ;  Moosavi-Dezfooli et al., 2016 ;  Papernot et al., 2016a ;  Kurakin et al., 2016 ;  Madry et al., 2017 ;  Carlini and Wagner, 2017 ;  Gilmer et al., 2018 ). It is thus of utmost importance to ensure that models be stable against perturbations in the input space. Various regularization schemes have been proposed to improve the stability of models. For linear classifiers and support vector machines ( Cortes and Vapnik, 1995 ), this goal is attained via an L 2 regularization which maximizes classification margins and reduces overfitting to the training data. This regularization technique has been widely used for neural networks as well and shown to promote generalization ( Hinton, 1987 ;  Krogh and Hertz, 1992 ;  Zhang et al., 2018 ). However, it remains unclear whether or not L 2 regularization increases classification margins and stability of a network, especially for deep architectures with intertwining nonlinearity. In this paper, we suggest ensuring robustness of nonlinear models via a Jacobian regularization scheme. We illustrate the intuition behind our regularization approach by visualizing the classification margins of a simple MNIST digit classifier in  Figure 1  (see Appendix A for more). Decision cells of a neural network, trained without regularization, are very rugged and can be unpredictably unstable (Figure 1a). On average, L 2 regularization smooths out these rugged boundaries but does not necessarily increase the size of decision cells, i.e., does not increase classification margins (Figure 1b). In contrast, Jacobian regularization pushes decision boundaries farther away from each training data point, enlarging decision cells and reducing instability (Figure 1c). The goal of the paper is to promote Jacobian regularization as a generic scheme for increasing robustness while also being agnostic to the architecture, domain, or task to which it is applied. In Under review as a conference paper at ICLR 2020 (a) Without regularization (b) With L 2 regularization (c) With Jacobian regularization support of this, after presenting the Jacobian regularizer, we evaluate its effect both in isolation as well as in combination with multiple existing approaches that are intended to promote robustness and generalization. Our intention is to showcase the ease of use and complimentary nature of our proposed regularization. Domain experts in each field should be able to quickly incorporate our regularizer into their learning pipeline as a simple way of improving the performance of their state-of-the-art system. The rest of the paper is structured as follows. In Section 2 we motivate the usage of Jacobian regularization and develop a computationally efficient algorithm for its implementation. Next, the effectiveness of this regularizer is empirically studied in Section 3. As regularlizers constrain the learning problem, we first verify that the introduction of our regularizer does not adversely affect learning in the case when input data remain unperturbed. Robustness against both random and adversarial perturbations is then evaluated and shown to receive significant improvements from the Jacobian regularizer. We contrast our work with the literature in Section 4 and conclude in Section 5.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes AdaScale SGD, a practical but principled algorithm that more reliably scales training by adapting to the gradient's variance. We provide theoretical results that formalize this approximate scale invariance, and perform large-scale empirical evaluations on five training benchmarks. Results align well with our theory, as AdaScale systematically preserves model quality across many scales. This includes training ImageNet with batch size 32k and Transformer with 262k max tokens per batch.",
        "Abstract": "When using distributed training to speed up stochastic gradient descent, learning rates must adapt to new scales in order to maintain training effectiveness. Re-tuning these parameters is resource intensive, while fixed scaling rules often degrade model quality. We propose AdaScale SGD, a practical and principled algorithm that is approximately scale invariant. By continually adapting to the gradient’s variance, AdaScale often trains at a wide range of scales with nearly identical results. We describe this invariance formally through AdaScale’s convergence bounds. As the batch size increases, the bounds maintain final objective values, while smoothly transitioning away from linear speed-ups. In empirical comparisons, AdaScale trains well beyond the batch size limits of popular “linear learning rate scaling” rules. This includes large-scale training without model degradation for machine translation, image classification, object detection, and speech recognition tasks. The algorithm introduces negligible computational overhead and no tuning parameters, making AdaScale an attractive choice for large-scale training.\n",
        "Introduction": "  INTRODUCTION Large datasets and large models underlie much of the recent success of machine learning. Training such models is time consuming, however, as stochastic gradient descent algorithms can require days or weeks to train effectively. Thus, procedures that speed up SGD are valuable. Faster training enables consideration of more data and models, which expands the capabilities of machine learning. To speed up SGD, distributed systems can process thousands of training examples per iteration. But training at large scales also creates a major algorithmic challenge. Specifically, learning rates must adapt to each scale. Without choosing these training parameters carefully, scaled SGD frequently trains low-quality models, producing a waste of resources rather than a useful model. To adapt learning rates, \"fixed scaling rules\" are standard but unreliable strategies.  Goyal et al. (2017)  popularized \"linear learning rate scaling,\" which can work well, especially for computer vision tasks ( Krizhevsky, 2014 ;  Devarakonda et al., 2017 ;  Jastrzębski et al., 2018 ;  Smith et al., 2018 ;  Lin et al., 2019 ). For other problems or larger scales, however, linear scaling often fails. This fact is well-known in theory ( Yin et al., 2018 ;  Jain et al., 2018 ;  Ma et al., 2018 ) and in practice ( Goyal et al., 2017 ). Other fixed scaling rules are also undependable.  Golmant et al. (2018)  test three rules-linear, root, and identity-and conclude that each one often degrades model quality.  Shallue et al. (2019)  compute near-optimal parameters for many tasks and scales, and the results do not align with any fixed rule. To ensure effective training, the authors recommend avoiding such rules and re-tuning parameters for each new scale-an inconvenient and resource-intensive solution. We propose AdaScale SGD. A practical but principled algorithm, AdaScale more reliably scales training by adapting to the gradient's variance. Decreased gradient variance is the fundamental im- pact of large batch sizes. Thus, scaling provides little gain if the variance is already \"small\" at small scales. In such cases, AdaScale increases the learning rate conservatively, and large-scale training progresses similarly to the small-batch setting. For iterations with \"large\" gradient variance, Ada- Scale increases the learning rate aggressively, and the per-iteration progress dramatically increases. AdaScale is approximately scale invariant, a quality that simplifies large-batch training. With no changes to learning rates or other inputs, AdaScale can train at many scales with similar results. This leads to two important innovations: (i) AdaScale improves the translation of training configurations between scales, which is useful for scaling up tasks or adapting to dynamic resource availability; and (ii) AdaScale works at scale with simple learning rate schedules, which eliminates the need for \"warm-up\" heuristics ( Goyal et al., 2017 ). Qualitatively, AdaScale and warm-up have similar effects on learning rates, but with AdaScale, this behavior emerges from a principled and adaptive mechanism, not hand-tuned parameters. We provide theoretical results that formalize this approximate scale invariance. Bounds for all scales converge to identical objective values. In contrast, the linear scaling rule requires fewer iterations but compromises model quality and training stability, causing divergence as the batch size increases. We perform large-scale empirical evaluations on five training benchmarks. Tasks include image clas- sification, machine translation, object detection, and speech recognition. The results align well with our theory, as AdaScale systematically preserves model quality across many scales. This includes training ImageNet with batch size 32k and Transformer with 262k max tokens per batch. To provide context for our description of AdaScale,  Figure 1  includes results from a simple scaling experiment using CIFAR-10 data. These results illustrate the concept of scale invariance, AdaScale's qualitative impact on learning rates, and a failure case for the linear scaling rule.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Deep learning has achieved remarkable accuracy on a variety of tasks, and is able to learn effective feature representations of data. However, deep neural networks and their embeddings exhibit some shortcomings, such as the existence of adversarial examples and the fact that they may correspond to flipping predictive features. This paper presents a direct example of such a shortcoming, wherein one can construct pairs of images that appear completely different to a human but are nearly identical in terms of their learned feature representations.",
        "Abstract": "An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations.",
        "Introduction": "  INTRODUCTION Beyond achieving remarkably high accuracy on a variety of tasks (Krizhevsky et al., 2012; He et al., 2015; Collobert & Weston, 2008), a major appeal of deep learning is the ability to learn effective feature representations of data. Specifically, deep neural networks can be thought of as linear classifiers acting on learned feature representations (also known as feature embeddings). A major goal in representation learning is for these embeddings to encode high-level, interpretable features of any given input (Goodfellow et al., 2016; Bengio et al., 2013; Bengio, 2019). Indeed, learned representations turn out to be quite versatile-in computer vision, for example, they are the driving force behind transfer learning Girshick et al. (2014); Donahue et al. (2014), and image similarity metrics such as VGG distance Dosovitskiy & Brox (2016a); Johnson et al. (2016); Zhang et al. (2018). These successes and others clearly illustrate the utility of learned feature representations. Still, deep networks and their embeddings exhibit some shortcomings that are at odds with our idealized model of a linear classifier on top of interpretable high-level features. For example, the existence of adver- sarial examples (Biggio et al., 2013; Szegedy et al., 2014)-and the fact that they may correspond to flipping predictive features Ilyas et al. (2019)-suggests that deep neural networks make predictions based on features that are vastly different from what humans use, or even recognize. (This message has been also corroborated by several recent works (Brendel & Bethge, 2019; Geirhos et al., 2019; Jetley et al., 2018; Zhang & Zhu, 2019).) In fact, we show a more direct example of such a short- coming (c.f. Section 2), wherein one can construct pairs of images that appear completely different to a human but are nearly identical in terms of their learned feature representations.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Autoencoders (AEs) are a class of neural networks consisting of an encoder and decoder part, used for a variety of applications such as outlier detection, data compression, and image enhancement. Despite their widespread use, the (deep) autoencoder model is not well understood. This paper aims to deepen our understanding of the autoencoder through theoretical analysis and experimental investigations.",
        "Abstract": "In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck.\nAutoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox.\nResearchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning.\nDespite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE.\nWe demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks.\nThe number of channels in the bottleneck, on the other hand, is secondary in importance.\nFurthermore, we show empirically, that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input.\nCopying does not occur, despite training the CAE for 1,000 epochs on a tiny (~ 600 images) dataset.\nWe believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs.",
        "Introduction": "  INTRODUCTION Autoencoders (AE) are an integral part of the neural network toolkit. They are a class of neural net- works that consist of an encoder and decoder part and are trained by reconstructing datapoints after encoding them. Due to their conceptual simplicity, autoencoders often appear in teaching materi- als as introductory models to the field of deep unsupervised learning. Nevertheless, autoencoders have enabled major contributions in the application and research of the field. The main areas of application include outlier detection ( Xia et al., 2015 ;  Chen et al., 2017 ;  Zhou & Paffenroth, 2017 ;  Baur et al., 2019 ), data compression ( Yildirim et al., 2018 ;  Cheng et al., 2018 ;  Dumas et al., 2018 ), and image enhancement ( Mao et al., 2016 ;  Lore et al., 2017 ). In the early days of deep learning, autoencoders were a crucial tool for the training of deep models. Training large (by the standards of the time) models was challenging, due to the lack of big datasets and computational resources. One way around this problem was to pre-train some or all layers of the network greedily by treating them as autoencoders with one hidden layer ( Bengio et al., 2007 ). Subsequently,  Erhan et al. (2009)  demonstrated that autoencoder pre-training also benefits generalization. Currently, researchers in the field of representation learning frequently rely on autoencoders for learning nuanced and high-level representations of data ( Kingma & Welling, 2013 ;  Tretschk et al., 2019 ;  Shu et al., 2018 ;  Makhzani et al., 2015 ;  Berthelot et al., 2018 ). However, despite its widespread use, we propose that the (deep) autoencoder model is not well understood. Many papers have aimed to deepen our understanding of the autoencoder through theo- retical analysis ( Nguyen et al., 2018 ;  Arora et al., 2013 ;  Baldi, 2012 ;  Alain & Bengio, 2012 ). While such analyses provide valuable theoretical insight, there is a significant discrepancy between the the- oretical frameworks and actual behavior of autoencoders in practice, mainly due to the assumptions made (e.g., weight tying, infinite depth) or the simplicity of the models under study. Others have approached this issue from a more experimental angle ( Arpit et al., 2015 ;  Bengio et al., 2013 ;  Le, 2013 ;  Vincent et al., 2008 ;  Berthelot et al., 2019 ). Such investigations are part of an ongoing effort to understand the behavior of autoencoders in a variety of settings.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes and analyzes noise regularization, a method well-studied in the context of regression and classification, for the purpose of conditional density estimation. By adding small random perturbations to the data during training, the conditional density estimate is smoothed and tends to generalize better. We show that adding noise during maximum likelihood estimation is equivalent to penalizing the second derivatives of the conditional log-probability. We also show that the proposed regularization scheme is asymptotically consistent, converging to the unbiased maximum likelihood estimator. We empirically demonstrate its effectiveness on three different neural network based models and demonstrate that, when properly regularized, neural network based CDE is able to improve upon state-of-the art non-parametric estimators.",
        "Abstract": "Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce. ",
        "Introduction": "  INTRODUCTION While regression analysis aims to describe the conditional mean E[y|x] of a response y given inputs x, many problems such as risk management and planning under uncertainty require gaining insight about deviations from the mean and their associated likelihood. The stochastic dependency of y on x can be captured by modeling the conditional probability density p(y|x). Inferring such a density function from a set of empirical observations {(x n , y n )} N n=1 is typically referred to as conditional density estimation (CDE) and is the focus of this paper. In the recent machine learning literature, there has been a resurgence of interest in high-capacity density models based on neural networks ( Dinh et al., 2017 ;  Ambrogioni et al., 2017 ;  Kingma & Dhariwal, 2018 ). Since this line of work mainly focuses on the modelling of images based on large scale data sets, over-fitting and noisy observations are of minor concern in this context. In contrast, we are interested in CDE in settings where data may be scarce and noisy. When combined with maximum likelihood estimation, the flexibility of such high-capacity models results in over-fitting and poor generalization. While regression typically assumes Gaussian conditional noise, CDE uses expressive distribution families to model deviations from the conditional mean. Hence, the over- fitting problem tends to be even more severe in CDE than in regression. Classical regularization of the neural network weights such as weight decay ( Pratt & Hanson, 1989 ) has been shown to be effective for regression and classification. However, in the context of CDE, the output of the neural network merely controls the parameters of a density model such as a Gaussian Mixture or Normalizing Flow. This makes the standard regularization methods in the parameter space less effective and harder to analyze. Aiming to address this issue, we propose and analyze noise regularization, a method well-studied in the context of regression and classification, for the purpose of conditional density estimation. In that, the paper attempts to close a gap in previous research. By adding small random perturbations to the data during training, the conditional density estimate is smoothed and tends to generalize better. In fact, we show that adding noise during maximum likelihood estimation is equivalent to penalizing the second derivatives of the conditional log-probability. Visually, the respective regularization term punishes very curved or even spiky density estimators in favor of smoother variants, which proves to be a favorable inductive bias in many applications. Moreover, under some regularity conditions, we show that the proposed regularization scheme is asymptotically consistent, converging to the unbiased maximum likelihood estimator. This does not only support the soundness of the proposed Under review as a conference paper at ICLR 2020 method but also endows us with useful insight in how to set the regularization intensity relative to the data dimensionality and training set size. Overall, the proposed noise regularization scheme is easy to implement and agnostic to the parameterization of the CDE model. We empirically demonstrate its effectiveness on three different neural network based models. The experimental results show that noise regularization outperforms other regularization methods significantly and consistently across various data sets. Finally, we demonstrate that, when properly regularized, neural network based CDE is able to improve upon state-of-the art non-parametric estimators, even when only 400 training observations are available.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents the first attempt to apply pre-trained contextual embeddings to source code. We introduce CuBERT, a BERT model trained on a corpus of Python programs collected from GitHub. We evaluate CuBERT on five classification tasks and a multi-headed pointer prediction task, and show that it outperforms baseline LSTM models supported by Word2Vec embeddings, and Transformers trained from scratch. We also demonstrate that finetuning works well even for smaller datasets and fewer training epochs. We plan to make the models and datasets publicly available.",
        "Abstract": "The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.",
        "Introduction": "  INTRODUCTION Modern software development places a high value on writing clean and readable code. This helps other developers understand the author's intent so that they can maintain and extend the code. De- velopers use meaningful identifier names and natural-language documentation to make this hap- pen (Martin, 2008). As a result, source code contains substantial information that can be exploited by machine-learning algorithms. Sequence modeling on source code has been shown to be success- ful in a variety of software-engineering tasks, such as code completion ( Hindle et al., 2012 ;  Raychev et al., 2014 ), source code to pseudocode mapping ( Oda et al., 2015 ), API-sequence prediction ( Gu et al., 2016 ), program repair ( Pu et al., 2016 ;  Gupta et al., 2017 ), and natural language to code mapping ( Iyer et al., 2018 ), among others. The distributed vector representations of tokens, called token (or word) embeddings, are a crucial component of neural methods for sequence modeling. Learning useful embeddings in a supervised setting with limited data is often difficult. Therefore, many unsupervised learning approaches have been proposed to take advantage of large amounts of unlabeled data that are more readily available. This has resulted in ever more useful pre-trained token embeddings ( Mikolov et al., 2013a ;  Pen- nington et al., 2014 ). However, the subtle differences in the meaning of a token in varying contexts are lost when each word is associated with a single representation. Recent techniques for learning contextual embeddings ( McCann et al., 2017 ;  Peters et al., 2018 ;  Radford et al., 2018 ; 2019;  Devlin et al., 2019 ;  Yang et al., 2019 ) provide ways to compute representations of tokens based on their surrounding context, and have shown significant accuracy improvements in downstream tasks, even with only a small number of task-specific parameters. Inspired by the success of pre-trained contextual embeddings for natural languages, we present the first attempt to apply the underlying techniques to source code. In particular, BERT ( Devlin et al., 2019 ) produces a bidirectional Transformer encoder ( Vaswani et al., 2017 ) by training it to Under review as a conference paper at ICLR 2020 predict values of masked tokens and whether two sentences follow each other in a natural discourse. The pre-trained model can be finetuned for downstream supervised tasks and has been shown to produce state-of-the-art results on a number of NLP benchmarks. In this work, we derive contextual embedding of source code by training a BERT model on source code. We call our model CuBERT, short for Code Understanding BERT. In order to achieve this, we curate a massive corpus of Python programs collected from GitHub. GitHub projects are known to contain a large amount of duplicate code. To avoid biasing the model to such duplicated code, we perform deduplication using the method of  Allamanis (2018) . The resulting corpus has 6.6M unique files with a total of 2 billion words. We also train Word2Vec embeddings ( Mikolov et al., 2013a ;b), namely, continuous bag-of-words (CBOW) and Skipgram embeddings, on the same corpus. For evaluating CuBERT, we create a benchmark of five classifi- cation tasks, ranging from classification of source code according to presence or absense of certain classes of bugs, to mismatch between a function's natural language description and its body, to pre- dicting the right kind of exception to catch for a given code fragment. These tasks are motivated by prior work in this space, but unfortunately, the associated datasets come from different languages and varied sources. We want to ensure that there is no overlap between pre-training and finetuning datasets, and that all of the tasks are defined on Python code. We therefore create new datasets for the five tasks after carefully separating the pre-training and finetuning corpora. To evaluate Cu- BERT's effectiveness on a more complex task, we create a task for joint classification, localization and repair of variable misuse bugs ( Vasic et al., 2019 ), which involves predicting two pointers. We finetune CuBERT on each of the classification tasks and compare the results with multi-layered bidirectional LSTM ( Hochreiter & Schmidhuber, 1997 ) models. We train the LSTM models from scratch and also using pre-trainined Word2Vec embeddings. Our results show that CuBERT con- sistently outperforms these baseline models by 2.9-22% across the tasks. We perform a number of additional studies by varying the sampling strategies used for training Word2Vec models, by varying program lengths, and by comparing against Transformer models trained from scratch. In addition, we also show that CuBERT can be finetuned effectively using only 33% of the task-specific labeled data and with only 2 epochs, and that it attains results competitive to the baseline models trained with the full datasets and much larger number of epochs. CuBERT when finetuned on the variable misuse localization and repair task, produces high classification, localization and localization+repair accuracies. The contributions of this paper are as follows: • We present the first attempt at pre-training a BERT contextual embedding of source code. • We show the efficacy of the pre-trained contextual embedding on five classification tasks. Our results show that the finetuned models outperform the baseline LSTM models sup- ported by Word2Vec embeddings, and Transformers trained from scratch. Further, the finetuning works well even for smaller datasets and fewer training epochs. We also evalu- ate CuBERT on a multi-headed pointer prediction task. • We plan to make the models and datasets publicly available for use by others.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel generative model, Generative RKM (Gen-RKM), based on the Restricted Kernel Machines (RKMs) framework. Gen-RKM is a probabilistic graphical model with latent variables that is capable of learning a disentangled representation from multiple views of data. The proposed model is compared to existing generative models such as Variational Auto-Encoders (VAEs), Generative Adversarial Networks (GANs) and Pixel Recurrent Neural Networks (PixelRNNs). Results show that Gen-RKM is able to learn a disentangled representation from multiple views of data and generate high-quality images.",
        "Abstract": "We introduce a novel framework for generative models based on Restricted Kernel Machines (RKMs) with multi-view generation and uncorrelated feature learning capabilities, called Gen-RKM. To incorporate multi-view generation, this mechanism uses a shared representation of data from various views. The mechanism is flexible to incorporate both kernel-based, (deep) neural network and convolutional based models within the same setting. To update the parameters of the network, we propose a novel training procedure which jointly learns the features and shared representation. Experiments demonstrate the potential of the framework through qualitative evaluation of generated samples.",
        "Introduction": "  INTRODUCTION In the past decade, interest in generative models has grown tremendously, finding applications in multiple fields such as, generated art, on-demand video, image denoising (Vincent et al., 2010), exploration in reinforcement learning (Florensa et al., 2018), collaborative filtering (Salakhutdinov et al., 2007), inpainting (Yeh et al., 2017) and many more. Some examples of graphical models based on a probabilistic framework with latent variables are Variational Auto-Encoders (Kingma & Welling, 2014) and Restricted Boltzmann Machines (RBMs) (Smolensky, 1986; Salakhutdinov & Hinton, 2009). More recently proposed models are based on adversarial training such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and its many variants. Furthermore, auto-regressive models such as Pixel Recurrent Neural Networks (PixelRNNs) (Van Den Oord et al., 2016) model the conditional distribution of every individual pixel given previous pixels. All these approaches have their own advantages and disadvantages. For example, RBMs perform both learning and Bayesian inference in graphical models with latent vari- ables. However, such probabilistic models must be properly normalized, which requires evaluating intractable integrals over the space of all possible variable configurations (Salakhutdinov & Hinton, 2009). Currently GANs are considered as the state-of-the-art for generative modeling tasks, pro- ducing high-quality images but are more difficult to train due to unstable training dynamics, unless more sophisticated variants are applied. Many datasets are comprised of different representations of the data, or views. Views can corre- spond to different modalities such as sounds, images, videos, sequences of previous frames, etc. Although each view could individually be used for learning tasks, exploiting information from all views together could improve the learning quality (Pu et al., 2016; Liu & Tuzel, 2016; Chen & De- noyer, 2017). Also, it is among the goals of the latent variable modelling to model the description of data in terms of uncorrelated or independent components. Some classical examples are Indepen- dent Component Analysis; Hidden Markov models (Rabiner & Juang, 1986); Probabilistic Principal Component Analysis (PCA) (Tipping & Bishop, 1999); Gaussian-Process Latent variable model (Lawrence, 2005) and factor analysis. Hence, when learning a latent space in generative models, it becomes interesting to find a disentangled representation. Disentangled variables are generally considered to contain interpretable information and reflect separate factors of variation in the data for e.g. lighting conditions, style, colors, etc. The definition of disentanglement in the literature is not precise, however many believe that a representation with statistically independent variables is a good starting point (Schmidhuber, 1992; Ridgeway, 2016). Such representations extract informa- tion into a compact form which makes it possible to generate samples with specific characteristics Under review as a conference paper at ICLR 2020 (Chen et al., 2018; Bouchacourt et al., 2018; Tran et al., 2017; Chen et al., 2016). Additionally, these representations have been found to generalize better and be more robust against adversarial attacks (Alemi et al., 2017). In this work, we propose an alternative generative mechanism based on the framework of Restricted Kernel Machines (RKMs) (Suykens, 2017), called Generative RKM (Gen-RKM). RKMs yield a representation of kernel methods with visible and hidden units establishing links between Kernel PCA, Least-Squares Support Vector Machines (LS-SVM) (Suykens et al., 2002) and RBMs. This framework has a similar energy form as RBMs, though there is a non-probabilistic training proce- dure where the eigenvalue decomposition plays the role of normalization. Recently, Houthuys & Suykens (2018) used this framework to develop tensor-based multi-view classification models and Schreurs & Suykens (2018) showed how kernel PCA fits into this framework.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to combining traditional and neural models for system-identification. The approach involves composing hybrid models, extrapolation testing, and enforcing stochastic loss functions. This paradigm is applied to simulated and real systems, and the results demonstrate that these models can decompose into deterministic, predictable, and stochastic components and can handle more complex systems than traditional models.",
        "Abstract": "Combining domain knowledge models with neural models has been challenging.  End-to-end trained neural models often perform better (lower Mean Square Error) than domain knowledge models or domain/neural combinations, and the combination is inefficient to train.  In this paper, we demonstrate that by composing domain models with machine learning models, by using extrapolative testing sets, and invoking decorrelation objective functions, we create models which can predict more complex systems. The models are interpretable, extrapolative, data-efficient, and capture predictable but complex non-stochastic behavior such as unmodeled degrees of freedom and systemic measurement noise.  We apply this improved modeling paradigm to several simulated systems and an actual physical system in the context of system identification.   Several ways of composing domain models with neural models are examined for time series, boosting, bagging, and auto-encoding on various systems of varying complexity and non-linearity.  Although this work is preliminary, we show that the ability to combine models is a very promising direction for neural modeling.",
        "Introduction": "  INTRODUCTION Modeling has been used for many years to explain, predict, and control the real world. Traditional models include science/math equations, algorithms, simulations, parametric models which capture domain knowledge, and interpolative models such as cubic splines or polynomial least squares among others which do not have explanatory value but can interpolate between known values well. The nonpredictable part of the signal is captured by a stochastic noise model. The domain/physical models predict n l-dimensional output vectors, Y ∈ R n×l given n k- dimen- sional input vectors, X ∈ R n×k with adjustable parameters, θ used to obtain the best fit (first term in Eq. 1). The unmodeled non-deterministic part of the data is often attributed to random noise fit to various stochastic models N (φ) with parameters φ (2nd term, Eq. (1)). This traditional approach has been very successful. The advantages of a good model include high data efficiency, interpretable, the ability to extrapolate to predict outputs from inputs beyond the range of the training input data, and composable (multiple models can be combined to solve more complex problems). However, this traditional approach has limitations. Complex systems often have degrees of freedom which are not modeled by the traditional models. These unmodeled degrees of freedom or sys- tematic errors of the measurement are not modeled adequately by the noise model. In addition, the parameters of the physical models, θ can be in error or be time dependent. In these cases the behavior of unmodeled part of the system is not random and thus the usual combined deterministic-stochastic model is inadequate. Neural models NN(X; W ), e.g. neural network models, are fundamentally just another form of parametric models where X are the inputs and W are the weight parameters. However, neural modes have unique properties to exploit. First, neural models can handle high dimensional input- output relations with complex patterns. Second, like interpolative models such as cubic splines or polynomials, NNs are sufficiently expressive to fit many possible relations but are often not good at Under review as a conference paper at ICLR 2020 extrapolation, (see below). Trask et al. (2018)  Third, neural models do not require handcrafting basis functions. Hence, neural models have the potential for describing unmodeled degrees of freedom, systematic errors, and nonstationary behavior. In this paper, neural modeling is combined with traditional modeling to achieve the advantages of both traditional and neural models and compensate for the problems mentioned above using follow- ing steps. (1) Composing Hybrid Models. We examine several ways of creating hybrid models: boosting, ensemble, and cyclical autoencoder ( Fig. 1 ). Combining domain models and neural mod- els requires assumptions about relationship between various system and noise. For example, Eq. (1) makes an implicit assumption that the models are composed by addition but there are many other possible assumptions. Unlike most boosting approaches, we use different model classes and loss functions for the various stages. (2) Extrapolation Testing. An extension to the traditional machine learning approach of dividing the data set into test and training portions is extended to include both interpolative and extrapolative testing sets as a stringent test of modeling power. (3) Stochastic Loss. Unlike previous approaches, the quality of the hybrid models to produce truly stochastic residuals is enforced using novel loss functions that enforce appropriate correlation of residuals. In this work, this paradigm is applied to system-identification (SysID) for simulated and real sys- tems. The results demonstrate that these models decompose into deterministic, predictable, and stochastic components and can handle more complex systems",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new training strategy for obtaining calibrated, trustworthy probabilities for both in-domain samples and under domain shift, which can be applied to a wide range of data modalities and model architectures. The strategy combines an entropy-encouraging loss with an adversarial calibration loss term. The approach is evaluated under 10 different perturbations and 10 different noise levels not seen during training. Results show that the proposed approach results in better calibration and technical trustworthiness of predictions for diverse types of out-of-domain samples and perturbations, compared to the state-of-the-art.",
        "Abstract": "To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield well-calibrated (and thus trustworthy) predictions for both in-domain samples as well as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as well as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, we propose an efficient yet general modelling approach for obtaining well-calibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of perturbations. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets, network architectures and perturbation strategies and observe that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions for both in-domain and out-of domain samples. ",
        "Introduction": "  INTRODUCTION To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world appli- cations, trustworthiness of deployed models is key. Not only in safety-critical applications such as autonomous driving or medicine ( Helldin et al., 2013 ;  Caruana et al., 2015 ;  Leibig et al., 2017 ), but also in dynamic open world systems in industry it is crucial for predictive models to be uncertainty- aware. Only if predictions are calibrated in the case of any gradual domain shift, covering the entire spectrum from in-domain (\"known unknowns\") to truly out-of-domain samples (\"unknown unknowns\"), they can be trusted. In particular in industrial and IoT settings, deployed models may encounter erroneous and inconsistent inputs far away from the input domain throughout the life-cycle; in addition, the distribution of the input data may gradually move away from the distribution of the training data (e.g. due to wear and tear of the assets, maintenance procedures or change in usage patterns). A variety of approaches to account for predictive uncertainty exist. They include post-processing steps for trained neural networks, where for example a validation set, drawn from the same distribution as the training data, is used to rescale the logit vectors returned by a trained neural network such that in-domain predictions are well calibrated ( Platt, 1999 ;  Guo et al., 2017 ). Orthogonal approaches have been proposed where trust scores and other measures for out-of-distribution (OOD) detection are derived, typically also based on trained networks ( Liang et al., 2018 ;  Jiang et al., 2018 ;  Papernot & McDaniel, 2018 ); however these latter approaches are designed to detect only truly OOD samples and do not consider the continuum of domain shifts from in-domain to truly OOD. Alternative avenues towards intrinsically uncertainty-aware networks have been followed by training probabilistic models. In particular, a lot of research effort has been put into training Bayesian neural networks, where typically a prior distribution over the weights is specified and, given the training data, a posterior distribution over the weights is inferred. This distribution can then be used to quantify predictive uncertainty. Since exact inference is untractable, a range of approaches for approximate inference has Under review as a conference paper at ICLR 2020 been proposed.In particular approaches based on variational approximations have recently received a lot of attention and range from estimators of the fully factorized posterior ( Blundell et al., 2015 ), to the interpretation of Gaussian dropout as performing approximate Bayesian inference ( Gal & Ghahra- mani, 2016 ) and facilitating a complex posterior using normalising flows ( Louizos & Welling, 2017 ). Since such Bayesian approaches often come at a high computational cost, alternative non-Bayesian approaches have been proposed, that can also account for predictive uncertainty. These include ensemble approaches, where smooth predictive estimates can be obtained by training ensembles of neural networks using adversarial examples ( Lakshminarayanan et al., 2017 ), and evidential deep learning, where predictions of a neural net are modelled as subjective opinions by placing a Dirichlet distribution on the class probabilities ( Sensoy et al., 2018 ). Both for Bayesian and non-Bayesian approaches, uncertainty-awareness and the quality of predictive uncertainty are typically evaluated by analysing the behaviour of the predictive entropy for out-of-domain predictions in form of grad- ual perturbations (e.g. rotation of an image), adversarial examples or held-out classes. However, while an increasing predictive entropy for increasingly strong perturbations can be an indicator for uncertainty-awareness, simply high predictive entropy is not sufficient for trustworthy predictions. Models can only be trusted if the confidence of predictions is calibrated, that is if the entropy matches the actual accuracy of the model. For example, if the entropy is too high, the model will yield under-confident predictions and similarly, if the entropy is too low, predictions will be over-confident. Notably, the focus of related work introduced above has been on image data and it remains unclear how these approaches perform for other data modalities, in particular when modelling sequences with long-range dependencies using complex architectures such as LSTMs ( Hochreiter & Schmidhuber, 1997 ) or GRUs ( Cho et al., 2014 ). Here, we propose an efficient yet general modelling approach for obtaining calibrated, trustworthy probabilities for both in-domain samples as well as under domain shift that can readily be applied to a wide range of data modalities and model architectures. More specifically, we first introduce a simple loss function to encourage high entropy on wrong predictions and combine this with an adversarial calibration loss term. Since in practical applications it is a priori not clear what type or magnitude of domain drift will occur, we evaluate calibration under doamin drift for 10 different perturbations and 10 different noise levels not seen during training. Our contribution in this paper is three-fold. (i) we illustrate the limitations of entropy as measure for trustworthy predictions and motivate the use of the expected calibration error for quantifying technical robustness ( Dawid, 1982 ;  DeGroot & Fienberg, 1983 ;  Niculescu-Mizil & Caruana, 2005 ;  Naeini et al., 2015 ;  Guo et al., 2017 ). (ii) we introduce a new training strategy combining an entropy-encouraging loss with an adversarial calibration loss term and demonstrate that this results in better calibration and technical trustworthiness of predictions for diverse types of out-of-domain samples and perturbations, compared to the state-of-the-art. (iii) We apply the concept of uncertainty-awareness and trustwor- thiness to sequence models and demonstrate that our approach substantially improves predictive uncertainty over existing approaches when classifying long sequences. While previous studies only compared predictive entropy for one simple architecture (LeNet) and typically one type of domain shift ( Sensoy et al., 2018 ;  Louizos & Welling, 2017 ), we here present an extensive comparison of 4 different architectures across 10 different perturbation strategies.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel framework for encoding abstract priors, called attribution priors, in which differentiable functions of a model's axiomatic feature attributions are directly regularized during training. This framework is a generalization of gradient-based regularization and can be used to encode meaningful domain knowledge more effectively than existing methods. A novel feature attribution method, expected gradients, is also introduced which extends integrated gradients and is naturally suited to being regularized under an attribution prior. The framework is applied to three different prediction tasks, resulting in improved deep models that are more interpretable and generalize better to noisy data, reduce prediction error and better capture biological signal, and develop a sparser model and improve performance when learning from limited training data.",
        "Abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.",
        "Introduction": "  INTRODUCTION Recent work on interpreting machine learning models has focused on feature attribution methods. Given an input feature, a model, and a prediction on a particular sample, such methods assign a number to the input feature that represents how important the input feature was for making the prediction. Previous literature about such methods has focused on the axioms they should satisfy ( Lundberg and Lee, 2017 ;  Sundararajan et al., 2017 ;  Štrumbelj and Kononenko, 2014 ;  Datta et al., 2016 ), and how attribution methods can give us insight into model behavior ( Lundberg et al., 2018a ;b;  Sayres et al., 2019 ;  Zech et al., 2018 ). These methods can be an effective way of revealing problems in a model or a dataset. For example, a model may place too much importance on undesirable features, rely on many features when sparsity is desired, or be sensitive to high frequency noise. In such cases, we often have a prior belief about how a model should treat input features, but for neural networks it can be difficult to mathematically encode this prior in terms of the original model parameters.  Ross et al. (2017b)  introduce the idea of regularizing explanations to train models that better agree with domain knowledge. Given a binary variable indicating whether each feature should or should not be important for predicting on each sample in the dataset, their method penalizes the gradients of unimportant features. However, two drawbacks limit the method's applicability to real-world problems. First, gradients don't satisfy the theoretical guarantees that modern feature attribution methods do ( Sundararajan et al., 2017 ). Second, it is often difficult to specify which features should be important in a binary manner. More recent work has stressed that incorporating intuitive, human priors will be necessary for developing robust and interpretable models ( Ilyas et al., 2019 ). Still, it remains challenging to encode meaningful, human priors like \"have smoother attribution maps\" or \"treat this group of features similarly\" by penalizing the gradients or parameters of a model. In this work, we propose an expanded framework for encoding abstract priors, called attribution priors, in which we directly regularize differentiable functions of a model's axiomatic feature attributions during training. This framework, which can be seen as a generalization of gradient-based regularization ( LeCun et al., 2010 ;  Ross et al., 2017b ;  Yu et al., 2018 ;  Jakubovitz and Giryes, 2018 ;  Roth et al., 2018 ), can be used to encode meaningful domain knowledge more effectively than existing methods. Furthermore, we introduce a novel feature attribution method - expected gradients - which extends integrated gradients ( Sundararajan et al., 2017 ), is naturally suited to being regularized Under review as a conference paper at ICLR 2020 under an attribution prior, and avoids hyperparameter choices required by previous methods. Using attribution priors, we build improved deep models for three different prediction tasks. On images, we use our framework to train a deep model that is more interpretable and generalizes better to noisy data by encouraging the model to have piecewise smooth attribution maps over pixels. On gene expression data, we show how to both reduce prediction error and better capture biological signal by encouraging similarity among gene expression features using a graph prior. Finally, on a patient mortality prediction task, we develop a sparser model and improve performance when learning from limited training data by encouraging a skewed distribution of the feature attributions.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes efficient black box adversarial attacks using stochastic derivative free optimization (DFO) methods with only access to the logits of the classifier. The proposed model requires a limited number of queries while outperforming the state of the art in terms of attack success rate. A new objective function is designed to suit classical derivative free optimization, and a new property of deep neural networks is highlighted, showing that they are not robust to single shot tiled attacks. A large spectrum of evolution strategies and other derivative-free optimization methods are explored using the Nevergrad framework.",
        "Abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.",
        "Introduction": "  INTRODUCTION Despite their success, deep learning algorithms have shown vulnerability to adversarial attacks ( Big- gio et al., 2013 ;  Szegedy et al., 2014 ), i.e. small imperceptible perturbations of the inputs, that lead the networks to misclassify the generated adversarial examples. Since their discovery, adversarial attacks and defenses have become one of the hottest research topics in the machine learning commu- nity as serious security issues are raised in many critical fields. They also question our understanding of deep learning behaviors. Although some advances have been made to explain theoretically ( Fawzi et al., 2016 ;  Sinha et al., 2017 ;  Cohen et al., 2019 ;  Pinot et al., 2019 ) and experimentally ( Good- fellow et al., 2015 ;  Xie et al., 2018 ;  Meng & Chen, 2017 ;  Samangouei et al., 2018 ;  Araujo et al., 2019 ) adversarial attacks, the phenomenon remains misunderstood and there is still a gap to come up with principled guarantees on the robustness of neural networks against maliciously crafted at- tacks. Designing new and stronger attacks helps building better defenses, hence the motivation of our work. First attacks were generated in a setting where the attacker knows all the information of the network (architecture and parameters). In this white box setting, the main idea is to perturb the input in the direction of the gradient of the loss w.r.t. the input ( Goodfellow et al., 2015 ;  Kurakin et al., 2016 ;  Carlini & Wagner, 2017 ;  Moosavi-Dezfooli et al., 2016 ). This case is unrealistic because the attacker has only limited access to the network in practice. For instance, web services that propose commercial recognition systems such as Amazon or Google are backed by pretrained neural networks. A user can query this system by sending an image to classify. For such a query, the user only has access to the inference results of the classifier which might be either the label, probabilities or logits. Such a setting is coined in the literature as the black box setting. It is more realistic but also more challenging from the attacker's standpoint. As a consequence, several works proposed black box attacks by just querying the inference results of a given classifier. A natural way consists in exploiting the transferability of an adversarial attack, based on the idea that if an example fools a classifier, it is more likely that it fools another one ( Pa- pernot et al., 2016a ). In this case, a white box attack is crafted on a fully known classifier. Papernot Under review as a conference paper at  ICLR 2020 et al. (2017)  exploited this property to derive practical black box attacks. Another approach within the black box setting consists in estimating the gradient of the loss by querying the classifier ( Chen et al., 2017 ;  Ilyas et al., 2018a ;b). For these attacks, the PGD attack ( Kurakin et al., 2016 ;  Madry et al., 2018a ) algorithm is used and the gradient is replaced by its estimation. In this paper, we propose efficient black box adversarial attacks using stochastic derivative free optimization (DFO) methods with only access to the logits of the classifier. By efficient, we mean that our model requires a limited number of queries while outperforming the state of the art in terms of attack success rate. At the very core of our approach is a new objective function particularly designed to suit classical derivative free optimization. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. It leverages results and ideas from ∞ -attacks. We also explore a large spectrum of evolution strategies and other derivative-free optimization methods thanks to the Nevergrad framework ( Rapin & Teytaud, 2018 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a method for calibrating regression problems in machine learning tasks. It reveals the flaws in the current definition of calibrated regression uncertainty and proposes a new definition. It also proposes a simple scaling method that can reduce the calibration error, evaluated on large scale real world vision datasets.",
        "Abstract": "Predicting not only the target but also an accurate measure of uncertainty is important for many applications and in particular safety-critical ones. In this work we study the calibration of uncertainty prediction for regression tasks which often arise in real-world systems. We show that the existing definition for calibration of a regression uncertainty [Kuleshov et al. 2018] has severe limitations in distinguishing informative from non-informative uncertainty predictions. We propose a new definition that escapes this caveat and an evaluation method using a simple histogram-based approach inspired by reliability diagrams used in classification tasks. Our method clusters examples with similar uncertainty prediction and compares the prediction with the empirical uncertainty on these examples. We also propose a simple scaling-based calibration that preforms well in our experimental tests. We show results on both a synthetic, controlled problem and on the object detection bounding-box regression task using the COCO and KITTI  datasets.",
        "Introduction": "  INTRODUCTION Regression problems arise in many real-world machine learning tasks. To name just a few: Depth from a single image ( Eigen et al., 2014 ), Object localization and Acoustic localization ( Vera-Diaz et al., 2018 ). Many of these tasks are solved by deep neural networks used within decision making pipelines which require the machine learning block not only to predict the target but to also output its confidence in the prediction. For example, the commonly used Kalman-Filter tracking algorithm (Blackman, 2004) requiring variance estimation for the observed object's location estimation. In addition, we may want the system to output a final uncertainty, reflecting real-world empirical Under review as a conference paper at ICLR 2020 probabilities, to allow a safety-critical system such as a self-driving car agent to take appropriate actions when confidence drops. In practice, using the confidence in the localization of objects has been shown to improve the non-maximal suppression stage and consequently the overall detection performance ( He et al., 2018 ). Similarly, (Feng et al., 2018) describe a probabilistic 3D vehicle detector for Lidar point clouds that can model both classification and spatial uncertainty. To provide uncertainty estimation, each prediction produced by the machine learning module during inference should be a distribution over the target domain. There are several approaches for achieving this, most common are Bayesian neural networks ( Gal, 2016 ;  Gal & Ghahramani, 2016 ), ensembles ( Lakshminarayanan et al., 2017 ) and outputting a parametric distribution directly ( Nix & Weigend, 1994 ). Bayesian neural networks place a probability distribution over the network parameters, which is translated to an uncertainty in the prediction, providing a technically sound approach but with overhead at inference time. In the direct approach, outputs of the network represent the parameters of the output distribution for either discrete ( Niculescu-Mizil & Caruana, 2005 ) or continuous ( Nix & Weigend, 1994 ) distributions. Note that the direct approach naturally captures the aleatoric uncertainty (inherent observation noise), but captures less the epistemic uncertainty (uncertainty in the model) ( Kendall & Gal, 2017 ). We chose as a test case for our calibration method, the direct approach for producing uncertainty: we transform the network output from a single scalar to a Gaussian distribution by taking the scalar as the mean and adding a branch that predicts the standard deviation (STD) as in ( Lakshminarayanan et al., 2017 ). While this is probably the simplest form, it is commonly used in practice, and our analysis is applicable to more complex distributions as well as other approaches. Adjusting the output distributions to match the observed empirical ones via a post process is called uncertainty calibration. It was shown that modern deep networks tend to be over confident in their predictions ( Guo et al., 2017 ). The same study revealed that for classification, Platt Scaling ( Platt, 1999 ), a simple scaling of the pre-activation of the last layer, achieves well calibrated confidence estimates ( Guo et al., 2017 ). In this paper we show that a similar simple scaling strategy, applied to the standard deviations of the output distributions, can calibrate regression algorithms as well. One major question is how to define calibration for regression, where the model outputs a continuous distribution over possible predictions. In recent work ( Kuleshov et al., 2018 ) suggested a definition based on credible intervals where if we take the p percentiles of each predicted distribution the output should fall below them for exactly p percent of the data. Based on this definition the authors further suggested a calibration evaluation metric and re-calibration method. While this seems very sensible and has the advantage of considering the entire distribution, we found serious flaws in this definition. The main problem arises from averaging over the whole dataset. We show, both empirically and analytically, that one can calibrate using this evaluation metric practically any output distribution, even one which is entirely uncorrelated with the empirical uncertainty as can be seen in  Fig. 1 . We elaborate on this property of the evaluation method described in ( Kuleshov et al., 2018 ) in Section 2 and show empirical evidence in Section 4. We propose a new simple definition for calibration for regression, which is closer to the standard one for classification. Calibration for classification can be viewed as expecting the output for every single data point to correctly predict its error, in terms of misclassification probability. In a similar fashion, we define calibration for regression by simply replacing the misclassification probability with the mean square error. Based on this definition, we propose a new calibration evaluation metric similar to the Expected Calibration Error (ECE) ( Naeini et al., 2015 ), which groups examples into interval bins with similar uncertainty, and then measures the discrepancy between each bin's parameters and the parameters of the empirical distribution within the bin. An additional dispersion measure completes our set of diagnostic tools by revealing cases where the individual uncertainty outputs are uninformative as they all return similar values. Finally, we propose a calibration method where we re-adjust the predicted uncertainty, in our case the outputted Gaussian variance, by minimizing the negative-log-likelihood (NLL) on a separate re-calibration set. We show good calibration results on a real-world dataset using a simple parametric model which scales the uncertainty by a constant factor. As opposed to ( Kuleshov et al., 2018 ), we show that our approach cannot calibrate predicted uncertainty that is uncorrelated with the real uncertainty, as one would expect. To summarize, our main contributions are: Under review as a conference paper at ICLR 2020 • Revealing the fundamental flaws in the current definition of calibrated regression uncertainty ( Kuleshov et al., 2018 ) • A new proposed definition for calibrated uncertainty in regression tasks • A simple scaling method that can reduce the calibration error similar to temperature scaling for classification ( Guo et al., 2017 ), evaluated on large scale real world vision datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes CONQUR, a general framework for integrating policy-consistent backups with regression-based function approximation for Q-learning and for managing the search through the space of possible regressors. CONQUR provides a computationally effective means for minimizing the effects of delusional bias in Q-learning, while admitting scaling to practical problems. We introduce novel augmentations of standard Q-regression to increase the degree of policy consistency across training batches, and define a search space over Q-regressors to allow consideration of multiple sets of policy commitments. We also introduce heuristics for guiding the search over regressors, and provide experimental results on the Atari suite demonstrating that CONQUR can offer improvements over Q-learning.",
        "Abstract": "Delusional bias is a fundamental source of error in approximate Q-learning. To date, the only techniques that explicitly address delusion require comprehensive search using tabular value estimates. In this paper, we develop efficient methods to mitigate delusional bias by training Q-approximators with labels that are \"consistent\" with the underlying greedy policy class. We introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class. We also propose a search framework that allows multiple Q-approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-learning in a variety of Atari games, sometimes dramatically.",
        "Introduction": "  INTRODUCTION Q-learning ( Watkins & Dayan, 1992 ;  Sutton & Barto, 2018 ) lies at the heart of many of the recent successes of deep reinforcement learning (RL) ( Mnih et al., 2015 ;  Silver et al., 2016 ), with recent advancements (e.g.,  van Hasselt (2010) ;  Bellemare et al. (2017) ;  Wang et al. (2016) ;  Hessel et al. (2017) ) helping to make it among the most widely used methods in applied RL. Despite these successes, many properties of Q-learning are poorly understood, and it is challenging to successfully apply deep Q-learning in practice. When combined with function approximation, Q-learning can become unstable ( Baird, 1995 ;  Boyan & Moore, 1995 ;  Tsitsiklis & Roy, 1996 ;  Sutton & Barto, 2018 ). Various modifications have been proposed to improve convergence or approximation error ( Gordon, 1995 ; 1999;  Szepesvári & Smart, 2004 ;  Melo & Ribeiro, 2007 ;  Maei et al., 2010 ;  Munos et al., 2016 ); but it remains difficult to reliably attain both robustness and scalability. Recently,  Lu et al. (2018)  identified a source of error in Q-learning with function approximation known as delusional bias. It arises because Q-learning updates the value of state-action pairs using estimates of (sampled) successor-state values that can be mutually inconsistent given the policy class induced by the approximator. This can result in unbounded approximation error, divergence, policy cycling, and other undesirable behavior. To handle delusion, the authors propose a policy-consistent backup operator that maintains multiple Q-value estimates organized into information sets. Each information set has its own backed-up Q-values and corresponding \"policy commitments\" responsible for inducing these values. Systematic management of these sets ensures that only consistent choices of maximizing actions are used to update Q-values. All potential solutions are tracked to prevent premature convergence on any specific policy commitments. Unfortunately, the proposed algorithms use tabular representations of Q-functions, so while this establishes foundations for delusional bias, the function approximator is used neither for generalization nor to manage the size of the state/action space. Consequently, this approach is not scalable to RL problems of practical size. In this work, we propose CONQUR (CONsistent Q-Update Regression), a general framework for integrating policy-consistent backups with regression-based function approximation for Q-learning and for managing the search through the space of possible regressors (i.e., information sets). With suitable search heuristics, our framework provides a computationally effective means for minimizing the effects of delusional bias in Q-learning, while admitting scaling to practical problems. Our main contributions are as follows. First we define novel augmentations of standard Q-regression to increase the degree of policy consistency across training batches. While testing exact consistency Under review as a conference paper at ICLR 2020 is expensive, we introduce an efficient soft-consistency penalty that promotes consistency of new labels with earlier policy commitments. Second, drawing on the information-set structure of  Lu et al. (2018) , we define a search space over Q-regressors to allow consideration of multiple sets of policy commitments. Third, we introduce heuristics for guiding the search over regressors, which is critical given the combinatorial nature of information sets. Finally, we provide experimental results on the Atari suite ( Bellemare et al., 2013 ) demonstrating that CONQUR can offer (sometimes dramatic) improvements over Q-learning. We also show that (easy-to-implement) consistency penalization on its own (i.e., without search) can improve over both standard and double Q-learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a new method, RigL, for training sparse neural networks without the need of a \"lucky\" initialization. RigL is memory and computationally efficient, requiring memory only proportional to the size of the sparse model and computation proportional to the number of nonzero parameters in the model. RigL is also accurate, achieving performance that matches or exceeds the performance of pruning based approaches. RigL works by infrequently using instantaneous gradient information to inform a re-wiring of the network, allowing the optimization to escape local minima.",
        "Abstract": "Sparse neural networks have been shown to yield computationally efficient networks with improved inference times.  There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017;Zhu & Gupta, 2018; Louizos et al., 2017; Li et al., 2016; Guo et al., 2016).  This limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires less floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results with ResNet-50, MobileNet v1 and MobileNet v2 on the ImageNet-2012 dataset. Finally,  we  provide  some  insights  into  why  allowing  the  topology  to change during the optimization can overcome local minima encountered when the topology remains static.",
        "Introduction": "  INTRODUCTION The parameter and floating point operation (FLOP) efficiency of sparse neural networks is now well demonstrated on a variety of problems ( Han et al., 2015 ;  Srinivas et al., 2017 ). Some work has even shown inference time speedups are possible on Recurrent Neural Networks (RNNs) ( Kalchbrenner et al., 2018 ) and Convolutional Neural Networks (ConvNets) ( Park et al., 2016 ). Currently, the most accurate sparse models are obtained with techniques that require, at a minimum, the cost of training a dense model in terms of memory and FLOPs ( Zhu & Gupta, 2018 ;  Guo et al., 2016 ), and sometimes significantly more ( Molchanov et al., 2017 ). This paradigm has two main limitations: 1. The maximum size of sparse models is limited to the largest dense model that can be trained. Even if sparse models are more parameter efficient, we can't use pruning to train models that are larger and more accurate than the largest possible dense models. 2. It is inefficient. Large amounts of computation must be performed for parameters that are zero valued or that will be zero during inference. Additionally, it remains unknown if the performance of the current best pruning algorithms are an upper bound on the quality of sparse models.  Gale et al. (2019)  found that three different dense-to- sparse training algorithms all achieve about the same sparsity / accuracy trade-off. However, this is far from conclusive proof that no better performance is possible. In this work we show the surprising result that dynamic sparse training, which includes the method we introduce below, can find more accurate models than the current best approaches to pruning initially dense networks. Importantly, our method does not change the FLOPs required to execute the model during training, allowing one to decide on a specific inference cost prior to training. The Lottery Ticket Hypothesis ( Frankle & Carbin, 2019 ) hypothesized that if we can find a sparse neural network with iterative pruning, then we can train that sparse network from scratch, to the Under review as a conference paper at ICLR 2020 same level of accuracy, by starting from the original initial conditions. In this paper we introduce a new method for training sparse models without the need of a \"lucky\" initialization; for this reason, we call our method \"The Rigged Lottery\" or RigL * . We show that this method is: • Memory efficient: It requires memory only proportional to the size of the sparse model. It never requires storing quantities that are the size of the dense model. This is in contrast to  Dettmers & Zettlemoyer (2019)  which requires storing the momentum for all parameters, even those that are zero valued. • Computationally efficient: The amount of computation required to train the model is pro- portional to the number of nonzero parameters in the model. • Accurate: The performance achieved by the method matches and sometimes exceeds the performance of pruning based approaches. Our method works by infrequently using instantaneous gradient information to inform a re-wiring of the network. We show that this allows the optimization to escape local minima where it would otherwise become trapped if the sparsity pattern were to remain static. Crucially, as long as the full gradient information is needed less than every 1 1−sparsity iterations, then the overall work remains proportional to the model sparsity.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an analysis of algorithms for variational inference, which aim to learn a generative model and construct a tractable variational approximation of the model parameters. Two main classes of stochastic gradient-ascent algorithms for optimising the model parameters are discussed: the importance weighted autoencoder (IWAE) and the reweighted wake-sleep (RWS) algorithm. It is shown that directly optimising the proposal distribution, as done by RWS, is preferable to optimising the IWAE multi-sample objective due to the need for reparametrisations and the φ-gradient breakdown. A generic adaptive importance-sampling framework for variational inference, termed adaptive importance sampling for learning (AISLE), is presented which admits not only RWS but also the IWAE-DREG and IWAE-STL gradients as special cases.",
        "Abstract": "The importance weighted autoencoder (IWAE) (Burda et al., 2016) is a popular variational-inference method which achieves a tighter evidence bound (and hence a lower bias) than standard variational autoencoders by optimising a multi-sample objective, i.e. an objective that is expressible as an integral over $K > 1$ Monte Carlo samples. Unfortunately, IWAE crucially relies on the availability of reparametrisations and even if these exist, the multi-sample objective leads to inference-network gradients which break down as $K$ is increased (Rainforth et al., 2018). This breakdown can only be circumvented by removing high-variance score-function terms, either by heuristically ignoring them (which yields the 'sticking-the-landing' IWAE (IWAE-STL) gradient from Roeder et al. (2017)) or through an identity from Tucker et al. (2019) (which yields the 'doubly-reparametrised' IWAE (IWAE-DREG) gradient). In this work, we argue that directly optimising the proposal distribution in importance sampling as in the reweighted wake-sleep (RWS) algorithm from Bornschein & Bengio (2015) is preferable to optimising IWAE-type multi-sample objectives. To formalise this argument, we introduce an adaptive-importance sampling framework termed adaptive importance sampling for learning (AISLE) which slightly generalises the RWS algorithm. We then show that AISLE admits IWAE-STL and IWAE-DREG (i.e. the IWAE-gradients which avoid breakdown) as special cases.",
        "Introduction": "  Introduction Let x be some observation and let z be some latent variable taking values in some space Z. These are modeled via the generative model p θ (z, x) = p θ (z)p θ (x|z) which gives rise to the marginal likelihood p θ (x) = Z p θ (z, x) dz of the model parameters θ. In this work, we analyse algorithms for variational inference, i.e. algorithms which aim to 1. learn the generative model, i.e. find a value θ which is approximately equal to the maximum-likelihood estimate (MLE) θ ml := arg max θ p θ (x); 2. construct a tractable variational approximation q φ,x (z) of p θ (z|x) = p θ (z, x)/p θ (x), i.e. find the value φ such that q φ ,x (z) is as close as possible to p θ (z|x) in some suitable sense. A few comments about this setting are in order. Firstly, as is common in the literature, we restrict our presentation to a single latent representation-observation pair (z, x) to avoid notational clutter - the extension to multiple independent observations is straightforward. Secondly, we assume that no parameters are shared between the generative model p θ (z, x) and the variational approximation q φ,x (z). This is common in neural-network applications but could be relaxed. Thirdly, our setting is general enough to cover amortised inference. For this reason, we often refer to φ as the parameters of an inference network. Two main classes of stochastic gradient-ascent algorithms for optimising ψ := (θ, φ) which employ K ≥ 1 Monte Carlo samples ('particles') to reduce errors have been proposed. • IWAE. The importance weighted autoencoder (IWAE) ( Burda et al., 2016 ) max- imizes a joint lower bound L K ψ ≤ p θ (x) whose bias decreases as K → ∞. The gradients of this objective can be unbiasedly approximated via the Monte-Carlo method. Unfortunately, the signal-to-noise ratio of the IWAE φ-gradient vanishes as K grows ( Rainforth et al., 2018 ). Two modified IWAE φ-gradients avoid this breakdown by removing high-variance 'score-function' terms: Under review as a conference paper at ICLR 2020 - IWAE-STL. The 'sticking-the-landing' IWAE (IWAE-STL) φ-gradient ( Roeder et al., 2017 ) heuristically drops the problematic score-function terms from the IWAE φ-gradient. This induces bias for the IWAE objective. - IWAE-DREG. The 'doubly-reparametrised' IWAE (IWAE-DREG) φ-gradient ( Tucker et al., 2019 ) unbiasedly removes the problematic score-function terms from the IWAE φ-gradient using a formal identity. • RWS. The reweighted wake-sleep (RWS) algorithm ( Bornschein & Bengio, 2015 ) optimises two separate objectives for θ and φ. Its gradients are approximated by self-normalised importance sampling with K particles: this induces a bias which vanishes as K → ∞. RWS can be viewed as an adaptive importance-sampling approach which iteratively improves its proposal distribution while simultaneously optimising θ via stochastic approximation. Crucially, the RWS φ-gradients do not degenerate as K → ∞. Of these two methods, the IWAE is the most popular and  Tucker et al. (2019)  demonstrated empirically that RWS can break down, conjecturing that this is due to the fact that RWS does not optimise a joint objective (for θ and φ). Meanwhile, the IWAE-STL gradient performed consistently well despite lacking a firm theoretical footing. Yet, IWAE suffers from the above-mentioned φ-gradient breakdown and exhibited inferior empirical performance to RWS ( Le et al., 2019 ). Thus, it is not clear whether the multi-sample objective approach of IWAE or the adaptive importance-sampling approach of RWS is preferable. In this work, we show that directly optimising the proposal distribution, e.g. as done by RWS, is preferable to optimising the IWAE multi-sample objective because (a) the multi-sample objective typically relies on reparametrisations and, even if these are available, leads to the φ-gradient breakdown, (b) modifications of the IWAE φ-gradient which avoid this breakdown (i.e. IWAE-STL and IWAE-DREG) can be justified in a more principled manner by taking an RWS-type adaptive importance-sampling view. This conclusion was already reached by  Le et al. (2019)  based on numerical experiments. They demonstrated that the need for reparametrisations can make IWAE inferrior to RWS e.g. for discrete latent variables. Our work complements theirs by formalising this argument. To this end, we slightly generalise the RWS algorithm to obtain a generic adaptive importance-sampling framework for variational inference which we term adaptive importance sampling for learning (AISLE) for ease of reference. We then show that AISLE admits not only RWS but also the IWAE-DREG and IWAE-STL gradients as special cases.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the representation of insideness in Deep Neural Networks (DNNs) for image segmentation. We analyze the segmentation of closed curves, reducing insideness to a minimum representation by eliminating other components. We analytically demonstrate that two state-of-the-art network architectures, DNNs with dilated convolutions and convolutional LSTMs, can exactly solve the insideness problem for any given curve. Experiments with synthetically generated closed curves show that when using standard training strategies, the DNNs do not learn general solutions for insideness, even though they are sufficiently complex to capture the long-range relationships. The only network that achieves almost full generalization is a recurrent network with a training strategy designed to encourage a specific mechanism for dealing with long-range relationships. These results add to the growing body of works that show that DNNs have problems in learning to solve some elemental visual tasks.",
        "Abstract": "Image segmentation aims at grouping pixels that belong to the same object or region. At the heart of image segmentation lies the problem of determining whether a pixel is inside or outside a region, which we denote as the \"insideness\" problem. Many Deep Neural Networks (DNNs) variants excel in segmentation benchmarks, but regarding insideness, they have not been well visualized or understood: What representations do DNNs use to address the long-range relationships of insideness? How do architectural choices affect the learning of these representations? In this paper, we take the reductionist approach by analyzing DNNs solving the insideness problem in isolation, i.e. determining the inside of closed (Jordan) curves. We demonstrate analytically that state-of-the-art feed-forward and recurrent architectures can implement solutions of the insideness problem for any given curve. Yet, only recurrent networks could  learn these general solutions when the training enforced a specific \"routine\" capable of breaking down the long-range relationships. Our results highlights the need for new training strategies that decompose the learning into appropriate stages, and that lead to the general class of solutions necessary for DNNs to understand insideness.",
        "Introduction": "  INTRODUCTION Image segmentation is necessary for complete image understanding. A key component of image segmentation is to determine whether a pixel is inside or outside a region, ie. the \"insideness\" prob- lem ( Ullman, 1984 ; 1996). Deep Neural Networks (DNNs) have been tremendously successful in image segmentation benchmarks, but it is not well understood whether DNNs represent insideness or how. Insideness has been overlooked in DNNs for segmentation since they have been mainly applied to the modality of \"semantic segmentation\", ie. labelling each pixel with its object category ( Ronneberger et al., 2015 ;  Yu & Koltun, 2016 ;  Visin et al., 2016 ;  Badrinarayanan et al., 2017 ;  Chen et al., 2018b ;  Long et al., 2015 ;  Lateef & Ruichek, 2019 ). In such cases, insideness is not necessary since a solution can rely only on object recognition. Yet, the recent need to solve more sophisticated visual tasks has fueled the development of DNNs with the ability to segment individual object instances, rather than object categories ( Li et al., 2016 ; 2017;  Song et al., 2018 ;  Chen et al., 2018a ;  Hu et al., 2018 ;  Maninis et al., 2018 ;  Liu et al., 2018b ;  He et al., 2017 ). In these segmentation modalities, insideness plays a central role, especially when there are few cues besides the boundaries of the objects, e.g. when there is lack of texture and color, and objects are unfamiliar. Thus, insideness is necessary to achieve true generalization in image segmentation. In this paper, we investigate derived and learned insideness-related representations in DNNs for segmentation. We take the reductionist approach by isolating insideness from other components in image segmentation. We analyze the segmentation of closed curves, similar to the methodology in Minsky & Papert's historic book Perceptrons ( Minsky & Papert, 1969 ). In this way, we distill insideness to a minimum representation by eliminating other components. We analytically demonstrate that two state-of-the-art network architectures, namely, DNNs with dilated convolutions ( Yu & Koltun, 2016 ;  Chen et al., 2018b ) and convolutional LSTMs (ConvL- STMs) ( Xingjian et al., 2015 ), among other networks, can exactly solve the insideness problem for any given curve with network sizes that are easily implemented in practice. The proofs draw on Under review as a conference paper at ICLR 2020 algorithmic ideas from classical work on visual routines ( Ullman, 1984 ; 1996), namely, the ray- intersection method and the coloring method, to derive equivalent neural networks that implement these algorithms. Then, in a series of experiments with synthetically generated closed curves, we evaluate the capabilities of these DNNs to learn the insideness problem. The experiments show that when using standard training strategies, the DNNs do not learn general solutions for insideness, even though these DNNs are sufficiently complex to capture the long-range relationships. The only net- work that achieves almost full generalization in all tested cases is a recurrent network with a training strategy designed to encourage a specific mechanism for dealing with long-range relationships. These results add to the growing body of works that show that DNNs have problems in learning to solve some elemental visual tasks ( Linsley et al., 2018 ;  Liu et al., 2018a ;  Wu et al., 2018 ;  Shalev- Shwartz et al., 2017 ).  Shalev-Shwartz et al. (2017)  introduced several tasks that DNNs can in theory solve, as it was shown mathematically, but the networks were unable to learn, not even for the given dataset, due to difficulties in the optimization with gradient descent. In contrast, the challenges we report for insideness are related to poor generalization rather than optimization, as our experi- ments show the networks succeed in solving insideness for the given dataset.  Linsley et al. (2018)  introduced new architectures that better capture the long-range dependencies in images. Here, we show that the training strategy has a big impact in capturing the long-range dependencies. Even if the DNNs we tested had the capacity to capture such long-range dependencies, they do not learn a general solution with the standard training strategies.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper explores the use of Graph Convolutional Networks (GCNs) for semi-supervised learning on attributed graphs, and proposes a novel method for directed hypergraphs called Directed Hypergraph Network (DHN). Generalization error bounds for a one-layer GNN are established, and DHN's effectiveness is demonstrated through detailed experimentation on real-world data. The paper provides new empirical benchmarks for soft-SSL on directed hypergraphs and makes the code available to foster reproducible research.",
        "Abstract": "Graph-based semi-supervised learning (SSL) assigns labels to initially unlabelled vertices in a graph.\nGraph neural networks (GNNs), esp. graph convolutional networks (GCNs), inspired the current-state-of-the art models for graph-based SSL problems.\nGCNs inherently assume that the labels of interest are numerical or categorical variables.\nHowever, in many real-world applications such as co-authorship networks, recommendation networks, etc., vertex labels can be naturally represented by probability distributions or histograms.\nMoreover, real-world network datasets have complex relationships going beyond pairwise associations.\nThese relationships can be modelled naturally and flexibly by hypergraphs.\nIn this paper, we explore GNNs for graph-based SSL of histograms.\nMotivated by complex relationships (those going beyond pairwise) in real-world networks, we propose a novel method for directed hypergraphs.\nOur work builds upon existing works on graph-based SSL of histograms derived from the theory of optimal transportation.\nA key contribution of this paper is to establish generalisation error bounds for a one-layer GNN within the framework of algorithmic stability.\nWe also demonstrate our proposed methods' effectiveness through detailed experimentation on real-world data.\nWe have made the code available.",
        "Introduction": "  INTRODUCTION In the last decade, deep learning models have been successfully embraced in many different fields and proved to achieve unprecedented performance on a vast range of applications  Krizhevsky et al. (2012) ;  Goodfellow et al. (2014) ;  Bahdanau et al. (2015) ;  LeCun et al. (2015) . Graph Convolutional Network (GCN)  Kipf & Welling (2017)  was recently proposed as an adaptation of a particular deep learning model (i.e., convolutional neural networks  Lecun et al. (1998) ) to enable handling graph-structured data. GCN was shown to be, in particular, effective in semi-supervised learning on attributed graphs. GCNs have inspired the current state-of-the art models for graph-based SSL  Wu et al. (2019a) ;  Veličković et al. (2018) ;  Vashishth et al. (2019) . GCNs inherently assume that the labels of interest are numerical or categorical variables. However, in many real-world applications such as co-authorship networks, recommendation net- works, etc., vertex labels can be naturally represented by probability distributions or histograms. Moreover, these real-world network datasets have complex relationships going beyond pairwise as- sociations. Such relationships can be modelled naturally and flexibly by hypergraphs. Moreover, hypergraphs can encode additional relationships with directions as illustrated in  Figure 1  and these hypergraphs are directed hypergraphs  Gallo et al. (1993) . Inspired by a prior work  Solomon et al. (2014)  that generalised label propagation to graph-based soft SSL setting and motivated by the fact that GNNs have inspired state-of-the-art models for traditional graph-based SSL, we make the following contributions. • We explore GNNs for soft SSL in which vertex labels are probability distributions. Mo- tivated by real-world applications, we propose DHN (Directed Hypergraph Network), a novel method for directed hypergraphs. DHN can be applied for soft-SSL using existing tools from optimal transportation (Section 3). • We provide generalisation error bounds for a one-layer GNN within the framework of algo- rithmic stability. We establish that such models, which use filters with bounded eigenvalues Under review as a conference paper at ICLR 2020 independent of graph size, can satisfy the strong notion of uniform stability and thus is gen- eralisable. In particular, the algorithmic stability of a one-layer GNN depends on the largest absolute eigenvalue of the graph convolution filter (Section 4). • We demonstrate DHN's effectiveness through detailed experimentation on real-world data. In particular, we demonstrate superiority over state-of-the-art hypergraph-based neural net- works. We provide new empirical benchmarks for soft-SSL on directed hypergraphs and make the code available to foster reproducible research (Section 5).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a framework for learning adaptive, high-dimensional loss functions through back-propagation that shape the loss landscape such that it can be efficiently optimized with gradient descent. This framework involves an inner and an outer loop, where the inner loop updates the optimizee with the meta-loss and the outer loop optimizes the meta-loss function by minimizing a task-loss. We show that our learned meta-loss functions improves over directly learning via the task-loss itself while maintaining the generality of the task-loss. Additionally, we demonstrate how extra information can be utilized to shape the loss landscapes at meta-train time. After training the meta-loss function, the task-specific losses are no longer required since the training of optimizees can be performed entirely by using the meta-loss function alone.",
        "Abstract": "We present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for training such loss functions, targeted at maximizing the performance of model learn- ing with them. We observe that the loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time.",
        "Introduction": "  INTRODUCTION Specifically, the purpose of this work is to encode learning strategies into an adaptive high-dimensional loss function, or a meta-loss, which generalizes across multiple training contexts or tasks. Inspired by inverse reinforcement learning ( Ng et al., 2000 ), our work combines the learning to learn paradigm of meta-learning with the generality of learning loss landscapes. We construct a unified, fully differentiable framework that can learn model-agnostic loss functions to provide a strong learning signal for a large range of model classes, such as classifiers, regressors or control policies. The contributions of this work are as follows: i) we present a framework for learning adaptive, high-dimensional loss functions through back-propagation that shape the loss landscape such that it can be efficiently optimized with gradient descent. This framework involves an inner and an outer loop. In the inner loop, a model or an optimizee is trained with gradient descent using the loss coming from our learned meta-loss function. Fig. 1 shows the pipeline for updating the optimizee with the meta-loss. The outer loop optimizes the meta-loss function by minimizing a task-loss, such as a standard regression or reinforcement-learning loss, that is induced by the updated optimizee. We show that our learned meta-loss functions improves over directly learning via the task-loss itself while maintaining the generality of the task-loss. ii) We show how we can utilize extra information that helps shape the loss landscapes at meta-train time. This extra information can take on various forms, such as exploratory signals or expert demonstrations for RL tasks. After training the meta-loss function, the task-specific losses are no longer required since the training of optimizees can be performed entirely by using the meta-loss function alone, without requiring the extra information given at meta-train time. In this way, our meta-loss can find more efficient ways to optimize the original task loss.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an improvement to unsupervised landmark learning, which aims to find pose representations from image data without the need for direct pose-level annotation. The proposed method explicitly encourages the model to factorize the reconstruction task into separate foreground and background reconstructions, where only the foreground reconstruction is conditioned on learned landmarks. This factorization allows for state-of-the-art landmark results with fewer learned landmarks, and fewer landmarks allocated to modeling background content. The overall quality of the reconstructed frame is improved via the factorized rendering, and an application to the video-prediction task is included.",
        "Abstract": "Unsupervised landmark learning is the task of learning semantic keypoint-like\nrepresentations without the use of expensive keypoint-level annotations. A popular approach is to factorize an image into a pose and appearance data stream, then to reconstruct the image from the factorized components. The pose representation should capture a set of consistent and tightly localized landmarks in order to facilitate reconstruction of the input image. Ultimately, we wish for our learned landmarks to focus on the foreground object of interest. However, the reconstruction task of the entire image forces the model to allocate landmarks to model the background. This work explores the effects of factorizing the reconstruction task into separate foreground and background reconstructions, conditioning only the foreground reconstruction on the unsupervised landmarks. Our experiments demonstrate that the proposed factorization results in landmarks that are focused on the foreground object of interest. Furthermore, the rendered background quality is also improved, as the background rendering pipeline no longer requires the ill-suited landmarks to model its pose and appearance. We demonstrate this improvement in the context of the video-prediction.",
        "Introduction": "  INTRODUCTION Pose prediction is a classical computer vision task that involves inferring the location and configu- ration of deformable objects within an image. It has applications in human activity classification, finding semantic correspondences across multiple object instances, and robot planning to name a few. One of the caveats of this task is that annotation is very expensive. Individual object \"parts\" need to be carefully and consistently annotated with pixel-level precision. Our work focuses on the task of unsupervised landmark learning, which aims to find unsupervised pose representations from image data without the need for direct pose-level annotation. A good visual landmark should be tightly localized, consistent across multiple object instances, and grounded on the foreground object of interest. Tight localization is important because many objects (such as persons) are highly deformable. A landmark localized to a smaller, rigid area of the object will offer more precise pose information in the event of object motion. Consistency across multiple object instances is also important, as we wish for our landmarks to apply to all instances within a visual category. Finally, and most relevant to our proposed method, we want our landmarks to focus on the foreground objects. A landmark that fires on the background is a wasted landmark, as the background is constantly changing, and yields little information regarding the pose of our foreground object of interest. Many unsupervised landmark learning methods perturb an input training image with various trans- formations, then require the model to learn semantic correspondences across the transformed vari- ants to piece together the unaltered input image. The primary issue with this approach is it penalizes the entire image reconstruction when we care only about the foreground, resulting in landmarks being allocated to the background. This poses a number of issues, including increased memory requirements (more landmarks required to capture the foreground) and lower landmark reliability (landmarks assigned to background are unstable). Our proposed method aims to reduce the likeli- Under review as a conference paper at ICLR 2020 hood of landmarks being allocated to the background, thereby improving overall landmark quality and reducing the number of landmarks required to achieve state-of-the-art performance. Our work builds upon existing methods in image-reconstruction-guided landmark learning tech- niques ( Jakab et al., 2018 ; Lorenz et al., 2019). We explicitly encourage our model to factorize the reconstruction task into separate foreground and background reconstructions, where only the foreground reconstruction is conditioned on learned landmarks. Our contributions are as follows: 1. We propose an improvement to reconstruction-guided unsupervised landmark learning that al- lows the landmarks to better focus on the foreground. 2. We demonstrate through empirical analysis that our proposed factorization allows for state-of- the-art landmark results with fewer learned landmarks, and that fewer landmarks are allocated to modeling background content. 3. We demonstrate that the overall quality of the reconstructed frame is improved via the factorized rendering, and include an application to the video-prediction task.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces deep neural networks (DNNs) and convolutional neural networks (CNNs) and their impressive success in various applications. It then discusses the immense computational requirements of DNNs and the hardware level innovations that have enabled their success. Finally, it examines network-specific optimization techniques, such as network compression, pruning, and regularization, that aim to reduce the number of edges in the network, but often require retraining the pruned network, leading to significant computational waste.",
        "Abstract": "Deep neural networks have demonstrated unprecedented success in various knowledge management applications. However, the networks created are often very complex, with large numbers of trainable edges which require extensive computational resources. We note that many successful networks nevertheless often contain large numbers of redundant edges. Moreover, many of these edges may have negligible contributions towards the overall network performance. In this paper, we propose a novel iSparse framework and experimentally show, that we can sparsify the network, by 30-50%, without impacting the network performance. iSparse leverages a novel edge significance score, E, to determine the importance of an edge with respect to the final network output. Furthermore, iSparse can be applied both while training a model or on top of a pre-trained model, making it a  retraining-free approach - leading to a minimal computational overhead. Comparisons of iSparse against PFEC, NISP, DropConnect, and Retraining-Free on benchmark datasets show that iSparse leads to effective network sparsifications.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs), particularly convolutional neural networks (CNN), have shown im- pressive success in many applications, such as facial recognition (Lawrence et al., 1997), time series analysis (Yang et al., 2015), speech recognition (Hinton et al., 2012), object classification (Liang & Hu, 2015), and video surveillance (Karpathy & et. at., 2014). As the term \"deep\" neural networks implies, this success often relies on large networks, with large number of trainable edges (weights) (Huang et al., 2017; Zoph et al., 2018; He et al., 2016; Simonyan & Zisserman, 2015). While a large number of trainable edges help generalize the network for complex and diverse pat- terns in large-scale datasets, this often comes with enormous computation cost to account for the non-linearity of the deep networks (ReLU, sigmoid, tanh). In fact, DNNs owe their recent suc- cess to hardware level innovations that render the immense computational requirements practi- cal (Ovtcharov & et. al., 2015; Matthieu Courbariaux et al., 2015). However, the benefits of hard- ware solutions and optimizations that can be applied to a general purpose DNN or CNN are limited and these solutions are fast reaching their limits. This has lead to significant interest in network- specific optimization techniques, such as network compression (Choi & et. al., 2018), pruning (Li et al., 2016; Yu et al., 2018), and regularization (Srivastava & et. al., 2014; Wan et al., 2013), aim to reduce the number of edges in the network. However, many of these techniques require retraining the pruned network, leading to the significant amount of computational waste.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new benchmark for offline optimization of reinforcement learning (RL) agents on Atari 2600 games using the replay data of a deep Q-network (DQN) agent. The benchmark is used to investigate the contributions of exploitation vs. exploration in off-policy deep RL, improve reproducibility of deep RL research, and facilitate the design of simpler off-policy deep RL algorithms. Results show that the logged DQN data is sufficient for optimizing strong Atari agents offline without any environment interaction, and a simple Q-learning algorithm called Random Ensemble Mixture (REM) outperforms more complex RL algorithms. The insights gained from the offline experiments are used to develop an online variant of REM, which performs comparably with online QR-DQN.",
        "Abstract": "This paper advocates the use of offline (batch) reinforcement learning (RL) to help (1) isolate the contributions of exploitation vs. exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the design of simpler deep RL algorithms. We propose an offline RL benchmark on Atari 2600 games comprising all of the replay data of a DQN agent. Using this benchmark, we demonstrate that recent off-policy deep RL algorithms, even when trained solely on logged DQN data, can outperform online DQN. We present Random Ensemble Mixture (REM), a simple Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. The REM algorithm outperforms more complex RL agents such as C51 and QR-DQN on the offline Atari benchmark and performs comparably in the online setting.",
        "Introduction": "  INTRODUCTION Deep neural networks have become a critical component of modern reinforcement learning (RL) ( Sut- ton and Barto, 2018 ). The seminal work of  Mnih et al. (2013 ; 2015) on deep Q-networks (DQN) has demonstrated that it is possible to train neural networks using Q-learning ( Watkins and Dayan, 1992 ) to achieve human-level performance in playing Atari 2600 games ( Bellemare et al., 2013 ) directly from raw pixels. Recent progress in mastering Go ( Silver et al., 2016 ) and advances in robotic control ( Levine et al., 2016 ;  OpenAI et al., 2018 ;  Kalashnikov et al., 2018 ) present additional supporting evidence for the enormous potential of deep RL. Off-policy RL algorithms such as Q-learning are attractive because they disentangle data collection and policy optimization and offer more sample efficient solutions than on-policy algorithms ( Sutton et al., 2000 ;  Schulman et al., 2015 ;  Mnih et al., 2016 ). Importantly, off-policy techniques can leverage the vast amount of existing offline logged data for real-world applications such as digital advertis- ing ( Strehl et al., 2010 ;  Bottou et al., 2013 ), education ( Mandel et al., 2014 ), and healthcare ( Shortreed et al., 2011 ). Since online RL is often unsafe to deploy in the real world, offline RL algorithms are the only feasible solution for many practical decision making problems ( Dulac-Arnold et al., 2019 ). Nevertheless, off-policy RL algorithms when combined with neural networks can be unstable or even divergent ( Baird, 1995 ;  Boyan and Moore, 1995 ;  Tsitsiklis and Van Roy, 1997 ). In the absence of theoretical guarantees for off-policy deep RL, recent advances (see  Hessel et al. (2018)  for an overview) are largely governed by empirical results on a popular benchmark suite of Atari 2600 games ( Bellemare et al., 2013 ). Unfortunately, these empirical results are difficult to reproduce ( Henderson et al., 2018 ) and the contribution of novel RL algorithms is often conflated with many other design choices ( Clary et al., 2019 ;  Khetarpal et al., 2018 ). Given the empirical nature of deep off-policy RL, it is crucial to come up with simpler and reproducible experimental settings and study the relative importance of different components of deep RL algorithms. It is also important to strive for finding successful RL algorithms that are as simple as possible. This paper advocates the use of offline (batch) RL to help (1) isolate the contributions of exploitation vs. exploration in off-policy deep RL, (2) improve reproducibility of deep RL research, and (3) facilitate the design of simpler off-policy deep RL algorithms. To this end, we propose a new benchmark for offline optimization of RL agents on Atari 2600 games using all of the replay data of a DQN agent ( Mnih et al., 2015 ). Using this benchmark, we investigate the following questions: 1. Is it possible to train successful Atari agents based solely on offline data? 2. Can one design simple and effective alternatives to intricate RL algorithms in the offline setting? 3. Are the insights gained from the offline setting useful for developing effective online algorithms? The contributions of this paper can be summarized as: • An offline RL benchmark is proposed for evaluating and designing RL algorithms on Atari 2600 games without exploration, based on the logged replay data of a DQN agent comprising 50 million (observation, action, reward, next observation) tuples per game. This reduces the computation cost of the experiments considerably and helps improve reproducibility of deep RL research by standardizing training using a fixed offline dataset. The replay dataset used in our experiments will be released to enable offline optimization of RL algorithms on a common ground. • Contrary to recent work ( Zhang and Sutton, 2017 ;  Fujimoto et al., 2019 ), we find that the logged DQN data is sufficient for optimizing strong Atari agents offline without any environment interaction. For instance, QR-DQN ( Dabney et al., 2018b ) trained offline on the DQN replay dataset significantly outperforms online DQN ( Mnih et al., 2015 ). • A simple and novel Q-learning algorithm called Random Ensemble Mixture (REM) is presented, which enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM, trained using the DQN replay dataset, outperforms more complex RL algorithms such as offline QR-DQN and surpasses the gains from online C51 ( Bellemare et al., 2017 ) over online DQN (Figure 1a). • We use the insights gained from the offline experiments to develop an online variant of REM, which performs comparably with online QR-DQN (Figure 1b).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an algorithm to transfer policies between tasks with significant differences in state transitions via a policy adaptation mechanism. The algorithm combines supervised reference trajectory tracking and unsupervised reinforcement learning to adapt the source policy to the target domain directly. The approach is capable of robustly transferring policies between tasks, even in the presence of nonlinear and time-varying differences in the dynamic model of the systems. The empirical results show that the approach enjoys significantly reduced sample complexity in solving the task and is capable of achieving optimal behavior in the target domain.",
        "Abstract": "Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can \\textbf{``Adapt-to-Learn\"}, that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties.  We show through theory and experiments that our method leads to a significantly reduced sample complexity of transferring the policies between the tasks.",
        "Introduction": "  INTRODUCTION Lack of principled mechanisms to quickly and efficiently transfer policies learned between domains has become the major bottleneck in Reinforcement Learning (RL). This inability to transfer or adapt policies is one major reason why RL has still not proliferated physical application like robotics. Since RL agents cannot quickly transfer policies, the agent is forced to learn every task from scratch, which is both time and sample expensive. Warm-start, a method in which weights from one neural network are transferred to another, has been reasonably successful for supervised learning. However, this method can often lead to mixed and even negative results in RL ( Joshi & Chowdhary, 2018 ;  Taylor & Stone, 2009 ). Our main contribution is an algorithm to transfer policies between tasks with significant differences in state transitions via a policy adaptation mechanism. Unlike the majority of existing work in trans- fer learning for RL, our approach does not merely use the transferred policy to warm start (initialize the parameter of the target network with learned source network) policy learning in the target do- main. Neither does it rely on a multitude of simulations across randomly generated source domains. Instead, we combine supervised reference trajectory tracking and unsupervised reinforcement learn- ing to adapt the source policy to the target domain directly. We show through theory and experiments that our method enjoys significantly reduced sample complexity in solving the task. Adapt-to-Learn is inspired by the fact that combined adaptation of behaviors and learning through experience is a primary mechanism of learning in biological creatures ( Krakauer & Mazzoni, 2011 ;  Fryling et al., 2011 ).Inspired by this ability of biological creatures, we seek to answer the question: Will learning to combine intrinsic adaptation reward with environment reward lead to more efficient transfer of policies between domains? Imitation Learning (IL) ( Duan et al., 2017 ;  Zhu et al., 2018 ) seems to play a crucial part in biological learning, and as such has been widely studied in RL. How- ever, the key is, when presented with a new situation, animals do not just imitate, but quickly adapt existing behaviors, and improve them through further experience. In particular, an animal learning to walk on a different terrain does not just imitate its existing gait, but adapts it to the new environ- ment.The theory behind such adaptation in reference tracking control problems has been typically restricted to deterministic dynamical systems with well-defined reference trajectories ( Åström & Wittenmark, 2013 ;  Chowdhary et al., 2013 ). This ability to adapt and incorporate further learning through optimization on the environment reward is one key difference between our method and ex- Under review as a conference paper at ICLR 2020 isting imitation learning and Guided Policy Search (GPS) methods ( Levine & Koltun, 2013 ). Unlike IL and GPS, our method transfers policies between task with significant differences in the transition models. Moreover, by mixing environment reward with intrinsic adaptation rewards, we ensure that the agent quickly adapts and also learns to acquire skills beyond what the source policy can teach. We posit that the presented method can be the foundation of a broader class of RL algorithms that can choose seamlessly between learning through RL to supervised adaptive imitation. Our empir- ical results show that approach is capable of robustly transferring policies between tasks, even in the presence of nonlinear and time-varying differences in the dynamic model of the systems. In particular, we show that it suffices to execute adapted greedy policies to ensure −optimal behavior in the target domain. Related work: D-RL has recently enabled agents to learn policies for complex robotic tasks in simulation ( Peng et al., 2016 ;  2017b ;  Liu & Hodgins, 2017 ;  Heess et al., 2017 ). However, D-RL has been plagued by the curse of sample complexity. Therefore, the capabilities demonstrated in the simulated environment are hard to replicate in the real world. This learning inefficiency of RL has led to significant work in the field of TL ( Taylor & Stone, 2009 ). A significant body of literature on transfer in RL is focused on initialized RL in the target domain using learned source policy; known as jump-start/warm-start methods ( Taylor et al., 2005 ;  Ammar et al., 2012 ;  2015 ). Some examples of these transfer architectures include transfer between similar tasks ( Banerjee & Stone, 2007 ), transfer from human demonstrations ( Peters & Schaal, 2006 ) and transfer from simulation to real ( Peng et al., 2017a ;  Ross et al., 2011 ;  Yan et al., 2017 ). Efforts have also been made in exploring accelerated learning directly on real robots, through Guided Policy Search (GPS) ( Levine et al., 2015 ) and parallelizing the training across multiple agents using meta-learning ( Levine et al., 2016 ;  Nagabandi et al., 2018 ;  Zhu et al., 2018 ). Sim-to-Real transfers have been widely adopted in the recent works and can be viewed as a subset of same domain transfer problems. Daftry et al. ( Daftry et al., 2016 ) demonstrated the policy transfer for control of aerial vehicles across different vehicle models and environments. Christiano et al. ( Christiano et al., 2016 ) transferred policies from simulation to real using an inverse dynamics model estimated interacting with the real robot. Through learning over an adversarial loss, the agents are trained to achieve robust policies across various environments ( Wulfmeier et al., 2017 ). However, these and other reported architectures do not necessarily lead to improved sample efficiency, handle relatively minor changes in the transition model, and are even known to cause negative transfer. In contrast, our approach directly adapts the source policies to target with significant transition model difference while interacting with the environment. It enjoys empirically and rigorously proven sample efficiency guarantees of order O(nH), depending polynomially on the horizon length \"H\".",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes Adversarial Inductive Transfer Learning (AITL), the first adversarial method of inductive transfer learning, to address discrepancies in both the input and output spaces. AITL is evaluated on pharmacogenomics datasets in terms of the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPR). Results show that AITL achieves a substantial improvement compared to the baselines, demonstrating the potential of transfer learning for drug response prediction.",
        "Abstract": "We propose Adversarial Inductive Transfer Learning (AITL), a method for addressing discrepancies in input and output spaces between source and target domains. AITL utilizes adversarial domain adaptation and multi-task learning to address these discrepancies. Our motivating application is pharmacogenomics where the goal is to predict drug response in patients using their genomic information. The challenge is that clinical data (i.e. patients) with drug response outcome is very limited, creating a need for transfer learning to bridge the gap between large pre-clinical pharmacogenomics datasets (e.g. cancer cell lines) and clinical datasets. Discrepancies exist between 1) the genomic data of pre-clinical and clinical datasets (the input space), and 2) the different measures of the drug response (the output space). To the best of our knowledge, AITL is the first adversarial inductive transfer learning method to address both input and output discrepancies. Experimental results indicate that AITL outperforms state-of-the-art pharmacogenomics and transfer learning baselines and may guide precision oncology more accurately.",
        "Introduction": "  INTRODUCTION Deep neural networks (Goodfellow et al., 2016) have demonstrated the state-of-the-art performance in different problems, ranging from computer vision and natural language processing to genomics (Eraslan et al., 2019) and medicine (Topol, 2019). However, these networks often require a large number of samples for training, which is challenging and sometimes impossible to obtain in the real world applications. Transfer learning (Pan & Yang, 2009) attempts to solve this challenge by leveraging the knowledge in a source domain, a large data-rich dataset, to improve the generalization performance on a small target domain. Training a model on the source domain and testing it on the target domain violates the i.i.d assumption that the train and test data are from the same distribution. The discrepancy in the input space decreases the prediction accuracy on the test data, which leads to poor generalization (Zhang et al., 2019). Many methods have been proposed to minimize the discrepancy between the source and the target domains using different metrics such as Jensen Shannon Divergence (Ganin & Lempitsky, 2014), Maximum Mean Discrepancy (Gretton et al., 2012), and Margin Disparity Dis- crepancy (Zhang et al., 2019). While transductive transfer learning (e.g. domain adaptation) uses a labeled source domain to improve generalization on an unlabeled target domain, inductive transfer learning (e.g. few-shot learning) uses a labeled source domain to improve the generalization on a labeled target domain where label spaces are different in the source and the target domains (Pan & Yang, 2009). Adversarial domain adaptation has shown great performance in addressing the discrepancy in the input space for different applications (Schoenauer-Sebag et al., 2019; Hosseini-Asl et al., 2018; Pin- heiro, 2018; Zou et al., 2018; Tsai et al., 2018; Long et al., 2018; Chen et al., 2017; Tzeng et al., 2017), however, adversarial adaptation to address the discrepancies in both the input and output spaces has not yet been explored. Our motivating application is pharmacogenomics (Smirnov et al., 2017) where the goal is to predict response to a cancer drug given the genomic data (e.g. gene ex- pression). Since clinical datasets in pharmacogenomics (patients) are small and hard to obtain, many studies have focused on large pre-clinical pharmacogenomics datasets such as cancer cell lines as a proxy to patients (Barretina et al., 2012; Iorio et al., 2016). A majority of the current methods are trained on cell line datasets and then tested on other cell line or patient datasets (Sharifi-Noghabi Under review as a conference paper at ICLR 2020 et al., 2019b; Geeleher et al., 2014). However, cell lines and patients data, even with the same set of genes, do not have identical distributions due to the lack of an immune system and the tumor mi- croenvironment in cell lines (Mourragui et al., 2019). Moreover, in cell lines, the response is often measured by the drug concentration that reduces viability by 50% (IC50), whereas in patients, it is often based on changes in the size of the tumor and measured by metrics such as response evaluation criteria in solid tumors (RECIST) (Schwartz et al., 2016). This means that drug response prediction is a regression problem in cell lines but a classification problem in patients. Therefore, discrepan- cies exist in both the input and output spaces in pharmacogenomics datasets. Table A1 provides the definition of these biological terms. In this paper, we propose Adversarial Inductive Transfer Learning (AITL), the first adversarial method of inductive transfer learning. Different from existing methods for transfer learning, AITL adapts not only the input space but also the output space. Our motivating application is transfer learning for pharmacogenomics datasets. In our driving application, the source domain is the gene expression data obtained from the cell lines and the target domain is the gene expression data ob- tained from patients. Both domains have the same set of genes (i.e., raw feature representation). Discrepancies exist between the gene expression data in the input space, and the measure of the drug response in the output space. AITL learns features for the source and target samples and uses these features as input for a multi-task subnetwork to predict drug response for both the source and the target samples. The output space discrepancy is addressed by the multi-task subnetwork, which has one shared layer and separate classification and regression towers, and assigns binary labels (called cross-domain labels) to the source samples. The multi-task subnetwork also alleviates the problem of small sample size in the target domain by sharing the first layer with the source domain. To address the discrepancy in the input space, AITL performs adversarial domain adaptation. The goal is that features learned for the source samples should be domain-invariant and similar enough to the features learned for the target samples to fool a global discriminator that receives samples from both domains. Moreover, with the cross-domain binary labels available for the source samples, AITL further regularizes the learned features by class-wise discriminators. A class-wise discrimi- nator receives source and target samples from the same class label and should not be able to predict the domain accurately. We evaluated the performance of AITL and state-of-the-art inductive and adversarial transductive transfer learning baselines on pharmacogenimcs datasets in terms of the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPR). In our experiments, AITL achieved a substantial improvement compared to the baselines, demon- strating the potential of transfer learning for drug response prediction, a crucial task of precision oncology.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an abstract summarizing the introduction of a study on type 1 diabetes (T1D). It outlines the challenges of managing T1D, including the need for continual decisions about insulin administration, and the potential of an artificial pancreas (AP) to improve glucose control. It also discusses the advances in technology that have enabled the development of an AP, as well as the remaining challenges in developing control algorithms.",
        "Abstract": "Individuals with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer in order to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 21k hours of simulated data across 30 patients, RL approaches outperform baseline control algorithms (increasing time spent in normal glucose range from 71% to 75%) without requiring meal announcements. Moreover, these approaches are adept at leveraging latent behavioral patterns (increasing time in range from 58% to 70%). This work demonstrates the potential of deep RL for controlling complex physiological systems with minimal expert knowledge. ",
        "Introduction": "  INTRODUCTION Type 1 diabetes (T1D) is a chronic disease affecting 20-40 million people worldwide ( You & Henneberg, 2016 ), and its incidence is increasing ( Tuomilehto, 2013 ). People with T1D cannot produce insulin, a hormone that signals cells to uptake glucose in the bloodstream. Without insulin, the body must metabolize energy in other ways that, when relied on repeatedly, can lead to life- threatening conditions ( Kerl, 2001 ). Tight glucose control improves both short- and long-term outcomes for people with diabetes, but can be difficult to achieve in practice ( Diabetes Control and Complications Trial Research Group, 1995 ). Typically, blood glucose is controlled by a combination of basal insulin (to control baseline blood glucose levels) and bolus insulin (to control glucose spikes after meals). To control blood glucose levels, individuals with T1D must continually make decisions about how much basal and bolus insulin to self-administer. This requires careful measurement of glucose levels and carbohydrate intake, resulting in at least 15-17 data points a day. If the individual uses a continuous glucose monitor (CGM), this can increase to over 300 data points, or a blood glucose reading every 5 minutes ( Coffen & Dahlquist, 2009 ). Combined with an insulin pump, a wearable device that automates the delivery of insulin, CGMs present an opportunity for closed-loop control. Such a system, known as an 'artificial pancreas' (AP), automatically anticipates the amount of required insulin and delivers the appropriate dose. This would be life-changing for individuals with T1D. For many years, researchers have worked towards the creation of an AP for blood glucose control ( Kadish, 1964 ;  Bequette, 2005 ;  Bothe et al., 2013 ). Though the technology behind CGMs and insulin pumps has advanced, there remains significant room for improvement when it comes to the control algorithms ( Bothe et al., 2013 ;  Pinsker et al., 2016 ). Current approaches often fail to maintain sufficiently tight glucose control and require meal announcements.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the behavior of minibatch stochastic gradient descent (SGD) and its effects on the final performance of popular network architectures. We argue that SGD exhibits two regimes with different behaviors: a noise dominated regime and a curvature dominated regime. Our experiments demonstrate that in the noise dominated regime, the final training loss and test accuracy are independent of batch size under a constant epoch budget, and the optimal learning rate increases as the batch size rises. In the curvature dominated regime, the optimal learning rate is independent of batch size, and the training loss and test accuracy degrade with increasing batch size. We also find that the critical learning rate which separates the two regimes varies between architectures. Our results provide evidence that the noise in stochastic gradients can enhance generalization in some cases, and suggest novel hyper-parameter tuning strategies that may reduce the cost of identifying the optimal learning rate and optimal epoch budget.",
        "Abstract": "This paper makes two contributions towards understanding how the hyperparameters of stochastic gradient descent affect the final training loss and test accuracy of neural networks. First, we argue that stochastic gradient descent exhibits two regimes with different behaviours; a noise dominated regime which typically arises for small or moderate batch sizes, and a curvature dominated regime which typically arises when the batch size is large. In the noise dominated regime, the optimal learning rate increases as the batch size rises, and the training loss and test accuracy are independent of batch size under a constant epoch budget. In the curvature dominated regime, the optimal learning rate is independent of batch size, and the training loss and test accuracy degrade as the batch size rises. We support these claims with experiments on a range of architectures including ResNets, LSTMs and autoencoders. We always perform a grid search over learning rates at all batch sizes. Second, we demonstrate that small or moderately large batch sizes continue to outperform very large batches on the test set, even when both models are trained for the same number of steps and reach similar training losses. Furthermore, when training Wide-ResNets on CIFAR-10 with a constant batch size of 64, the optimal learning rate to maximize the test accuracy only decays by a factor of 2 when the epoch budget is increased by a factor of 128, while the optimal learning rate to minimize the training loss decays by a factor of 16. These results confirm that the noise in stochastic gradients can introduce beneficial implicit regularization.",
        "Introduction": "  INTRODUCTION Stochastic gradient descent (SGD) is the most popular optimization algorithm in deep learning, but it remains poorly understood. A number of papers propose simple scaling rules that predict how changing the learning rate and batch size will influence the final performance of popular network architectures ( Hoffer et al., 2017 ;  Goyal et al., 2017 ;  Smith et al., 2017 ;  Jastrzębski et al., 2017 ). Some of these scaling rules are contradictory, and  Shallue et al. (2018)  argue that none of these simple prescriptions work reliably across multiple architectures. Some papers claim SGD with Momentum significantly outperforms SGD without Momentum ( Sutskever et al., 2013 ), but others observe little difference between both algorithms in practice ( Kidambi et al., 2018 ;  Zhang et al., 2019 ). We hope to clarify this debate. We argue that minibatch stochastic gradient descent exhibits two regimes with different behaviours: a noise dominated regime and a curvature dominated regime ( Ma et al., 2017b ;  McCandlish et al., 2018 ;  Liu & Belkin, 2018 ). The noise dominated regime typically arises for small or moderate batch sizes, while the curvature dominated regime typically arises when the batch size is large. The curvature dominated regime may also arise if the epoch budget is small or the loss is poorly conditioned ( McCandlish et al., 2018 ). Our extensive experiments demonstrate that, 1. In the noise dominated regime, the final training loss and test accuracy are independent of batch size under a constant epoch budget, and the optimal learning rate increases as the batch size rises. In the curvature dominated regime, the optimal learning rate is independent of batch size, and the training loss and test accuracy degrade with increasing batch size. The critical learning rate which separates the two regimes varies between architectures. 2. If specific assumptions are satisfied, then the optimal learning rate is proportional to batch size in the noise dominated regime. These assumptions hold for most tasks. However we Under review as a conference paper at ICLR 2020 observe a square root scaling rule when performing language modelling with an LSTM. This is not surprising, since consecutive gradients in a language model are not independent. 3. SGD with Momentum and learning rate warmup do not outperform vanilla SGD in the noise dominated regime, but they can outperform vanilla SGD in the curvature dominated regime. There is also an active debate regarding the role of stochastic gradients in promoting generalization. It has been suspected for a long time that stochastic gradients sometimes generalize better than full batch gradient descent ( Heskes & Kappen, 1993 ;  LeCun et al., 2012 ). This topic was revived by  Keskar et al. (2016) , who showed that the test accuracy often falls if one holds the learning rate constant and increases the batch size, even if one continues training until the training loss ceases to fall. Many authors have studied this effect ( Jastrzębski et al., 2017 ;  Smith & Le, 2017 ;  Chaudhari & Soatto, 2018 ), but to our knowledge no paper has demonstrated a clear generalization gap between small and large batch training under a constant step budget on a challenging benchmark while simultaneously tuning the learning rate. This phenomenon has also been questioned by a number of authors.  Shallue et al. (2018)  argued that one can reduce the generalization gap between small and large batch sizes if one introduces additional regularization (we note that this is consistent with the claim that stochastic gradients can enhance generalization).  Zhang et al. (2019)  suggested that a noisy quadratic model is sufficient to describe the performance of neural networks on both the training set and the test set. In this work, we verify that small or moderately large batch sizes substantially outperform very large batches on the test set in some cases, even when compared under a constant step budget. However the batch size at which the test accuracy begins to degrade can be larger than previously thought. We find that the test accuracy of a 16-4 Wide-ResNet (Zagoruyko & Komodakis, 2016) trained on CIFAR-10 for 9725 updates falls from 94.7% at a batch size of 4096 to 92.8% at a batch size of 16384. When performing language modelling with an LSTM on the Penn TreeBank dataset for 16560 updates ( Zaremba et al., 2014 ), the test perplexity rises from 81.7 to 92.2 when the batch size rises from 64 to 256. We observe no degradation in the final training loss as the batch size rises in either model. These surprising results motivated us to study how the optimal learning rate depends on the epoch budget for a fixed batch size. As expected, the optimal test accuracy is maximized for a finite epoch budget, consistent with the well known phenomenon of early stopping ( Prechelt, 1998 ). Meanwhile the training loss falls monotonically as the epoch budget increases, consistent with classical optimization theory. More surprisingly, the learning rate that maximizes the final test accuracy decays very slowly as the epoch budget increases, while the learning rate that minimizes the training loss decays rapidly. 1 These results provide further evidence that the noise in stochastic gradients can enhance generalization in some cases, and they suggest novel hyper-parameter tuning strategies that may reduce the cost of identifying the optimal learning rate and optimal epoch budget. We describe the noise dominated and curvature dominated regimes of SGD with and without Mo- mentum in section 2. We focus on the analogy between SGD and stochastic differential equations ( Gardiner et al., 1985 ; Welling & Teh, 2011;  Mandt et al., 2017 ;  Li et al., 2017 ), but our primary contributions are empirical and many of our conclusions can be derived from different assumptions ( Ma et al., 2017b ;  Zhang et al., 2019 ). In section 3, we provide an empirical study of the relationship between the optimal learning rate and the batch size under a constant epoch budget, which verifies the existence of the two regimes in practice. In section 4, we study the relationship between the optimal learning rate and the batch size under a constant step budget, which confirms that stochastic gradients can introduce implicit regularization enhancing the test set accuracy. Finally in section 5, we fix the batch size and consider the relationship between the optimal learning rate and the epoch budget.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the problem of devising optimal attacks on Deep Reinforcement Learning (RL) policies. Optimal attacks are formulated as a Markov Decision Process (MDP) and trained using Deep RL algorithms. Experiments are conducted on OpenAI Gym environments with discrete and continuous state-action spaces, and results show that optimal attacks outperform existing attacks. The paper also discusses how the methods used to train RL policies impact their resilience to attacks, and suggests that training methods specifically tailored to Partially Observable MDPs (POMDPs) result in more resistant policies.",
        "Abstract": "Control policies, trained using the Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through  numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modelled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies. ",
        "Introduction": "  INTRODUCTION Advances in Deep Reinforcement Learning (RL) have made it possible to train end-to-end policies achieving superhuman performance on a large variety of tasks, such as playing Atari games  Mnih et al. (2013 ; 2016), playing  Go Silver et al. (2016 ; 2017), as well as controlling systems with continuous state and action spaces  Lillicrap et al. (2016) ;  Schulman et al. (2015) ;  Levine et al. (2016) . Recently, some of these policies have been shown to be vulnerable to adversarial attacks at test time see e.g.  Huang et al. (2017) ;  Pattanaik et al. (2018) . Even if these attacks only introduce small perturbations to the successive inputs of the policies, they can significantly impact the average collected reward by the agent. So far, attacks on RL policies have been designed using heuristic techniques, based on gradient methods, essentially the Fast Gradient Sign Method (FGSM). As such, they do not explicitly aim at minimizing the reward collected by the agent. In contrast, in this paper, we investigate the problem of casting optimal attacks with well-defined objectives, for example minimizing the average reward collected by the agent. We believe that casting optimal attacks is crucial when assessing the robustness of RL policies, since ideally, the agent should learn and apply policies that resist any possible attack (of course with limited and reasonable amplitude). For a given policy learnt by the agent, we show that the problem of devising an optimal attack can be formulated as a Markov Decision Process (MDP), defined through the system dynamics, the agent's reward function and policy. We are mainly interested in black-box attacks: to devise them, the adversary only observes the variables from which she builds her reward. For example, if the objective is to lead the system to a certain set of (bad) states, the adversary may just observe the states as the system evolves. If the goal is to minimize the cumulative reward collected by the agent, the Under review as a conference paper at ICLR 2020 adversary may just observe the system states and the instantaneous rewards collected by the agent. In black-box attacks, the aforementioned MDP is unknown, and optimal attacks can be trained using RL algorithms. The action selected by the adversary in this MDP corresponds to a perturbed state to be fed to the agent's policy. As a consequence, the action space is typically very large. To deal with this issue, our optimal attack is trained using DDPG  Lillicrap et al. (2016) , a Deep RL algorithm initially tailored to continuous action space. We train and evaluate optimal attacks for some OpenAI Gym  Brockman et al. (2016)  environments with discrete action spaces (discrete MountainCar, Cartpole, and Pong), and continuous state-action spaces (continuous MountainCar and continuous LunarLander). As expected, optimal attacks outper- form existing attacks. We discuss how the methods used to train RL policies impact their resilience to attacks. We show that the damages caused by attacks are upper bounded by a quantity directly related to the smoothness of the policy under attack 1 . Hence, training methods such as DDPG leading to smooth policies should be more robust. Finally, we remark that when under attack, the agent faces an environment with uncertain state feedback, and her sequential decision problem can be modelled as a Partially Observable MDP (POMDP). This suggests that training methods specifically tailored to POMDP (e.g. DRQN  Hausknecht & Stone (2015) ) result in more resistant policies. These observations are confirmed by our experiments.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Cooperative multiagent reinforcement learning (MARL) is a field of study that investigates how multiple agents can learn to coordinate as a team to maximize a global objective. This paper proposes a novel approach to MARL that combines agent-specific rewards with team rewards to enable agents to learn complex coordination tasks. The proposed approach does not require extensive domain knowledge or manual tuning, and avoids the risk of changing the underlying problem. Results from experiments demonstrate that the proposed approach is effective in solving complex coordination tasks.",
        "Abstract": "Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Also, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods such as MADDPG on a number of difficult coordination benchmarks. ",
        "Introduction": "  INTRODUCTION Cooperative multiagent reinforcement learning (MARL) studies how multiple agents can learn to coordinate as a team toward maximizing a global objective. Cooperative MARL has been applied to many real world applications such as air traffic control ( Tumer and Agogino, 2007 ), multi-robot coordination ( Sheng et al., 2006 ;  Yliniemi et al., 2014 ), communication and language ( Lazaridou et al., 2016 ;  Mordatch and Abbeel, 2018 ), and autonomous driving ( Shalev-Shwartz et al., 2016 ). Many such environments endow agents with a team reward that reflects the team's coordination objective, as well as an agent-specific local reward that rewards basic skills. For instance, in soccer, dense local rewards could capture agent-specific skills such as passing, dribbling and running. The agents must then coordinate when and where to use these skills in order to optimize the team objective, which is winning the game. Usually, the agent-specific reward is dense and easy to learn from, while the team reward is sparse and requires the cooperation of all or most agents. Having each agent directly optimize the team reward and ignore the agent-specific reward usually fails or is sample-inefficient for complex tasks due to the sparsity of the team reward. Conversely, having each agent directly optimize the agent-specific reward also fails because it does not capture the team's objective, even with state of the art multiagent RL algorithms such as MADDPG ( Lowe et al., 2017 ). One solution to this problem is to use reward shaping, where extensive domain knowledge about the task is used to create a proxy reward function ( Rahmattalabi et al., 2016 ). Constructing this proxy reward function is difficult in complex environments, and is domain-dependent. Apart from requiring domain knowledge and manual tuning, this approach also poses risks of changing the underlying problem itself ( Ng et al., 1999 ). Simple approaches to creating a proxy reward via linear combinations of the two objectives also fail to solve or generalize to complex coordination tasks ( Devlin et al., 2011 ;  Williamson et al., 2009 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the implications of weight-space symmetry for the optimization of neural network parameters. It provides insights into three observations on neural network landscapes, including the existence of numerous connected high-dimensional plateaus extending across the landscape due to weight-space symmetries, and the number of saddles that can grow exponentially in neural network landscapes. It also proposes a novel low-loss path finding algorithm to find barriers between partner minima, and provides theoretical and numerical evidence for the existence of first-order permutation saddles.",
        "Abstract": "Neural network training depends on the structure of the underlying loss landscape, i.e. local minima, saddle points, flat plateaus, and loss barriers. In relation to the structure of the landscape, we study the permutation symmetry of neurons in each layer of a deep neural network, which gives rise not only to multiple equivalent global minima of the loss function but also to critical points in between partner minima. In a network of $d-1$ hidden layers with $n_k$ neurons in layers $k = 1, \\ldots, d$, we construct continuous paths between equivalent global minima that lead through a `permutation point' where the input and output weight vectors of two neurons in the same hidden layer $k$ collide and interchange. We show that such permutation points are critical points which lie inside high-dimensional subspaces of equal loss, contributing to the global flatness of the landscape. We also find that a permutation point for the exchange of neurons $i$ and $j$ transits into a flat high-dimensional plateau that enables all $n_k!$ permutations of neurons in a given layer $k$ at the same loss value. Moreover, we introduce higher-order permutation points by exploiting the hierarchical structure in the loss landscapes of neural networks, and find that the number of $K$-th order permutation points is much larger than the (already huge) number of equivalent global minima -- at least by a polynomial factor of order $K$. In two tasks, we demonstrate numerically with our path finding method that continuous paths between partner minima exist: first, in a toy network with a single hidden layer on a function approximation task and, second, in a multilayer network on the MNIST task. Our geometric approach yields a lower bound on the number of critical points generated by weight-space symmetries and provides a simple intuitive link between previous theoretical results and numerical observations.",
        "Introduction": "  INTRODUCTION The structure of the loss landscape plays an important role in the optimization of neural network parameters. A large number of numerical ( Dauphin et al., 2014 ;  Goodfellow et al., 2014 ;  Li et al., 2018 ;  Sagun et al., 2014 ; 2016;  Ballard et al., 2017 ; Garipov et al., 2018;  Draxler et al., 2018 ;  Sagun et al., 2017 ;  Baity-Jesi et al., 2018 ) and theoretical ( Choromanska et al., 2015 ;  Rasmussen, 2003 ;  Freeman and Bruna, 2016 ;  Soudry and Carmon, 2016 ;  Nguyen and Hein, 2017 ) studies have explored the properties of the loss landscape. In particular, in a multilayer network of d − 1 hidden layers with n neurons each, there are (n!) d−1 equivalent configurations corresponding to the permutation of neuron indices in each layer of the network ( Goodfellow et al., 2016 ;  Bishop, 1995 ). The permutation symmetries give rise to a loss landscape where any given global minimum in the weight space must have (n!) d−1 − 1 completely equivalent partner minima. This property of neural network landscapes is called weight-space symmetry. Several ( Saad and Solla, 1995 ;  Amari et al., 2006 ;  Wei et al., 2008 ) works explored the implications of weight-space symmetry for training dynamics in two-layer networks and found that training dynamics slow down near the singular regions caused by weight-space symmetry.  Dauphin et al. (2014) ;  Orhan and Pitkow (2017)  argue that optimization paths may get close to the singular regions induced by weight-space symmetry and this, in turn, slows down training for deep neural networks. Exploiting weight-space symmetries, we give insights into and partial explanations of three observations on neural network landscapes. Under review as a conference paper at ICLR 2020 Observation 1. Training dynamics are slow near singular regions caused by weight-space symmetry and stochastic gradient descent might travel near these regions throughout training ( Saad and Solla, 1995 ;  Wei et al., 2008 ;  Amari et al., 2006 ;  Dauphin et al., 2014 ;  Orhan and Pitkow, 2017 ). Observation 2. The Hessian of the loss function has numerous almost-zero eigenvalues throughout training, thus the landscape is flat in many directions ( Sagun et al., 2017 ;  Papyan, 2018 ;  Ghorbani et al., 2019 ). Related to observation 1 and 2, we prove the existence of numerous connected high-dimensional plateaus extending across the landscape due to weight-space symmetries. Observation 3. The number of saddles can grow exponentially in neural network landscapes ( Auer et al., 1996 ;  Dauphin et al., 2014 ;  Choromanska et al., 2015 ). Related to observation 3, we prove that there are at least polynomially many more saddles than the global minima due to weight-space symmetries in neural networks, without any further assumptions. In addition, we propose a novel low-loss path finding algorithm to find barriers between partner minima. We start from the known permutation symmetries and consider continuous low-loss paths that connect two equivalent global minima by merging the weight vectors of two neurons in a specific way. At a so-called permutation point, where the distance between the input and output weight vectors of the two neurons vanishes, the indices of the two neurons can be interchanged at no extra cost. After the change, the system returns on the 'mirrored' path back to the original configuration - except for the permutation of one pair of indices. Surprisingly, we find that we can permute all neuron indices in the same layer at the same cost as the loss at a permutation point reached by moving along the path that merges a single pair of neurons. These constant-loss permutations are possible because each permutation point lies in a high-dimensional plateau of critical points. Our theory can be extended to higher-order saddles and provides explicit lower bounds for the number of first- and higher-order permutation points. Numerically, we confirm the existence of first-order permutation saddles. In particular, the specific contributions of our work are: • A simple low-loss path-finding algorithm linking partner global minima via a permutation point, implemented by minimization under a single scalar constraint (distance of weight vectors). • The theoretical characterization of permutation points, for example that these are critical points and several permutation points are connected via paths at equal loss. • A lower bound for the number of first- and higher-order permutation points and their corresponding plateaus. • Numerical demonstrations of the path finding method in multilayer neural networks trained on MNIST.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to optimize the inference process of Convolutional Neural Networks (CNNs) by pruning. The proposed method, referred to as semantic pruning, is performed by measuring each filter's contribution (activation magnitude) toward recognition of a target object class and removing filters with the lowest contributions. The pruned and unpruned filters are analyzed for correlation, overlap and other measures allowing us to determine class-wise interactions. The contribution of this paper is to investigate the sensitivity of object classification to semantic pruning and to study the interference between class wise filters in CNNs.",
        "Abstract": "Convolutional Neural Networks (CNN) have achieved state-of-the-art performance in different computer vision tasks, but at a price of being computationally and power intensive. At the same time, only a few attempts were made toward a deeper understanding of CNNs. In this work, we propose to use semantic pruning technique toward not only CNN optimization but also as a way toward getting some insight information on convolutional filters correlation and interference. We start with a pre-trained network and prune it until it behaves as a single class classifier for a selected class. Unlike the more traditional approaches which apply retraining to the pruned CNN, the proposed semantic pruning  does not use retraining. Conducted experiments showed that a) for each class there is a pruning ration which allows removing filters with either an increase or no loss of classification accuracy, b) pruning can improve the interference between filters used for classification of different classes c) effect between classification accuracy and correlation between pruned filters groups specific for different classes. ",
        "Introduction": "  INTRODUCTION Convolutional Neural Networks (CNN) provides high quality features that are used in vari- ous tasks such as object recognition ( Ouyang et al., 2015 ;  Wang et al., 2015 ;  Lin et al., 2017a ;  Hu et al., 2017 ), scene classification ( Zhou et al., 2014 ;  Ren & Li, 2015 ), semantic segmen- tation  Girshick et al. (2014) ;  Husain et al. (2017) , image captioning ( Anderson et al., 2017 ;  Rennie et al., 2017 ;  Yao et al., 2017 ;  Lu et al., 2017 ), raw audio generation ( van den Oord et al., 2016 ), data analytics ( Najafabadi et al., 2015 ). Unlike engineered features, where one or multiple features have been designed with specific domain knowledge (SIFT ( Lowe, 1999 ) ,SURF ( Bay et al., 2008 ), LBP ( Ojala et al., 1994 ), Canny ( Canny, 1986 ), etc.), features in CNN are obtained by di- rectly learning representations (filters) from the data without explicit domain expertise. While using a large number of learned filters in CNNs creates high-quality features, it also re- quires significant computational resources. The filter learning and the inference processes need resources such as GPU or TPUs to tackle the computational overhead. Recently, multiple opti- mization methods were proposed, with the target to reduce the computational overhead of either the learning or the inference processes. For instance, in the learning optimization approach, the following works have been done ( Zhang et al., 2018 ;  Lin et al., 2017b ). On the other hand, the number of the current works targeting the optimization of the inference are: reduced precision computation ( Rastegari et al., 2016 ;  Courbariaux et al., 2016 ;  Zhou et al., 2016 ), look-up and pre- computing ( Abdiyeva et al., 2018 ;  Brendel & Bethge, 2019 ), pruning ( Anwar et al., 2015 ;  Li et al., 2016 ;  Zhu & Gupta, 2017 ;  Raghu et al., 2017 ;  Morcos et al., 2018 ;  Ma et al., 2019 ) and knowledge distillation ( Hinton et al., 2015 ). The work described in this paper continues in the direction of inference optimization by pruning. However, the pruning is only used as a tool for studying the interpretability of CNNs filter. Pruning is the process of removing redundant filters from the network ( Anwar et al., 2015 ;  Li et al., 2016 ;  Zhang et al., 2017 ;  Zylberberg, 2017 ;  Zhu & Gupta, 2017 ;  Raghu et al., 2017 ;  Morcos et al., 2018 ;  Ma et al., 2019 ). In current approaches, the pruning process consists of the following steps: train CNN, remove redundant filters and retrains the network to preserve the original accuracy. The pro- posed pruning method, however, does not involve retraining of the pruned network. The motivation of this work is the interpretation of CNNs: pruned and unpruned filters are analyzed to evaluate Under review as a conference paper at ICLR 2020 class-wise relations and dependencies. In particular, the pruning applied to AlexNet is used to ex- plore and study whether one can obtain a new single class classifier by dropping the selected filters in the original multi-class classifier instead of an expensive training process of a new classifier. The interpretability of the network resulting from pruning can be seen as a filter-wise and signal- wise network dissection: The removal of filters in a controlled manner allows us to determine how different classified objects affects other classes, how the density of the classes in the learned feature space affects the accuracy of a classification and how the structure of the learned multi-class feature space allows to improve the accuracy of a single-class classification. Therefore, the pruning used in this work can also be seen as an approach to CNN understanding ( Bau et al., 2017 ;  Zylberberg, 2017 ;  Kindermans et al., 2017 ;  Morcos et al., 2018 ). The proposed method, referred to as semantic pruning, is performed as follows: we start from a CNN trained to distinguish q classes. Then we reduce the multi-class CNN to the single class classification problem, by removing unrelated to the selected class labels. To remove filters, we measure each filter's contribution (activation magnitude) toward recognition of a target object class. Filters with the highest contributions were preserved, and the remaining filters were dropped from the network. The pruned and unpruned filters are analyzed for correlation, overlap and other measures allowing us to determine class-wise interactions. The contribution of this paper can be summarized as the following: • investigated the sensitivity of object classification to semantic pruning, • study of the interference between class wise filters in CNN, This paper is organized as follows. Section 2 introduces semantic pruning concept. Section 3 describes the experiments and the results and Section 4 concludes the findings.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents two improvements to state-of-the-art image augmentation techniques, Cutout and SamplePairing, for use in neural networks. The proposed methods, Copyout and CopyPairing, are evaluated in several experiments and an analysis of the reasons for their improved performance is provided. The results demonstrate that these methods can lead to better generalization and improved performance with smaller datasets.",
        "Abstract": "Image augmentation is a widely used technique to improve the performance of convolutional neural networks (CNNs). In common image shifting, cropping, flipping, shearing and rotating are used for augmentation. But there are more advanced techniques like Cutout and SamplePairing.\n\nIn this work we present two improvements of the state-of-the-art Cutout and SamplePairing techniques. Our new method called Copyout takes a square patch of another random training image and copies it onto a random location of each image used for training. The second technique we discovered is called CopyPairing. It combines Copyout and SamplePairing for further augmentation and even better performance.\n\nWe apply different experiments with these augmentation techniques on the CIFAR-10 dataset to evaluate and compare them under different configurations. In our experiments we show that Copyout reduces the test error rate by 8.18% compared with Cutout and 4.27% compared with SamplePairing. CopyPairing reduces the test error rate by 11.97% compared with Cutout and 8.21% compared with SamplePairing.\n\nCopyout and CopyPairing implementations are available at https://github.com/anonym/anonym.",
        "Introduction": "  INTRODUCTION Image augmentation is a data augmentation method that generates more training data from the existing training samples. This way the neural network is trained with more different images. This helps to generalize better on new and unknown images and leads to better performance with smaller datasets. Image augmentation is especially useful in domains where training data is limited or expensive to obtain like in biomedical applications. Image augmentation is a regularization technique like Dropout ( Hinton et al., 2012 ) and weight regularization ( Ng, 2004 ). But it is different in one way. It is not part of the network itself which makes it relatively easy and cheap to apply. Images can be augmented on runtime. While the GPU runs the neural network, the CPU can do the augmentation for the next batch in an asynchronous manner. Two state-of-the-art techniques for advanced image augmentation are called Cutout ( Devries & Taylor, 2017 ) and SamplePairing ( Inoue, 2018 ). In our work we show two improvements to these techniques. Our main contributions are: 1. Copyout image augmentation described in section 2 2. CopyPairing image augmentation described in section 3 3. Several experiments to compare them in Section 4.3 ff. 4. An analysis of the reasons why these methods improve performance in Section 5 Under review as a conference paper at ICLR 2020",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the optimization of molecules for pharmaceuticals, OLED materials, and photovoltaics. It examines various algorithms and approaches, such as gradient descent, the simplex method, variational autoencoders, Gaussian processes, neural networks, and recurrent neural networks, to optimize molecules in a discrete and sparse space. It also explores how to map a continuous space to the space of molecules and their properties, and how to use semi-supervised VAEs to specify desired properties directly.",
        "Abstract": "Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. However, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, we encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, harmonizing hidden representations of each atom between SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, our All SMILES VAE learns an almost bijective mapping between molecules and latent representations near the high-probability-mass subspace of the prior. Our SMILES-derived but molecule-based latent representations significantly surpass the state-of-the-art in a variety of fully- and semi-supervised property regression and molecular property optimization tasks.",
        "Introduction": "  INTRODUCTION The design of new pharmaceuticals, OLED materials, and photovoltaics all require optimization within the space of molecules (Pyzer-Knapp et al., 2015). While well-known algorithms ranging from gradient descent to the simplex method facilitate efficient optimization, they generally assume a continuous search space and a smooth objective function. In contrast, the space of molecules is discrete and sparse. Molecules correspond to graphs, with each node labeled by one of ninety-eight naturally occurring atoms, and each edge labeled as a single, double, or triple bond. Even within this discrete space, almost all possible combinations of atoms and bonds do not form chemically stable molecules, and so must be excluded from the optimization domain, yet there remain as many as 10 60 small molecules to consider (Bohacek et al., 1996). Moreover, properties of interest are often sensitive to even small changes to the molecule (Stumpfe & Bajorath, 2012), so their optimization is intrinsically difficult. Efficient, gradient-based optimization can be performed over the space of molecules given a map between a continuous space, such as R n or the n-sphere, and the space of molecules and their properties (Sanchez-Lengeling & Aspuru-Guzik, 2018). Initial approaches of this form trained a variational autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014) on SMILES string representations of molecules (Weininger, 1988) to learn a decoder mapping from a Gaussian prior to the space of SMILES strings (Gómez-Bombarelli et al., 2018). A sparse Gaussian process on molecular properties then facilitates Bayesian optimization of molecular properties within the latent space (Dai et al., 2018; Gómez-Bombarelli et al., 2018; Kusner et al., 2017; Samanta et al., 2018), or a neural network regressor from the latent space to molecular properties can be used to perform gradient descent on molecular properties with respect to the latent space (Aumentado-Armstrong, 2018; Jin et al., 2018; Liu et al., 2018; Mueller et al., 2017). Alternatively, semi-supervised VAEs condition the decoder on the molecular properties (Kang & Cho, 2018; Lim et al., 2018), so the desired properties can be specified directly. Recurrent neural networks have also been trained to model SMILES strings directly, and tuned with transfer learning, without an explicit latent space or encoder (Gupta et al., 2018; Segler et al., 2017).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a method for training deep learning models using stochastic gradient descent (SGD). The training data is split and stored across a number of compute nodes capable of working in parallel. The algorithm involves computing a random approximation of the gradient of the loss function associated with data stored on each node, aggregating the approximations, and broadcasting the aggregated vector back to the nodes. Each node then performs an update of the model parameters.",
        "Abstract": "Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD (Bernstein et al., 2018), have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM. In this paper, we perform a general analysis of sign-based methods for non-convex optimization. Our analysis is built on intuitive bounds on success probabilities and does not rely on special noise distributions nor on the boundedness of the variance of stochastic gradients. Extending the theory to distributed setting within a parameter server framework, we assure exponentially fast variance reduction with respect to number of nodes, maintaining 1-bit compression in both directions and using small mini-batch sizes. We validate our theoretical findings experimentally.",
        "Introduction": "  INTRODUCTION One of the key factors behind the success of modern machine learning models is the availability of large amounts of training data ( Bottou & Le Cun, 2003 ;  Krizhevsky et al., 2012 ;  Schmidhuber, 2015 ). However, the state-of-the-art deep learning models deployed in industry typically rely on datasets too large to fit the memory of a single computer, and hence the training data is typically split and stored across a number of compute nodes capable of working in parallel. Training such models then amounts to solving optimization problems of the form min x∈R d f (x) := 1 M M m=1 f m (x), (1) where f m : R d → R represents the non-convex loss of a deep learning model parameterized by x ∈ R d associated with data stored on node m. Arguably, stochastic gradient descent (SGD) ( Robbins & Monro, 1951 ;  Vaswani et al., 2019 ;  Qian et al., 2019 ) in of its many variants ( Kingma & Ba, 2015 ;  Duchi et al., 2011 ;  Schmidt et al., 2017 ;  Zeiler, 2012 ;  Ghadimi & Lan, 2013 ) is the most popular algorithm for solving (1). In its basic implementation, all workers m ∈ {1, 2, . . . , M } in parallel compute a random approximation g m (x k ) of ∇f m (x k ), known as the stochastic gradient. These approximations are then sent to a master node which performs the aggregation The aggregated vector is subsequently broadcast back to the nodes, each of which performs an update of the form x k+1 = x k − γ kĝ (x k ), thus updating their local copies of the parameters of the model.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes QXplore, a new exploration formulation that seeks novelty in the predicted reward landscape instead of novelty in the state space. QXplore exploits the inherent reward-space signal from the computation of temporal difference error (TD-error) in value-based RL, and explicitly promotes visiting states where the current understanding of reward dynamics is poor. Results demonstrate the utility of QXplore for efficient learning on a variety of complex benchmark environments with continuous controls and sparse rewards.",
        "Abstract": "A major challenge in reinforcement learning is exploration, especially when reward landscapes are sparse. Several recent methods provide an intrinsic motivation to explore by directly encouraging agents to seek novel states. A potential disadvantage of pure state novelty-seeking behavior is that unknown states are treated equally regardless of their potential for future reward. In this paper, we propose an exploration objective using the temporal difference error experienced on extrinsic rewards as a secondary reward signal for exploration in deep reinforcement learning. Our objective yields novelty-seeking in the absence of extrinsic reward, while accelerating exploration of reward-relevant states in sparse (but nonzero) reward landscapes. This objective draws inspiration from dopaminergic pathways in the brain that influence animal behavior. We implement the objective with an adversarial Q-learning method in which Q and Qx are the action-value functions for extrinsic and secondary rewards, respectively. Secondary reward is given by the absolute value of the TD-error of Q. Training is off-policy, based on a replay buffer containing a mix of trajectories sampled using Q and Qx.  We characterize performance on a set of continuous control benchmark tasks, and demonstrate comparable or faster convergence on all tasks when compared with other state-of-the-art exploration methods.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning (RL) has recently achieved impressive results across several challenging domains, such as playing games ( Mnih et al., 2016 ;  Silver et al., 2017 ;  OpenAI, 2018 ;  Baker et al., 2019 ) and controlling robots ( OpenAI et al., 2018 ; Kalashnikov et al., 2018). In many of these tasks, a well-shaped reward function is critical to learning performant policies. On the other hand, deep RL still remains challenging for tasks where the reward function is sparse. In these settings, state-of-the-art RL methods often perform poorly and train very slowly, if at all, due to the low probability of observing improved rewards by following the current optimal policy or with a naive exploration policy such as -greedy sampling. The challenge of learning from sparse rewards is typically framed as a problem of exploration, inspired by the notion that a successful RL agent must efficiently explore the state space of its environment in order to find improved sources of reward. One common exploration paradigm is to directly determine the novelty of states and to encourage the agent to visit states with the highest novelty. In small MDPs this can be achieved through counting how many times each state has been visited. This approach often performs poorly in high-dimensional or continuous state spaces, but recent work ( Tang et al., 2017 ;  Bellemare et al., 2016 ;  Fu et al., 2017 ) using count-like statistics have shown success on benchmark tasks with complex state spaces. Another paradigm for exploration learns a dynamic model of the environment and computes a novelty measure proportional to the error of the model in predicting transitions in the environment. This exploration method relies on the core assumption that well-modeled regions of the state space are similar to previously visited states and thus are less interesting than other regions of state space. Predictions of the transition dynamics can be directly computed ( Pathak et al., 2017 ;  Stadie et al., 2015 ;  Savinov et al., 2019 ;  Burda et al., 2019a ), or related to an information gain objective on the state space, as described in VIME ( Houthooft et al., 2016 ) and EMI ( Kim et al., 2018 ). Several exploration methods have recently been proposed that capitalize on the function approxima- tion properties of neural networks. Random network distillation (RND) trains a function to predict the output of a randomly-initialized neural network from an input state, and uses the approximation Under review as a conference paper at ICLR 2020 error as a reward bonus for a separately-trained RL agent ( Burda et al., 2019b ). Similarly, DORA ( Fox et al., 2018 ) trains a network to predict zero on observed states and deviations from zero are used to indicate unexplored states. An important shortcoming of existing exploration methods is that they only incorporate information about states and therefore assume all unobserved states are equally motivating, regardless of their viability for future reward. The viability of this assumption is highly task dependent: While games like Montezuma's Revenge or Super Mario Bros, where novelty correlates highly with success, can be attacked effectively by state novelty methods alone ( Burda et al., 2019b ;  Pathak et al., 2017 ;  Ecoffet et al., 2019 ;  Kim et al., 2018 ), other tasks such as hide-and-seek or some Atari games where novelty and utility are less correlated tend to frustrate state novelty methods ( Burda et al., 2019b ;  Baker et al., 2019 ;  Burda et al., 2019a ).  Baker et al. (2019)  explored using both RND and a simple state counting baseline to discover skills such as navigation and block-pushing in a hide-and-seek environment. However, the authors found that careful construction of the state representation used for novelty seeking was necessary to discover any such skills, as novelty in the full state space did not correspond to novelty in the intuitive sense ( Baker et al., 2019 ). Instead of focusing on the state-space, this work uses the temporal difference error (TD-error) which provides a signal into novelty in the reward landscape. Past works have also utilized information from the reward landscape as a learning signal to various extents. Schmidhuber et. al. first describe using reward misprediction and model prediction error for exploration ( Schmidhuber, 1991 ;  Thrun & Möller, 1991 ; 1992). However, the work was primarily concerned with model-building and system- identification in small MDPs, and used reward prediction error rather than TD-error.  Later, Gehring & Precup (2013)  used TD-error as a negative signal to constrain exploration to focus on states that are well understood by the value function to avoid common failure modes. Related to maximizing TD-error is maximizing the variance or KL-divergence of a posterior distribution over MDPs or Q-functions, which can be used as a measure of uncertainty ( Osband & Van Roy, 2017 ;  O'Donoghue et al., 2017 ;  Chen et al., 2017 ;  Fox et al., 2018 ;  Osband et al., 2018 ). Posterior uncertainty over Q-functions can be used for information gain in the reward or Q-function space, as opposed to information gain in the state space as described by VIME among others ( Houthooft et al., 2016 ;  Kim et al., 2018 ), though posterior uncertainty methods have thus-far largely been used for local exploration as an alternative to dithering methods such as -greedy sampling, though  Osband et al. (2018)  do apply posterior uncertainty to Montezuma's Revenge. In this paper we propose QXplore, a new exploration formulation that seeks novelty in the predicted reward landscape instead of novelty in the state space. QXplore exploits the inherent reward-space signal from the computation of temporal difference error (TD-error) in value-based RL, and explicitly promotes visiting states where the current understanding of reward dynamics is poor. In the following sections, we describe QXplore and demonstrate its utility for efficient learning on a variety of complex benchmark environments with continuous controls and sparse rewards.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method to automate the process of searching for the right training data distribution for deep neural networks (DNNs). The proposed method uses ensemble Active Learning (AL) to build data subsets of a large labeled training dataset that give more accurate DNNs in less training time. The method is evaluated on three popular image classification benchmarks, studying the impact of key design choices, and the robustness of the selected subsets to changes in model architecture. The results have significant practical implications for DNN training with stochastic gradient methods, as well as for storage of large training datasets.",
        "Abstract": "Deep Neural Networks (DNNs) often rely on very large datasets for training. Given the large size of such datasets, it is conceivable that they contain certain samples that either do not contribute or negatively impact the DNN's optimization. Modifying the training distribution in a way that excludes such samples could provide an effective solution to both improve performance and reduce training time. In this paper, we propose to scale up ensemble Active Learning methods to perform acquisition at a large scale (10k to 500k samples at a time). We do this with ensembles of hundreds of models, obtained at a minimal computational cost by reusing intermediate training checkpoints. This allows us to automatically and efficiently perform a training data distribution search for large labeled datasets. We observe that our approach obtains favorable subsets of training data, which can be used to train more accurate DNNs than training with the entire dataset. We perform an extensive experimental study of this phenomenon on three image classification benchmarks (CIFAR-10, CIFAR-100 and ImageNet), analyzing the impact of initialization schemes, acquisition functions and ensemble configurations. We demonstrate that data subsets identified with a lightweight ResNet-18 ensemble remain effective when used to train deep models like ResNet-101 and DenseNet-121. Our results provide strong empirical evidence that optimizing the training data distribution can provide significant benefits on large scale vision tasks.",
        "Introduction": "  INTRODUCTION Deep Neural Networks (DNNs) have become the dominant approach for addressing supervised learning problems. They are trained using stochastic gradient methods, where (i) data is subsampled into mini-batches, and (ii) the network parameters are iteratively updated by the gradient of the pa- rameter weights, relative to a loss function for the given labeled mini-batch of data. The architecture of the DNN, hyper-parameters of the training process, and distribution of the dataset used are all crucial ingredients that impact the final performance of the DNN. Given the remarkable success of this recipe for supervised learning, there is significant interest in automating the end-to-end process of applying DNNs to real-world problems ( Wong et al., 2018 ;  He et al., 2018 ;  2019 ). While there has been a considerable effort towards methods and frameworks that automate DNN architecture search ( Elsken et al., 2018 ;  Luo et al., 2018 ;  Liu et al., 2018 ;  Xie et al., 2019 ;  Dong & Yang, 2019 ) and training hyper-parameter search ( Golovin et al., 2017 ;  Mutny & Krause, 2018 ;  Kandasamy et al., 2019 ); the process of searching for the right training data dis- tribution (also called dataset curation) is still performed by experts, requiring several heuristics and significant manual effort. With the rapid growth in the availability of labeled data for large-scale vi- sion tasks, to the order of billions of samples ( Sun et al., 2017 ;  Zeki Yalniz et al., 2019 ), automating the training data subset search would make the application of DNNs much easier for non-experts, and potentially lead to datasets and models that outperform those that were curated by hand. With the cost of data storage also becoming increasingly important, the ability to automatically identify data to remove from the training distribution is appealing from a practical perspective. In this paper, we present a simple yet effective method to perform a training data subset search by using ensemble Active Learning (AL). The typical goal of AL is to to select, from a large unlabeled dataset, the smallest possible training set to label in order to solve a specific task ( Cohn et al., 1994 ). We instead propose to use AL to build data subsets of a large labeled training dataset that give more Under review as a conference paper at ICLR 2020 accurate DNNs in less training time. We tackle several key issues that have not been addressed so far by state-of-the-art AL methods. The first is the difficulty in scaling the number of models for ensemble AL. While it seems intuitive that more ensembles can improve performance, existing studies show no gains in AL performance beyond 10 models, and even recommend the use of only 5 models ( Lakshminarayanan et al., 2017 ;  Ovadia et al., 2019 ). Implicit ensemble methods for AL have been found similarly ineffective, but only evaluated in settings with with < 10 models ( Beluch et al., 2018 ). In this study, we propose the use of implicit ensembles with hundreds of training checkpoints from different experimental runs, and empirically demonstrate the effectiveness of this approach. Second, we switch to a large-scale experimental setting compared to what is typically used for AL experiments. For example,  Beluch et al. (2018)  and  Sinha et al. (2019)  only acquire 40k and 64k samples at a time respectively on ImageNet, never use more than 30% of the dataset, and do not compare to the full dataset performance. In contrast, for ResNet-18 training on ImageNet, we acquire up to 500k samples at a time and use 80% of the dataset, improving top-1 accuracy by 0.5% over a model trained with the entire dataset. Third, we demonstrate the transferability of the subsets obtained with our approach between DNNs with very different network capacities, ranging from 10-layer ResNets ( He et al., 2016 ) to 121-layer DenseNets ( Huang et al., 2016 ). Given the rapid pace of model development, this shows that our data subsets remain valuable even after a particular model is surpassed by newer architectures. This has been difficult to achieve in prior AL studies ( Lowell et al., 2018 ), and not investigated in detail for large-scale vision tasks. To summarize, our contributions are as follows: (i) we propose a simple approach to scale up en- semble AL methods to hundreds of models with a negligible computational overhead at train time; (ii) we evaluate several methods to reduce the size of existing large datasets with AL; and (iii) we conduct a detailed empirical study on three popular image classification benchmarks, studying the impact of key design choices, and the robustness of the selected subsets to changes in model archi- tecture. Our study has significant practical implications for DNN training with stochastic gradient methods, as well as for storage of large training datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to automated code synthesis using neural networks, specifically targeting a general-purpose low-level programming language. We propose a discriminator-based system for generating a training corpus, which iteratively creates sub-corpora measuring their functional properties against those of the problems it is attempting to solve. We demonstrate that our approach far exceeds uniform sampling in terms of find-rates, and produces a wider range of programs than a general baseline using genetic programming. Our approach offers an effective way to generate a training corpus for a high-dimensional program search space, capable of finding a wide range of unseen useful programs based only on input/output examples, without any labelled training data.",
        "Abstract": "Current work on neural code synthesis consists of increasingly sophisticated architectures being trained on highly simplified domain-specific languages, using uniform sampling across program space of those languages for training. By comparison, program space for a C-like language is vast, and extremely sparsely populated in terms of `useful' functionalities; this requires a far more intelligent approach to corpus generation for effective training. We use a genetic programming approach using an iteratively retrained discriminator to produce a population suitable as labelled training data for a neural code synthesis architecture. We demonstrate that use of a discriminator-based training corpus generator, trained using only unlabelled problem specifications in classic Programming-by-Example format, greatly improves network performance compared to current uniform sampling techniques.",
        "Introduction": "  INTRODUCTION Automated code synthesis is increasingly being studied as a way to lower the entry bar for non- experts to create computer software, and to aid in generally taming the complexity of large-scale systems by allowing engineers to specify their intentions at a higher level of abstraction. The ap- proach of neural code synthesis in particular has recently gained a lot of attention, applying advances in neural networks to the problem of automated synthesis. We specifically study the approach of programming by example, in which a small set of input-output examples are presented to the system to serve as a guide to the desired functionality of a program. Based on an analysis of these examples the synthesis system returns a source-code program able to replicate that functionality. Recent research in this field demonstrates promising results, including DeepCoder  Balog et al. (2017)  and  Zohar & Wolf (2018) . However, research to date is limited to using domain-specific languages and often linear sequential programs without conditions or loops. We also take a neural-network-based approach to this problem in an attempt to gain inter-program inference across the training examples given to our system, potentially allowing the system to learn general aspects of programming to help synthesize new programs from unseen input/output exam- ples. Unlike existing recent work, however, we target a general-purpose low-level programming language for code synthesis with a much larger search space of possible programs. This presents a major challenge in generating a training corpus for the neural network. Where related research has used uniform sampling methods through program search space ( Sun et al. (2018)Chen et al. (2017) ), or even enumerative approaches( Balog et al. (2017) ), such approaches are wholly inadequate over larger search volumes - with sparse sampling producing very poor inference results. To solve this training corpus generation problem we propose a novel discriminator-based system, in which new sub-corpora are iteratively created, continually measuring their functional properties against those of the problems it is attempting to solve. This process works by learning how similar the I/O mappings of generated programs are to I/O problems requested by users; by selecting pro- grams which result in increasingly similar I/O mappings we simultaneously choose programs with similar underlying source code features, until we are able to solve I/O problems requested by users. We demonstrate that the resultant training corpus is greatly superior to a conventionally generated corpus via uniform sampling, when using a more generalised programming language for synthesis. We measure the performance of our approach by comparing against similar research on neural code synthesis which uses uniform or enumerative sampling for training, demonstrating that our Under review as a conference paper at ICLR 2020 discriminator-informed corpus generation approach far exceeds uniform sampling, by a factor of 2, in terms of find-rates. We also compare against a general baseline using genetic programming (GP); this baseline produces a surprising result that GP has a broader range of programs found, although its probability of resolving any given user-provided problem is worse. Our approach offers an effective way to generate a training corpus for a high-dimensional program search space, capable of finding a wide range of unseen useful programs based only on input/output examples, without any labelled training data. At a high level our research also demonstrates that the structure of the training corpus provided to a neural network greatly affects its performance on general purpose code generation tasks, and we argue that it should therefore represent a core focus of the code synthesis community's efforts alongside work on neural network and language structures. In the remainder of this paper we firstly assess the literature in the field, focusing on neural code synthesis and specifically its corpus generation techniques. In Sec. 3 we then present the method- ology we use to build our system, based on both a synthesis network and a discriminator network for corpus generation. We then evaluate our approach in Sec. 4 by comparing it against traditional training corpus generation approaches for neural code synthesis.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a framework for enabling generalization of discrete action policies to solve tasks using unseen sets of actions in reinforcement learning. It introduces a method to represent an action with a dataset reflecting its diverse characteristics, and employs a generalizable unsupervised learning approach to embed these datasets. Additionally, it proposes a method to use learned action representations in reinforcement learning, and regularization methods to enable learning of generalizable policies.",
        "Abstract": "A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances. In this work, we address one such setting which requires solving a task with a novel set of actions. Empowering machines with this ability requires generalization in the way an agent perceives its available actions along with the way it uses these actions to solve tasks. Hence, we propose a framework to enable generalization over both these aspects: understanding an action’s functionality, and using actions to solve tasks through reinforcement learning. Specifically, an agent interprets an action’s behavior using unsupervised representation learning over a collection of data samples reflecting the diverse properties of that action. We employ a reinforcement learning architecture which works over these action representations, and propose regularization metrics essential for enabling generalization in a policy. We illustrate the generalizability of the representation learning method and policy, to enable zero-shot generalization to previously unseen actions on challenging sequential decision-making environments. Our results and videos can be found at sites.google.com/view/action-generalization/",
        "Introduction": "  INTRODUCTION Imagine visiting your friend for the first time, and you decide to cook your favorite dish there. But since you have never been in their kitchen before, there could be certain tools you have never seen, like an odd-shaped sponge. However, by looking at its porous texture or observing its interaction with water, you can understand that this object can absorb liquid. Later during cooking when you want to clean the table, you can select that sponge since you can relate its absorbing characteristics with another tool you have used for cleaning. Just like in this scenario, our tasks often involve making selections from novel or unseen entities. When we encounter such choices, we examine them to first understand their functionality which informs our selection process while solving a task. Can machines also understand previously unseen choices and subsequently use them for solving tasks? From a reinforcement learning perspective, this brings an interesting question of how to enable generalization of discrete action policies to solve tasks using unseen sets of actions. Prior work in deep reinforcement learning has explored generalization over environments ( Cobbe et al., 2018 ;  Nichol et al., 2018 ), and tasks (Finn et al., 2017;  Parisi et al., 2018 ). However, action space generalization is relatively unexplored and is crucial for agents to be flexible in the face of novel circumstances, like selecting an unseen sponge for a known task of cleaning in above example. In this work, our goal is to develop a framework that reflects the two phases of solving action general- ization: (1) general understanding of unseen discrete actions from their characteristic information (like appearance or behaviors), and (2) training a policy to solve tasks by utilizing this general understanding. However, an action can have diverse behaviors and hence requires a collection of data (e.g. different viewpoints, videos or state trajectories of how it effects on environment) to sufficiently express this diversity. Hence, the primary challenge is to develop a generalizable unsupervised learning method which can extract an action's characteristics from a dataset constituting its diverse effects. To this end, we propose to embed actions' datasets by extending the work on hierarchical variational autoencoders ( Edwards & Storkey, 2017 ). The obtained embeddings reflect an action's general utility, and can be used as action representations in the downstream task of reinforcement learning. However, conventional reinforcement learning Under review as a conference paper at ICLR 2020 Goal Moving Ball Goal Goal Initial State Scenario A Scenario B (a) Chain Reaction Tool Environment (CREATE) Scenario A Scenario B Initial State (b) Shape Stacking algorithms utilize the available actions in a way that best optimizes a reward. This directly incen- tivizes a policy to overfit to the actions seen during training, just like the problem of overfitting to training data in supervised learning. To address this challenge, we formulate this problem as risk minimization ( Vapnik, 1992 ) for reinforcement learning, and propose regularization objectives to enforce generalization of policy to unseen actions. The main contributions of this paper are: (1) introducing the problem and a proposed solution to enable action space generalization in reinforcement learning, (2) representing an action with a dataset reflecting its diverse characteristics, and employing a generalizable unsupervised learning approach to embed these datasets. (3) a method to use learned action representations in reinforcement learning, and regularization methods to enable learning of generalizable policies.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel technique for extending graph representations with metadata embedding dimensions while debiasing the remaining (topology) dimensions. The Metadata-Orthogonal Node Embedding Training (MONET) unit is a GNN algorithm which jointly embeds graph topology and graph metadata while enforcing linear decorrelation between the two embedding spaces. Analysis and experimental results on real world graphs show that MONET can successfully \"debias\" topology embeddings while relegating metadata information to separate metadata embeddings.",
        "Abstract": "Are Graph Neural Networks (GNNs) fair? In many real world graphs, the formation of edges is related to certain node attributes (e.g. gender, community, reputation). In this case, any GNN using these edges will be biased by this information, as it is encoded in the structure of the adjacency matrix itself.  In this paper, we show that when metadata is correlated with the formation of node neighborhoods, unsupervised node embedding dimensions learn this metadata. This bias implies an inability to control for important covariates in real-world applications, such as recommendation systems. \n\nTo solve these issues, we introduce the Metadata-Orthogonal Node Embedding Training (MONET) unit, a general model for debiasing embeddings of nodes in a graph. MONET achieves this by ensuring that the node embeddings are trained on a hyperplane orthogonal to that of the node metadata. This effectively organizes unstructured embedding dimensions into an interpretable topology-only, metadata-only division with no linear interactions. We illustrate the effectiveness of MONET though our experiments on a variety of real world graphs, which shows that our method can learn and remove the effect of arbitrary covariates in tasks such as preventing the leakage of political party affiliation in a blog network, and thwarting the gaming of embedding-based recommendation systems.",
        "Introduction": "  INTRODUCTION Graph embeddings - continuous, low-dimensional vector representations of nodes - have been eminently useful in network visualization, node classification, link prediction, and many other graph learning tasks ( 10 ). While graph embeddings can be estimated directly by unsupervised algorithms using the graph's structure (e.g. 24; 28;  15 ; 25), there is often additional (non-relational) information available for each node in the graph. This information, frequently referred to as node attributes or node metadata, can contain information that is useful for prediction tasks including demographic, geo-spatial, and/or textual features. The interplay between a node's metadata and edges is a rich and active area of research. Interestingly, in a number of cases, this metadata can be measurably related to a graph's structure (21), and in some instances there may be a causal relationship (the node's attributes influence the formation of edges). As such, metadata can enhance graph learning models (31; 20), and conversely, graphs can be used as regularizers in supervised and semi-supervised models of node features (32;  11 ). Furthermore, metadata are commonly used as evaluation data for graph embeddings ( 8 ). For example, node embeddings trained on a Flickr user graph were shown to predict user-specified Flickr \"interests\" (24). This is presumably because users (as nodes) in the Flickr graph tend to follow users with similar interests, which illustrates a potential causal connection between node topology and node metadata. However, despite the usefulness and prevalence of metadata in graph learning, there are instances where it desirable to design a system to avoid the effects of a particular kind of sensitive data. For instance, the designers of a recommendation system may want to make recommendations independent of a user's demographic information or location. At first glance, this may seem like an artificial dilemma - surely one could just avoid the problem by not adding such sensitive attributes to the model. However, such an approach (ignoring a sensitive Under review as a conference paper at ICLR 2020 attribute) does not control for any existing correlations that may exist between the sensitive metadata and the edges of a node. In other words, if the edges of the graph are correlated with sensitive metadata, then any algorithm which does not explicitly model and remove this correlation will be biased as a result of it. Surprisingly, almost all of the existing work in the area (31; 35) has ignored this important realization.  1  In this work, we seek to refocus the discussion about graph learning with node metadata. To this end, we propose a novel, general technique for extending graph representations with metadata embedding dimensions while debiasing the remaining (topology) dimensions. Specifically, our contributions are the following: 1. The Metadata-Orthogonal Node Embedding Training (MONET) unit, a novel GNN al- gorithm which jointly embeds graph topology and graph metadata while enforcing linear decorrelation between the two embedding spaces. 2. Analysis which proves that a naive approach (adding metadata embeddings without MONET) leaks metadata information into topology embeddings, and that the MONET unit does not. 3. Experimental results on real world graphs which show that MONET can successfully \"debias\" topology embeddings while relegating metadata information to separate metadata embeddings.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method for adapting a classifier trained on a source domain to a target domain with a different label proportion. The method assumes that the joint distribution of the source and target domains is shifted due to a shift in the label proportion, and that the conditional probability of the labels given the observations is the same in both domains. The proposed method is evaluated on a real-world dataset of medical diagnoses and shows improved performance over existing methods.",
        "Abstract": "Label shift refers to the phenomenon where the marginal probability p(y) of observing a particular class changes between the training and test distributions, while the conditional probability p(x|y) stays fixed. This is relevant in settings such as medical diagnosis, where a classifier trained to predict disease based on observed symptoms may need to be adapted to a different distribution where the baseline frequency of the disease is higher. Given estimates of p(y|x) from a predictive model, one can apply domain adaptation procedures including Expectation Maximization (EM) and Black-Box Shift Estimation (BBSE) to efficiently correct for the difference in class proportions between the training and test distributions. Unfortunately, modern neural networks typically fail to produce well-calibrated estimates of p(y|x), reducing the effectiveness of these approaches. In recent years, Temperature Scaling has emerged as an efficient approach to combat miscalibration. However, the effectiveness of Temperature Scaling in the context of adaptation to label shift has not been explored. In this work, we study the impact of various calibration approaches on shift estimates produced by EM or BBSE. In experiments with image classification and diabetic retinopathy detection, we find that calibration consistently tends to improve shift estimation. In particular, calibration approaches that include class-specific bias parameters are significantly better than approaches that lack class-specific bias parameters, suggesting that reducing systematic bias in the calibrated probabilities is especially important for domain adaptation.",
        "Introduction": "  INTRODUCTION Imagine we train a classifier in country A to predict whether or not a person has a disease based on observed symptoms, and that we hope to deploy this classifier in country B, which has poorer access to healthcare. If the prevalence of the disease in country B is higher in than in country A, the classifier might systematically misdiagnose people as not having the disease. How can we adapt the classifier to cope with the difference in the baseline prevalence of the disease in the two countries? Formally, let y denote our labels (e.g. whether or not a person is diseased), and let x denote the observed symptoms. Let us denote the joint distribution (x, y) in country A (our \"source\" domain) as P, and let us denote the distribution in country B (our \"target\" domain, where we do not have labels) as Q. How can we adapt a classifier trained to estimate p(y|x) (the conditional probability in distribution P) so that it can instead estimate q(y|x) (the conditional probability in distribution Q)? Absent assumptions about the nature of the shift between P and Q, this problem is intractable. However, if the disease generates similar symptoms in both countries, we can assume that p(x|y) = q(x|y), and that the shift in the joint distribution q(x, y) is due to a shift in the label proportion q(y). Formally, we assume that q(x, y) = p(x|y)q(y). This is known as label shift or prior probability shift ( Amos, 2008 ), and it corresponds to anti-causal learning (i.e. predicting the cause y from its effects x) ( Schoelkopf et al., 2012 ). Anti-causal learning is appropriate for diagnosing diseases given observations of symptoms because diseases cause symptoms.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a supervised learning setting to learn the preconditioner of the Dual space Preconditioned Gradient Descent (DPGD) algorithm of Maddison et al. (2019). We present a general methodology that allows to effectively learn the preconditioner while avoiding issues related to this task. We implement this methodology in dimension one and 50 and show that this can lead to dramatic speed-ups.",
        "Abstract": "Preconditioning an minimization algorithm improve its convergence and can lead to a minimizer in one iteration in some extreme cases. There is currently no analytical way for finding a suitable preconditioner. We present a general methodology for learning the preconditioner and show that it can lead to dramatic speed-ups over standard optimization techniques.\n   ",
        "Introduction": "  INTRODUCTION Many problems arising in applied mathematics can be formulated as the minimization of a convex function f : The resolution of an optimization problem is usually tackled using an optimization algorithm that produces a sequence of iterates converging to some minimizer of f ( Nesterov (2018) ). The gradient descent algorithm is a standard optimization algorithm that converges linearly (i.e exponentially fast) if f is regular enough. Preconditioned methods ( Nemirovsky & Yudin (1983) ;  Lu et al. (2016) ;  Maddison et al. (2019) ) are powerful optimization algorithms that converges linearly under weaker assumptions that gradient descent. The performance of these methods relies heavily on the pre- conditioning - the task of choosing the hyperparameter called the preconditioner. In the case of the Dual space Preconditioned Gradient Descent (DPGD) of  Maddison et al. (2019) , an optimal preconditioning can lead to convergence in one iteration. The preconditioner is a function that has to be selected properly w.r.t. f to obtain the desired linear convergence. Although  Maddison et al. (2019)  gives some hints to precondition DPGD, there is currently no analytical way for finding a suitable preconditioner. In this paper, we propose to learn the preconditioner of DPGD using a neural network. We make the following contribution: • We propose a supervised learning setting to learn the preconditioner of an optimization algorithm ( DPGD, Maddison et al. (2019) ) • We present a general methodology that allows to effectively learn the preconditioner while avoiding issues related to this task • We implement this methodology in dimension one and 50 and show that this can lead to dramatic speed-ups. The remainder is organized as follows. The next section provides background knowledge on the DPGD algorithm. Then, in section 3 we present our supervised learning setting. Finally, we apply this methodology in section 4. Additional developments are provided in the appendix as well as postponed proofs.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a learning-based approach to instruction-following that is robust to human commands. An agent policy is trained to respond to synthetic, template-based language as well as natural language text. The agent is tested in a 3D room environment with 26 fine-grained motor actions to identify and manipulate visually-realistic models from the ShapeNet dataset. Results show that agents trained with powerful pretrained language encoders can satisfy human instructions with substantially above-chance accuracy. The paper also analyzes the generalization that supports robustness to human instructions, finding that contextual word representations are essential for capturing both lexical and phrasal equivalence.",
        "Abstract": "Recent work has described neural-network-based agents that are trained to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the instructions that such agents are trained to follow are typically generated from templates (by an environment simulator), and do not reflect the varied or ambiguous expressions used by real people. We address this issue by integrating language encoders that are pretrained on large text corpora into a situated, instruction-following agent. In a procedurally-randomized first-person 3D world, we first train agents to follow synthetic instructions requiring the identification, manipulation and relative positioning of visually-realistic objects models. We then show how these abilities can transfer to a context where humans provide instructions in natural language, but only when agents are endowed with language encoding components that were pretrained on text-data. We explore techniques for integrating text-trained and environment-trained components into an agent, observing clear advantages for the fully-contextual phrase representations computed by the well-known BERT model, and additional gains by integrating a self-attention operation optimized to adapt BERT's representations for the agent's tasks and environment. These  results bridge the gap between two successful strands of recent AI research: agent-centric behavior optimization and text-based representation learning. ",
        "Introduction": "  INTRODUCTION Developing machines that can follow natural human commands, particularly those pertaining to an environment shared by both machine and human, is a long-standing and elusive goal of AI ( Wino- grad, 1972 ). Recent work has applied end-to-end, representation-learning-based methods to this challenge, where a neural-network-based agent is optimized to process language input, perceive its surroundings and execute appropriate movements jointly ( Oh et al., 2017 ; Hermann et al., 2017;  Chaplot et al., 2018 ). End-to-end learning promises a way to deal flexibly with the complexity of the physical, visual and linguistic world without relying on (potentially brittle) hand-crafted fea- tures, rules or policies. Nevertheless, the cost of this flexibility is the large number of environment interactions (samples) required for a randomly-initialized network to learn behaviour policies from raw experience. To make the approach feasible, many studies thus employ a synthetic language that is generated on demand from templates by the environment simulator ( Chevalier-Boisvert et al., 2018 ;  Jiang et al., 2019 ;  Yu et al., 2018b ;a). The studies that do combine both end-to-end learning with natural-language data do so in less realistic grid-like environments ( Misra et al., 2017 ) or grant agents access to privileged global observations to make learning more tractable ( Misra et al., 2018 ). Here, we take a different learning-based approach to instruction-following that is robust to human commands. We train agent policies to respond to synthetic, template-based language but also endow them with powerful language encoders that are pretrained on natural language text. The synthetic instructions that our agents are trained to follow require mastery of 26 fine-grained motor actions in order to identify and manipulate visually-realistic models from the ShapeNet dataset ( Chang et al., 2015 ) in a 3D room. The visual realism of the world makes it possible to elicit diverse natural human ways of instructing and/or referring to things, and to study the agents' robustness to this diversity. Unsurprisingly, we find that agents that are trained on template-based commands do not cope well with the diversity and variation of natural keyboard-typed language. In contrast, when powerful Under review as a conference paper at ICLR 2020 pretrained language encoders are integrated into our agent, we find that agents can satisfy human instructions with substantially above-chance accuracy. To better analyze the generalization that supports this robustness, we probe trained agents with specific modifications to their training instructions. We find that methods based on conventional (context-free) word embeddings support generalization that is driven by lexical similarity (executing lift a vehicle when trained to lift a car), but capture phrasal equivalence less well (failing at put a plate on the container when trained to put the dish on the tray). In con- trast, methods that integrate contextual word representations support both types of generalization. We also find that robustness to synonyms can often be improved when pretrained encoders are com- bined with an additional (cross-modal) self-attention tuned to the agent's environmental objectives. Ablation experiments highlight the role that WordPiece tokenization ( Schuster & Nakajima, 2012 ) plays in robustness to human instructions. This motivates the addition of typo noise to our training pipeline, which further improves the accuracy of the agent's responses. Overall, our principal contributions are the following: • We train an agent that can both interpret human language commands and overcome a similar range of behavioural and environmental challenges to state-of-the-art policy-learning approaches ( Table 1 ). Under review as a conference paper at ICLR 2020 • We develop the techniques of transfer-learning from a text representation-learning model to an embodied agent. Note that this is very different from transfer between language classification tasks ( Collobert & Weston, 2008 ;  Devlin et al., 2018 ); in our evaluations the agent must interpret unfamiliar instructions zero-shot (i.e. without additional learning steps), and must realize complex language-conditional behaviours (decoding ≈ 50 appropriate actions) in spite of this uncertainty.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel Adversarial Predictive Normalize Maximum Likelihood (pNML) scheme as a defense and detector against adversarial attacks on deep neural networks (DNNs). The pNML scheme is based on a min-max game where the goal is to be as close as possible to a reference learner, who knows the true label but is restricted to use a learner from a given hypothesis class. The proposed method uses a simple targeted adversarial attack as a method of refinement to create a hypothesis class and use it to predict the test label. Results demonstrate robustness to state-of-the-art adversarial attacks on MNIST and CIFAR10 datasets, and show that by combining adversarial sample detection, the method is able to overcome a defense-aware adaptive attack.",
        "Abstract": "Adversarial attacks were shown to be very effective in degrading the performance of neural networks. By slightly modifying the input, an almost identical input is misclassified by the network. To address this problem, we adopt the universal learning framework. In particular, we follow the recently suggested Predictive Normalized Maximum Likelihood (pNML) scheme for universal learning, whose goal is to optimally compete with a reference learner that knows the true label of the test sample but is restricted to use a learner from a given hypothesis class. In our case, the reference learner is using his knowledge on the true test label to perform minor refinements to the adversarial input. This reference learner achieves perfect results on any adversarial input. The proposed strategy is designed to be as close as possible to the reference learner in the worst-case scenario.  Specifically, the defense essentially refines the test data according to the different hypotheses, where each hypothesis assumes a different label for the sample. Then by comparing the resulting hypotheses probabilities, we predict the label and detect whether the sample is adversarial or natural. Combining our method with adversarial training we create a robust scheme which can handle adversarial input along with detection of the attack. The resulting scheme is demonstrated empirically.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have shown to have state-of-the-art performance in many machine learning tasks ( Goodfellow et al., 2016a ). Despite the impressive performance, it has been found that DNNs are susceptible to adversarial attacks ( Szegedy et al., 2013 ;  Biggio et al., 2013 ). These attacks cause the network to under-perform by adding specially crafted noise to the input, such that the original and modified inputs are almost indistinguishable. In the case of an image classification task, adversarial attacks can be characterized according to three properties ( Carlini et al., 2019 ): the adversary goal, the adversary capabilities, and the adversary knowledge. (i) The adversary goal may be to simply cause misclassification. This is referred to as an untargeted attack. Alternatively, the goal can be to have the model misclassify certain examples from a source class into a target class of its choice. This is referred to as a targeted attack. (ii) The adversary capabilities relate to the strength of the perturbation the adversary is allowed to cause the data, i.e., the distance between the original sample and the adversarial sample under a certain distance metric must be smaller than . (iii) The adversary knowledge represents what knowledge the adversary has about the model. This can range from a fully white-box scenario, where the adversary has complete knowledge about the model and its parameters, to a black-box scenario where the adversary does not know the model and only has a limited number of queries on it. Out of the many different adversarial attack algorithms, gradient optimization-based attacks are known to be the most powerful kind of attacks ( Carlini et al., 2019 ). In these attacks, the adversary uses the gradients of a loss function with respect to the input in order to find the perturbation that minimizes the performance of the network. Such attacks include, among others, Fast Gradient Sign Method (FGSM) ( Goodfellow et al., 2014 ), Projected Gradient Descent (PGD) ( Madry et al., 2017 ) and CW-attack ( Carlini & Wagner, 2017 ). Adversarial defenses can be separated into two categories according to their goal: to increase robust- ness against adversarial examples ( Madry et al., 2017 ) and to detect whether an example is natural Under review as a conference paper at ICLR 2020 or adversarial ( Li & Li, 2017 ;  Feinman et al., 2017 ). While recent years have shown growing inter- est in understanding and defending against adversarial examples, no solution has been found yet to white-box settings, where the adversary has full access to the network. The universal prediction framework considers both the stochastic setting in which the true relation between the features and labels is given by a stochastic function, and the individual setting in which no probabilistic connection between the data and the labels is assumed. The Predictive Normalize Maximum Likelihood (pNML) as the universal learning solution is proposed for the batch learning in the individual settings (Fogel & Feder, 2018a;  Bibas et al., 2019a ; b ). The pNML scheme gives the optimal solution for a min-max game where the goal is to be as close as possible to a reference learner, who knows the true label but is restricted to use a learner from a given hypothesis class. Further elaboration on the pNML learner is presented in section 3. We propose the Adversarial pNML scheme as a new adversarial defense and detector. Adversarial pNML improves DNNs robustness against adversarial attacks and allows the detection of adversarial samples. Based on the pNML, we restrict the genie learner, a learner who knows the true test label, to perform minor refinements to the adversarial examples. Our method essentially uses an existing model trained with adversarial training. Then, we compose a hypothesis class. Each member in the class assumes a different label for the input. The member refines the input sample based on the assumed label. Finally, by comparing the resulting hypotheses probabilities, we can predict the true label along with detection that the sample is adversarial. In this paper, we propose a novel scheme that uses a simple targeted adversarial attack as a method of refinement. By performing that simple refinement we create a hypothesis class and use it to pre- dict the test label. We demonstrate our method robustness to state-of-the-art adversarial attacks as PGD ( Madry et al., 2017 ) for MNIST and CIFAR10 datasets. We show that by combining adver- sarial sample detection, which is an inherent property of our method, we manage to overcome a defense-aware adaptive attack. Unlike existing methods that attempt to remove the adversary per- turbation ( Samangouei et al., 2018 ;  Song et al., 2017 ;  Guo et al., 2017 ), our method is unique in the sense it does not remove the perturbation but rather exploits the adversarial subspace properties.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Time2Vec, a learnable vector representation for time that can be used in any machine learning architecture. Experiments on several (synthesized and real-world) datasets show that using Time2Vec instead of the time itself offers a boost in performance. This model-agnostic representation for time obviates the need for hand-crafting features and can be easily combined with many models or architectures.",
        "Abstract": "Time is an important feature in many applications involving events that occur synchronously and/or asynchronously. To effectively consume time information, recent studies have focused on designing new architectures. In this paper, we take an orthogonal but complementary approach by providing a model-agnostic vector representation for time, called Time2Vec, that can be easily imported into many existing and future architectures and improve their performances. We show on a range of models and problems that replacing the notion of time with its Time2Vec representation improves the performance of the final model.",
        "Introduction": "  INTRODUCTION In building machine learning models, \"time\" is often an important feature. Examples include predicting daily sales for a company based on the date (and other available features), predicting the time for a patient's next health event based on their medical history, and predicting the song a person is interested in listening to based on their listening history. The input for problems involving time can be considered as a sequence where, rather than being identically and independently distributed (iid), there exists a dependence across time (and/or space) among the data points. The sequence can be either synchronous, i.e. sampled at regular intervals, or asynchronous, i.e. sampled at different points in time. In both cases, time may be an important feature. For predicting daily sales, for instance, it may be useful to know if it is a holiday or not. For predicting the time for a patient's next encounter, it is important to know the (asynchronous) times of their previous visits. Recurrent neural networks (RNNs) do not typically treat time itself as a feature, typically assuming that inputs are synchronous. When time is known to be a relevant feature, it is often fed in as yet another input dimension ( Choi et al., 2016 ;  Du et al., 2016 ;  Li et al., 2018b ). In practice, RNNs often fail at effectively making use of time as a feature. To help the RNN make better use of time, several researchers design hand-crafted features of time that suit their specific problem and feed those features into the RNN ( Choi et al., 2016 ;  Baytas et al., 2017 ;  Kwon et al., 2019 ). Hand-crafting features, however, can be expensive and requires domain expertise about the problem. Many recent studies aim at obviating the need for hand-crafting features by proposing general- purpose-as opposed to problem specific-architectures that better handle time ( Neil et al., 2016 ;  Zhu et al., 2017 ;  Mei & Eisner, 2017 ;  Hu & Qi, 2017 ;  Upadhyay et al., 2018 ;  Li et al., 2018a ). We follow an orthogonal but complementary approach to these recent studies by developing a general- purpose model-agnostic representation for time that can be potentially used in any architecture. In particular, we develop a learnable vector representation (or embedding) for time as a vector representation can be easily combined with many models or architectures. We call this vector representation Time2Vec. To validate the effectiveness of Time2Vec, we conduct experiments on several (synthesized and real-world) datasets and integrate it with several architectures. Our main result is to show that on a range of problems and architectures that consume time, using Time2Vec instead of the time itself offers a boost in performance.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a novel technique, Nodal Optimization for Recurrent Meta-Learning (NORML), which uses a recurrent neural network based meta-learner to learn how to make parameter updates for a task learner. NORML is evaluated on the Mini-ImageNet dataset and is shown to improve on existing optimization-based and memory-based methods. An ablation study is done showing the effects of the different components of NORML and a comparison is done between NORML and Model Agnostic Meta-Learning (MAML) using the Omniglot dataset. The comparison demonstrates that NORML makes superior parameter updates than the updates made by gradient descent.",
        "Abstract": "Meta-learning is an exciting and powerful paradigm that aims to improve the effectiveness of current learning systems. By formulating the learning process as an optimization problem, a model can learn how to learn while requiring significantly less data or experience than traditional approaches. Gradient-based meta-learning methods aims to do just that, however recent work have shown that the effectiveness of these approaches are primarily due to feature reuse and very little has to do with priming the system for rapid learning (learning to make effective weight updates on unseen data distributions). This work introduces Nodal Optimization for Recurrent Meta-Learning (NORML), a novel meta-learning framework where an LSTM-based meta-learner performs neuron-wise optimization on a learner for efficient task learning. Crucially, the number of meta-learner parameters needed in NORML, increases linearly relative to the number of learner parameters. Allowing NORML to potentially scale to learner networks with very large numbers of parameters. While NORML also benefits from feature reuse it is shown experimentally that the meta-learner LSTM learns to make effective weight updates using information from previous data-points and update steps.",
        "Introduction": "  INTRODUCTION Humans have a remarkable capability to learn useful concepts from a small number of examples or a limited amount of experience. In contrast most machine learning methods require large, labelled datasets to learn effectively. Little is understood about the actual learning algorithm(s) used by the human brain, and how it relates to machine learning algorithms like backpropagation ( Lillicrap & Körding (2019) ).  Botvinick et al. (2019)  argues that inductive bias and structured priors are some of the main factors that enable fast learning in animals. In order to build general-purpose systems we must be able to design and build learning algorithms that can quickly and effectively learn from a limited amount of data by utilizing prior knowledge and experience. Supervised few-shot learning aims to challenge machine learning models to learn new tasks by lever- aging only a handful of labelled examples.  Vinyals et al. (2016)  introduces the few-shot learning problem for image classification, where a model is tasked to classify a number of images while being provided either 1 or 5 examples of each class (hereafter referred to as 1-shot and 5-shot learning). One way to approach this problem is by way of meta-learning, a broad family of techniques that aim to learn how to learn ( Thrun & Pratt (1998) ). One particularly powerful group of approaches, known as memory-based methods, use memory architectures that can leverage prior information to assist in learning ( Santoro et al. (2016) ;  Ravi & Larochelle (2017) ). Optimization-based methods ( Finn et al. (2017) ) is another exciting area that aim to learn an initial set of parameters that can quickly adapt to new, unseen tasks with relatively little training data. This work introduces a novel technique where a recurrent neural network based meta-learner learns how to make parameter updates for a task learner. The meta-learner and the learner are jointly trained so as to learn how to learn new tasks with little data. This approach allows one to utilize aspects from both optimization-based and memory-based meta-learning methods. The recurrent architecture of the meta-learner can use important prior information when updating the learner, while the learner can learn a set of initial task parameters are easily optimized for a new task. The vanishing gradient challenge faced by gradient-based optimization is solved by using a Long short- term memory (LSTM) based meta-learner ( Hochreiter & Schmidhuber (1997) ). Memory-based Under review as a conference paper at ICLR 2020 methods ( Ravi & Larochelle (2017) ) that use all of the learner's parameters as input to a meta- learner tend to break down when using a learner with a large number of parameters ( Andrychowicz et al., 2016 ). The technique proposed in this work, Nodal Optimization for Recurrent Meta-Learning (NORML), solves this scaling problem by learning layer-wise update signals used to update the learners's parameters. NORML is evaluated on the Mini-ImageNet dataset and is shown to improve on existing optimization-based and memory-based methods. An ablation study is done showing the effects of the different components of NORML. Furthermore a comparison is done between NORML and Model Agnostic Meta-Learning (MAML) using the Omniglot dataset. The comparison demonstrates that NORML makes superior parameter updates than the updates made by gradient descent.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents an approach to reinforcement learning that leverages the complementary properties of model-free reinforcement learning and model-based optimal control. The proposed method views model predictive control (MPC) as a way to simultaneously approximate and optimize a local Q function via simulation, and Q learning as a way to improve MPC using real-world data. The approach is tested on simulated continuous control tasks and compared against information theoretic MPC and soft Q-Learning, demonstrating faster learning with fewer system interactions and better performance. The paper also compares the approach to domain randomization on sim-to-sim tasks, concluding that learning from data generated on the true system is able to overcome biases and adapt to the real dynamics.",
        "Abstract": "Model-free Reinforcement Learning (RL) algorithms work well in sequential decision-making problems when experience can be collected cheaply and model-based RL is effective when system dynamics can be modeled accurately. However, both of these assumptions can be violated in real world problems such as robotics, where querying the system can be prohibitively expensive and real-world dynamics can be difficult to model accurately. Although sim-to-real approaches such as domain randomization attempt to mitigate the effects of biased simulation, they can still suffer from optimization challenges such as local minima and hand-designed distributions for randomization, making it difficult to learn an accurate global value function or policy that directly transfers to the real world. In contrast to RL, Model Predictive Control (MPC) algorithms use a simulator to optimize a simple policy class online, constructing a closed-loop controller that can effectively contend with real-world dynamics. MPC performance is usually limited by factors such as model bias and the limited horizon of optimization. In this work, we present a novel theoretical connection between information theoretic MPC and entropy regularized RL and develop a Q-learning algorithm that can leverage biased models. We validate the proposed algorithm on sim-to-sim control tasks to demonstrate the improvements over optimal control and reinforcement learning from scratch. Our approach paves the way for deploying reinforcement learning algorithms on real-robots in a systematic manner.",
        "Introduction": "  INTRODUCTION Deep reinforcement learning algorithms have recently generated great interest due to their successful application to a range of difficult problems including Computer Go ( Silver et al., 2016 ) and high- dimensional control tasks such as humanoid locomotion ( Lillicrap et al., 2015 ;  Schulman et al., 2015 ). While these methods are extremely general and can learn policies and value functions for complex tasks directly from data, they can also be sample inefficient, and partially-optimized so- lutions can be arbitrarily poor. These challenges severely restrict RL's applicability to real systems such as robots due to data collection challenges and safety concerns. One straightforward way to mitigate these issues is to learn a policy or value function entirely in a high-fidelity simulator ( Shah et al., 2017 ;  Todorov et al., 2012 ) and then deploy the optimized policy on the real system. However, this approach can fail due to model bias, external disturbances, the subtle differences between the real robot's hardware and poorly modeled phenomena such as friction and contact dynamics. Sim-to-real transfer approaches based on domain randomization ( Sadeghi & Levine, 2016 ;  Tobin et al., 2017 ) and model ensembles ( Kurutach et al., 2018 ;  Shyam et al., 2019 ) aim to make the policy robust by training it to be invariant to varying dynamics. However, learning a globally consistent value function or policy is hard due to optimization issues such as local optima and covariate shift between the exploration policy used for learning the model and the actual control policy executed on the task ( Ross & Bagnell, 2012 ). Model predictive control (MPC) is a widely used method for generating feedback controllers that repeatedly re-optimizes a finite horizon sequence of controls using an approximate dynamics model that predicts the effect of these controls on the system. The first control in the optimized sequence is executed on the real system and the optimization is performed again from the resulting next state. However, the performance of MPC can suffer due to approximate or simplified models and lim- Under review as a conference paper at ICLR 2020 ited lookahead. Therefore the parameters of MPC, including the model and horizon H should be carefully tuned to obtain good performance. While using a longer horizon is generally preferred, real-time requirements may limit the amount of lookahead and a biased model can result in com- pounding model errors. In this work, we present an approach to RL that leverages the complementary properties of model- free reinforcement learning and model-based optimal control. Our proposed method views MPC as a way to simultaneously approximate and optimize a local Q function via simulation, and Q learning as a way to improve MPC using real-world data. We focus on the paradigm of entropy regularized reinforcement learning where the aim is to learn a stochastic policy that minimizes the cost-to-go as well as KL divergence with respect to a prior policy. This approach enables faster convergence by mitigating the over-commitment issue in the early stages of Q-learning and better exploration ( Fox et al., 2015 ). We discuss how this formulation of reinforcement learning has deep connections to information theoretic stochastic optimal control where the objective is to find control inputs that minimize the cost while staying close to the passive dynamics of the system ( Theodorou & Todorov, 2012 ). This helps in both injecting domain knowledge into the controller as well as mitigating issues caused by over optimizing the biased estimate of the current cost due to model error and the limited horizon of optimization. We explore this connection in depth and derive an infinite horizon information theoretic model predictive control algorithm based on  Williams et al. (2017) . We test our approach called Model Predictive Q Learning (MPQ) on simulated continuous control tasks and compare it against information theoretic MPC and soft Q-Learning ( Haarnoja et al., 2017 ), where we demonstrate faster learning with fewer system interactions and better performance as compared to MPC and soft Q-Learning even in the presence of sparse rewards. The learned Q function allows us to truncate the MPC planning horizon which provides additional computational benefits. Finally, we also compare MPQ versus domain randomization(DR) on sim-to-sim tasks. We conclude that DR approaches can be sensitive to the hand-designed distributions for randomizing parameters which causes the learned Q function to be biased and suboptimal on the true system's parameters, whereas learning from data generated on true system is able to overcome biases and adapt to the real dynamics.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the fault tolerance of artificial neural networks (NNs) and their applications in safety-critical tasks. It explores the potential of neuromorphic hardware (NH) to improve the computational power of machine learning systems, and the need for robustness to weight faults in AI safety. The paper draws on systems biology to study the effect of deleting components of biological networks, and applies this to the fault tolerance of NNs. The results of this research can be directly applied to NH and biological networks.",
        "Abstract": "The loss of a few neurons in a brain rarely results in any visible loss of function. However, the insight into what “few” means in this context is unclear. How many random neuron failures will it take to lead to a visible loss of function? In this paper, we address the fundamental question of the impact of the crash of a random subset of neurons on the overall computation of a neural network and the error in the output it produces. We study fault tolerance of neural networks subject to small random neuron/weight crash failures in a probabilistic setting. We give provable guarantees on the robustness of the network to these crashes. Our main contribution is a bound on the error in the output of a network under small random Bernoulli crashes proved by using a Taylor expansion in the continuous limit, where close-by neurons at a layer are similar. The failure mode we adopt in our model is characteristic of neuromorphic hardware, a promising technology to speed up artificial neural networks, as well as of biological networks. We show that our theoretical bounds can be used to compare the fault tolerance of different architectures and to design a regularizer improving the fault tolerance of a given architecture. We design an algorithm achieving fault tolerance using a reasonable number of neurons. In addition to the theoretical proof, we also provide experimental validation of our results and suggest a connection to the generalization capacity problem.",
        "Introduction": "  INTRODUCTION Understanding the inner working of artificial neural networks (NNs) is currently one of the most pressing questions ( 20 ) in learning theory. As of now, neural networks are the backbone of the most successful machine learning solutions (37;  18 ). They are deployed in safety-critical tasks in which there is little room for mistakes ( 10 ; 40). Nevertheless, such issues are regularly reported since attention was brought to the NNs vulnerabilities over the past few years (37;  5 ;  24 ;  8 ). Fault tolerance as a part of theoretical NNs research. Understanding complex systems requires understanding how they can tolerate failures of their components. This has been a particularly fruitful method in systems biology, where the mapping of the full network of metabolite molecules is a computationally quixotic venture. Instead of fully mapping the network, biologists improved their understanding of biological networks by studying the effect of deleting some of their components, one or a few perturbations at a time ( 7 ;  12 ). Biological systems in general are found to be fault tolerant ( 28 ), which is thus an important criterion for biological plausibility of mathematical models. Neuromorphic hardware (NH). Current Machine Learning systems are bottlenecked by the under- lying computational power ( 1 ). One significant improvement over the now prevailing CPU/GPUs is neuromorphic hardware. In this paradigm of computation, each neuron is a physical entity ( 9 ), and the forward pass is done (theoretically) at the speed of light. However, components of such hardware are small and unreliable, leading to small random perturbations of the weights of the model (41). Thus, robustness to weight faults is an overlooked concrete Artificial Intelligence (AI) safety problem ( 2 ). Since we ground the assumptions of our model in the properties of NH and of biological networks, our fundamental theoretical results can be directly applied in these computing paradigms.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to adversarial training which is robust against multiple types of p norm bounded attacks. We propose a modified PGD-based algorithm called multi steepest descent (MSD) which more naturally incorporates the different perturbations within the PGD iterates, and show empirically that our approach improves upon past work by being applicable to standard network architectures, easily scaling beyond the MNIST dataset, and outperforming past results on robustness against multiple perturbation types.",
        "Abstract": "Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers, but the vast majority has defended against single types of attacks. Recent work has looked at defending against multiple attacks, specifically on the MNIST dataset, yet this approach used a relatively complex architecture, claiming that standard adversarial training can not apply because it \"overfits\" to a particular norm. In this work, we show that it is indeed possible to adversarially train a robust model against a union of norm-bounded attacks, by using a natural generalization of the standard PGD-based procedure for adversarial training to multiple threat models. With this approach, we are able to train standard architectures which are robust against l_inf, l_2, and l_1 attacks, outperforming past approaches on the MNIST dataset and providing the first CIFAR10 network trained to be simultaneously robust against (l_inf, l_2, l_1) threat models, which achieves adversarial accuracy rates of (47.6%, 64.3%, 53.4%) for (l_inf, l_2, l_1) perturbations with epsilon radius = (0.03,0.5,12).",
        "Introduction": "  INTRODUCTION Machine learning algorithms have been shown to be susceptible to adversarial examples ( Szegedy et al., 2014 ) through the existence of data points which can be adversarially perturbed to be mis- classified, but are \"close enough\" to the original example to be imperceptible to the human eye. Methods to generate adversarial examples, or \"attacks\", typically rely on gradient information, and most commonly use variations of projected gradient descent (PGD) to maximize the loss within a small perturbation region, usually referred to as the adversary's threat model. Since then, a number of heuristic defenses have been proposed to defend against this phenomenon, e.g. distillation ( Papernot et al., 2016 ) or more recently logit-pairing ( Kannan et al., 2018 ). However, as time goes by, the original robustness claims of these defenses typically don't hold up to more advanced adversaries or more thorough attacks ( Carlini & Wagner, 2017 ;  Engstrom et al., 2018 ;  Mosbach et al., 2018 ). One heuristic defense that seems to have survived (to this day) is to use adversarial training against a PGD adversary ( Madry et al., 2018 ), which remains quite popular due to its simplicity and apparent empirical robustness. The method continues to perform well in empirical benchmarks even when compared to recent work in provable defenses, although it comes with no formal guarantees. Some recent work, however, pointed out that adversarial training against ∞ perturbations \"overfits\" to the ∞ threat model, and used this as motivation to propose a more complicated architecture in order to achieve robustness to multiple perturbation types on the MNIST dataset ( Schott et al., 2019 ). In this work, we offer a alternative viewpoint: while adversarial training can overfit to the individual threat models, we show that it is indeed possible to use adversarial training to learn a model which is simultaneously robust against multiple types of p norm bounded attacks (we consider ∞ , 2 , and 1 attacks, but the approach can apply to more general attacks). First, we show while simple generalizations of adversarial training to multiple threat models can achieve some degree of robustness against the union of these threat models, the performance is inconsistent and converges to suboptimal tradeoffs which may not actually minimize the robust objective. Second, we propose a slightly modified PGD-based algorithm called multi steepest descent (MSD) for adversarial training which more naturally incorporates the different perturbations within the PGD iterates, further improving the adversarial training approach by directly minimizing the robust optimization objective. Third, we show empirically that our approach improves upon past work by being applicable to standard Under review as a conference paper at ICLR 2020 network architectures, easily scaling beyond the MNIST dataset, and outperforming past results on robustness against multiple perturbation types.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes the Adaptive Thermostat Monte Carlo (ATMC) sampler, a stochastic gradient MCMC method for Bayesian inference in deep learning. ATMC dynamically adjusts the amount of momentum and noise applied to each model parameter, and is shown to outperform optimization methods in terms of accuracy, log-likelihood and uncertainty calibration. The paper also introduces the ResNet++ architecture, which is designed to be compatible with ATMC and to reduce the need for hyper-parameter tuning. Experiments on Cifar-10 and ImageNet demonstrate the effectiveness of ATMC and ResNet++.",
        "Abstract": "Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness.\nMarkov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters.\nDespite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind  optimization methods for large scale deep learning tasks.\nWe aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network.\nATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients.\nWe use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the  absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline.",
        "Introduction": "  Introduction In contrast to optimization approaches in machine learning that derive a single estimate for the weights of a neural network, Bayesian inference aims at deriving a posterior distribution over the weights of the network. This makes it possible to sample model instances from the distribution over the weights and offers unique advantages. Multiple model instances can be aggregated to obtain robust uncertainty estimates over the network's predictions; uncertainty estimates are crucial in domains such as medical diagnosis and autonomous driving where following a model's incorrect predictions can result in catastrophe ( Kendall & Gal, 2017 ). Sampling a distribution, as opposed to optimizing a loss, is less prone to overfitting and more training doesn't decrease test performance. Bayesian inference can also be applied to differential privacy, where each individual sample has increased privacy guarantees ( Wang et al., 2015 ), and to reinforcement learning, where one can leverage model uncertainty to balance between exploration and exploitation ( Osband & Van Roy, 2017 ). Traditional Markov Chain Monte Carlo (MCMC) methods like HMC ( Neal et al., 2011 ) are a standard class of methods for generating samples from the posterior distribution over model parameters. These methods are seldom applied in deep learning because they have traditionally failed to scale well with large datasets and many parameters ( Rajaratnam & Sparks, 2015 ). Stochastic Gradient MCMC (SG-MCMC) methods have fared somewhat better in scaling to large datasets due to their close relationship to stochastic optimization methods. For example the SGLD sampler ( Welling & Teh, 2011 ) amounts to performing stochastic gradient descent while adding Gaussian noise to each parameter update. Despite these improvements, samplers like SGLD are only guaranteed to converge to the correct Under review as a conference paper at ICLR 2020 Algorithm 1 The ATMC sampler. The algorithm accepts the initialized model parameters θ 0 , step size h, pre-conditioner m, and momentum noise D c . distribution when the step size is annealed to zero; additional control variates have been developed to mitigate this to some extent (Ahn et al., 2012;  Ding et al., 2014 ). The objective of this work is to make Bayesian inference practical for deep learning by mak- ing SG-MCMC methods scale to large models and datasets. The contributions described in this work fall in three categories. We first propose the Adaptive Thermostat Monte Carlo (ATMC) sampler that offers improved convergence and stability. ATMC dynamically adjusts the amount of momentum and noise applied to each model parameter. Secondly, we improve an existing second order numerical integration method that is needed for the ATMC sampler. Third, since ATMC, like other SG-MCMC samplers, is not directly compat- ible with stochastic regularization methods such as batch normalization (BatchNorm) and Dropout (see Sect. 4), we construct the ResNet++ network by taking the original ResNet architecture ( He et al., 2016 ), removing BatchNorm and introducing SELUs ( Klambauer et al., 2017 ), Fixup initialization ( Zhang et al., 2019a ) and weight normalization ( Salimans & Kingma, 2016 ). We design ResNet++ so that its parameters are easy to sample from and the gradients are well-behaved even in the absence of BatchNorm. We show that the ATMC sampler is able to outperform optimization methods in terms of accuracy, log-likelihood and uncertainty calibration in the following settings. First, when using the ResNet++ architecture for both the ATMC sampler and the optimization baseline, the ATMC sampler significantly outperforms the optimization baseline on both Cifar-10 and ImageNet. Secondly, when using the standard ResNet for the optimization baseline and the ResNet++ for the ATMC sampler, multiple samples of the ATMC that approximate the predictive posterior of the model are still able to outperform the optimization baseline on ImageNet. Using the ResNet++ architecture, the ATMC sampler reduces the need for hyper-parameter tuning since it does not require early stopping, does not use stochastic regularization, is not prone to over-fitting on the training data and avoids a carefully tuned learning rate decay schedule.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper explores the task of multi-modal image-to-image (I2I) translation, which involves transforming images from one domain to another. We propose a pretraining approach for style encoders, which simplifies and speeds up the training compared to competing approaches. We provide a study of the importance of different losses and regularization terms for multi-modal I2I translation networks, and show that the pretrained latent embeddings is not dependent on the target domain and generalizes well to other domains. We achieve state-of-the art results on several benchmarks in terms of style capture and transfer, and diversity of results.",
        "Abstract": "Image-to-image (I2I) translation aims to translate images from one domain to another. To tackle the multi-modal version of I2I translation, where input and output domains have a one-to-many relation, an extra latent input is provided to the generator to specify a particular output. Recent works propose involved training objectives to learn a latent embedding, jointly with the generator, that models the distribution of possible outputs. Alternatively, we study a simple, yet powerful pre-training strategy for multi-modal I2I translation. We first pre-train an encoder, using a proxy task, to encode the style of an image, such as color and texture, into a low-dimensional latent style vector. Then we train a generator to transform an input image along with a style-code to the output domain. Our generator achieves state-of-the-art results on several benchmarks with a training objective that includes just a GAN loss and a reconstruction loss, which simplifies and speeds up the training significantly compared to competing approaches. We further study the contribution of different loss terms to learning the task of multi-modal I2I translation, and finally we show that the learned style embedding is not dependent on the target domain and generalizes well to other domains.",
        "Introduction": "  INTRODUCTION Image-to-Image (I2I) translation is the task of transforming images from one domain to another (e.g., semantic maps → scenes, sketches → photo-realistic images, etc.). Many problems in computer vision and graphics can be cast as I2I translation, such as photo-realistic image synthesis ( Chen & Koltun (2017) ;  Isola et al. (2017) ;  Wang et al. (2018a) ), super-resolution ( Ledig et al. (2017) ), colorization ( Zhang et al. (2016 ; 2017a)), and inpainting ( Pathak et al. (2016) ). Therefore, I2I translation has recently received significant attention in the literature. One main challenge in I2I translation is the multi-modal nature for many such tasks - the relation between an input domain A and an output domain B is often times one-to-many, where a single input image I A i ∈ A can be mapped to different output images from domain B. For example, a sketch of a shoe or a handbag can be mapped to corresponding objects with different colors or styles, or a semantic map of a scene can be mapped to many scenes with different appearance, lighting and/or weather conditions. Since I2I translation networks typically learn one-to-one mappings due to their deterministic nature, an extra input is required to specify an output mode to which an input image will be translated. Simply injecting extra random noise as input proved to be ineffective as shown in ( Isola et al. (2017) ;  Zhu et al. (2017b) ), where the generator network just learns to ignore the extra noise and collapses to a single or few modes (which is one form of the mode collapse problem). To overcome this problem,  Zhu et al. (2017b)  proposed BicycleGAN, which learns to encode the distribution of different possible outputs into a latent vector z, and then learns a deterministic mapping G : (A, z) → B. So, depending on the latent vector z, a single input I A i ∈ A can be mapped to multiple outputs in B. While BicycleGAN requires paired training data, several works ( Lee et al. (2018) ;  Huang et al. (2018) ) extended it to the unsupervised case, where images in domains A and B are not in correspondence ('unpaired'). One main component of unpaired I2I is a cross-cycle consistency constraint, where the network generates an intermediate output by swapping the styles of a pair of images, then swaps the style between the intermediate output again to reconstruct the original images. This enforces that the latent vector z preserves the encoded style information when translated from an image i to another image j and back to image i again. This constraint can also be applied to paired training data, where it encourages style/attribute transfer between images. However, training BicycleGAN ( Zhu et al. (2017b) ) or its unsupervised counterparts ( Huang et al. (2018) ;  Lee et al. (2018) ) is not trivial. For example, Under review as a conference paper at ICLR 2020 BicycleGAN combines the objectives of both conditional Variational Auto-Encoders (cVAEs) ( Sohn et al. (2015) ) and a conditional version of Latent Regressor GANs (cLR-GANs) ( Donahue et al. (2016) ;  Dumoulin et al. (2016) ) to train their network. The training objective of ( Huang et al. (2018) ;  Lee et al. (2018) ) is even more involved to handle the unsupervised setup. In this work, we aim to simplify the training of general purpose multi-modal I2I translation networks, while also improving the diversity and expressiveness of different styles in the output domain. Our approach is inspired by the work of  Meshry et al. (2019)  which utilizes a staged training strategy to re-render scenes under different lighting, time of day, and weather conditions. We propose a pretraining approach for style encoders, in multi-modal I2I translation networks, which makes the training simpler and faster by requiring fewer losses/constraints. Our approach is also inspired by the standard training paradigm in visual recognition of first pretraining on a proxy task, either large supervised datasets (e.g., ImageNet) ( Krizhevsky et al. (2012) ;  Sun et al. (2017) ;  Mahajan et al. (2018) ) or unsupervised tasks (e.g.,  Doersch et al. (2015) ;  Noroozi & Favaro (2016) ), and then fine-tuning (transfer learning) on the desired task. Similarly, we propose to pretrain the encoder using a proxy task that encourages capturing style into a latent space. Our goal is to highlight the importance of pretraining for I2I networks and demonstrate that a simple approach can be very effective for multi-modal image synthesis. In particular, we make the following contributions: • We explore style pretraining and its generalization for the task of multi-modal I2I translation, which simplifies and speeds up the training compared to competing approaches. • We provide a study of the importance of different losses and regularization terms for multi-modal I2I translation networks. • We show that the pretrained latent embeddings is not dependent on the target domain and generalizes well to other domains (transfer learning). • We achieve state-of-the art results on several benchmarks in terms of style capture and transfer, and diversity of results.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel method for training highly flexible posterior approximations for deep neural networks. The method starts with a coarse, mean-field approximation of the posterior and makes iterative, local refinements to it by sampling the values of additive auxiliary variables. The refinement iterations are repeated for each posterior sample, resulting in samples from a highly complex distribution. We show that the procedure is guaranteed to improve the resulting evidence lower bound (ELBO) without posing a significant computational overhead.",
        "Abstract": "Variational inference (VI) is a popular approach for approximate Bayesian inference that is particularly promising for highly parameterized models such as deep neural networks.  A key challenge of variational inference is to approximate the posterior over model parameters with a distribution that is simpler and tractable yet sufficiently expressive. In this work, we propose a method for training highly flexible variational distributions by starting with a coarse approximation and iteratively refining it. Each refinement step makes cheap, local adjustments and only requires optimization of simple variational families. We demonstrate theoretically that our method always improves a bound on the approximation (the Evidence Lower BOund) and observe this empirically across a variety of benchmark tasks.  In experiments, our method consistently outperforms recent variational inference methods for deep learning in terms of log-likelihood and the ELBO.  We see that the gains are further amplified on larger scale models, significantly outperforming standard VI and deep ensembles on residual networks on CIFAR10.",
        "Introduction": "  INTRODUCTION Uncertainty plays a crucial role in a multitude of machine learning applications, ranging from weather prediction to drug discovery. Poor predictive uncertainty risks potentially poor outcomes, especially in domains such as medical diagnosis or autonomous vehicles where some forms of high confidence errors may be especially costly (Amodei et al., 2016). Thus, it is becoming increasingly important that the underlying model provides high quality uncertainty estimates along with its pre- dictions. Yet, possibly the most widely used models, deep neural networks ( LeCun et al., 2015 ), are unable to accurately quantify model uncertainty. They are often overconfident in their predictions, even when their predictions are incorrect ( Guo et al., 2017 ;  Ovadia et al., 2019 ). By marginalizing over a posterior distribution over the parameters given the training data, Bayesian inference provides a principled approach to capturing uncertainty. In contrast, standard training of neural networks employs a point estimate of the parameters, which cannot account for model un- certainty. Unfortunately, exact Bayesian inference is intractable in general for neural networks. To model epistemic uncertainty, variational inference (VI) instead approximates the true posterior with a simpler distribution. The most widely used one for neural networks is the mean-field approxi- mation, where the posterior is represented using an independent Gaussian distribution over all the weights. Variational inference is appealing since it reduces the problem of inference to an optimiza- tion problem, minimizing the discrepancy between the true posterior and the variational posterior. The key challenge, however, is the task of training expressive posterior approximations that can capture the true posterior without significantly increasing the computational costs. This paper describes a novel method for training highly flexible posterior approximations. The idea is to start with a coarse, mean-field approximation q(w) and make iterative, local refinements to it. The regions of the local refinements are determined by sampling the values of additive auxiliary variables. The model parameters w are expressed using a number of auxiliary variables a k ( Figure 1  left) for k = 1, . . . , K that leave the marginal distribution unchanged. In each iteration, we sample the value of an auxiliary variable according to the current variational approximation q(a k ) and refine the approximation by conditioning on the newly sampled value q(w) ≈ p(w|x, y, a 1:k ) ( Figure 1  right illustrates the process). Each refinement step makes cheap, local adjustments to the variational posterior in the region of the sampled auxiliary variables. At the end, we draw one sample from Under review as a conference paper at ICLR 2020 the refined q(w). The refinement iterations have to be repeated for each posterior sample. The algorithm results in samples from a highly complex distribution, starting from a simple mean-field approximation. While the distribution of the samples is difficult to quantify, we show that it is not limited to factorized, uni-modal forms, and that the procedure is guaranteed to improve the resulting ELBO without posing a significant computational overhead.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a strategy for building classifiers that are certifiably robust against label-flipping attacks. The proposed defense provides pointwise certifications guaranteeing that the classifier's prediction would not be different had it been trained on data with some number of labels flipped. This work represents the first pointwise certified defense to data poisoning attacks and provides a defense against a worst-case adversary that can make a training set perturbation to specifically target each test point individually.",
        "Abstract": "This paper considers label-flipping attacks, a type of data poisoning attack where an adversary relabels a small number of examples in a training set in order to degrade the performance of the resulting classifier. In this work, we propose a strategy to build classifiers that are certifiably robust against a strong variant of label-flipping, where the adversary can target each test example independently. In other words, for each test point, our classifier makes a prediction and includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee test-time robustness to adversarial manipulation of the input to a classifier. Further, we obtain these certified bounds with no additional runtime cost over standard classification. On the Dogfish binary classification task from ImageNet, in the face of an adversary who is allowed to flip 10 labels to individually target each test point, the baseline undefended classifier achieves no more than 29.3% accuracy; we obtain a classifier that maintains 64.2% certified accuracy against the same adversary.",
        "Introduction": "  INTRODUCTION Modern classifiers, despite their widespread empirical success, are known to be susceptible to adver- sarial attacks. In this paper, we are specifically concerned with so-called \"data-poisoning\" attacks (formally, causative attacks [ Barreno et al. 2006 ;  Papernot et al. 2018 ]), where the attacker manip- ulates some aspects of the training data in order to cause the learning algorithm to output a faulty classifier. Work in this area includes label-flipping attacks ( Xiao et al., 2012 ), where the labels of a training set can be adversarially manipulated to decrease performance of the trained classifier; general data poisoning, where both the training inputs and labels can be manipulated ( Steinhardt et al., 2017 ); and backdoor attacks ( Chen et al., 2017 ;  Tran et al., 2018 ), where the training set is corrupted so as to cause the classifier to deviate from its expected behavior when triggered by a spe- cific pattern, without causing a noticeable drop in overall accuracy. However, unlike the alternative \"test-time\" adversarial setting, where reasonably effective defenses exist to build adversarially ro- bust classifiers, relatively little work has been done on building classifiers that are certifiably robust to targeted data poisoning attacks. In this work, we propose a strategy for building classifiers that are certifiably robust against label- flipping attacks. In particular, we propose a pointwise certified defense-by this we mean that with each prediction, the classifier includes a certification guaranteeing that its prediction would not be different had it been trained on data with some number of labels flipped. Existing works on certified defenses make statistical guarantees over the entire test set, but they make no guarantees as to the robustness of a prediction on any particular test point. Thus, while these algorithms provide general robustness to any single corruption, a determined adversary could still cause a specific test point to be misclassified. We therefore consider the threat of a worst-case adversary that can make a training set perturbation to specifically target each test point individually. This motivates a defense that can certify each of its individual predictions, as we present here. To the best of our knowledge, this work represents the first pointwise certified defense to data poisoning attacks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a system that combines an image encoder, language encoder, relational network, and image decoder to generate a target image of a scene after an action has been performed. The system is trained in a GAN setting that conditions on both a source image and the action text description. The system is implemented in PyTorch and trained on a synthetic dataset and evaluated on synthetic and real-world images. The results show that the proposed system is able to generate realistic images that accurately depict the effect of the action on the scene.",
        "Abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.",
        "Introduction": "  INTRODUCTION For many human-robot interaction scenarios, a long-term vision has been to develop systems where humans can simply instruct robots using natural language commands. This aim encompasses several sub-challenges in robotics and artificial intelligence, including natural language understanding, symbol grounding, task and motion planning, to name only a few. However, a first step for solving these challenging tasks is to study how to best combine the data representations from different domains, which is still a topic of research. This paper studies whether a perceptual system can via simulation train a computational model to predict the output of actions. The aim is to generate an output image that shows the effect of a certain action on objects when the instruction to change an object is given in natural language.  Figure 1  shows a simplified visualization of the main idea where the input image contains several objects of different shape, size, and color, and the input instruction is to manipulate one of the objects in some way (move, remove, add, replace). The output of the model is a synthetic generated image that shows the effect that the action had on the scene. To successfully solve the task of depicting the effect of a certain action, the model must further address a number of different sub-challenges, including; 1) image encoding, 2) language learning, 3) relational learning, and 4) image generation. The key requirement for implementing these human cognitive processes in computational modeling is the data representation for each of the different required domains and how to combine and use their shared representations. There are several works in the literature that combines some of the aforementioned sub-challenges for solving problems such as image captioning  You et al. (2016) , image editing  Chen et al. (2018) , image generation from text descriptions  Reed et al. (2016a) , visual question answering  Santoro et al. (2017) ;  Yang et al. (2016) , 3D reconstructed object conditioned on a 2D image  Girdhar et al. (2016) ;  Weber et al. (2018) , paired robot action and linguistic translation  Yamada et al. (2018) , and Vision-and-Language Navigation (VLN)  Anderson et al. (2018) . However, the challenge of how to combine all the four sub-challenges and learn their shared representations still requires more research. In this work, we propose a system that combines an image encoder, language encoder, relational network, and image decoder and train it in a GAN setting that conditions on both a source image and the action text description to generate a target image of the scene after the action has been performed. The system is implemented in PyTorch and trained on a synthetic data-set and evaluated on synthetic and real-world images. The dataset and source code can be downloaded from [link to appear].",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper studies the large dimensional behavior of the Gram matrix G = {φ(x i ) φ(x j )} n i,j=1 for a given data representation model x → φ(x). Specifically, the paper aims to quantify the information encoded by the representation model on a set of real data x 1 , . . . , x n by studying the spectral information (i.e., eigenvalues and dominant eigenvectors) of the Gram matrix. The paper relies on the notion of concentrated vectors, which are generated by applying successive Lipschitz operations to Gaussian random vectors, and shows that the spectral behavior of the Gram matrix computed on these representations is the same as on a Gaussian Mixture Model (GMM) with the same p-dimensional means and covariances. The paper presents experimental results on GAN-data and real images from the Imagenet dataset to demonstrate that the sufficient statistics to characterize the quality of a given representation network are only the first and second order statistics of the representations.",
        "Abstract": "This paper shows that deep learning (DL) representations of data produced by generative adversarial nets (GANs) are random vectors which fall within the class of so-called concentrated random vectors. Further exploiting the fact that Gram matrices, of the type G = X'X with X = [x_1 , . . . , x_n ] ∈ R p×n and x_i independent concentrated random vectors from a mixture model, behave asymptotically (as n, p → ∞) as if the x_i were drawn from a Gaussian mixture, suggests that DL representations of GAN-data can be fully described by their first two statistical moments for a wide range of standard classifiers. Our theoretical findings are validated by generating images with the BigGAN model and across different popular deep representation networks.",
        "Introduction": "  INTRODUCTION The performance of machine learning methods depends strongly on the choice of the data repre- sentation (or features) on which they are applied. This data representation should ideally contain relevant information about the learning task in order to achieve learning with simple models and small amount of samples. Deep neural networks ( Rumelhart et al., 1988 ) have particularly shown impressive results by automatically learning representations from raw data (e.g., images). However, due to the complex structure of deep learning models, the characterization of their hidden represen- tations is still an open problem ( Bengio et al., 2009 ). Specifically, quantifying what makes a given deep learning representation better than another is a fundamental question in the field of Representation Learning ( Bengio et al., 2013 ). Relying on ( Montavon et al., 2011 ) a data representation is said to be good when it is possible to build simple models on top of it that are accurate for the given learning problem.  Montavon et al. (2011)  have notably quantified the layer-wise evolution of the representation in deep networks by comput- ing the principal components of the Gram matrix G = {φ (x i ) φ (x j )} n i,j=1 at each layer for n input data x 1 , . . . , x n , where φ (x) is the representation of x at layer of the given DL model, and the number of components controls the model simplicity. In their study, the impact of the repre- sentation at each layer is quantified through the prediction error of a linear predictor trained on the principal subspace of G . Pursuing on this idea, given a certain representation model x → φ(x), we aim in this arti- cle at theoretically studying the large dimensional behavior, and in particular the spectral infor- mation (i.e., eigenvalues and dominant eigenvectors), of the corresponding Gram matrix G = {φ(x i ) φ(x j )} n i,j=1 in order to determine the information encoded (i.e., the sufficient statistics) by the representation model on a set of real data x 1 , . . . , x n . Indeed, standard classification and regression algorithms -along with the last layer of a neural network ( Yeh et al., 2018 )- retrieve the data information directly from functionals or the eigenspectrum of G 1 . To this end, though, one needs a statistical model for the representations given the distribution of the raw data (e.g., images) which is generally unknown. Yet, due to recent advances in generative models since the advent of Generative Adversarial Nets ( Goodfellow et al., 2014 ), it is now possible to generate complex data Under review as a conference paper at ICLR 2020 structures by applying successive Lipschitz operations to Gaussian random vectors. In particular, GAN-data are used in practice as substitutes of real data for data augmentation ( Antoniou et al., 2017 ). On the other hand, the fundamental concentration of measure phenomenon ( Ledoux, 2005 ) tells us that Lipschitz-ally transformed Gaussian vectors satisfy a concentration property. Precisely, defining the class of concentrated vectors x ∈ E through concentration inequalities of f (x), for any real Lipschitz observation f : E → R, implies that deep learning representations of GAN-data fall within this class of random vectors, since the mapping x → φ(x) is Lipschitz. Thus, GAN-data are concentrated random vectors and thus an appropriate statistical model of realistic data. Targeting classification applications by assuming a mixture of concentrated random vectors model, this article studies the spectral behavior of Gram matrices G in the large n, p regime. Precisely, we show that these matrices have asymptotically (as n, p → ∞ with p/n → c < ∞) the same first- order behavior as for a Gaussian Mixture Model (GMM). As a result, by generating images using the BigGAN model ( Brock et al., 2018 ) and considering different commonly used deep representation models, we show that the spectral behavior of the Gram matrix computed on these representations is the same as on a GMM model with the same p-dimensional means and covariances. A surprising consequence is that, for GAN data, the aforementioned sufficient statistics to characterize the quality of a given representation network are only the first and second order statistics of the representations. This behavior is shown by simulations to extend beyond random GAN-data to real images from the Imagenet dataset ( Deng et al., 2009 ). The rest of the paper is organized as follows. In Section 2, we introduce the notion of concentrated vectors and their main properties. Our main theoretical results are then provided in Section 3. In Section 4 we present experimental results. Section 5 concludes the article.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a solution to the out-of-distribution detection problem, which is a measure of uncertainty in neural network classifications. The simplest approach is to use the softmax output of a neural net as a confidence measure, and setting a threshold for the softmax output can to some extent distinguish between \"in distribution\" and \"out-of-distribution\" samples. This baseline is improved by using an ensemble of classifiers (and adversarial training). Other, more computationally demanding approaches are Bayesian Neural Networks, nearest neighbor methods, and other related works.",
        "Abstract": "A simple method for obtaining uncertainty estimates for Neural Network classifiers (e.g. for out-of-distribution detection) is to use an ensemble of independently trained networks and average the softmax outputs. While this method works, its results are still very far from human performance on standard data sets. We investigate how this method works and observe three fundamental limitations: \"Unreasonable\" extrapolation, \"unreasonable\" agreement between the networks in an ensemble, and the filtering out of features that distinguish the training distribution from some out-of-distribution inputs, but do not contribute to the classification. To mitigate these problems we suggest \"large\" initializations in the first layers and changing the activation function to sin(x) in the last hidden layer. We show that this combines the out-of-distribution behavior from nearest neighbor methods with the generalization capabilities of neural networks, and achieves greatly improved out-of-\ndistribution detection on standard data sets (MNIST/fashionMNIST/notMNIST, SVHN/CIFAR10).",
        "Introduction": "  INTRODUCTION When a neural network classifies inputs, we often need to have some measure of the uncertainty involved in the prediction. In particular, in safety critical applications we would like to know when the classifier received an input that is different from the inputs it has been trained on - in such cases it could be dangerous to just use the \"best guess\", instead we may want to choose some safe action or involve a human. One solution for this \"out-of-distribution detection\" problem could be to add a label \"unknown\" and augment the training set with inputs that do not belong to any of the labels of the classifier and which get the new label \"unknown\". However, the danger is that the classifier learns the particular type of \"known unknowns\" it was trained on and does not generalize to the \"unknown unknowns\". To avoid this, we treat out-of-distribution detection as a \"one class classification\" problem, i.e. we want to learn the input distribution without help of out of distribution inputs. The simplest approach is to use the softmax output of a neural net as a confidence measure. It is well known that the softmax output tends to be \"overconfident\", so we cannot interpret it directly as a probability for the chosen class. However, it still tends to be more confident for samples from the correct distribution than for outliers, so setting a threshold for the softmax output can to some extent distinguish between \"in distribution\" and \"out-of-distribution\" samples. This is the \"baseline method\" for outlier detection suggested in (Hendrycks & Gimpel, 2017). In ( Lakshminarayanan et al., 2017 ) this baseline is improved by using an ensemble of classifiers (and adversarial training): Often each random initialization of a neural network gives an overconfi- dent classifier, but different classifiers disagree - averaging the softmax output over an ensemble of classifiers then gives an improved signal. Other, more computationally demanding approaches are Bayesian Neural Networks (( Neal, 1996 ), ( Barber & Bishop, 1998 ), ( Blundell et al., 2015 )), nearest neighbor methods (e.g. ( Mandelbaum & Weinshall, 2017 ), ( Jiang et al., 2018 ), ( Papernot & McDaniel, 2018 ), ( Frosst et al., 2019 )). For some further (less closely) related works, see appendix A.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an experimental technique called Partial Backpropagation, which freezes weights that have gradients very near to zero and are less likely to change, with the rest of the weights trained normally. This technique is applied to a variety of vision and language applications, and results in a slight drop in accuracy with no harm in accuracy for lesser freezing. The paper also visualizes the distribution of gradients from several layers in a VGG19 convolutional network to better understand their distributions.",
        "Abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.",
        "Introduction": "  INTRODUCTION The immense success of deep neural networks we are witnessing since the deep learning revolution occurred is surprising. A large variety of vision and language applications ranging from image classification, object detection, image synthesis, image super-resolution, image captioning, language modeling....etc. has proved that neural networks possess a powerful capability of learning very complex data. However, training these networks to perform as expected is very time-consuming and requires powerful graphical processing units (GPUs). A recently published open-source project by NVIDIA 1 claimed that training a generative adversarial network (GAN) took more than 6 days on 8 Tesla V100 GPUs. However, we argue that a lot of parameters involved during training are important for update only for the first few epochs (in our experiments, the first two epochs only), and can be frozen for the rest of the training epochs. The backpropagation algorithm is the base algorithm used to optimize deep neural networks. For each weight, a gradient is computed with respect to the loss which indicates the amount a weight should change. Large gradients correspond to a large change that will occur in the weight, while small ones (near to zero) indicate that the weight is nearly optimized and does not need much change. In particular, if a gradient for a particular weight is zero or close to zero, this means that it has either reached its optimal solution, or it is stuck at a saddle point. The former means that the weight has a good value and is less likely to change throughout the training and can be kept frozen. In this paper, we wish to show the redundancy of weights in a neural network that have no influence and can be kept frozen during training. In particular, we demonstrate that fully training a model with all its weights is required for the first two epochs only. To justify this, we propose an experimental technique named Partial Backpropagation, which freezes weights that have gradients very near to zero and are less likely to change, with the rest of the weights trained normally. This induces a very slight drop in accuracy (and no harm in accuracy for lesser freezing). An overview of Under review as a conference paper at ICLR 2020 our experimental technque is shown in  Figure 1 . Note that in Figure 1(b), the red weights are frozen and not removed or zeroed out. We can further visualize the histogram of gradients across the network layers to have a better understanding of their distributions. In  Figure 2 , we visualize the distribution of gradients from several layers in a VGG19 convolutional network ( Simonyan & Zisserman, 2015 ). In particular, we visualize the gradients of layers 3, 7, 10 and 13 after training for 2 epochs. We can see a large number of gradients with values very near to zero, suggesting that a lot of weights in these layers have already been optimized and are less likely to change throughout the training.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents theoretical and empirical evidence that competent learning can be achieved in evolutionary time without backpropagation. We prove that, if the classifier to be learned is linear, then evolution can create a neural network that classifies well, almost certainly and within polynomial number of generations, individuals per generation, and neurons per individual. We also validate our theorem through experiments on the MNIST data set and propose a second biologically plausible ANN-like mechanism, based on dopaminergic plasticity. Our results suggest that effective brain circuits specializing in classification tasks could have evolved.",
        "Abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.",
        "Introduction": "  INTRODUCTION In his Turing award lecture, neural networks pioneer Geoff Hinton opined that \"evolution can't get gradients because a lot of what determines the relationship between the genotype and the phenotype is outside your control\" ( Hinton, 2019 ). We beg to differ. Evolution does have what amounts to an effective oracle access to the (indeed, complex and intractable) mapping from genotype to pheno- type. The well-established equations of population genetics governing evolution under recombina- tion ( Bürger, 2000 ;  Chastain et al., 2014 ) describe the way whereby the distribution of genotypes in the population is updated from one generation to the next, informed by the empirical fitness of the phenotypes during lifetime; and these equations do bear a similarity to gradient descent and, even closer, to no-regret learning ( Chastain et al., 2014 ). In this paper, we show that, in fact, quite effective training of neural nets can be carried out, without backpropagation, in evolutionary time. The towering empirical success of ANNs has brought into focus their profound incongruity with what we know about the brain: backpropagation requires that and synaptic weights and plasticity be informed by downstream events. Clever versions of ANNs have been proposed recently that avoid this criticism: ANNs whose backward weights are random and fixed ( Lillicrap et al., 2016 ) and a variant that also uses random feedback weights but with zero initial conditions ( Nø kland, 2016 ), a backpropagation interpretation of STDP (a widely accepted theory of plasticity) ( Bengio et al., 2015 ), unsupervised learning using STDP ( Diehl & Cook, 2015 ), ANNs driven by neural competition ( Krotov & Hopfield, 2019 ), or ANNs with target value propagation at each layer rather than the loss gradient ( Lee et al., 2015 ). Here we take a very different approach. We believe that, while forward neural computation is coterminous with life, backpropagation (i.e., feedback to individual neurons and synapses about their contribution to the overall performance of the circuit) can be effectively carried out over evolutionary Under review as a conference paper at ICLR 2020 time. Suppose that the brain circuitry for a particular classification task, such as \"food/not food\", is encoded in the animal's genes, assuming each gene to have two alleles 0 and 1. A (haploid) genotype is a bit string. Crucially, we assume that the weight of each link of the neural network is a fixed sparse linear function of the genes. Evolution proceeds in generations. At each generation, a gene is an independent binary variable with fixed probability of 1. A population is sampled from this distribution of genotypes, and it experiences a sequence of inputs to the brain circuit. Fitness of each genotype depends, to some small degree, on the animal's success over its lifetime in the specific classification task. In the next generation, the allele frequencies will change slightly, depending on how each allele of each gene fared cumulatively (over both all inputs and all genotypes containing it) in the classification task. These changes follow the standard population genetics equations for the weak selection regime, see  Bürger (2000) ;  Chastain et al. (2014) ; weak selection means that the classification task is only one of the many biological functions (digestion, locomotion, other brain circuits, etc.) that affect the animal's fitness. The question is, can competent brain circuits evolve this way? We offer theoretical evidence that this is indeed the case 1 . In Section 2 we prove that, if the classifier to be learned is linear, then evolution does indeed succeed in creating a neural network that classifies well, almost certainly and within polynomial (in the dimension of the problem) number of generations, individuals per generation, and neurons per individual. We also validate our theorem through experiments on the MNIST data set. Our experiments are not meant to compete with what is happening in the ANN world. We want to make the point that competent learning can happen in life: NNE with a single hidden layer already gives surprisingly good accuracy rates (more than 90% accuracy for classifying the MNIST digits 0 to 4). There is a different way of looking at, and motivating, our results, namely from the point of view of the study of the brain in connection to evolution. \"Nothing in biology makes sense except in the light of evolution,\" Theodosius Dobzhansky famously proclaimed. Neuroscientists have espoused this point of view, and evolutionary arguments come up often in the study of the brain, see for example  Bosman & Aboitiz (2015) . However, we are not aware of a technical discussion in the literature of the obvious existential question: Is the architecture of the brain susceptible to evolution through natural selection? Can brain circuits evolve? Our mathematical and empirical results in this paper on NNE strongly suggest that, indeed, effective brain circuits specializing in classification tasks could have evolved. We also propose a second biologically plausible ANN-like mechanism, based on dopaminergic plas- ticity. It was recently established experimentally ( Yagishita et al., 2014 ) that weights in certain synapses (in this case from the cortex to the striatum in the mouse brain, but not only) are increased if dopamine was released within 0.5-2 seconds after the synapse's firing. Intuitively, this is a rein- forcement plasticity mechanism that \"rewards\" synapses whose firing led to a favorable outcome. Inspired by this experiment, we define dopaminergic neural nets (DNN), in which the weight of a link that fired (that is, both nodes fired during the current training example) is modified by a multiple of ( 1 4 − err 2 ), where err is the error of the current training example. That is, links that fired are rewarded if the result was good, and actually also punished if it was not. Our experiments show that such DNNs can also learn to classify quite well, comparable to SGD.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper explores the potential of pre-trained embedding models such as BERT, GPT-2 and InferSent to be used as a form of emergent, flexible knowledge representation harvested from text corpora. It demonstrates how this knowledge can be extracted and utilized to solve real-world problems with little or no additional training, and proposes new evaluation metrics for the development of embedding models with optimal semantic geometries.",
        "Abstract": "Many applications of linguistic embedding models rely on their value as pre-trained inputs for end-to-end tasks such as dialog modeling, machine translation, or question answering. This position paper presents an alternate paradigm: Rather than using learned embeddings as input features, we instead treat them as a common-sense knowledge repository that can be queried via simple mathematical operations within the embedding space. We show how linear offsets can be used to (a) identify an object given its description, (b) discover relations of an object given its label, and (c) map free-form text to a set of action primitives. Our experiments provide a valuable proof of concept that language-informed common sense reasoning, or `reasoning in the linguistic domain', lies within the grasp of the research community. In order to attain this goal, however, we must reconsider the way neural embedding models are typically trained an evaluated. To that end, we also identify three empirically-motivated evaluation metrics for use in the training of future embedding models.",
        "Introduction": "  INTRODUCTION This position paper casts pre-trained embedding models like BERT ( Devlin et al., 2018 ), GPT-2 ( Radford et al. ) and InferSent ( Conneau et al., 2017 ) in a new role. Rather than using the learned hidden states as pre-trained features for downstream tasks, we instead view them as a form of emer- gent, flexible knowledge representation harvested from a rich body of text corpora. We show how the knowledge implicitly encoded in the embedding space can be extracted and utilized to solve real-world problems with little or no additional training, and we argue that the effectiveness of this method can be vastly increased in future embedding models. There is, of course, inherent unpredictability of this approach. In contrast to hand-curated symbolic knowledge, harvested representational knowledge is imprecise, at times unreliable, and difficult to anticipate - but it is also fascinating, spontaneous, and fluidly creative in ways that traditional knowledge systems seldom replicate. Unlike a knowledge graph, these vector-based representations can explore questions like, \"What is the combination of fear and sound?\" or \"What is like a river without water? 1 \" Queries, in this context, are linear algebra operations on the embedded representations of input text, with answers found by seeking the embedded word or sentence with the greatest cosign similarity to the calculated result. This requires a startlingly high level of semantic structure within the geom- etry of the learned representations, and gentle probing of current state-of-the-art embedding spaces reveals that they are not yet up to the task. The potential exists, but only in nascent form. The purpose of this paper is to unequivocally demonstrate that potential, quantify it where possible, and propose new evaluation metrics for the development of embedding models with optimal semantic geometries.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the reinforcement learning (RL) problem and the Markov Decision Process (MDP) which includes a discount factor 0 ≤ γ ≤ 1 that exponentially reduces the present value of future rewards. It is proposed that a prior belief of the hazard rate p(λ) implies a specific discount function, and that the canonical case in RL of discounting future rewards according to d(t) = γ t is consistent with the belief that there exists a single hazard rate λ = e −γ known with certainty. An algorithm is proposed that approximates hyperbolic discounting while building on successful Q-learning tools and their associated theoretical guarantees. The efficacy of the approximation scheme is demonstrated in a proposed Pathworld environment which is characterized by an uncertain per-time-step risk to the agent. The paper also considers higher-dimensional deep RL agents in the Arcade Learning Environment (ALE), where the benefits of hyperbolic discounting are measured. The paper questions the RL paradigm of learning policies through a single discount function and proposes a practical approach for training an agent which discounts future rewards by a hyperbolic (or other non-exponential) discount function and acts according to this.",
        "Abstract": "Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process.  The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences.  Here we extend earlier work of Kurth-Nelson and Redish and propose an efficient deep reinforcement learning agent that acts via hyperbolic discounting and other non-exponential discount mechanisms. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL.  Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over state-of-the-art methods.",
        "Introduction": "  INTRODUCTION The standard treatment of the reinforcement learning (RL) problem is the Markov Decision Process (MDP) which includes a discount factor 0 ≤ γ ≤ 1 that exponentially reduces the present value of future rewards (Bellman, 1957; Sutton & Barto, 1998). A reward r t received in t-time steps is devalued to γ t r t , a discounted utility model introduced by Samuelson (1937). This establishes a time- preference for rewards realized sooner rather than later. The decision to exponentially discount future rewards by γ leads to value functions that satisfy theoretical convergence properties (Bertsekas, 1995). The magnitude of γ also plays a role in stabilizing learning dynamics of RL algorithms (Prokhorov & Wunsch, 1997; Bertsekas & Tsitsiklis, 1996) and has recently been treated as a hyperparameter of the optimization (OpenAI, 2018; Xu et al., 2018). However, both the magnitude and the functional form of this discounting function establish priors over the solutions learned. The magnitude of γ chosen establishes an effective horizon for the agent of 1/(1 − γ), far beyond which rewards are neglected (Kearns & Singh, 2002). This effectively imposes a time-scale of the environment, which may not be accurate. Further, the exponential discounting of future rewards is consistent with a prior belief that there is a known constant per-time-step hazard rate (Sozou, 1998) or probability of dying of 1 − γ (Lattimore & Hutter, 2011). Additionally, discounting future values exponentially and according to a single discount factor γ does not harmonize with the measured value preferences in humans 1 and animals (Mazur, 1985; 1997; Ainslie, 1992; Green & Myerson, 2004; Maia, 2009). A wealth of empirical evidence has been amassed that humans, monkeys, rats and pigeons instead discount future returns hyperbolically, where d k (t) = 1 1+kt , for some positive k > 0 (Ainslie, 1975; 1992; Mazur, 1985; 1997; Frederick et al., 2002; Green et al., 1981; Green & Myerson, 2004). This discrepancy between the time-preferences of animals from the exponential discounted measure of value might be presumed irrational. But Sozou (1998) showed that hyperbolic time-preferences is mathematically consistent with the agent maintaining some uncertainty over the prior belief of the hazard rate in the environment. Hazard rate h(t) measures the per-time-step risk the agent incurs as it acts in the environment due to a potential early death. Precisely, if s(t) is the probability that the Under review as a conference paper at ICLR 2020 agent is alive at time t then the hazard rate is h(t) = − d dt lns(t). We consider the case where there is a fixed, but potentially unknown hazard rate h(t) = λ ≥ 0. The prior belief of the hazard rate p(λ) implies a specific discount function Sozou (1998). Under this formalism, the canonical case in RL of discounting future rewards according to d(t) = γ t is consistent with the belief that there exists a single hazard rate λ = e −γ known with certainty. Further details are available in Appendix A. Common RL environments are also character- ized by risk, but often in a narrower sense. In deterministic environments like the original Ar- cade Learning Environment (ALE) (Bellemare et al., 2013) stochasticity is often introduced through techniques like no-ops (Mnih et al., 2015) and sticky actions (Machado et al., 2018) where the action execution is noisy. Physics sim- ulators may have noise and the randomness of the policy itself induces risk. But even with these stochastic injections the risk to reward emerges in a more restricted sense. In Section 2 we show that a prior distribution reflecting the uncertainty over the hazard rate, has an as- sociated discount function in the sense that an MDP with either this hazard distribution or the discount function, has the same value function for all policies. This equivalence implies that learning policies with a discount function can be interpreted as making them robust to the as- sociated hazard distribution. Thus, discounting serves as a tool to ensure that policies deployed in the real world perform well even under risks they were not trained under. We propose an algorithm that approximates hyperbolic discounting while building on successful Q- learning (Watkins & Dayan, 1992) tools and their associated theoretical guarantees. We show learning many Q-values, each discounting exponentially with a different discount factor γ, can be aggregated to approximate hyperbolic (and other non-exponential) discount factors. We demonstrate the efficacy of our approximation scheme in our proposed Pathworld environment which is characterized both by an uncertain per-time-step risk to the agent. Conceptually, Pathworld emulates a foraging environment where an agent must balance easily realizable, small meals versus more distant, fruitful meals. We then consider higher-dimensional deep RL agents in the ALE, where we measure the benefits of hyperbolic discounting. This approximation mirrors the work of Kurth-Nelson & Redish (2009); Redish & Kurth-Nelson (2010) which empirically demonstrates that modeling a finite set of µAgents simultaneously can approximate hyperbolic discounting function. Our method then generalizes to other non-hyperbolic discount functions and uses deep neural networks to model the different Q-values from a shared representation. Surprisingly and in addition to enabling new non-exponential discounting schemes, we observe that learning a set of Q-values is beneficial as an auxiliary task (Jaderberg et al., 2016). Adding this multi-horizon auxiliary task often improves over a state-of-the-art baseline, Rainbow (Hessel et al., 2018) in the ALE (Bellemare et al., 2013). This work questions the RL paradigm of learning policies through a single discount function which exponentially discounts future rewards through the following contributions: 1. Hazardous MDPs. We formulate MDPs with hazard present and demonstrate an equivalence between undiscounted values learned under hazards and (potentially non- exponentially) discounted values without hazard. 2. Hyperbolic (and other non-exponential)-agent. A practical approach for training an agent which discounts future rewards by a hyperbolic (or other non-exponential) discount function and acts according to this. 3. Multi-horizon auxiliary task. A demonstration of multi-horizon learning over many γ simultaneously as an effective auxiliary task.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a privacy-preserving representation learning method for a client-server setup. The method is based on Variational Autoencoders (VAE) and is augmented with two novel loss terms to enforce disentanglement of the private and public parts of the representation. Experiments demonstrate that the proposed method learns a semantically meaningful privacy-preserving disentangled representation of client data.",
        "Abstract": "Deep learning and latest machine learning technology heralded an era of success in data analysis. Accompanied by the ever increasing performance, reaching super-human performance in many areas, is the requirement of amassing more and more data to train these models. Often ignored or underestimated, the big data curation is associated with the risk of privacy leakages. The proposed approach seeks to mitigate these privacy issues. In order to sanitize data from sensitive content, we propose to learn a privacy-preserving data representation by disentangling into public and private part, with the public part being shareable without privacy infringement. The proposed approach deals with the setting where the private features are not explicit, and is estimated though the course of learning. This is particularly appealing, when the notion of sensitive attribute is ``fuzzy''. We showcase feasibility in terms of classification of facial attributes and identity on the CelebA dataset. The results suggest that private component can be removed in the cases where the the downstream task is known a priori (i.e., ``supervised''), and the case where it is not known a priori (i.e., ``weakly-supervised'').",
        "Introduction": "  INTRODUCTION In recent years, learning with DNNs has brought impressive advances to the state-of-the-art across a wide variety of machine-learning tasks and applications. Yet, these approaches are generally only capable of significant performance leaps when large amounts of training data are provided for training purposes. However, building and curating such large data corpora comes with many strings attached. Firstly, it is cumbersome, expensive as well as time consuming to amass sufficient and foremost clean data. Apart from that, amassing large amounts of data increases the risk of privacy creep, i.e. subtly encoding privacy related information  Narayanan & Shmatikov (2006) ;  Backstrom et al. (2007) . In this regard, many datasets have been released into the public domain that are unintentionally permeated with private information about individuals, raising serious concerns about data privacy. In  Narayanan & Shmatikov (2006)  anonymization could be reversed by making use of publicly available data, e.g. movie reviews. In  Wachinger et al. (2015)  showed, that the surface of magnet resonance images (MRI) of the brain can be used to identify people potentially allow for unintended disease progression. Growing privacy concerns will entails the risk of becoming a major deterrent in the widespread adoption of machine learning and the attainment of their concomitant benefits. Therefore, reliable and accurate privacy-preserving methodologies are needed, which is why the topic lately has enjoyed increased attention in the research community. Several efforts have been made in machine learning to develop algorithms that preserve user privacy while achieving reasonable predictive power. This is even more challenging in the client-server scenario. Therein, the clients are supposed to send information to the server, which in turn then performs operations such as training for the client. The crucial aspect in this case is about the client's confidential data. As an example, consider a set of clients that aim at collaboratively learning an attribute classifier on face images, while preserving the identities of the individuals. Ideally the trained model, which classifies non-sensitive attributes (e.g., having glasses or not) with high accuracy, at the same time fails in classifying sensitive attributes of them (e.g., gender). A standard approach to address the privacy issue in the client-server setup is to anonymize the data of clients. This is often achieved by directly obfuscating the private part(s) of the data and/or adding random noise to raw data. Consequently, it is the noise level controlling the trade-off between Under review as a conference paper at ICLR 2020 predictive quality and user privacy (e.g.,  Differential Privacy Dwork (2006) ;  Abadi et al. (2016) ). These approaches, associate a privacy budget with all operations on the dataset. Complex training procedures run the risk of exhausting the budget before convergence. Another widely adopted solution is rely on encoded data representation. Following this notion, instead of the client's data a feature representation is transferred to the server instead. Unfortunately, the extracted features may still contain rich information, which can breach user privacy  Osia et al. (2017 ; 2018). Specifically, in the example above, an attacker can exploit the eavesdropped features to reconstruct the raw image, and hence the person on the raw image can be re-identified from the reconstructed image  Mahendran & Vedaldi (2015) .In addition, the extracted features can also be exploited by an attacker to infer private attributes  Salem et al. (2019) . A recent solution to such a problem has been federated learning  McMahan et al. (2016) ;  Geyer et al. (2017) , which allows us to collaboratively train a centralized model while keeping the training data decentralized. The idea behind this strategy is that the clients transfer the training gradients of data to the server instead of the data itself. While such an approach is appealing to train a neural network with data hosted in different clients, it does not allow the use of a centralized model for making a prediction at test time. Furthermore, transferring the models between clients and server entails significant data transmission, which considerably prolongs training. What is more, averaging the gradients across the clients further slows the backpropagation. Most importantly, all of the aforementioned methods are mainly applicable when the private at- tributes are known a priori. That his, these approaches typically fail to prevent privacy attacks when the private information contained in the dataset is not explicitly identified. Some scenarios also defy simple annotation of private content, e.g. imagery of military and civilian commodities. In such scenarios, it is highly desirable to automatically remove content that may be subject to sensitive information. This situation is further aggravated in domains such as low-shot learning with scarcity of training examples and associated privacy labels, entailing ambiguity w.r.t. sensitive features (i.e., sensitive features are the ones reveal privacy). In this paper, we focus on the following fundamental questions: how can we learn representations of a dataset in order to minimize the amount of information which could be revealed about the identity of each client? The main goal is to enable an analyst to learn relevant properties (e.g. regular labels non-privacy infringing) of a dataset as a whole, while protecting the privacy of the individual contributors (private labels, which can identify a client). This assumes the database is held by a trusted server that can learn a privacy-preserving representations, i.e. by sanitizing the identity- related information from the latent representation. Specifically, we postulate the decomposition of the latent representation into two latent factors: style and content. In this regard, style captures the private aspects of the data, whereas content encodes the public part. Thus, it is the public part that should be used for training downstream tasks as it can be transferred without compromising privacy. Following this notion, style encodes patterns that are shared among the samples of each client. In contrast to that, the content encodes information about concepts, which is shared across clients. Ultimately, this implies a disentanglement in the feature space in. private features and public features. Our proposed method is built on top of the popular Variational Autoencoders (VAE)  Kingma & Welling (2013) , where it used as core representation learning paradigm. VAE consists of two net- works, namely an encoder and a decoder. The former maps a data sample to a latent representation, while the latter maps this representation back to data space. VAE networks are trained by mini- mizing a cost function that encourages learning a latent representation, which leads to realistic data synthesis while simultaneously ensuring sufficient diversity in the synthesized data. This is achieved by minimizing the distance between input and reconstruction subject to distributional regularization on the latent space. In addition, to the VAE entailed cost functions, we augment the loss space with additional terms namely \"content classification loss\" and \"style confusion loss\". In context of a su- pervised setup, the former is utilized to enforce for target predictability for a downstream task, while the latter encourage preserving the privacy. We show that, this two additional terms can enforce dis- entanglement of the private and public parts of the representation, as well utilization in context of weakly supervised scenario, where the downstream attributes are not known a priori. In summary, the main contributions of this paper are the three-fold. First, we pose the privacy- preserving representation learning problem as learning disentangled representations in a client- Under review as a conference paper at ICLR 2020 confusion loss classification loss reconstruction loss content style (a) (b) server setup. Second, we propose to learn disentangled representations from the client-level su- pervision by adding two novel loss terms to VAE. Third, we demonstrate experimentally that our proposed method learns a semantically meaningful privacy-preserving disentangled representation of client data.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the use of differential equations in a variety of disciplines, including physics, engineering, economics, chemistry, biology, medicine, and neural networks. It also discusses the difficulty of finding analytical solutions in closed form and the use of numerical integration methods, such as Runge-Kutta, to obtain approximate solutions. The paper further examines the importance of adaptive step size strategies in Runge-Kutta methods, which can increase efficiency by several orders of magnitude.",
        "Abstract": "Initial value problems, i.e. differential equations with specific, initial conditions, represent a classic problem within the field of ordinary differential equations(ODEs). While the simplest types of ODEs may have closed-form solutions, most interesting cases typically rely on iterative schemes for numerical integration such as the family of Runge-Kutta methods. They are, however, sensitive to the strategy the step size is adapted during integration, which has to be chosen by the experimenter. In this paper, we show how the design of a step size controller can be cast as a learning problem, allowing deep networks to learn to exploit structure in the initial value problem at hand in an automatic way. The key ingredients for the resulting Meta-Learning Runge-Kutta (MLRK) are the development of a good performance measure and the identification of suitable input features. Traditional approaches suggest the local error estimates as input to the controller. However, by studying the characteristics of the local error function we show that including the partial derivatives of the initial value problem is favorable. Our experiments demonstrate considerable benefits over traditional approaches. In particular, MLRK is able to mitigate sudden spikes in the local error function by a faster adaptation of the step size. More importantly, the additional information in the form of partial derivatives and function values leads to a substantial improvement in performance. The source code can be found at https://www.dropbox.com/sh/rkctdfhkosywnnx/AABKadysCR8-aHW_0kb6vCtSa?dl=0",
        "Introduction": "  INTRODUCTION Differential equations in their general form cover an extremely wide variety of disciplines: While many applications are rather intuitive, as for instance simple Newtonian physics and engineering, other more exotic use cases include the governing of price evolution in economics ( Black & Scholes, 1973 ), the study of rates in chemical reactions ( Scholz & Scholz, 2014 ), and the modeling of population growths in biology ( Lotka, 1925 ;  Volterra, 1926 ). In medicine, differential equations may be used to model cancer growth ( Ilea et al., 2013 ), diabetes and the glucose metabolism ( Esna-Ashari et al., 2017 ) as well as for pharmaceutical drug design ( Deuflhard, 2000 ). Recently, differential equations have also been used as a way to design neural networks ( Chen et al., 2018 ). Unfortunately, finding an analytical solution in closed form is in many cases very difficult, if not impossible. Therefore, a variety of numerical integration methods have been developed to obtain accurate, but approximate solutions. Arguably, the most prominent ones are Runge-Kutta methods, a family of integration methods for initial value problems. However, setting up Runge-Kutta involves several design choices, one of which is the step size controller. Using an adaptive step size strategy instead of a constant step size can often increase efficiency by several orders of magnitude, c.f . ( Söderlind, 2006 ). Their performance is hampered by the fact that they only make use of hand-designed features.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method to reduce the representation error of a generative prior by combining a trained GAN with an untrained Deep Decoder. The proposed hybrid model is evaluated on compressive sensing tasks using both in-distribution and out-of-distribution images. Results show that the hybrid model consistently yields higher PSNRs than various GAN priors across a wide variety of undersampling ratios, while also outperforming a standalone Deep Decoder. The hybrid model also performs well on far out-of-distribution images, where it has comparable performance to the Deep Decoder model.",
        "Abstract": "Generative models, such as GANs, have demonstrated impressive performance as natural image priors for solving inverse problems such as image restoration and compressive sensing. Despite this performance, they can exhibit substantial representation error for both in-distribution and out-of-distribution images, because they maintain explicit low-dimensional learned representations of a natural signal class. In this paper, we demonstrate a method for removing the representation error of a GAN when used as a prior in inverse problems by modeling images as the linear combination of a GAN with a Deep Decoder. The deep decoder is an underparameterized and most importantly unlearned natural signal model similar to the Deep Image Prior.  No knowledge of the specific inverse problem is needed in the training of the GAN underlying our method.  For compressive sensing and image superresolution, our hybrid model exhibits consistently higher PSNRs than both the GAN priors and Deep Decoder separately, both on in-distribution and out-of-distribution images.  This model provides a method for extensibly and cheaply leveraging both the benefits of learned and unlearned image recovery priors in inverse problems.",
        "Introduction": "  INTRODUCTION Generative Adversarial Networks (GANs) show promise as priors for solving imaging inverse problems such as inpainting, compressive sensing, super-resolution, and others. For example, they have been shown to perform as well as common sparsity based priors on compressed sensing tasks using 5-10x fewer measurements, and also perform well in nonlinear blind image deblurring ( Bora et al., 2017 ;  Asim et al., 2018 ). The typical inverse problem in imaging is to reconstruct an image given incomplete or corrupted measurements of that image. Since there may be many potential reconstructions that are consistent with the measurements, this task requires a prior assumption about the structure of the true image. A traditional prior assumption is that the image has a sparse representation in some basis. Provided the image is a member of a known class for which many examples are available, a GAN can be trained to approximate the distribution of images in the desired class. The generator of the GAN can then be used as a prior, by finding the point in the range of the generator that is most consistent with the provided measurements. We use the term \"GAN prior\" to refer to generative convolutional neural networks which learn a mapping from a low dimensional latent code space to the image space, for example with the DCGAN, GLO, or VAE architectures (Radford et al., 2015;  Bojanowski et al., 2017 ; Kingma & Welling, 2013). Challenges in the training of GANs involve selecting hyperparameters, like the dimensionality of the model manifold; difficulties in training, such as mode collapse; and the fact than GANs are not directly optimizing likelihood. Because of this, their performance as image priors is severely limited by representation error ( Bora et al., 2017 ). This effect is exaggerated when reconstructing images which are out of the training distribution, in which case the GAN prior typically fails completely to give a sensible solution to the inverse problem. In contrast, untrained deep neural networks also show promise in solving imaging inverse problems, by leveraging architectural bias of a convolutional network as a structural prior instead of a learned representation ( Ulyanov et al., 2018 ;  Heckel & Hand, 2018 ). These methods are independent of any training data or image distribution, and therefore are robust to shifts in data distribution that Under review as a conference paper at ICLR 2020 are problematic for GAN priors. Recent work by  Heckel & Hand (2018)  presents an untrained decoder-style network architecture, the Deep Decoder, that is an efficient image representation and as a consequence works well as an image prior. In particular, it can represent images more efficiently than with wavelet thresholding. When used for denoising tasks, it outperforms BM3D, considered the state-of-the-art among untrained denoising methods. The Deep Decoder is similar to the Deep Image Prior, but it can be underparameterized, having fewer optimizable parameters than the image dimensionality, and consequently does not need any algorithmic regularization, such as early stopping. In this paper, we propose a simple method to reduce the representation error of a generative prior by studying image models which are linear combinations of a trained GAN with an untrained Deep Decoder. We build a method that capitalizes on the strengths of both methods: we want strong performance for all natural images and not just those close to a training distribution, and we want improved performance when given images are near a provided training distribution. The former comes from the Deep Decoder, and the latter comes from the GAN. We demonstrate the performance of this method on compressive sensing tasks using both in-distribution and out-of-distribution images. For in-distribution images, we find that the hybrid model consistently yields higher PSNRs than various GAN priors across a wide variety of undersampling ratios (sometimes by 10+ dB), while also consistently outperforming a standalone Deep Decoder (by around 1 dB). Performance improvements over the GAN prior also hold in the case of imaging on far out-of-distribution images, where the hybrid model and Deep Decoder model have comparable performance. A major challenge of the field is to build algorithms for solving inverse problems that are at least as good as both learned and recently discovered unlearned methods. Any new method should be at least as good as either approach separately. The literature contains multiple answers to this question, including invertible neural networks, optimizing over all weights of a trained GAN in an image-adaptive way, and more. This paper provides a significantly simpler method to get the benefits of both learned and unlearned methods, surprisingly by simply taking the linear combination of both models.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a study of the robustness of disentangled generative models to adversarial attacks. It introduces a hierarchical disentangled VAE, Seatbelt-VAE, which is more robust to adversarial attacks than β-TCVAEs and β-TCDLGMs. The paper demonstrates that β-TCVAEs are significantly more robust to adversarial attacks via their latents than vanilla VAEs, and that Seatbelt-VAEs are more robust to attacks that act to maximize the evidence lower bound for the adversarial input.",
        "Abstract": "This paper is concerned with the robustness of VAEs to adversarial attacks. We highlight that conventional VAEs are brittle under attack but that methods recently introduced for disentanglement such as β-TCVAE (Chen et al., 2018) improve robustness, as demonstrated through a variety of previously proposed adversarial attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE that is designed to be significantly more robust to adversarial attacks than existing approaches, while retaining high quality reconstructions.",
        "Introduction": "  INTRODUCTION Unsupervised learning of disentangled latent variables in generative models remains an open research problem, as is an exact mathematical definition of disentangling (Higgins et al., 2018). Intuitively, a disentangled generative model has a one-to-one correspondence between each input dimension of the generator and some interpretable aspect of the data generated. For VAE-derived models (Kingma & Welling, 2013; Rezende et al., 2014) this is often based around rewarding independence between latent variables. Factor VAE (Kim & Mnih, 2018), β-TCVAE (Chen et al., 2018) and HFVAE (Esmaeili et al., 2019) have shown that the evidence lower bound can be decomposed to obtain a term capturing the degree of independence between latent variables of the model, the total correlation. By up-weighting this term, we can obtain better disentangled representations under various metrics compared to β-VAEs (Higgins et al., 2017a). Disentangled representations, much like PCA or factor analysis, are not only human-interpretable but also offer more informative and robust latent space representations. In addition, information theoretic interpretations of deep learning show that having a disentangled hidden layer within a discriminative deep learning model increases robustness to adversarial attack (Alemi et al., 2017). Adversarial attacks on deep generative models, more difficult than those on discriminative models (Tabacof et al., 2016; Gondim-Ribeiro et al., 2018; Kos et al., 2018), attempt to fool a model into reconstructing a chosen target image by adding distortions to the original input image. Generally, the most effective attack mode involves making the latent-space representation of the distorted input match that of the target image (Gondim-Ribeiro et al., 2018; Kos et al., 2018). This kind of attack is particularly relevant to applications where the encoder's output is used downstream. Projections of data from VAEs, disentangled or not, are used for tasks such as: text classification (Xu et al., 2017); discrete optimisation (Kusner et al., 2017); image compression (Theis et al., 2017; Townsend et al., 2019); and as the perceptual part of a reinforcement learning algorithm (Ha & Schmidhuber, 2018; Higgins et al., 2017b), the latter of which uses a disentangled VAE's encoder to improve the robustness of the agent to domain shift. Here we demonstrate that β-TCVAEs are significantly more robust to 'latent-space' attack than standard VAEs, and are generally more robust to attacks that act to maximise the evidence lower bound for the adversarial input. The robustness of these disentangled models is highly relevant because of the use-cases for VAEs highlighted above. However, imposing additional disentangling constraints on a VAE training objective degrades the quality of resulting drawn or reconstructed images (Higgins et al., 2017a; Chen et al., 2018). We sought whether more powerful, expressive models, can help ameliorate this and in doing so built Under review as a conference paper at ICLR 2020 a hierarchical disentangled VAE, Seatbelt-VAE, drawing on works like Ladder VAEs (Sønderby et al., 2016) and BIVA (Maaløe et al., 2019). We demonstrate that Seatbelt-VAEs are more robust to adversarial attacks than β-TCVAEs and β-TCDLGMs (the latter a simple generalisation we make of β-TC penalisation to hierarchical VAEs). See  Figure 1  for a demonstration. Rather than being concerned with human-interpretable controlled generation by our models, which has been the focus of much research into disentangling, instead we are interested in the robustness afforded by disentangled representations. Thus our key contributions are: • A demonstration that β-TCVAEs are significantly more robust to adversarial attacks via their latents than vanilla VAEs. • The introduction of Seatbelt-VAE, a hierarchical version of the β-TCVAE, designed to further increase robustness to various types of adversarial attack, while also giving better perceptual quality of reconstructions even when regularised.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a generic approach to identify optimal policies of a given Non Markovian Decision Process (NMRDP) by learning a latent state space from a set of similar and different trajectories. The proposed approach requires weaker domain specific knowledge than existing methods, and is demonstrated on a toy NMRDP grid-world environment and a real-life data set of tourists visiting Salzburg.",
        "Abstract": "Expanding Non Markovian Reward Decision Processes (NMRDP) into Markov Decision Processes (MDP) enables the use of state of the art Reinforcement Learning (RL) techniques to identify optimal policies. In this paper an approach to exploring NMRDPs and expanding them into MDPs, without the prior knowledge of the reward structure, is proposed. The non Markovianity of the reward function is disentangled under the assumption that sets of similar and dissimilar trajectory batches can be sampled. More precisely, within the same batch, measuring the similarity between any couple of trajectories is permitted, although comparing trajectories from different batches is not possible. A modified version of the triplet loss is optimised to construct a representation of the trajectories under which rewards become Markovian.",
        "Introduction": "  INTRODUCTION One of the major objectives in reinforcement learning is to identify optimal policies of a given Markov Decision Process (MDP), whose existence is typically ensured if the environment is Marko- vian ( Sutton et al., 1998 ). However, this assumption is usually not fulfilled in many real life prob- lems. A first possible cause is \"partial observability\", when the real state space is observable only through a projection on a sub-space. For instance, consider a (cleaning) robot that uses the distance from the walls in four directions as a state space. This representation of the environment does not necessarily correspond to the exact location in the house as different places share the exact same distances to walls. This setting is usually framed as a Partially Observed Markov Decision Process (POMDP) ( Shani et al., 2013 ;  Cassandra, 1998 ;  Murphy, 2000 ;  Hausknecht & Stone, 2015 ;  Zhu et al., 2018 ). A second possible cause is that reward functions might depend on the whole trajectory. Consider the example of a waiter/server robot whose state space is the current position in the coffee shop and the current interaction with the clients (taking orders, getting and delivering beverages). The objective is to train this robot to bring coffee when ordered. A possible reinforcement signal is to reward the agent when it processes an order that was not satisfied in the past. As the states do not include past requests, nor their fulfilment, the environment is not Markovian and the proposed reward is a function of the trajectory up to the present. This setting is a Non-Markovian Reward Decision Process (NMRDP) ( Bacchus et al., 1997 ;  Thiébaux et al., 2006 ;  Bakker, 2002 ;  Bacchus et al., 1996 ). Both POMDP and NMRDP are special cases of Non Markovian Decision Processes. The second above example (waiter robot) is actually a multiple task problem. Planning in this setting is a challenging problem because it requires more than optimally and independently solving each task ( Toro Icarte et al., 2018 ). It is possible that a global optimal policy for the whole sequence of tasks induces sub-optimal policies for each one of the sub-tasks. Let us take a naive example of an agent building a shelter which requires gathering wood and stones from nearby resources. To gather wood/stones optimally, the agent should look for the nearest forest/quarry. However, if it wants to optimally gather all the resources, it should not just consider how close the forests or the quarries are, but how close they are to each other as it will have to walk from one to the other. Notice that this example shares similarities with the k-server problem ( Manasse et al., 1990 ). In those cases, the nature of the task is not encoded in the state space, so the Markov assumption is not verified. Casting the problems as an NMRDP requires using a (stage by stage) reward function that encodes the tasks specifications. In the shelter building scenario, the reward as a function of the states, while none of the tasks is achieved, is different from when stones or wood have been collected. If it is possible to map the previously observed states (the trajectory up to the present) Under review as a conference paper at ICLR 2020 into the current task, the problem can be expanded into a standard MDP, where it is known how to identify optimal policies. Motivated by the shared similarities of the described problems, we shall focus on learning to identify current tasks from the trajectory of visited states ( Wilson et al., 2007 ). Ideally, if annotated trajectory data sets are available, learning a trajectory representation function that expands the NMRDP into an MDP would be a supervised task. However this is a strong as- sumption. Indeed if a simple way of creating such data sets existed, it would be used to expand the NMRDP's state space. Another issue, is that given a set of tasks, there might exist multiple non Markovian reward functions of interest. For instance, consider an agent whose goal is to maximise customers satisfaction. In this case the reinforcement signals are the declared satisfaction of said customers (in the form of a rating for example). However even if different customers expect the same goals to be accomplished by the robot, their perception of the performance might differ. This example can be generalised to the case where several experts (or simply users) are training a robot to perform some tasks, possibly different from one expert to the other. They share a common NMRDP environment with different reward/reinforcement functions as the set of tasks (or their ap- preciations/preferences) might be different from one expert to the other. The same augmented MDP can be used for all of their respective NMRDP as they share the same latent global set of tasks (more or less whatever the robot can physically do). This training can be made by the expert inde- pendently but it will certainly be faster and more efficient if made collectively. In these cases, in order to identify the full set of tasks and annotate the trajectories, a central authority must be aware of the reward function of each expert. This might be impossible due to tractability reasons, or pri- vacy constraints. Fortunately, the experts can easily provide \"similarity\" between trajectories with respect to the latent task, i.e., whether the robot was performing the same task or not at that time, without specifying precisely the task. In the customer example, the evaluations given by the same client can be used to compare if two trajectories are associated with the same task or not, but not if those trajectories come from batches associated to two different customers. Indeed, establishing comparisons between the tasks associated to subsets of the observed trajectories without actually knowing the actual task is possible. As a consequence, we are going to assume that learning is done using batches of trajectories, where we will have a task similarity measure within each batch. In order to identify the tasks, standard techniques identify a latent state space in which policies become Markovian, thus optimal ones exist. Unfortunately these approaches require domain spe- cific knowledge in the form of \"relevant propositions\" ( Bacchus et al., 1997 ;  Thiébaux et al., 2006 ;  Bakker, 2002 ;  Toro Icarte et al., 2018 ), either by estimating the hidden state or by constructing ad- ditional features. For example, in the waiter robot case, received orders are the relevant propositions that the agent needs to save in order to identify current tasks. Our major contribution in this paper is a generic approach to discover a latent state space given a set of similar and different trajectories. This assumption requires weaker domain specific knowledge. Once the latent state space is available, multi-task and meta-learning related approaches provide ef- ficient tools to identify optimal policies ( Andrychowicz et al., 2017 ; Colas et al., 2019) or to learn a general purpose policy from which learning a specific task's optimal policy can be done in few steps ( Duan et al., 2017 ;  Rakelly et al., 2019 ). The remaining of the paper is organised as follows: we introduce the mathematical tools used in this paper and we explain how they relate to our work in Section 2. We specify in Section 3 a subset of NMRDPs that can be expanded with a Markovian latent space into MDPs and we provide a possible structure of such expansion. In Section 4, we propose an algorithm to learn trajectory representation functions that approximate the tasks' latent state space and thus approximates the proposed equivalent MDPs. Section 5 is dedicated to empirical results. We consider toy NMRDPs grid-world environment (where we solve a multi-task problem using learned representations) and a real-life data set of tourists visiting Salzburg (where we learn a trajectory representations that clusters their paths according to their destinations).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the phenomenon of perturbation-based defenses against attacks on Convolutional Neural Networks. We show that many defense techniques can be interpreted as input perturbations, and that the relationship between channel distortion and robustness is consistent across different families of input perturbations. We also analyze why some state-of-the-art attacks are sensitive to perturbation-based defenses, and provide insight into how an attacker might avoid them.",
        "Abstract": "The existence of adversarial examples, or intentional mis-predictions constructed from small changes to correctly predicted examples, is one of the most significant challenges in neural network research today. Ironically, many new defenses are based on a simple observation - the adversarial inputs themselves are not robust and small perturbations to the attacking input often recover the desired prediction. While the intuition is somewhat clear, a detailed understanding of this phenomenon is missing from the research literature. This paper presents a comprehensive experimental analysis of when and why perturbation defenses work and potential mechanisms that could explain their effectiveness (or ineffectiveness) in different settings.",
        "Introduction": "  INTRODUCTION The attacks on Convolutional Neural Networks, such as Carlini & Wanger ( Carlini & Wagner, 2017 ) or PGD ( Madry et al., 2017 ), generate strategically placed modifications that can be easily dominated by different types of perturbations resulting in correct predictions ( Dziugaite et al., 2016 ;  Roth et al., 2019 ). This suggests that the standard adversarial examples are not robust. Many defense techniques explicitly leverage this property and can be retrospectively interpreted as perturbations of the input images. However, a detailed understanding of this phenomenon is lacking from the research literature including: (1) what types of perturbations work and what is their underlying mechanism, (2) whether all attacks exhibit this property, and (3) possible counter-measures attackers can employ to defeat perturbation defenses. We can interpret a large number of recent defenses as a type of input perturbations, for example, feature squeezing ( Xu et al., 2017 ), frequency or JPEG compression ( Dziugaite et al., 2016 ), randomized smoothing ( Cohen et al., 2019 ), and perturbation of network structure or the inputs randomly ( Jafarnia- Jahromi et al., 2018 ;  Zhang & Liang, 2019 ;  Guo et al., 2017 ). The defense techniques exhibit very similar gains in robustness. To show it, we start with a simple model where every example is passed through a lossy channel (stochastic or deterministic) prior to model inference. This channel induces a small perturbation to the input. We optimize the perturbation to be small enough as not to affect the prediction accuracy on clean examples but large enough to dominate any adversarial attack. We find that this trade-off is surprisingly consistent across very different families of input perturbations, where the relationship between channel distortion (the L 2 distance between channel input and output) and robustness is very similar. Why are some state-of-the-art attacks are sensitive to perturbation-based defenses? We find that many attacks execute an optimization procedure that finds an adversarial image that is very close to the original image in terms of of L 1 , L 2 , or L ∞ norm. The resultant optimum, i.e., the adversarial image, tends to exhibit a higher level of instability than natural examples, which we demonstrate from the perspective of a first-order and second-order analysis. By instability we mean that small perturbations of the example can affect the prediction confidences. The unification of perturbation-based defense also gives us some insight into how an attacker might avoid them.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new framework called instability analysis, which measures the robustness of the outcome of optimizing a neural network to SGD noise. Through instability analysis, the authors provide insights into the behavior of unpruned networks, including the emergence of sparse, matching subnetworks in more challenging settings. Results demonstrate that instability analysis is a valuable tool for investigating the behavior of neural networks and provide empirical evidence in support of the lottery ticket hypothesis.",
        "Abstract": "We uncover a connection between two seemingly unrelated empirical phenomena: mode connectivity and sparsity. On the one hand, there is growing catalog of situations where, across multiple runs, SGD learns weights that fall into minima that are connected (mode connectivity). A striking example is described by Nagarajan & Kolter (2019). They observe that test error on MNIST does not change along the linear path connecting the end points of two independent SGD runs, starting from the same random initialization. On the other hand, there is the lottery ticket hypothesis of Frankle & Carbin (2019), where dense, randomly initialized networks have sparse subnetworks capable of training in isolation to full accuracy.\n\nHowever, neither phenomenon scales beyond small vision networks. We start by proposing a technique to find sparse subnetworks after initialization. We observe that these subnetworks match the accuracy of the full network only when two SGD runs for the same subnetwork are connected by linear paths with the no change in test error. Our findings connect the existence of sparse subnetworks that train to high accuracy with the dynamics of optimization via mode connectivity. In doing so, we identify analogues of the phenomena uncovered by Nagarajan & Kolter and Frankle & Carbin in ImageNet-scale architectures at state-of-the-art sparsity levels.",
        "Introduction": "  INTRODUCTION The lottery ticket hypothesis ( Frankle & Carbin, 2019 ) conjectures that neural networks contain sparse subnetworks that are capable of training in isolation from initialization to full accuracy. The sole empirical evidence in support of the lottery ticket hypothesis is a series of experiments using a procedure called iterative magnitude pruning (IMP). IMP returns a subnetwork of the original, randomly initialized network by training the network to completion, pruning the lowest-magnitude weights ( Han et al., 2015 ), and resetting each remaining weight to its original initialization. On small networks for MNIST and CIFAR-10, IMP subnetworks can match the accuracy of the full network (we refer to such subnetworks as matching subnetworks) at sparsity levels far beyond those at which randomly reinitialized or randomly pruned subnetworks can do the same. The lottery ticket hypothesis offers a new perspective on the role of overparameterization and raises the tantalizing prospect that there may exist much smaller neural networks that are capable of re- placing the larger models we typically train today. Unfortunately, in more challenging settings, there is no empirical evidence that the lottery ticket hypothesis holds. IMP subnetworks of VGG and Resnet-style networks on CIFAR-10 and ImageNet perform no better than other kinds of sparse networks ( Liu et al., 2019 ;  Gale et al., 2019 ). In this paper, we describe a new framework called instability analysis, which measures whether the outcome of optimizing a network is robust to SGD noise (in which case we call it stable). Insta- bility analysis offers a range of new insights into the behavior of unpruned networks. For example, the outcome of optimization becomes stable to SGD noise early in training (3% for Resnet-20 on CIFAR-10 and 20% on Resnet-50 for ImageNet). Moreover, it distinguishes known cases where IMP succeeds and fails to find a matching subnetwork; namely, IMP subnetworks are only matching when they are stable. It also allows us to identify new scenarios where sparse, matching subnet- works emerge early in training in more challenging settings, including Resnet-50 and Inception-v3 on ImageNet. In doing so, our results demonstrate that instability analysis is a valuable scientific tool for investigating the behavior of neural networks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a technique to mimic the benefits of large batches in Generative Adversarial Networks (GANs) without the computational costs of actually using large batches. Core-set selection is used to sub-sample a large batch to produce a smaller batch, which is then used to train the GAN. This technique yields many of the benefits of having large batches with much less computational overhead, and is generic, so can be applied to nearly all GAN variants.",
        "Abstract": "BigGAN suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large minibatch sizes. This finding is interesting but also discouraging -- large batch sizes  are slow and expensive to emulate on conventional hardware. Thus, it would be nice if there were some trick by which we could generate batches that were effectively big though small in practice. In this work, we propose such a trick, inspired by the use of Coreset-selection in active learning. When training a GAN, we draw a large batch of samples from the prior and then compress that batch using Coreset-selection. To create effectively large batches of real images, we create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset-selection on those projected embeddings at training time. We conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it helps us use GANs to reach a new state of the art in anomaly detection.",
        "Introduction": "  INTRODUCTION Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ) have become a popular research topic. Arguably the most impressive results have been in image synthesis ( Brock et al., 2018 ;  Sal- imans et al., 2018 ;  Miyato et al., 2018 ;  Zhang et al., 2018 ; 2017), but they have also been applied fruitfully to text generation ( Fedus et al., 2018 ; Guo et al., 2018), domain transfer learning ( Zhu et al., 2017 ;  Zhang et al., 2017 ;  Isola et al., 2017 ), and various other tasks ( Xian et al., 2018 ;  Ledig et al., 2017 ;  Zhu & Bento, 2017 ). Recently,  Brock et al. (2018)  substantially improved the results of  Zhang et al. (2018)  by using very large mini-batches during training. The effect of large mini-batches in the context of deep learning is well-studied ( Smith et al., 2017 ;  Goyal et al., 2017 ;  Keskar et al., 2016 ;  Shallue et al., 2018 ) and general consensus is that they can be helpful in many circumstances, but the results of  Brock et al. (2018)  suggest that GANs benefit disproportionately from large batches. In fact,  Table 1  of  Brock et al. (2018)  shows that for the Frechet Inception Distance (FID) metric ( Heusel et al., 2017 ) on the ImageNet dataset, scores can be improved from 18.65 to 12.39 simply by making the batch eight times larger. Unfortunately, increasing the batch size in this manner is not always possible since it increases the computational resources required to train these models - often beyond the reach of conventional hardware. The experiments from the BigGAN paper require a full 'TPU Pod'. The 'unofficial' open source release of BigGAN works around this by accumulating gradients across 8 different V100 GPUs and only taking an optimizer step every 8 gradient accumulation steps. Future research on GANs would be much easier if we could have the gains from large batches without these pain points. In this paper, we take steps toward accomplishing that goal by proposing a technique that allows for mimicking large batches without the computational costs of actually using large batches. In this work, we use Core-set selection (Agarwal et al., 2005) to sub-sample a large batch to produce a smaller batch. The large batches are then discarded, and the sub-sampled, smaller, batches are used to train the GAN. Informally, this procedure yields small batches with 'coverage' similar to that of the large batch - in particular the small batch tries to 'cover' all the same modes as are covered in the large batch. This technique yields many of the benefits of having large batches with much less computational overhead. Moreover, it is generic, and so can be applied to nearly all GAN variants.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the impact of low-latency constraints on Sequence-to-Sequence (S2S) models, and compares two architectures for implementing deterministic \"wait-k\" decoders for online machine translation. We propose improved training techniques for wait-k by first using uni-directional encoders and training across multiple values of k, and a novel update mechanism for the transformer model to improve online decoding. Our results show that the proposed techniques improve the performance of S2S models for online machine translation.",
        "Abstract": "Neural sequence-to-sequence models are at the basis of state-of-the-art solutions for sequential prediction problems such as machine translation and speech recognition. The models typically assume that the entire input is available when starting target generation. In some applications, however, it is desirable to start the decoding process before the entire input is available, e.g. to reduce the latency in automatic speech recognition. We consider state-of-the-art wait-k decoders, that first read k tokens from the source and then alternate between reading tokens from the input and writing to the output. We investigate the sensitivity of such models to the value of k that is used during training and when deploying the model, and the effect of updating the hidden states in transformer models as new source tokens are read. We experiment with German-English translation on the IWSLT14 dataset and the larger WMT15 dataset. Our results significantly improve over earlier state-of-the-art results for  German-English translation on the WMT15 dataset across different latency levels.",
        "Introduction": "  INTRODUCTION Sequence-to-Sequence (S2S) models are state-of-the-art for tasks where source and target sequences have different lengths, including automatic speech recognition, machine translation, speech transla- tion, text-to-speech synthesis, etc. The most common models are composed of an encoder that reads the entire input sequence, while a decoder (often equipped with an attention mechanism) iteratively produces the next output token given the input and the partial output decoded so far. While these models perform very well in the typical offline decoding use case, few studies consider how S2S models are affected by low-latency constraints, and which architectures and strategies are the most efficient. Low-latency decoding is desirable for applications such as online speech recognition, and as-you-type machine translation. In such scenarios, the decoding process starts before the entire input sequence is available, and the output sequence is produced in an on-the-fly manner. However, if we consider for instance machine translation, online prediction generally comes at the cost of re- duced translation quality and more research is needed to reach the grail of natural and high-quality online speech-to-speech interpretation. In this paper we consider deterministic \"wait-k\" decoders that are state of the art for low-latency decoding (Ma et al., 2019; Zheng et al., 2019b). These decoders first read k tokens from the source, after which they proceed to alternatingly produce a target symbol and read another source sym- bol. We compare two architectures to implement such models: one based on a 2D-convolutional sequence-to-sequence model (Elbayad et al., 2018), and one based on the attention-based trans- former architecture (Vaswani et al., 2017). For these models, we investigate the impact of the choice of k when training the models, and when using them to generate translations. For the transformer model, we also consider the effect of updating the hidden states of previous target symbols based on the full source context that is available at any moment. These updates are inspired from the 2D convolutional model, where such \"updates\" are an inherent consequence of the architecture. In summary, our contributions are the following: 1. We compare transformer and 2D convolutional architectures for online machine translation. 2. We propose improved training techniques for wait-k by first using uni-directional encoders and training across multiple values of k",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a method for discovering stochastic embeddings, called the Stochastic Prototype Embedding (SPE), which is an extension of the Prototypical Network (PN). SPE outperforms the only other fully-formulated method for stochastic supervised embeddings, the Hedged Instance Embedding (HIB), on a superset of the complete battery of experiments used to justify HIB. SPE is also more computation efficient to train than HIB, with complexity comparable to that of the PN, and has no hand-tuned parameters. Additionally, SPE attains more interpretable representations by disentangling class-discriminative features.",
        "Abstract": "Supervised deep-embedding methods project inputs of a domain to a representational space in which same-class instances lie near one another and different-class instances lie far apart. We propose a probabilistic method that treats embeddings as random variables. Extending a state-of-the-art deterministic method, Prototypical Networks (Snell et al., 2017), our approach supposes the existence of a class prototype around which class instances are Gaussian distributed. The prototype posterior is a product distribution over labeled instances, and query instances are classified by marginalizing relative prototype proximity over embedding uncertainty. We describe an efficient sampler for approximate inference that allows us to train the model at roughly the same space and time cost as its deterministic sibling. Incorporating uncertainty improves performance on few-shot learning and gracefully handles label noise and out-of-distribution inputs. Compared to the state-of-the-art stochastic method, Hedged Instance Embeddings (Oh et al., 2019), we achieve superior large- and open-set classification accuracy. Our method also aligns class-discriminating features with the axes of the embedding space, yielding an interpretable, disentangled representation.",
        "Introduction": "  INTRODUCTION Supervised deep-embedding methods map instances from an input space to a latent embedding space in which same-label pairs are near and different-label pairs are far. The embedding thus captures semantic relationships without discarding inter-class structure. In contrast, consider a standard neural network classifier with a softmax output layer trained with a cross-entropy loss. Although its penultimate layer might be treated as an embedding, the classifier's training objective attempts to orthogonalize all classes and thereby eliminate any information about inter-class structure. Supervised embedding methods are critical for large- and open-set classification tasks, and are popular for few-shot and lifelong learning tasks. Nearly all previous methods for deep embeddings are deterministic: an instance projects to a point in the embedding space. Deterministic embeddings fail to capture uncertainty due either to out-of- distribution inputs (e.g., data corruption) or label ambiguity (e.g., overlapping classes). Representing uncertainty is important for many reasons, including robust classification and decision making, informing downstream models, interpreting representations, and detecting out-of-distribution samples. In this article, we propose a method for discovering stochastic embeddings, where each embedded instance is a random variable whose distribution reflects the uncertainty in the embedding space. Our proposed method, the Stochastic Prototype Embedding (SPE), is an extension of the Prototypical Network (PN) ( Snell et al., 2017 ). As in the PN, our SPE assumes each class can be characterized by a prototype in the embedding space and an instance is classified based on its proximity to a prototype. In the case of the SPE, the embeddings and prototypes are Gaussian random variables, each class instance is assumed to be a Gaussian perturbation of the prototype, and a query instance is classified by marginalizing over the embedding uncertainty. Our main contribution is to show that SPE outperforms the only other fully-formulated method for stochastic supervised embeddings, the Hedged Instance Embedding (HIB) ( Oh et al., 2019 ), on a superset of the complete battery of experiments used to justify HIB. SPE is also more computation efficient to train than HIB, with complexity comparable to that of the PN, and has no hand-tuned parameters. We also demonstrate that embedding distributions are related to label uncertainty and input ambiguity. Finally, we explore an intriguing emergent property of SPE: that it attains more interpretable representations by disentangling class-discriminative features.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new algorithm to automatically determine the learning rate for a deep learning job in an autonomous manner. The proposed algorithm works across multiple datasets and models for different tasks such as natural and adversarial training. It is an 'optimistic' method, in the sense that it increases the learning rate to as high as possible by examining the training loss repeatedly. Experiments show that the proposed algorithm performs surprisingly well compared to the state-of-the-art.",
        "Abstract": "Training neural networks on image datasets generally require extensive experimentation to find the optimal learning rate regime. Especially, for the cases of adversarial training or for training a newly synthesized model, one would not know the best learning rate regime beforehand. We propose an automated algorithm for determining the learning rate trajectory, that works across datasets and models for both natural and adversarial training, without requiring any dataset/model specific tuning. It is a stand-alone, parameterless, adaptive approach with no computational overhead. We theoretically discuss the algorithm's convergence behavior. We empirically validate our algorithm extensively. Our results show that our proposed approach \\emph{consistently} achieves top-level accuracy compared to SOTA baselines in the literature in natural training, as well as in adversarial training.",
        "Introduction": "  INTRODUCTION Deep architectures are generally trained by minimizing a non-convex loss function via underlying optimization algorithm such as stochastic gradient descent or its variants. It takes a fairly large amount of time to find the best suited optimization algorithm and its optimal hyperparameters (such as learning rate, batch size etc.) for training a model to the desired accuracy, this being a major challenge for academicians and industry practitioners alike. Usually, such tuning is done by initial configuration optimization through grid search or random search (Bergstra et al., 2011; Snoek et al., 2012; Thornton et al.) . Recent works have also formulated it as a bandit problem (Li et al., 2017). However, it has been widely demonstrated that hyperparameters, especially the learning rate often needs to be dynamically adjusted as the training progresses, irrespective of the initial choice of configuration. If not adjusted dynamically, the training might get stuck in a bad minima, and no amount of training time can recover it. In this work, we focus on learning rate which is the foremost hyperparameter that one seeks to tune when training a deep learning model to get favourable results. Certain auto-tuning and adaptive variants of SGD, such as AdaGrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015) among others have been proposed that automatically adjust the learning rate as the training progresses, us- ing functions of gradient. Yet others have proposed fixed learning rate and/or batch size change regimes (Goyal et al., 2017), (Smith et al., 2018) for certain data set and model combination. In addition to traditional natural learning tasks where a good LR regime might already be known from past experiments, adversarial training for generating robust models is gaining a lot of pop- ularity off late. In these cases, tuning the LR would generally require time consuming multiple experiments, since the LR regime is unlikely to be known for every attack for every model and dataset of interest 1 . Moreover, new models are surfacing every day courtesy the state-of-the-art model synthesis systems, and new datasets are also becoming available quite often in different do- mains such as healthcare, automobile industy etc. In each of these cases, no prior LR regime would be known, and would require considerable manual tuning in the absence of a universal method, with demonstrated effectiveness over a wide range of tasks, models and datasets. Wilson et al. (2017) observed that solutions found by existing adaptive methods often generalize worse than those found by non-adaptive methods. Even though initially adaptive methods might Under review as a conference paper at ICLR 2020 display faster initial progress on the training set, their performance quickly plateaus on the test set, and learning rate tuning is required to improve the generalization performance of these methods. For the case of SGD with Momentum, learning rate (LR) step decay is very popular (Goyal et al., 2017),(Huang et al., 2017), ReduceLRonPlateau 2 . However, in certain junctures of training, in- creasing the LR can potentially lead to a quick, further exploration of the loss landscape and help the training to escape a sharp minima (having poor generalisation Keskar et al. (2016)). Further, recent works have shown that the distance traveled by the model in the parameter space determines how far the training is from convergence Hoffer et al. (2017). This inspires the idea that increasing the LR to take bigger steps in the loss landscape, while maintaining numerical stability might help in better generalization. The idea of increasing and decreasing the LR periodically during training has been demonstrated by Smith (2017); Smith & Topin (2017) in their cyclical learning rate method (CLR). This has also been shown by Loshchilov & Hutter (2016), in Stochastic Gradient Descent with Warm Restarts (SGDR, popularly referred to as Cosine Annealing with Warm Restarts). In CLR, the LR is varied period- ically in a linear manner, between a maximum and a minimum value, and it is shown empirically that such increase of learning rate is overall beneficial to the training compared to fixed schedules. In SGDR, the training periodically restarts from an initial learning rate, and then decreases to a min- imum learning rate through a cosine schedule of LR decay. The period typically increases in powers of 2. The authors suggest optimizing the initial LR and minimum LR for good performance. Schaul et al. (2013) had suggested an adaptive learning rate schedule that allows the learning rate to increase when the signal is non-stationary and the underline distribution changes. This is a com- putationally heavy method, requiring computing the Hessian in an online manner. Recently, there has been some work that explore gradients in different forms for hyperaparameter optimization. Maclaurin et al. (2015) suggest an approach by which they exactly reverse SGD with momentum to compute gradients with respect to all continuous learning parameters (referred to as hypergradients); this is then propagated through an inner optimization. Baydin et al. (2018) suggest a dynamic LR-tuning approach, namely, hypergradient descent, that apply gradient-based updates to the learning rate at each iteration in an online fashion. We propose a new algorithm to automatically determine the learning rate for a deep learning job in an autonomous manner that simply compares the current training loss with the best observed thus far to adapt the LR. The proposed algorithm works across multiple datasets and models for different tasks such as natural as well as adversarial training. It is an 'optimistic' method, in the sense that it increases the LR to as high as possible by examining the training loss repeatedly. We show through rigorous experimentation that in spite of its simplicity, the proposed algorithm performs surprisingly well as compared to the state-of-the-art.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper provides theoretical foundations for the regularisation technique known as dropout. It is shown that the dropout objective is a common lower bound on the objectives of a family of infinitely many models, which includes models corresponding to the three methods of evaluation: the arithmetic averaging, the geometric averaging, and the deterministic. This allows for model selection at validation time by evaluating the different methods of making predictions corresponding to individual models in the family. Additionally, it is demonstrated that the trained model is best viewed as deterministic, not as a stochastic model with a deterministic approximation. Finally, a cheap approximation to the bias of this model is used to get better results from model tuning.",
        "Abstract": "We push on the boundaries of our knowledge about dropout by showing theoretically that dropout training can be understood as performing MAP estimation concurrently for an entire family of conditional models whose objectives are themselves lower bounded by the original dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. The deterministic subvariant's bound is equal to its objective, and the highest amongst these models. It also exhibits the best model fit in our experiments. Together, these results suggest that the predominant view of deterministic dropout as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.",
        "Introduction": "  INTRODUCTION The regularisation technique known as dropout underpins numerous state-of-the-art results in deep learning ( Hinton et al. 2012 ;  Srivastava et al. 2014 ), and its application has received much attention in the form of optimisation ( Wang & Manning 2013 ) and attempts at explaining or improving its approximation properties ( Baldi & Sadowski 2013 ;  Zolna et al. 2017 ;  Ma et al. 2016 ). The dominant perspective today views dropout as either an implicit ensemble method ( Warde-Farley et al. 2013 ) or averaging over an approximate Bayesian posterior ( Gal & Ghahramani 2016a ). Regardless of which view we take, dropout training is carried out the same way, by minimising the expectation of the loss over randomly sampled dropout masks. However, at test time these views naturally lead to different algorithms: the Bayesian approach computes an arithmetic average as it marginalises out the weight uncertainty, while the ensemble approach typically uses the geometric average due to its close relationship to the loss. Collectively they are called MC dropout and neither is clearly better than the other ( Warde-Farley et al. 2013 ). A third way to make predictions is to \"turn dropout off\", that is, propagate expected values through the network in a single, deterministic pass. This deterministic (also known as standard) dropout is considered to be an excellent approximation to MC dropout. This situation is unsatisfactory as it does not provide theoretical grounding for dropout, without which the choice of dropout variant remains arbitrary. In this paper, we provide such theoretical foundations. First, we prove the dropout objective to be a common lower bound on the objectives of a family of infinitely many models. This family includes models corresponding to the three aforementioned methods of evaluation: the arithmetic averaging, the geometric averaging, and the deterministic. Thus by maximising the dropout objective we get a single set of parameters and many models that all have the same parameters but differ in how they make predictions. This allows us to train once and perform model selection at validation time by evaluating the different methods of making predictions corresponding to individual models in the family. Second, we turn the conventional perspective on its head by showing that while dropout training performs stochastic regularisation, the trained model is best viewed as deterministic, not as a stochastic model with a deterministic approximation. This paper is structured as follows. In §2, we revisit variational dropout ( Gal & Ghahramani 2016a ) and demonstrate that, despite common perception, sharing of masks is not necessary, neither in theory nor in practice. Then, by recasting dropout in a simple conditional form, we highlight the counterintuitive role played by the variational posterior. §3 contains our main contributions. Here we construct a family of conditional models whose MAP objectives are all lower bounded by the usual dropout objective, and identify a member of this family as best in terms of model fit. In §4, we select Under review as a conference paper at ICLR 2020 the best of this family in terms of generalisation to improve language modelling. Finally, creating a cheap approximation to the bias of this model allows us to get better results from model tuning.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Neural networks (NNs) have achieved state of the art performance across various domains and tasks, yet many aspects of them are not well understood. Recent advances in understanding the training and evaluation behavior of neural networks in the infinitely wide limit and in the strongly overparametrized regime have expressed the behavior of the neural network in terms of the neural tangent kernel (NTK). Belkin et al. (2018b) suggested a new picture of learning in the overparametrized regime, introducing an \"interpolating kernel method\" which successfully learns in this regime. This paper suggests that if overparametrized NNs are indeed linked to such kernel methods, then zero (or small) training error alone might not tell us much about the test performance, as the measure of function complexity could be determined by the NN architecture, initialization, and the optimization method.",
        "Abstract": "The recently developed link between strongly overparametrized neural networks (NNs) and kernel methods has opened a new way to understand puzzling features of NNs, such as their convergence and generalization behaviors. In this paper, we make the bias of initialization on strongly overparametrized NNs under gradient descent explicit. We prove that fully-connected wide ReLU-NNs trained with squared loss are essentially a sum of two parts: The first is the minimum complexity solution of an interpolating kernel method, while the second contributes to the test error only and depends heavily on the initialization. This decomposition has two consequences: (a) the second part becomes negligible in the regime of small initialization variance, which allows us to transfer generalization bounds from minimum complexity interpolating kernel methods to NNs; (b) in the opposite regime, the test error of wide NNs increases significantly with the initialization variance, while still interpolating the training data perfectly. Our work shows that -- contrary to common belief -- the initialization scheme has a strong effect on generalization performance, providing a novel criterion to identify good initialization strategies.",
        "Introduction": "  INTRODUCTION Neural networks (NNs) have celebrated many successes over the past decade and achieved state of the art performance across various domains and tasks. From a theoretical standpoint, however, many aspects of neural networks are not well understood, as for example illustrated by  Zhang et al. (2016) . Neural networks seem to contradict classical learning theory as, in many scenarios, they are able to fit random labels perfectly while still generalizing well when trained on the true labels. In addition, overparametrized neural networks frequently exhibit even improved test performance when the number of parameters is increased further ( Belkin et al., 2019 ). NN models thus often seem to avoid overfitting. Very recently there have been advances in understanding the training and evaluation behavior of neural networks in the infinitely wide limit ( Jacot et al., 2018 ;  Hayou et al., 2019 ) and also in the strongly overparametrized regime ( Du et al., 2018b ;  Li & Liang, 2018 ;  Allen-Zhu et al., 2018a ), which is close to the infinite limit NN. Both lines of work express the behavior of the neural network in terms of the so-called neural tangent kernel (NTK). In particular,  Du et al. (2018b)  showed in this way that, under mild conditions, strongly over-parametrized neural networks converge to a global minimum of zero training error. In another line of research,  Belkin et al. (2018b)  suggested a new picture of learning in the over- parametrized regime, introducing an \"interpolating kernel method\" which successfully learns in this regime. This method selects the least complex function that interpolates all data points perfectly, as opposed to traditional methods which balance the function's complexity against its goodness-of-fit. It is suggested that this picture of overparametrized learning could help understand neural networks. If overparametrized NNs are indeed linked to such kernel methods, then zero (or small) training error alone might not tell us much about the test performance, as the measure of function complexity could be determined by the NN architecture, initialization, and the optimization method. A bad NN design may lead to an unfavorable complexity measure that could result in a large generalization gap.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Colored Local Iterative Procedure 1 (CLIP), a novel node coloring scheme that improves the flexibility and power of Message Passing Neural Networks (MPNNs). CLIP is the first provably universal extension of MPNNs, and is shown to achieve state-of-the-art results on benchmark datasets while significantly outperforming traditional MPNNs and recent methods on graph property testing. The paper provides a precise mathematical definition for universal graph representations, as well as a general mechanism to design them using separable neural networks.",
        "Abstract": "In this paper, we show that a simple coloring scheme can improve, both theoretically and empirically, the expressive power of Message Passing Neural Networks (MPNNs). More specifically, we introduce a graph neural network called Colored Local Iterative Procedure (CLIP) that uses colors to disambiguate identical node attributes, and show that this representation is a universal approximator of continuous functions on graphs with node attributes. Our method relies on separability, a key topological characteristic that allows to extend well-chosen neural networks into universal representations. Finally, we show experimentally that CLIP is capable of capturing structural characteristics that traditional MPNNs fail to distinguish, while being state-of-the-art on benchmark graph classification datasets.",
        "Introduction": "  INTRODUCTION Learning good representations is seen by many machine learning researchers as the main reason behind the tremendous successes of the field in recent years ( Bengio et al., 2013 ). In image analy- sis ( Krizhevsky et al., 2012 ), natural language processing ( Vaswani et al., 2017 ) or reinforcement learning ( Mnih et al., 2015 ), groundbreaking results rely on efficient and flexible deep learning architectures that are capable of transforming a complex input into a simple vector while retaining most of its valuable features. The universal approximation theorem ( Cybenko, 1989 ;  Hornik et al., 1989 ;  Hornik, 1991 ;  Pinkus, 1999 ) provides a theoretical framework to analyze the expressive power of such architectures by proving that, under mild hypotheses, multi-layer perceptrons (MLPs) can uniformly approximate any continuous function on a compact set. This result provided a first theoret- ical justification of the strong approximation capabilities of neural networks, and was the starting point of more refined analyses providing valuable insights into the generalization capabilities of these architectures ( Baum and Haussler, 1989 ;  Geman et al., 1992 ;  Saxe et al., 2014 ;  Bartlett et al., 2018 ). Despite a large literature and state-of-the-art performance on benchmark graph classification datasets, graph neural networks yet lack a similar theoretical foundation ( Xu et al., 2019 ). Universality for these architectures is either hinted at via equivalence with approximate graph isomorphism tests (k-WL tests in  Xu et al. 2019 ;  Maron et al. 2019a ), or proved under restrictive assumptions (finite node attribute space in  Murphy et al. 2019 ). In this paper, we introduce Colored Local Iterative Procedure 1 (CLIP), which tackles the limitations of current Message Passing Neural Networks (MPNNs) by showing, both theoretically and experimentally, that adding a simple coloring scheme can improve the flexibility and power of these graph representations. More specifically, our contributions are: 1) we provide a precise mathematical definition for universal graph representations, 2) we present a general mechanism to design universal neural networks using separability, 3) we propose a novel node coloring scheme leading to CLIP, the first provably universal extension of MPNNs, 4) we show that CLIP achieves state of the art results on benchmark datasets while significantly outperforming traditional MPNNs as well as recent methods on graph property testing. The rest of the paper is organized as follows: Section 2 gives an overview of the graph representation literature and related works. Section 3 provides a precise definition for universal representations, as well as a generic method to design them using separable neural networks. In Section 4, we show that most state-of-the-art representations are not sufficiently expressive to be universal. Then, using the analysis of Section 3, Section 5 provides CLIP, a provably universal extension of MPNNs. Finally, Under review as a conference paper at ICLR 2020 Section 6 shows that CLIP achieves state-of-the-art accuracies on benchmark graph classification taks, as well as outperforming its competitors on graph property testing problems.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents the Domain Invariant Variational Autoencoder (DIVA), a deep generative model for domain generalization. DIVA extends the variational autoencoder (VAE) framework by introducing independent latent representations for a domain label, a class label and any residual variations in the input. This partitioning of the latent space encourages and guides the model to disentangle these sources of variation. DIVA is evaluated on a version of the MNIST dataset and a Malaria Cell Images dataset, and is shown to improve performance in both supervised and semi-supervised settings.",
        "Abstract": "We consider the problem of domain generalization, namely, how to learn representations given data from a set of domains that generalize to data from a previously unseen domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative model that tackles this problem by learning three independent latent subspaces, one for the domain, one for the class, and one for any residual variations. We highlight that due to the generative nature of our model we can also incorporate unlabeled data from known or previously unseen domains. To the best of our knowledge this has not been done before in a domain generalization setting. This property is highly desirable in fields like medical imaging where labeled data is scarce. We experimentally evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset where we show that (i) the learned subspaces are indeed complementary to each other, (ii) we improve upon recent works on this task and (iii) incorporating unlabelled data can boost the performance even further.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) led to major breakthroughs in a variety of areas like computer vision and natural language processing. Despite their big success, recent research shows that DNNs learn the bias present in the training data. As a result they are not invariant to cues that are irrelevant to the actual task ( Azulay & Weiss, 2018 ). This leads to a dramatic performance decrease when tested on data from a different distribution with a different bias. In domain generalization the goal is to learn representations from a set of similar distributions, here called domains, that can be transferred to a previously unseen domain during test time. A common motivating application, where domain generalization is crucial, is medical imaging ( Blanchard et al., 2011 ;  Muandet et al., 2013 ). For instance, in digital histopathology a typical task is the classification of benign and malignant tissue. However, the preparation of a histopathology image includes the staining and scanning of tissue which can greatly vary between hospitals. Moreover, a sample from a patient could be preserved in different conditions ( Ciompi et al., 2017 ). As a result, each patient's data could be treated as a separate domain ( Lafarge et al., 2017 ). Another problem commonly encountered in medical imaging is class label scarcity. Annotating medical images is an extremely time consuming task that requires expert knowledge. However, obtaining domain labels is surprisingly cheap, since hospitals generally store information about the patient (e.g., age and sex) and the medical equipment (e.g., manufacturer and settings). Therefore, we are interested in extending the domain generalization framework to be able to deal with additional unlabeled data, as we hypothesize that it can help to improve performance. In this paper, we propose to tackle domain generalization via a new deep generative model that we refer to as the Domain Invariant Variational Autoencoder (DIVA). We extend the variational autoencoder (VAE) framework ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ) by introducing independent latent representations for a domain label, a class label and any residual variations in the input x. Such partitioning of the latent space will encourage and guide the model to disentangle these sources of variation. Finally, by virtue of having a generative model we can naturally handle the semi-supervised scenario, similarly to  Kingma et al. (2014) . We evaluate our model on a version of the MNIST dataset where each domain corresponds to a specific rotation angle of the digits, as well as on a Malaria Cell Images dataset where each domain corresponds to a different patient. An implementation of DIVA can be found under (URL was removed to preserve anonymity).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. BBRT treats inference as a first-class citizen and can be applied to sequence- and graph-based models in the literature to produce state-of-the-art results on property optimization benchmark tasks. We explore various decoding strategies and introduce simple ranking methods to decide which outputs are fed back into the model. BBRT is an extensible tool for interpretable and user-centric molecular design applications.",
        "Abstract": "Machine learning algorithms for generating molecular structures offer a promising new approach to drug discovery. We cast molecular optimization as a translation problem, where the goal is to map an input compound to a target compound with improved biochemical properties. Remarkably, we observe that when generated molecules are iteratively fed back into the translator, molecular compound attributes improve with each step. We show that this finding is invariant to the choice of translation model, making this a \"black box\" algorithm. We call this method Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. This simple, powerful technique operates strictly on the inputs and outputs of any translation model. We obtain new state-of-the-art results for molecular property optimization tasks using our simple drop-in replacement with well-known sequence and graph-based models. Our method provides a significant boost in performance relative to its non-recursive peers with just a simple \"``for\" loop. Further, BBRT is highly interpretable, allowing users to map the evolution of newly discovered compounds from known starting points. ",
        "Introduction": "  INTRODUCTION Automated molecular design using generative models offers the promise of rapidly discovering new compounds with desirable properties. Chemical space is large, discrete, and unstructured, which together, present important challenges to the success of any molecular optimization campaign. Ap- proximately 10 8 compounds have been synthesized ( Kim et al., 2015 ) while the range of potential drug-like candidates is estimated to between 10 23 and 10 80 ( Polishchuk et al., 2013 ). Consequently, new methods for intelligent search are paramount. A recently introduced paradigm for compound generation treats molecular optimization as a transla- tion task where the goal is to map an input compound to a target compound with favorable properties ( Jin et al., 2019b ). This framework has presented impressive results for constrained molecular prop- erty optimization where generated compounds are restricted to be structurally similar to the source molecule. We extend this framework to unconstrained molecular optimization by treating inference, vis-à-vis decoding strategies, as a first-class citizen. We observe that generated molecules can be repeatedly fed back into the model to generate even better compounds. This finding is invariant to the choice of translation model, making this a \"black box\" algorithm. This invariance is particularly attrac- tive considering the recent emphasis on new molecular representations ( Gómez-Bombarelli et al., 2018 ;  Jin et al., 2018 ;  Dai et al., 2018 ;  Li et al., 2018 ;  Kusner et al., 2017 ;  Krenn et al., 2019 ). Using our simple drop-in replacement, our method can leverage these recently introduced molecular representations in a translation setting for better optimization. We introduce Black Box Recursive Translation (BBRT), a new inference method for molecular property optimization. Surprisingly, by applying BBRT to well-known sequence- and graph-based models in the literature, we can produce new state-of-the-art results on property optimization bench- mark tasks. Through an exhaustive exploration of various decoding strategies, we demonstrate the empirical benefits of using BBRT. We introduce simple ranking methods to decide which outputs are fed back into the model and find ranking to be an appealing approach to secondary property opti- mization. Finally, we demonstrate how BBRT is an extensible tool for interpretable and user-centric molecular design applications.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to Continual Learning in the single-head scenario. We propose a method that combines a regularization term with a memory module to learn a sequence of datasets without forgetting the previously learned tasks. We evaluate our approach on the MNIST and CIFAR-10 datasets and show that our method outperforms the state-of-the-art methods.",
        "Abstract": "In the field of Continual Learning, the objective is to learn several tasks one after the other without access to the data from previous tasks. Several solutions have been proposed to tackle this problem but they usually  assume that the user knows which of the tasks to perform at test time on a particular sample, or rely on small samples from previous data and most of them suffer of a substantial drop in accuracy when updated with batches of only one class at a time. In this article, we propose a new method, OvA-INN, which is able to learn one class at a time and without storing any of the previous data. To achieve this, for each class, we train a specific Invertible Neural Network to output the zero vector for its class. At test time, we can predict the class of a sample by identifying which network outputs the vector with the smallest norm. With this method, we show that we can take advantage of pretrained models by stacking an invertible network on top of a features extractor. This way, we are able to outperform state-of-the-art approaches that rely on features learning for the Continual Learning of MNIST and CIFAR-100 datasets. In our experiments, we are reaching 72% accuracy on CIFAR-100 after training our model one class at a time.",
        "Introduction": "  INTRODUCTION A typical Deep Learning workflow consists in gathering data, training a model on this data and finally deploying the model in the real world ( Goodfellow et al., 2016 ). If one would need to update the model with new data, it would require to merge the old and new data and process a training from scratch on this new dataset. Nevertheless, there are circumstances where this method may not apply. For example, it may not be possible to store the old data because of privacy issues (health records, sensible data) or memory limitations (embedded systems, very large datasets). In order to address those limitations, recent works propose a variety of approaches in a setting called Continual Learning ( Parisi et al., 2018 ). In Continual Learning, we aim to learn the parameters w of a model on a sequence of datasets D i = {(x j i , y j i )} ni j=1 with the inputs x j i ∈ X i and the labels y j i ∈ Y i , to predict p(y * |w, x * ) for an unseen pair (x * , y * ). The training has to be done on each dataset, one after the other, without the possibility to reuse previous datasets. The performance of a Continual Learning algorithm can then be measured with two protocols : multi-head or single-head. In the multi-head scenario, the task identifier i is known at test time. For evaluating performances on task i, the set of all possible labels is then Y = Y i . Whilst in the single-head scenario, the task identifier is unknown, in that case we have Y = ∪ N i=1 Y i with N the number of tasks learned so far. For example, let us say that the goal is to learn MNIST sequentially with two batches: using only the data from the first five classes and then only the data from the remaining five other classes. In multi-head learning, one asks at test time to be able to recognize samples of 0-4 among the classes 0-4 and samples of 5-9 among classes 5-9. On the other hand, in single-head learning, one can not assume from which batch a sample is coming from, hence the need to be able to recognize any samples of 0-9 among classes 0-9. Although the former one has received the most attention from researchers, the last one fits better to the desiderata of a Continual Learning system as expressed in  Farquhar & Gal (2018)  and ( van de Ven & Tolias, 2019 ). The single-head scenario is also notoriously harder than its multi-head counterpart ( Chaudhry et al., 2018 ) and is the focus of the present work.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a methodology to extract second-order representations of data points to improve the performance of artificial intelligence algorithms on distorted images. We analyze the existing literature for classification under distortion and organize them into frameworks based on data dependency. We then discuss the experimental setup and results of our proposed methodology.",
        "Abstract": "Deep neural networks represent data as projections on trained weights in a high dimensional manifold. This is a first-order based absolute representation that is widely used due to its interpretable nature and simple mathematical functionality. However, in the application of visual recognition, first-order representations trained on pristine images have shown a vulnerability to distortions. Visual distortions including imaging acquisition errors and challenging environmental conditions like blur, exposure, snow and frost cause incorrect classification in first-order neural nets. To eliminate vulnerabilities under such distortions, we propose representing data points by their relative positioning in a high dimensional manifold instead of their absolute positions. Such a positioning scheme is based on a data point’s second-order property. We obtain a data point’s second-order representation by creating adversarial examples to all possible decision boundaries and tracking the movement of corresponding boundaries. We compare our representation against first-order methods and show that there is an increase of more than 14% under severe distortions for ResNet-18. We test the generalizability of the proposed representation on larger networks and on 19 complex and real-world distortions from CIFAR-10-C. Furthermore, we show how our proposed representation can be used as a plug-in approach on top of any network. We also provide methodologies to scale our proposed representation to larger datasets.",
        "Introduction": "  INTRODUCTION In recent years, artificial intelligence systems achieved state-of-the-art performances in image clas- sification tasks ( Russakovsky et al., 2015 )( Krizhevsky et al., 2012 )( He et al., 2016 ). Specifically, classification algorithms surpassed top-5 human error rate of 5.1% on ImageNet ( Russakovsky et al., 2015 ). Even though these advancements are promising, images in these datasets do not cover diverse real-world scenarios. For instance, ImageNet consists of photographs parsed from Flickr, a popular image hosting service. The images on Flickr are generally high quality since users tend not to share distorted photographs. Distortions may include perceptually unpleasant camera related issues like blur, motion blur, overexposure, underexposure, and noise. Moreover, environmental conditions such as rain, snow, and frost can affect the field of view. These non-ideal conditions impact the performance of artificial intelligence (AI) algorithms ( Dodge & Karam, 2017 )( Hendrycks & Dietterich, 2019 ). These AI algorithms are primarily driven by deep neural nets that learn non-linear transformations to obtain discriminate representation spaces. Deep neural networks are trained to transform a data point into a representation space where linear classifiers can discriminate between classes ( Goodfellow et al., 2016 )( Krizhevsky et al., 2012 )( He et al., 2016 ). All the hidden layers are supervised to maximize the interclass distance while minimizing the intraclass distance to obtain linearly separable representations in the last layer. We formulate the inherent mechanisms behind the classification process as follows: Let f be an L layered neural network trained to distinguish between N classes. If x is any input to the network, the output for a classification application is given by f (x) = y where y is a (N × 1) vector. The class of x is the index of the maxima of y. Consider only the final fully connected layer f L parameterized by weights (W L ) and bias (b L ). We obtain y as, Under review as a conference paper at ICLR 2020 where f L−1 is the flattened output of the network just before the final fully connected layer. All the data points that span f L−1 representation space should be linearly separable for a well trained f L . Note that Eq. 1 is a filtering operation between the weight vectors and the representation. Hence, the final fully connected layer in a network can be considered as a linear filter set with N filters onto which the representations (f L−1 ) are linearly projected ( Wang et al., 2019 ). Projection refers to a dot product between the data point and the filters. The filter W i L , ∀i ∈ [1, N ] that has the largest projection or is maximally correlated with the data point represents the corresponding class. In the above setting, the original data point x is represented by its projection intensity f L−1 (x). This representation can be analyzed as a first-order point process ( Dorai-Raj et al., 2001 ). As an example, consider cities in the USA. All cities can be located on a map using their latitudes and longitudes as shown in Fig.1a. This is an instance of directly using the intensity - in this case the latitude and longitude - to represent a data point - in this case a city. The first-order representation is widely used for its intuitive nature, interpretability and ease of mathematical operation. However, under distortions and perturbations, this representation is not stable ( Azulay & Weiss, 2018 )( Goodfellow et al., 2014 ). The authors in  Goodfellow et al. (2014)  show the instability of such systems with adversarial images. In  Azulay & Weiss (2018) , the authors demonstrate that when images are translated by a few pixels, the network output and the hidden layer representations change drastically. We show that this change holds for distortions as well in Fig.2. Consider an image taken from MNIST ( LeCun et al., 1998 ) dataset. The image is subjected to five levels of progressive blurring all of which are visualized in Fig.2a. Fig.2b shows the t-SNE ( Maaten & Hinton, 2008 ) feature visualization of first order point representations of corresponding f L−1 (x BlurLvl ) for 10000 images in MNIST test set. The individual shapes and absolute locations of each cluster get progressively distorted with increasing levels of blur. Hence, it is natural that a network trained on original data performs poorly on distorted data. An alternative to representing a point by its intensity is by representing its influence on every other point in the subspace ( Dorai-Raj et al., 2001 ). This is the second-order property of data points. Consider the same example as before from Fig.1a. However, instead of latitudes and longitudes, the pairwise distance between all cities is provided. A Multi-Dimensional Scaling (MDS) algorithm is used to obtain the relative positioning of the cities with respect to each other as shown in Fig.1b. MDS algorithms ( Mead, 1992 ) are a class of algorithms that use second-order point representations to translate pairwise distances of N objects into an abstract cartesian space. In Sections 2 and 3 , we provide an intuitive methodology to extract the second-order representation of any data point. In Section 4, we categorize the existing literature for classification under distortion and organize them into frameworks based on data dependency. We describe the experimental setup and discuss the results in Section 5 and conclude our work in Section 6.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a self-supervised machine translation (SS-NMT) approach that uses the internal representations of an emergent neural machine translation (NMT) system to select useful data for training. This method resembles self-paced learning (SPL) in that it uses the emerging model hypothesis to select samples online that fit into its space. The paper focuses on analysing the selected distribution of data -order, difficulty and closeness to the final task- without imposing it beforehand. The mutual supervision of the primary and auxiliary tasks leads to a self-induced curriculum, which is the subject of the paper's analysis.",
        "Abstract": "Self-supervised neural machine translation (SS-NMT) learns how to extract/select suitable training data from comparable (rather than parallel) corpora and how to translate, in a way that the two tasks support each other in a virtuous circle.  SS-NMT has been shown to be competitive with state-of-the-art unsupervised NMT. In this study we provide an in-depth analysis of the sampling choices the SS-NMT model takes during training. We show that, without it having been told to do so, the model selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) a denoising curriculum. We observe that the dynamics of the mutual-supervision of both system internal representation types is vital for the extraction and hence translation performance. We show that in terms of the human Gunning-Fog Readability index (GF), SS-NMT starts by extracting and learning from Wikipedia data suitable for high school (GF=10--11) and quickly moves towards content suitable for first year undergraduate students (GF=13).",
        "Introduction": "  INTRODUCTION Human learners, when faced with a new task, generally focus on simple examples before applying their gained knowledge on more complex instances. This approach to learning based on sampling from a curriculum of increasing complexity has also been shown to be beneficial for machines and has been named curriculum learning (CL) ( Bengio et al., 2009 ) by the machine learning community. Previous research on curriculum learning has focused on selecting the best distribution of data - i.e. order, difficulty and closeness to the final task- to train a system. In this approach, data is prepared for the system to ease the learning task. In this work, we follow a complementary approach: we design a system that selects by itself the data to be trained on, and we analyse the selected distribution of data -order, difficulty and closeness to the final task- without imposing it beforehand. Our method resembles self-paced learning (SPL) ( Kumar et al., 2010 ), in that it uses the emerging model hypothesis to select samples online that fit into its space as opposed to most curriculum learning approaches that rely on judgements by the target hypothesis, i.e. an external teacher ( Hacohen & Weinshall, 2019 ) to design the curriculum. The task explored in our work is machine translation (MT). In particular, we focus on self-supervised machine translation (SS-NMT) ( Ruiter et al., 2019 ), which exploits the internal representations of an emergent neural machine translation (NMT) system to select useful data for training, where each selection decision is dependent on the current state of the model. Self-supervised learning ( Raina et al., 2007 ;  Bengio et al., 2013 ) involves a primary task (PT), for which labelled data is not available, and an auxiliary task (AT) that enables the PT to be learned by exploiting supervisory signals within the data. In the case of SS-NMT, both tasks -data extraction and learning NMT- enable and enhance each other, such that this mutual supervision leads to a self-induced curriculum, which is the subject to our analysis.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a convolutional neural network (CNN) based system for the recognition of handwritten Amharic characters. The proposed system is trained with a dataset of handwritten Amharic characters and tested on a separate test set. The proposed system is compared with existing methods and shows improved performance. The proposed system is also compared with deep learning methods used for other languages and shows promising results.",
        "Abstract": "Amharic language is an official language of the federal government of the Federal Democratic Republic of Ethiopia. Accordingly, there is a bulk of handwritten Amharic documents available in libraries, information centres, museums, and offices. Digitization of these documents enables to harness already available language technologies to local information needs and developments. Converting these documents will have a lot of advantages including (i) to preserve and transfer history of the country (ii) to save storage space (ii) proper handling of documents (iv) enhance retrieval of information through internet and other applications. Handwritten Amharic character recognition system becomes a challenging task due to inconsistency of a writer, variability in writing styles of different writers, relatively large number of characters of the script, high interclass similarity, structural complexity and degradation of documents due to different reasons. In order to recognize handwritten Amharic character a novel method based on deep neural networks is used which has recently shown exceptional performance in various pattern recognition and machine learning applications, but has not been endeavoured for Ethiopic script. The CNN model is trained and tested our database that contains 132,500 datasets of handwritten Amharic characters. Common machine learning methods usually apply a combination of feature extractor and trainable classifier. The use of CNN leads to significant improvements across different machine-learning classification algorithms. Our proposed CNN model is giving an accuracy of 91.83% on training data and 90.47% on validation data.",
        "Introduction": "  Introduction In recent years, there is much interest in the area of handwritten documents recognition. Between the handwritten documents and printed documents, automatic handwritten document recognition is more challenging. Handwritten characters written by different persons are not identical and vary in both size and shape. Numerous variations in writ- ing styles of individual character make the recognition task difficult. The similarities in distinct character shapes, the overlaps, and the interconnections of the neighbouring characters further complicate the problem. Recognition is an area that covers various fields such as, face recognition, finger print recognition, image recognition, charac- ter recognition, numerals recognition, etc. Handwritten Character Recognition System  Sarkhel et al. (2016)  is an intelligent system able to classify handwritten Characters as human see. There have been different methods that are used for offline handwritten document recognition. In the conventional methods with features engineered manually and using different classification algorithms to classify the characters based on the extracted features. On the other hand, deep learning algorithms such as convolutional neural networks are able to do the feature extraction by themselves from the raw images of the handwritten document and classify characters on those features learned. Deep learning methods show better performance in different researchers work for handwritten recognition task. Among the deep learning methods convolutional neural networks, which is the one proposed for this research work, are the most commonly used algorithms. Deep Neural networks consist of input layer and multiple nonlinear hidden layers and output layer, so the number of connections and trainable parameters are very large. The deep neural network needs very large set of examples to prevent over fitting. One class type of Deep Neural Network with comparatively smaller set of parameters and easier to train is Convolutional Neural Network (CNN)  Liang et al. (2016) . CNN is a multi-layer feed-forward neural network that extracts features and properties from the input data (images or sounds). CNN is trained with neural network back-propagation algorithm. CNN have the ability to learn from high-dimensional com- plex inputs, nonlinear mappings from very large number of data (images or sounds)  Maitra et al. (2015) . The advantage of CNN is that it automatically extracts the salient features which are invariant and a certain degree to shift and shape distortions of the input characters  Shin et al. (2016) .Another major advantage of CNN is the use of shared weight in convolutional layers, which means that the same filter is used for each input in the layer. The share weight reduces parameter number and improves performance  Bai et al. (2015a) . Recently, Convolutional Neural Network (CNN)  Lecun & Bengio (1995)  is found efficient for handwritten character recognition due to its distinct features. CNNs add the new dimension for image classification systems and recognizing visual patterns directly from pixel images with minimal pre-processing. In addition, CNN automatically provides some degree of translation invariance. A CNN based model was tested on UNIPEN  Guyon et al. (1994)  English character dataset and found recognition rates of 93.7 percent and 90.2 percent for lowercase and up- percase characters,  respectivelyYuan et al. (2012) . Amharic language has its own alphabet which is significantly different from other alphabets such as Latin alphabet. Although the Ethiopic alphabet called Fidel has recently been standardized to have 435 characters. However the most commonly Ethiopic script used by Amharic has 265 characters including 27 labialized characters (characters representing two sounds) ሏ ሟ ሯ ሷ ሿ ቋ ቈ ቧ ቷ ቿ ኋ ኗ ኟ ኳ ኰ ዟ ዧ ዷ ጇ ጓ ጧ ጯ ጿ ፏ ጐ ኈ ኧ and 34 base characters with six orders representing derived vocal sounds of the base character, 21 symbols for numerals and 8 punctuation marks. There is no capital and lower case distinction. When we see the features of Amharic characters they have the following basic characteristics: Each symbol is written according to the sound it have when pronounced. Vowels are created by modifying the base characters in some form. The symbols are written in disconnected manner e.g ሀ, ሁ, ሂ, ሃ. The direction of writing the script is from left to right and top to bottom sequence. There is a proportional space between characters and words. The Amharic language alphabet is conveniently written in a tabular form of seven columns as shown in  Table 1  where each column corresponds to vocal sounds in the order of ä, u, i, a, e, @, and o. Several handwritten scriptures and documents written in this language are available on paper or on any other material. Converting the handwritten documents into digital forms helps us to process, share and store them in electronic form. The conventional way of convert- ing the handwritten Amharic documents in to an electronic form, which is done by typing on the keyboard, is very time consuming, error prone and tedious. Due to the keyboard layout for Amharic characters which takes an average of two keystrokes to write one Amharic character, the conventional way of converting handwritten Amharic documents will be very difficult. This emphasizes the need for an automatic hand- written Amharic character recognition system which converts handwritten texts into machine-readable code that can be accepted by a computer for further processing. Amharic handwriting recognition is challenging due to mainly two reasons. First, it has huge number of symbols compared to that of the alphabet system. Second, most characters are very similar in shape. This is because of the minimal modification performed to get order of a character. For instance, ብ comes from በ, ሳ comes from ሰ , and ሁ comes from ሀ . Most core characters also show similarities to one another. One common feature is a mark of palatalization which sets off palatal ሸ from ሰ, ቸ from ተ, ጀ from ደ, ኘ from ነ and so on. The interclass variability is even minimal in case of handwritings where mostly these modifications are forgotten or placed at a wrong position. Lack of standard way of writing aggravates the problem by increasing the intra-class variability. Nowadays, it is becoming increasingly important to have information in digital format for increased efficiency in data storage, retrieval and to make them available for users. Although a lot of work and research has been done for handwritten character recognition for other languages like English and Asian languages such as Japanese, Chinese and Korean, there is only a few research at- tempts at Amharic language. A few works has been reported in scientific literature related to the recognition of Amharic printed and handwritten document recognition. In  Assabie & Bigün (2011)  they develop a recognition system for Ethiopic script using direction field tensor mechanism. Their system is developed by extracting primitive structural features and their spatial relationship. Since there is no standard database for Ethiopic text they use thirty pages scanned image from newspaper, books and clean printouts. The achieved performance for their system is 87%. They did not consider handwritten documents in their dataset. In  Birhanu & Sethuraman (2015)  they have used ANN approach for recognition of real life documents. They collected their dataset from 'Addis Zemen' newspaper, Amharic Bible, 'Federal NegaritGazeta' newspaper and the fiction 'FikerEskeMekabir'. The performance of their system for a new test set is 11.40% which is not satisfactory and the proposed system is trained with printed documents rather than handwritten. In  Meshesha & Jawahar (2007)  they develop a system which uses a principal component and linear discriminant analysis followed by a decision directed acyclic graph based support vector machine based classifier. Existing methods including those discussed above for Amharic document recognition systems, employ manually designed feature extractor and learned classifier and most of them use printed documents rather than handwritten. It is not easy to design an optimal feature extractor for a particular application. Hence, the performance of these algorithms is not satisfactory. Various methods have been proposed and high recognition rates are reported for the handwritten recognition of other languages. In  Bai et al. (2015b)  they propose shared-hidden-layer deep convolutional neural network (SHL-CNN) for image character recognition. In SHL-CNN, the hidden layers are made common across characters from different languages, performing a universal feature extraction process that aims at learning common character traits existed in different languages such as strokes, while the final softmax layer is made language dependent, trained based on characters from the destination language only. The effectiveness of the learned SHL-CNN is verified on both English and Chinese image character recog- nition tasks, showing that the SHL-CNN can reduce recognition errors by 16-30% relatively compared with models trained by characters of only one language using conventional CNN, and by 35.7% relatively compared with state-of-the-art methods. A modified LeNet-5 which is one of common CNN model with special settings of the number of neurons in each layer and the connecting way between some layers is proposed by  Yuan et al. (2012)  for offline handwritten English character recognition. They used the UNIPEN lowercase and uppercase dataset in their experiments and at- tain a recognition rate of 93.7% and 90.2% for uppercase and lowercase respectively. Authors in  Wu et al. (2014)  proposed handwritten recognition method for Chinese character based on relaxation convolutional neural network (R-CNN) and alternately trained relaxation convolutional neural network. The relaxation convolutional layer in their model, unlike the traditional convolution layer, does not require neurons within a feature map to share the same convolutional kernel, endowing the neural network with more expressive power. Authors in  Zhong et al. (2015a)  applied multi-pooling and data augmentation with non-linear transformation to a convolutional neural network (CNN) for multi-font printed Chinese character recognition (PCCR). They propose a multi-pooling layer on top of the final convolutional layer; this approach is found to be robust to spatial layout variations and deformations in multi-font printed Chinese characters. Outstanding recognition rate of 94.38% is achieved by combining the multi-pooling and data augmentation techniques and 99.74% by applying the multi- pooling and data augmentation techniques with non-linear transformation jointly. In  Yang et al. (2015)  the authors proposed an enhancement of deep convolutional neural network for recognition of online handwritten Chinese character. The enhancement is done by incorporating a variety of domain specific knowledge, including deformation, non-linear normalization, imaginary strokes, path signature and 8-directional features. The contribution in this work is twofold. First the domain specific technologies are investigated and integrated with the deep convolutional neural network to form a composite network to achieve improved performance. Second, the resulting deep con- volutional neural networks with diversity in their domain knowledge are combined using a hybrid serial-parallel strategy. A promising accuracy of 97.20% and 96.87% are achieved using CASIA-OLHWDB1.0 and CASIA-OLHWDB1.1 dataset for Chinese character respectively. In  He et al. (2015)  an effective method to analyze the recog- nition confidence of handwritten Chinese character based on softmax regression score of a high performance convolutional network is studied. In  Zhong et al. (2015b)  authors proposed a deeper architecture of CNN algorithm by using streamlined version of GoogLeNet. They used the ICDAR 2013 offline Chinese character recognition system competition dataset. With incorporation of traditional directional feature maps the proposed GoogLeNet models achieve an accuracy of 96.35% and 96.74% as single and ensemble models respectively. A handwritten Hangul character recognition sys- tem using deep convolutional neural network by proposing several novel techniques to increase the performance and training speed of the networks is done by  Kim & Xie (2015) . In  Anil et al. (2015)  Malayalam handwritten character recognition using the convolutional neural network is developed and in their work they discussed the CNN is better than the conventional handcrafted feature extractor based systems. Deep learning based large scale handwritten Devanagari character recognition is proposed by  Acharya et al. (2015)  with focus on the use of dropout and dataset increment approach to increase the test accuracy of their deep learning model. A combination of four dif- ferent pattern analysis techniques are used to develop a powerful and efficient system for handwritten Telgu character recognition system is proposed by  Soman et al. (2013) . Their system embodies convolutional neural networks, principal component analysis, support vector machines and multi-classifier systems. As compared to the handwrit- ten automated character recognition system discussed above the Amharic character recognition system is the least studied subject both in the conventional handcrafted feature extractor based systems as well as deep learning based convolutional neural networks. In the proposed method, both the feature extraction and classification tasks are done through learning from labelled data. This method overcomes the problems faced by the existing methods. Visual recognition system using convolutional neural networks  Lecun et al. (1998)  have shown a significant improvement in recent years. Record-breaking results have been obtained using these methods. This has motivated the researcher to investigate the success of the CNN algorithms on this challenging problem. Visual recognition using convolutional neural networks enables us to train the complete system from end to end.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a contrastive framework to learn sentence embeddings by contrasting multiple linguistic representations. The model is trained to map close input sentences to close representations while separating unrelated sentences. The framework is based on the distributional hypothesis that words within similar context share similar meaning, and is extended to sentences with the assumption that different views of the same sentence should lead to close representation. The dependency trees are used as a linguistic framework to mathematically describe the compositional structure of a sentence. This framework has the potential to benefit from the availability of efficient parser tools for various languages, making it possible to obtain such information almost freely.",
        "Abstract": "In this work, we propose a self-supervised method to learn sentence representations with an injection of linguistic knowledge. Multiple linguistic frameworks propose diverse sentence structures from which semantic meaning might be expressed out of compositional words operations. We aim to take advantage of this linguist diversity and learn to represent sentences by contrasting these diverse views. Formally, multiple views of the same sentence are mapped to close representations. On the contrary, views from other sentences are mapped further. By contrasting different linguistic views, we aim at building embeddings which better capture semantic and which are less sensitive to the sentence outward form.\n",
        "Introduction": "  INTRODUCTION We propose to learn sentence embeddings by contrasting multiple linguistic representations. The motivation is to benefit from linguistic structures diversity to discard noises inherent to each repre- sentation. We aim at encoding high-level representations by aligning the underlying shared infor- mation from multiple views. As illustrated in  Figure 1 , we train our model with a contrastive framework which aims at map- ping close input sentences to close representations while separating unrelated sentences. In Natural Language Processing (NLP), this framework has been widely used to learn word representations ( Mikolov et al., 2013a ;b) for example. This model relies on the distributional hypothesis which conjectures that words within similar context share similar meaning. Such framework has also been extended to sentences with the similar hypothesis that the meaning can be inferred from the context sentences ( Logeswaran & Lee, 2018 ;  Kiros et al., 2015 ). We propose to extend this framework by assuming that different views of the same sentence should lead to close representation. We considered the dependency trees, a linguistic framework that de- scribes the compositional structure of a sentence. As illustrated in  Figure 1 , in this framework, the sentence is mathematically described as an oriented acyclic graph where the nodes are words and edges describe the relations between words. Such structure has benefited from an important attention in the NLP community and efficient parser tools for various languages are available, which makes it possible to obtain such information almost freely in the sense it does not require additional hand annotated data.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes DRGAN, a modified Generative Adversarial Network (GAN) framework, to estimate dose-response from observational data. DRGAN builds on the framework introduced in GANITE (Yoon et al., 2018) and modifies the GAN framework to generate unobserved counterfactual outcomes from a standard treatment-effect dataset. DRGAN also introduces novel architectures for the generator and discriminator networks, as well as a new semi-synthetic data simulation for use in the dose-response setting. Experiments show that DRGAN outperforms existing benchmarks.",
        "Abstract": "The problem of estimating treatment responses from observational data is by now a well-studied one. Less well studied, though, is the problem of treatment response estimation when the treatments are accompanied by a continuous dosage parameter. In this paper, we tackle this lesser studied problem by building on a modification of the generative adversarial networks (GANs) framework that has already demonstrated effectiveness in the former problem. Our model, DRGAN, is flexible, capable of handling multiple treatments each accompanied by a dosage parameter. The key idea is to use a significantly modified GAN model to generate entire dose-response curves for each sample in the training data which will then allow us to use standard supervised methods to learn an inference model capable of estimating these curves for a new sample. Our model consists of 3 blocks: (1) a generator, (2) a discriminator, (3) an inference block. In order to address the challenge presented by the introduction of dosages, we propose novel architectures for both our generator and discriminator. We model the generator as a multi-task deep neural network. In order to address the increased complexity of the treatment space (because of the addition of dosages), we develop a hierarchical discriminator consisting of several networks: (a) a treatment discriminator, (b) a dosage discriminator for each treatment. In the experiments section, we introduce a new semi-synthetic data simulation for use in the dose-response setting and demonstrate improvements over the existing benchmark models.",
        "Introduction": "  INTRODUCTION Most of the methods developed in the causal inference literature focus on learning the effects of binary or categorical treatments (Bertsimas et al., 2017; Alaa et al., 2017; Alaa & van der Schaar, 2017; Athey & Imbens, 2016; Wager & Athey, 2018; Yoon et al., 2018). These treatments, though, are often administered at a certain dosage which can take on continuous values (such as vasopressors (Döpp-Zemel & Groeneveld, 2013)). In medicine, using a high dosage of a drug can lead to toxic effects while using a low dosage can result in no effect on the patient outcome (Wang et al., 2017). Moreover, the dosage levels used when choosing between multiple treatments for a patient are crucial for the decision (Rothwell et al., 2018). While admissible dosage intervals for drugs are often determined from clinical trials (Cook et al., 2015), these trials often have a small number of patients and use simplistic mathematical models to assign dosage levels to patients that do not take into account patient heterogeneity (Ursino et al., 2017). After drugs are approved through clinical trials, observational data collected about different treatment dosages prescribed to a diverse set of patients offers us the opportunity to learn individualized responses. As the relationships between treatment dosage efficacy, toxicity and patient features become more complex, estimating dose-response from observational data becomes particularly important in order to identify optimal dosages for each patient. Fortunately, there is a wealth of observational data available in the medical domain from electronic health records (Henry et al., 2016). Learning from observational data already presents significant challenges in the binary treatment setting. As explained by Spirtes (2009), in an observational treatment-effect dataset, only the factual outcome is present (i.e. the outcome for the treatment that was actually given) - the counterfactual outcomes are not observed. This problem is exacerbated in the dose-response setting in which the number of counterfactuals is no longer even finite. Moreover, the treatment assignment is non-random Under review as a conference paper at ICLR 2020 and instead is assigned according to the features associated with each sample. Due to the continuous nature of the dosage parameter, adjusting for the bias in the dosage assignments is significantly more complex than for binary (or even multiple) treatments. Thus, standard methods for adjusting for treatment selection bias cannot be easily extended to handle bias in the dosage parameter. In this paper we address the problem of dose-response estimation from observational data by building on the framework introduced in GANITE (Yoon et al., 2018). The key idea is to modify the GAN framework (Goodfellow et al., 2014) to generate the unobserved counterfactual outcomes from a standard treatment-effect dataset. Already, GANITE presents a significant modification to the original GAN framework - rather than the discriminator discriminating between entirely real or entirely fake samples, the discriminator is attempting to pick out the real component from a vector containing the real (factual) outcome from the dataset and the fake (counterfactual) outcomes generated by the generator. We also inherit this key difference from a standard GAN, but in addition we must make further modifications to the original GANITE framework in order to address the dosage problem. A naive attempt to extend Yoon et al. (2018) to the dosage setting might involve trying to define a discriminator that takes as input an entire dose-response curve for each treatment from the generator (with the outcome for the observed treatment-dosage pair replacing the generated one) and that tries to determine the factual treatment-dosage pair. This fails for two reasons: (1) we do not wish to assume prior knowledge of the functional form of the dose-response curves and so will have access to the generated dose-response curves only by evaluating them at given points (and so \"entire\" dose-response curves cannot be passed to the discriminator); (2) substituting the generator output for the factual treatment-dosage pair with the factual outcome will almost always create a discontinuity in the response curve and thus the factual treatment-dosage pair would be very easy to identify. We overcome these two hurdles by defining a discriminator that, rather than acting on the entire dose-response curves, acts on a finite set of points from each curve, as shown in  Fig. 1 . From among the chosen points, the discriminator will then attempt to pick out the factual one. To ensure that the entire dose-response curve is well-estimated, we sample the set of points randomly each time an input would be passed to the discriminator. If we were to fix a set of points in advance to compare for all treatments and samples then only the outcomes associated with these dosage levels would be well estimated. As our discriminator will be taking as input a set of random dosage-outcome pairs, we need to condition its behaviour to be like that of a function on a set. In particular, we draw on ideas from Zaheer et al. (2017) to ensure that the discriminator acts as a function on sets and its output does not depend on the order in which the elements of the set are given as input. In addition, we model the generator as a multi-task deep network capable of taking dosages as an input; this gives us the flexibility to learn heterogeneous dose-response curves for the different treatments. We also develop a hierarchical discriminator which breaks down the job of the discriminator into determining the factual treatment and determining the factual dosage using separate networks. We show in the experiments section that this approach significantly improves performance and is more stable than using a single network discriminator. Our contributions in this paper are 3-fold: (1) we propose DRGAN, a significantly modified GAN framework, capable of dose-response estimation, (2) we propose novel architectures for each of our networks, (3) we propose a new semi-synthetic data simulation for use in the dose-response setting. We show, using semi-synthetic experiments, that our model outperforms existing benchmarks. Methods for estimating the outcomes of treatments with an exposure dosage parameter that only employ observational data make use of the generalized propensity score (GPS) (Imbens, 2000; Imai & Van Dyk, 2004; Hirano & Imbens, 2004) or build on top of balancing methods for multiple treatments. Schwab et al. (2019) developed a neural network based method to estimate counterfactuals for multiple treatments and continuous dosages. The proposed Dose Response networks (DRNets) in Schwab et al. (2019) consist of a three level architecture with shared layers for all treatments, multi-task layers for each treatment and additional multi-task layers for dosage sub-intervals. More specifically, for each treatment w, the dosage interval [a w , b w ] is subdivided into E equally sized sub-intervals and a multi-task head is added for each sub-interval. Their model architecture extends the one in Shalit et al. (2017) by adding the multi-task heads for the dosage strata. However, the main advantage of using multi-task heads for dosage intervals would be the added flexibility in the model to learn potentially very different functions over different regions of the dosage interval. DRNets does not determine the dosage intervals dynamically and thus much of this flexbility is lost. We demonstrate in our experiments that DRGAN outperforms both GPS and DRNets. For a discussion of works that address treatment-response estimation without a dosage parameter, see Appendix A. Note that for such methods we cannot treat the dosage as an additional input due to the bias associated with its assignment.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a comparison between two different mathematical formulations of Hierarchical Sparse Coding (HSC): Hierarchical Lasso (Hi-La) and 2-Layers Sparse Predictive Coding (2L-SPC). The paper investigates the effect of top-down connections of PC, the consequences in terms of computations and convergence, and the qualitative differences concerning the learned atoms. The paper experiments with 4 different databases and varying the sparsity of each layer. Results show that 2L-SPC outperforms Hi-La in terms of prediction error, number of iterations needed for the state variables to reach stability, and convergence during the dictionary learning stage. The paper also discusses the qualitative differences between the features learned by both networks.",
        "Abstract": "Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layerwise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called Sparse Deep Predictive Coding (SDPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the SDPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of Lasso layers. A 2-layered SDPC and a Hi-La networks are trained on 3 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by SDPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the SDPC is faster to converge than for the Hi-La model. Third, we show that the SDPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the SDPC features are more generic and informative.",
        "Introduction": "  INTRODUCTION Finding a \"efficient\" representation to model a given signal in a concise and efficient manner is an inverse problem that has always been central to the machine learning community. Sparse Coding (SC) has proven to be one of the most successful methods to achieve this goal. SC holds the idea that signals (e.g. images) can be encoded as a linear combination of few features (called atoms) drawn from a bigger set called the dictionary ( Elad, 2010 ). The pursuit of optimal coding is usually decom- posed into two complementary subproblems: inference (coding) and dictionary learning. Inference consists in finding an accurate sparse representation of the input data considering the dictionaries are fixed, it could be performed using algorithms like ISTA & FISTA ( Beck & Teboulle, 2009 ), Match- ing Pursuit ( Mallat & Zhang, 1993 ), Coordinate Descent ( Li & Osher, 2009 ), or ADMM ( Heide et al., 2015 ). Once the representation is inferred, one can learn the atoms from the data using methods like gradient descent ( Rubinstein et al., 2010 ;  Kreutz-Delgado et al., 2003 ;  Sulam et al., 2018 ), or online dictionary learning ( Mairal et al., 2009a ). Consequently, SC offers an unsupervised framework to learn simultaneously basis vectors (e.g. atoms) and the corresponding input repre- sentation. SC has been applied with success to image restoration ( Mairal et al., 2009b ), feature extraction ( Szlam et al., 2010 ) and classification ( Yang et al., 2011 ;  Perrinet & Bednar, 2015 ). Inter- estingly, SC is also a field of interest for computational neuroscientists.  Olshausen & Field (1997)  first demonstrated that adding a sparse prior to a shallow neural network was sufficient to account for the emergence of neurons whose Receptive Fields (RFs) are spatially localized, band-pass and oriented filters, analogous to those found in the primary visual cortex (V1) of mammals ( Hubel & Wiesel, 1962 ). Because most of the SC algorithms are limited to single-layer network, they cannot model the hierarchical structure of the visual cortex. However, few solutions have been proposed to tackle Hierarchical Sparse Coding (HSC) as a global optimization problem ( Sulam et al., 2018 ;  Makhzani & Frey, 2013 ; 2015; Aberdam et al., 2019; Sulam et al., 2019). These methods are looking for an optimal solution of HSC without considering their plausibility in term of neuronal implementation. Consequently, the quest for reliable HSC formulation that is compatible with a neural implementation remains open. Under review as a conference paper at ICLR 2020  Rao & Ballard (1999)  introduce the Predictive Coding (PC) to model the effect of the interaction of cortical areas in the visual cortex. PC intends to solve the inverse problem of vision by combining feedforward and feedback connections. In PC, feedback connection carries prediction of the neural activity of the lower cortical area while feedforward pass prediction error to the higher cortical area. In such a framework, neural population are updated to minimize the unexpected component of the neural signal ( Friston, 2010 ). PC has been applied for supervised object recognition ( Wen et al., 2018 ;  Han et al., 2018 ;  Spratling, 2017 ) or unsupervised prediction of future video frames ( Lotter et al., 2016 ). Interestingly, PC is flexible enough to introduce a sparse prior to each layer. Therefore, one can consider PC as a bio-plausible formulation of the HSC problem. This formulation is to confront with the other bio-plausible HSC formulation that consists of a stack of independent Lasso problems ( Sun et al., 2017 ). To the best of our knowledge, no study has compared these two mathe- matically different formulations of the same problem of optimizing the Hierarchical Sparse Coding of images. What is the effect of top-down connection of PC? What are the consequences in term of computations and convergence? What are the qualitative differences concerning the learned atoms? The objective of this study is to experimentally answer these questions and to show that the PC framework could be successfully used for improving solutions to HSC problems. We start our study by defining the two different mathematical formulations to solve the HSC problem: the Hierarchical Lasso (Hi-La) that consists in stacking Lasso sub-problems, and the 2-Layers Sparse Predictive Coding (2L-SPC) that leverages PC into a deep and sparse network of bi-directionally connected layers. To experimentally compare both models, we train the 2L-SPC and Hi-La networks on 4 different databases and we vary the sparsity of each layer. First, we compare the overall prediction error of the two models and we break it down to understand its distribution among layers. Second, we analyze the number of iterations needed for the state variables of each network to reach their stability. Third, we compare the convergence of both models during the dictionary learning stage. Finally, we discuss the qualitative differences between the features learned by both networks in light of their activation probability.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a novel approach to training machine learning models on large amounts of annotated training data collected through crowdsourcing. The approach merges the two steps of aggregating subjective, weak, or noisy annotations, and training machine learning models. At training time, along with learning a model that predicts the ground truth, the approach also learns models of the difficulty of each example and the competence of each annotator in a generalizable manner. The proposed method is formulated in a way that allows it to be extended such that it can also learn decision function estimators for the annotators. This approach can be effectively used for training on crowdsourced data as well as on weakly labeled data, and can be used within frameworks such as Snorkel to significantly improve their performance.",
        "Abstract": "Many machine learning systems today are trained on large amounts of human-annotated data. Annotation tasks that require a high level of competency make data acquisition  expensive, while the resulting labels are often subjective, inconsistent, and may contain a variety of human biases. To improve data quality, practitioners often need to collect multiple annotations per example and aggregate them before training models. Such a multi-stage approach results in redundant annotations and may often produce imperfect ``ground truth'' labels that limit the potential of training supervised machine learning models. We propose a new end-to-end framework that enables us to: (i) merge the aggregation step with model training, thus allowing deep learning systems to learn to predict ground truth estimates directly from the available data, and (ii) model difficulties of examples and learn representations of the annotators that allow us to estimate and take into account their competencies. Our approach is general and has many applications, including training more accurate models on crowdsourced data, ensemble learning, as well as classifier accuracy estimation from unlabeled data. We conduct an extensive experimental evaluation of our method on 5 crowdsourcing datasets of varied difficulty and show accuracy gains of up to 25% over the current state-of-the-art approaches for aggregating annotations, as well as significant reductions in the required annotation redundancy.",
        "Introduction": "  INTRODUCTION The rising popularity and recent success of deep learning has resulted in machine learning systems that rely on large amounts of annotated training data (LeCun et al., 2015; Wu et al., 2016; Gulshan et al., 2016; Esteva et al., 2017). The most common, scalable way to collect such large amounts of training data is through crowdsourcing (Howe, 2006). Crowdsourcing works well in simple settings where annotation tasks do not require domain expertise-for example, in object detection and recognition tasks in natural images and videos (e.g., Deng et al., 2009; Kovashka et al., 2016). However, annotation in specialized domains such as medical pathology requires a certain level of competency and expertise from the annotators which makes annotation expensive. Moreover, often times there is high rate of disagreement even between experts, which results in increasingly subjective and inconsistent labels (Elmore et al., 2015; Hutson et al., 2019). A typical approach to dealing with subjectivity is to treat each annotation as simply noisy, collect multiple redundant labels per example (e.g., from different annotators), and then aggregate them using majority voting or other more advanced techniques (e.g., Dawid & Skene, 1979; Carpenter, 2008; Liu et al., 2012; Bachrach et al., 2012; Zhou et al., 2015; Zhou & He, 2016) to obtain a single \"ground truth\" label. At the expense of redundancy, this results in better data quality and more accurate estimates of the ground truth. More recently, the emerging systems for data programming and weak supervision also internally rely on label aggregation techniques similar to methods used for solving the crowdsourcing problem. Snorkel (Ratner et al., 2017; Bach et al., 2019) is a popular such system and was designed for efficient and low-cost creation of large-scale labeled datasets using programmatically generated, so-called weak labels. However, as we show in our empirical evaluation none of these systems solve label aggregation effectively in the presence of high subjectivity. We argue that to become more effective, these methods need to make use of meta-data and other types of information that may be available about the data instances and the annotators labeling them. To this end, we propose a novel approach that allows us to train accurate predictive models of the ground truth directly on the non-aggregated imperfectly labeled data. Our method merges the two steps of: (i) aggregating subjective, weak, or noisy annotations, and (ii) training machine learning models. At training time, along with learning a model that predicts the ground truth, we also learn models of the difficulty of each example and the competence of each annotator in a generalizable manner (i.e., these models can make predictions for previously unseen examples and annotators). Our approach can be effectively used for training on crowdsourced data as well as on weakly labeled data, and also be used within frameworks such as Snorkel (Ratner et al., 2017; Bach et al., 2019) and significantly improve their performance. We propose a method that can: 1. Learn truth estimators: Learn functions representing the underlying ground truth, while impos- ing almost no constraints (as opposed to prior work). In fact, we are able to leverage the capacity of deep neural networks along with the interpretability provided by Bayesian models, in order to obtain highly expressive estimators of the underlying truth. 2. Learn quality estimators: Learn functions that estimate the quality of each annotator. When annotators can be described by some features (e.g., age, gender, location, etc. of an Amazon Mechanical Turk annotator, instead of just an ID), our quality estimators are able to generalize to new, previously unseen, predictors. Previous work only considered estimating accuracies of a fixed set of predictors, without being able to leverage any information we might have about them. Furthermore, in contrast to previous work, we are also able to predict the per-instance predictor comptencies (i.e., our method can determine whether a human annotator is an expert for a subset of queries, instead of just estimating his/her overall accuracy), which is done by learning dependencies between the instances and the annotators. Finally, our approach is able to distinguish between multiple different types of errors by estimating the full confusion matrix for each instance-predictor pair. 3. Be easily extended: The truth and quality estimators can take arbitrary functional forms and fully leverage the expressivity of deep neural networks. Both human annotators and machine learning classifiers may sometimes be unable to make predictions about certain aspects of the ground truth (e.g., human annotators may be unsure about what the correct answer to a question). The proposed method is formulated in a way that allows it to be extended such that it can also learn decision function estimators for the annotators (i.e., estimators that predict whether an annotator will be able to provide a prediction for a given data instance). These estimators can have significant implications for data annotation systems where the cost of querying annotators is high (e.g., when these annotators are highly qualified, such as doctors or other kinds of domain experts). This is because it allows for better matching annotators to instances, thus reducing the required amount of annotation redundancy. An overview of the proposed approach and model is shown in  Figure 1 , and a detailed description is provided in Section 3.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the prevalence of isomorphic graphs in graph classification tasks, and the implications of this on the performance of machine learning models. We analyze 54 graph data sets used in graph classification, and find that in most of them there are isomorphic graphs, with proportions varying from 100% to 0%. We investigate the causes of isomorphic graphs, and express an upper bound for the generalization gap through the Radamacher complexity of a classifier and the number of isomorphic graphs in a data set. We evaluate a classification model's performance on isomorphic instances, and open-source new cleaned data sets that contain only non-isomorphic instances with no noisy target labels. Our findings provide a model-agnostic way to artificially increase performance on several widely used data sets.",
        "Abstract": "In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments. ",
        "Introduction": "  INTRODUCTION Recently there has been an increasing interest in the development of machine learning models that operate on graph structured data. Such models have found applications in chemoinformatics ( Ralaivola et al. (2005) ;  Rupp & Schneider (2010) ;  Ferré et al. (2017) ) and bioinformatics ( Borg- wardt et al. (2005) ;  Kundu et al. (2013) ), neuroscience ( Sharaev et al. (2018) ;  Jie et al. (2016) ;  Wang et al. (2016) ), computer vision ( Stumm et al. (2016) ) and system security ( Li et al. (2016) ), natu- ral language processing ( Glavaš &Šnajder (2013) ), and others ( Kriege et al. (2019) ;  Nikolentzos et al. (2019) ). One of the popular tasks that encompasses these applications is graph classification problem for which many graph kernels and graph neural networks have been developed. One of the implicit assumptions that many practitioners adhere to is that models that can distin- guish isomorphic instances from non-isomorphic ones possess higher expressiveness in classifica- tion problem and hence much efforts have been devoted to incorporate efficient graph isomorphism methods into the classification models. As the problem of computing complete graph invariant is GI-hard ( Gärtner et al. (2003) ), for which no known polynomial-time algorithm exists, other heuris- tics have been proposed as a proxy for deciding whether two graphs are isomorphic. Indeed, from the early days topological descriptors such Wiener index ( Wiener (1947a ;b)) attempted to find a single number that uniquely identifies a graph. Later, graph kernels that model pairwise similarities between graphs utilized theoretical developments in graph isomorphism literature. For example, graphlet kernel ( Shervashidze et al. (2009) ) is based on the Kelly conjecture (see also  Kelly (1957) ), anonymous walk kernel ( Ivanov & Burnaev (2018) ) derives insights from the reconstruction proper- ties of anonymous experiments (see also  Micali & Allen Zhu (2016) ), and WL kernel ( Shervashidze et al. (2011a) ) is based on an efficient graph isomorphism algorithm. For sufficiently large k, k- dimensional WL algorithm includes all combinatorial properties of a graph ( Cai et al. (1992a) ), so one may hope its power is enough for the data set at hand. Since only for k = Ω(n) WL algorithm is guaranteed to distinguish all graphs (for which the running time becomes exponential; see also  Fürer (2017) ), in the general case WL algorithm can be used only as a strong baseline for graph isomorphism. In similar fashion, graph neural networks exploit graph isomorphism algorithms and Under review as a conference paper at ICLR 2020 have been shown to be as powerful as k-dimensional WL algorithm (see for example  Maron et al. (2019) ;  Xu et al. (2018) ;  Morris et al. (2019) ). Experimental evaluation reveals that models based on the theoretical constructions with high com- binatorial power such as WL algorithm performs better than the models without them such as Vertex histogram kernel ( Vishwanathan et al. (2010) ) on a commonly used data sets. This could add addi- tional bias to results of comparison of classification algorithms since the models could simply apply a graph isomorphism method (or an efficient approximation) to determine a target label at the infer- ence time. However, purely judging on the accuracy of the algorithms in such cases would imply an unfair comparison between the methods as it does not measure correctly generalization ability of the models on the new test instances. As we discover, indeed many of the data sets used in graph classification have isomorphic instances so much that in some of them the fraction of the unique non-repeating graphs is as low as 20% of the total size. This challenges previous experimental re- sults and requires understanding of how influential isomorphic instances on the final performance of the models. Our contributions are: • We analyze the quality of 54 graph data sets which are used ubiquitously in graph classifica- tion comparison. Our findings suggest that in the most of the data sets there are isomorphic graphs and their proportion varies from as much as 100% to 0%. Surprisingly, we also found that there are isomorphic instances that have different target labels suggesting they are not suitable for learning a classifier at all. • We investigate the causes of isomorphic graphs and show that node and edge labels are important to identify isomorphic graphs. Other causes include numerical attributes of nodes and edges as well as the sizes of the data set. • We express an upper bound for the generalization gap through the Radamacher complexity of a classifier and the number of isomorphic graphs in a data set. This bound presents the- oretical evidence on how weightning of each graph in the training influences classification accuracy. • We evaluate a classification model's performance on isomorphic instances and show that even strong models do not achieve optimal accuracy even if the instances have been seen at the training time. Hence we show a model-agnostic way to artificially increase performance on several widely used data sets. • We open-source new cleaned data sets that contain only non-isomorphic instances with no noisy target labels. We give a set of recommendations regarding applying new models that work with graph structured data.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a probabilistic approach to unify open set recognition with continual learning in a single deep model. This model architecture gives rise to a natural way of open set recognition with statistical outlier rejection on the basis of the approximate posterior in Bayesian inference. The model is shared across tasks and automatically expands a single linear classifier head with units for new classes, thus not requiring explicit task labels during inference. The approach is demonstrated to incrementally learn the classes of two image and one audio dataset, as well as cross-dataset scenarios across modalities, while allowing for forward and backward transfer due to weight-sharing. The model is also able to distinguish between unseen data from various datasets and data belonging to known tasks.",
        "Abstract": "We introduce a unified probabilistic approach for deep continual learning based on variational Bayesian inference with open set recognition. Our model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across tasks. The open set recognition bounds the approximate posterior by fitting regions of high density on the basis of correctly classified data points and balances open set detection with recognition errors. Catastrophic forgetting is significantly alleviated through generative replay, where the open set recognition is used to sample from high density areas of the class specific posterior and reject statistical outliers. Our approach naturally allows for forward and backward transfer while maintaining past knowledge without the necessity of storing old data, regularization or inferring task labels. We demonstrate compelling results in the challenging scenario of incrementally expanding the single-head classifier for both class incremental visual and audio classification tasks, as well as incremental learning of datasets across modalities.",
        "Introduction": "  INTRODUCTION Most machine learning systems make the closed world assumption and are predominantly trained according to the isolated learning paradigm, where data is available at all times and is indepen- dently and identically distributed. However, in the context of continual learning, where tasks and data arrive in sequence, neither of these two principles is desirable. A neural network that is trained exclusively on a new task's data forgets past knowledge and suffers from an early identified phe- nomenon commonly referred to as catastrophic forgetting (McCloskey & Cohen, 1989). Moreover, to overcome the closed world assumption, inclusion of a \"background\" class is veritably insufficient as it is impossible to include all unseen concepts and classes explicitly in the loss function before- hand. Likewise, commonly applied thresholding of prediction values doesn't prevent resulting large confidences for unseen classes if the data is far away from any known data (Matan et al., 1990). Most of the existing continual learning literature concentrates efforts on either alleviating catas- trophic forgetting, maximizing knowledge transfer or addressing ways in which to efficiently store subsets of past data. These works have identified weight regularization (McCloskey & Cohen, 1989; Zenke et al., 2017; Kirkpatrick et al., 2017; Li & Hoiem, 2016; Nguyen et al., 2018) and rehearsal techniques (Ratcliff, 1990; Lopez-Paz & Ranzato, 2017; Rebuffi et al., 2017; Bachem et al., 2015) or have postulated methods based on complementary learning systems theory (O'Reilly & Norman, 2003) through dual-model with generative memory approaches (Gepperth & Karaoguz, 2016; Shin et al., 2017; Wu et al., 2018; Farquhar & Gal, 2018; Achille et al., 2018) as mechanisms against catastrophic inference. On the one hand, regularization techniques can work well in principle, but come with the caveat of relying on a new task's proximity to previous knowledge. On the other hand, training and storing separate models, including generative models for generative rehearsal, comes at increased memory cost and doesn't allow for full knowledge sharing, particularly to already stored models. Specifically, the transfer of already attained knowledge to benefit new tasks, known as for- ward transfer, as well as the potential positive impact of learning new concepts to aid in existing tasks, known as backward transfer, are crucial to any continual learning system. Generally speak- Under review as a conference paper at ICLR 2020 ing, most current approaches include a set of simplifications, such as considering separate classifiers for each new task, referred to as multi-head classifiers. This scenario prevents \"cross-talk\" between classifier units by not sharing them, which would otherwise rapidly decay the accuracy (Zenke et al., 2017; Kirkpatrick et al., 2017; Rusu et al., 2016; Shin et al., 2017; Gepperth & Karaoguz, 2016; Re- buffi et al., 2017; Achille et al., 2018; Nguyen et al., 2018) as newly introduced classes directly impact and confuse existing concepts. In the multi-head scenario task ids thus need to be encoded or are often assumed to be given by humans in order to know which classifier to use for prediction. Correspondingly, in generative replay, generative and discriminative models are taken to be separate models (Shin et al., 2017; Farquhar & Gal, 2018; Nguyen et al., 2018). Similar to regularization of a classifier, a generative model can suffer from the learned approximate posterior distribution devi- ating further from the true posterior with each further task increment. In order to avoid catastrophic forgetting induced by learning to generate on previously generated data, previous works even store a separate generative model per task (Farquhar & Gal, 2018), in analogy to the multi-head classifier. An extended review of recent continual learning methods is provided by Parisi et al. (2019). A parallel thread pursues a complementary component of identifying out-of-distribution and open set examples. While current continual learning approaches typically do not include this thread yet, it can be considered crucial to any system and a necessity in order to avoid encoding task labels and distinguishing seen from unknown data. Here, multiple methods rely on using confidence values as means of rejection through calibration (Liang et al., 2018; Lee et al., 2018b;a). Arguably this also includes Bayesian approaches using variational methods (Farquhar & Gal, 2018; Achille et al., 2018) or Monte-Carlo dropout (Gal & Ghahramani, 2015) to estimate uncertainties. Since the closed world assumption also holds true for Bayesian methods as the approximated posterior probability cannot be computed for unknown classes, misclassification still occurs, as the open space risk is unbounded (Boult et al., 2019). Recently Thomas et al. (2014); Bendale & Boult (2016); Dhamija et al. (2018) have proposed extreme value theory (EVT) based open set recognition to bound the open-space risk and balance it with recognition errors in deep neural networks. In this work we propose a probabilistic approach to unify open set recognition with continual learn- ing in a single deep model in order to remove or alleviate above mentioned common simplifications. Specifically, our contributions are: • We introduce a single model for continual learning that combines a joint probabilistic en- coder with a generative model and a linear classifier. Inspired by EVT based open set recognition (Bendale & Boult, 2016) for Softmax prediction layers, this model architecture gives rise to a natural way of open set recognition with statistical outlier rejection on the basis of the approximate posterior in Bayesian inference. The latter requires no upfront knowledge of open set data or corresponding modifications to loss or training procedure and can successfully prevent nonsensical predictions for unseen unknown data, a robust- ness feature that is currently not present in closed world continual learning systems. • We show how this EVT bound to the posterior can be used for both identification and rejection of statistically outlying unseen unknown data instances, as well as exclusion of generated samples from areas of low probability density. When used in generative replay this leads to significantly reduced catastrophic forgetting without storing real data. • We share our model across tasks and automatically expand a single linear classifier head with units for new classes, thus not requiring explicit task labels during inference. • We demonstrate that our approach can incrementally learn the classes of two image and one audio dataset, as well as cross-dataset scenarios across modalities, while allowing for forward and backward transfer due to weight-sharing. When presented with novel data our model is able to distinguish between unseen data from various datasets and data belong- ing to known tasks. We further show that our approach readily profits from recent model advances such as variational lossy auto-encoders (Gulrajani et al., 2017; Chen et al., 2017).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel algorithm, Way Off-Policy learning, which uses KL-control from a pre-trained prior model to reduce extrapolation error in batch reinforcement learning (BRL). Experiments show the effectiveness of WOP above strong baselines on both traditional RL tasks and on the challenging problem of open-domain dialog generation. Additionally, novel conversation rewards based on how human preferences are implicitly expressed in text are proposed. This work is the first to learn from implicit signals in conversation offline using batch RL.",
        "Abstract": "Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. This is a critical shortcoming for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms which use KL-control to penalize divergence from a pre-trained prior model of probable actions. This KL-constraint reduces extrapolation error, enabling effective offline learning, without exploration, from a fixed batch of data. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. This Way Off-Policy (WOP) algorithm is tested on both traditional RL tasks from OpenAI Gym, and on the problem of open-domain dialog generation; a challenging reinforcement learning problem with a 20,000 dimensional action space. WOP allows for the extraction of multiple different reward functions post-hoc from collected human interaction data, and can learn effectively from all of these. We test real-world generalization by deploying dialog models live to converse with humans in an open-domain setting, and demonstrate that WOP achieves significant improvements over state-of-the-art prior methods in batch deep RL.\n",
        "Introduction": "  INTRODUCTION In order to scale deep reinforcement learning (RL) to safety-critical, real-world domains, two abil- ities are needed. First, since collecting real-world interaction data can be expensive and time- consuming, algorithms must be able to learn from off-policy data no matter how it was generated, or how little correlation between the data distribution and the current policy. Second, it is often necessary to carefully test a policy before deploying it to the real world; for example, to ensure its behavior is safe and appropriate for humans. Thus, the algorithm must be able to learn offline first, from a static batch of data, without the ability to explore. This off-policy, batch reinforcement learning (BRL) setting represents a challenging RL problem. Most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy ( Fujimoto et al., 2018b ). Even models based on off-policy algorithms like Q-learning fail to learn in the offline, batch setting, when the model is not able to explore. If the batch data is not sufficient to cover the state-action space, BRL models can suffer from extrapolation error, learning unrealistic value estimates of state-action pairs not contained in the batch ( Fujimoto et al., 2018b ). It can be impossible to correct for extrapolation error when there is a mismatch in the distribution of state- actions pairs in the batch data, and the distribution induced by the learned policy. For example, if the policy learns to select actions which are not contained in the batch, it cannot learn a reasonable value function for those actions.  Figure 1  illustrates this concept, where the batch only covers a subset of possible policies. Extrapolation error is particularly problematic in high-dimensional state and action spaces (such as those inherent in language generation). We propose to resolve these issues by leveraging a pre-trained generative model of the state-action space, p(a|s), trained on known sequences of interaction data. While training with RL, we penalize divergence from this prior model with different forms of KL-control. This technique ensures that the RL model learns a policy that stays close the state-action distribution of the batch, combating Under review as a conference paper at ICLR 2020 extrapolation error. We also propose using dropout to obtain uncertainty estimates of the target Q- values, and use this lower bound to alleviate overestimation bias. We benchmark against a discrete adaptation of Batch Constrained Q-learning (BCQ) ( Fujimoto et al., 2018b ), a recently proposed state-of-the-art BRL algorithm for continuous domains, and show that our Way Off-Policy algorithm achieves superior performance in both a traditional RL domain, as well as in a challenging, under- explored, real-world reinforcement learning problem: using implicitly expressed human reactions in chat to improve open-domain dialog systems. When a machine learning system interacts with humans, ideally we would like to learn about the humans' preferences in order to improve its performance. Yet having humans manually indicate their preferences through explicit means like pressing a button (e.g.  Christiano et al. (2017) ) or submitting a feedback report, does not scale. Instead, we would like to be able to use humans' implicit reactions, such as the sentiment they express, or the length of the conversation, in order to improve the policy. However, applying off-policy batch RL to language generation is challenging because the number of potential combinations of words and sentences leads to a combinatorial explosion in the size of the state space. The action space - the set of frequent vocabulary words in the English language - is 20,000-dimensional. This compounds extrapolation error, making BRL even more difficult. However, when learning from human interactions in the wild, it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors (e.g.  Horton (2016) ). To support this work, we developed an interactive online platform that allows humans to chat with deep neural network dialog models running on a GPU; the BRL models trained for this study are available live at https://neural.chat/rl/. Through this platform we collected human re- sponses to a set of over 40 different dialog models over the course of several months. Using our Way Off-Policy algorithm, we are able to effectively learn from this batch of data, in spite of the fact that it was generated with a vastly different set of model architectures, which were trained on different datasets. Further, we use the batch to learn from many different reward functions designed post-hoc to extract implicit human preferences, something that is only possible with effective off-policy BRL. In summary, the contributions of this paper are: • A novel algorithm, Way Off-Policy learning, which is the first to propose using KL-control from a pre-trained prior model as a way to reduce extrapolation error in batch RL. • Experiments showing the effectiveness of WOP above strong baselines based on prior work (e.g.  Fujimoto et al. (2018b) ), on both traditional RL tasks and on the challenging problem of open-domain dialog generation. • A set of novel conversation rewards based on how human preferences are implicitly ex- pressed in text. We are the first work to learn from implicit signals in conversation offline using batch RL.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the utility of self-supervision on sequential data when additional data are unavailable, and proposes new types of self-supervision tasks. We refer to this approach as 'limited self-supervision.' We compare the utility of several different existing forms of self-supervision in our limited-data setting, identify consistent trends across supervision types, and demonstrate the utility of combining multiple different forms of self-supervision. We also propose a new form of self-supervision, piecewise-linear autoencoding, that trades off fine-grained signal modeling and long-term dependency propagation. Our findings suggest that there is a wide range of time-series and sequence classification tasks where limited self-supervision could improve performance, and present a methodological contribution in the form of a useful new type of self-supervision.",
        "Abstract": "Self-supervision, in which a target task is improved without external supervision, has primarily been explored in settings that assume the availability of additional data. However, in many cases, particularly in healthcare, one may not have access to additional data (labeled or otherwise). In such settings, we hypothesize that self-supervision based solely on the structure of the data at-hand can help. We explore a novel self-supervision framework for time-series data, in which multiple auxiliary tasks (e.g., forecasting) are included to improve overall performance on a sequence-level target task without additional training data. We call this approach limited self-supervision, as we limit ourselves to only the data at-hand. We demonstrate the utility of limited self-supervision on three sequence-level classification tasks, two pertaining to real clinical data and one using synthetic data. Within this framework, we introduce novel forms of self-supervision and demonstrate their utility in improving performance on the target task. Our results indicate that limited self-supervision leads to a consistent improvement over a supervised baseline, across a range of domains. In particular, for the task of identifying atrial fibrillation from small amounts of electrocardiogram data, we observe a nearly 13% improvement in the area under the receiver operating characteristics curve (AUC-ROC) relative to the baseline (AUC-ROC=0.55 vs. AUC-ROC=0.62). Limited self-supervision applied to sequential data can aid in learning intermediate representations, making it particularly applicable in settings where data collection is difficult.",
        "Introduction": "  INTRODUCTION Many problems involving sequential data, such as machine translation, sentiment analysis, and mor- tality prediction, are naturally framed as sequence-level tasks ( Harutyunyan et al., 2017 ;  Hassan et al., 2018 ;  Radford et al., 2017 ). Sequence-level tasks map a sequence of observations x 0:T to a single label y. Learning this mapping is often made challenging due to a high-D (dimension) low-N (number of samples) setting ( Nasrabadi, 2007 ). Such problems are particularly prevalent in healthcare tasks, which often involve limited quantities of labeled data captured at a high temporal resolution (e.g., electrocardiogram waveforms). In high-D low-N settings, researchers have had success with transfer learning techniques, by lever- aging additional data to learn intermediate representations that are then used in the target task. When additional data are unavailable, it may be possible to improve the intermediate learned representa- tion of the data with respect to the target task by considering additional tasks intrinsic to the data. In particular, we hypothesize that the structure of sequential data provides a rich source of innate supervision. For example, signal reconstruction or forecasting could improve the intermediate rep- resentation by capturing the underlying data-generating process. Such approaches are examples of self-supervision, where labels are derived from the input (as opposed to external sources). In this paper, we show that leveraging the sequential structure of the data at-hand can lead to im- proved performance on sequence-level tasks (i.e., the target task). More specifically, by consid- ering self-supervised auxiliary tasks (e.g., signal reconstruction), in addition to the sequence-level task, one can learn useful intermediate representations of the data. Past work investigating self- Under review as a conference paper at ICLR 2020 supervision for sequential data has focused on full-signal reconstruction ( Dai & Le, 2015 ), and to a lesser extent forecasting ( Ramachandran et al., 2016 ). Building on past work, we examine the utility of self-supervision on sequential data when additional data are unavailable, and we propose new types of self-supervision tasks. We refer to this approach as 'limited self-supervision.' We limit the self-supervision to the data at-hand, and focus on self-supervised auxiliary tasks relevant to sequential data ordered by time (i.e., time-series data). Our main contributions are as follows: • We demonstrate the efficacy of the proposed limited self-supervision framework for im- proving performance across datasets/tasks with no additional data. • We compare the utility of several different existing forms of self-supervision in our limited- data setting, identify consistent trends across supervision types, and demonstrate the utility of combining multiple different forms of self-supervision. • We propose a new form of self-supervision, piecewise-linear autoencoding, that trades off fine-grained signal modeling and long-term dependency propagation. We demonstrate that this is the best form of limited self-supervision across all tasks. Our work suggests that there is a wide range of time-series and sequence classification tasks where limited self-supervision could improve performance. It also shows the value of including multiple, simultaneous streams of auxiliary self-supervision. Our findings present a methodological contribu- tion, in the form of a useful new type of self-supervision, piecewise-linear autoencoding. Further, our empirical findings on when and how auxiliary tasks help can inform future work in developing self-supervision techniques.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel defense mechanism for deep neural networks against adversarial attacks. The proposed defense mechanism is based on the concept of increasing the embedding margin of the network, which is the minimal distance between two instances belonging to two different classes. Two procedures are proposed to increase the embedding margin, and a resilient classifier is trained using these procedures and standard kernel density estimation. An extensive empirical study is conducted to evaluate the proposed defense against all threat models, and the results indicate that the proposed defense achieves state-of-the-art detection.",
        "Abstract": "This paper is concerned with the defense of deep models against adversarial at-\ntacks. We develop an adversarial detection method, which is inspired by the cer-\ntificate defense approach, and captures the idea of separating class clusters in the\nembedding space so as to increase the margin. The resulting defense is intuitive,\neffective, scalable and can be integrated into any given neural classification model.\nOur method demonstrates state-of-the-art detection performance under all threat\nmodels.",
        "Introduction": "  INTRODUCTION Defending machine learning models from adversarial attacks has become an increasingly pressing issue as deep neural networks become associated with more critical aspects of society. Adversarial attacks can effectively fool deep models and force them to misclassify, using a slight but maliciously- designed distortion that is typically invisible to the human eye (Carlini & Wagner, 2017c;  Athalye et al., 2018 ;  Szegedy et al., 2013 ;  Goodfellow et al., 2014 ;  Kurakin et al., 2016 ). Despite numerous developments, defense mechanisms are still wanting. Many interesting ideas have been proposed to construct defense mechanisms for adversarial exam- ples. Among these are adversarial training ( Zuo et al., 2020 ;  Yan et al., 2018 ;  Tramèr et al., 2017 ;  Madry et al., 2017 ), ensemble methods ( Strauss et al., 2017 ), and randomization ( Dhillon et al., 2018 ) to name a few. These works consider both detection and resiliency. However, many of these defense ideas were found to be inadequate ( Athalye et al., 2018 ; Carlini et al., 2019; Carlini & Wag- ner, 2017b;  He et al., 2017 ). For example, adversarial training critically depends on the specific choice of adversarial attacks used to generate the adversarial training instances. As a result, often this method cannot withstand attacks based on different strategies. ( Engstrom et al., 2018 ). A more formal approach to adversarial defense is the certification approach ( Hein & An- driushchenko, 2017 ), which is designed to provide a lower bound for the penetration distortion required to fool a given network. Certified defense methods are referred to as being either \"exact\" or \"conservative\". In exact methods no distortion smaller than the certification bound can penetrate the deep neural network (DNN) ( Hein & Andriushchenko, 2017 ;  Wong & Kolter, 2017 ;  Wong et al., 2018 ;  Cohen et al., 2019 ). In \"conservative\" methods, the bound is merely a relative metric for comparing DNN robustness to adversarial examples ( Tsuzuku et al., 2018 ;  Zhang et al., 2019 ; Ding et al., 2018). Both exact and conservative methods have been criticized for being computationally expensive and unscalable ( Tjeng et al., 2018 ;  Cohen et al., 2019 ). It is interesting to view adversarial attacks through activation geometry in embedding layers. A trained deep classification model tends to organize instances into clusters in the embedding space, according to class labels. Classes with clusters in close proximity to one another, provide excellent opportunities for attackers to fool the model. This geometry explains the tendency of untargeted attacks to alter the label of a given image to that of an adjacent class in the embedding space as demonstrated in Figure 1a. Thus, if we can modify the model to increase the margin between clusters, while lowering (or not increasing) the activation sensitivity in the embedding space to input changes, we can make the network more immune to attacks. This embedding sensitivity can be quantified through a Lipschitz constant or directly via the Jacobian. Inspired by certificate defense methods, we developed an adversarial detection method, that captures the above separation in embedding space intuition. Ideally, we would like to lower bound the distor- Under review as a conference paper at ICLR 2020 tion, , required by the adversary to force a DNN F to misclassify x + , where x is an input image. We propose an approximation to such a bound which, while not formal, motivates a useful strategy for creating defense methods. The bound, η/||J F (x)||, which is similar to other known bounds, is given in terms of η, where η quantifies the \"embedding margin\" of the network, and J F (x), the Jacobian of F with respect to x (see details in Section 2). The embedding margin, for a given inter- mediate layer, is the minimal distance (under any p-norm) between two instances belonging to two different classes. This approximate relation motivates a strategy of penetration distortion maximiza- tion (PDM) whereby, we implicitly or explicitly maximize this lower bound without attempting to calculate it. The notion of increasing a DNN classification margin has been discussed in several contexts  Liu et al. (2016) ;  Hoffer & Ailon (2015) ;  Wen et al. (2016) . To apply the PDM approach we propose two procedures to increase the embedding margin. These two methods are complementary in the sense that we can benefit by applying them together. In conjunction, we use the reverse cross- entropy method of  Pang et al. (2018) , which tends to smooth the Jacobian. Our adversarial detection mechanism is constructed by training a resilient classifier using the above three procedures; we then apply standard kernel density estimation (KDE) on the embedding layer ( Feinman et al., 2017 ). We present an extensive empirical study focusing on the detection of adversarial examples under all threat models, in which we consider the FGSM, BIM, C&W and JSMA attacks. Our experimental procedure strictly adheres to the comprehensive evaluation desiderata proposed by Carlini et al. (2019). The results we obtain indicate that the proposed defense achieves state-of-the-art detection.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a simple latent shape representation obtained by applying Principal Component Analysis (PCA) on the Signed Distance Function (SDF) transformed shape. This representation is used in downstream tasks such as 3D shape reconstruction from a single view and 3D shape completion from a point cloud. Results show that the eigenSDF approach is competitive with state-of-the-art methods in standard benchmarks. The paper also suggests that more complex benchmarks may be needed to push forward the study of learned 3D shape reconstruction.",
        "Abstract": "Deep learning applied to the reconstruction of 3D shapes has seen growing interest. A popular approach to 3D reconstruction and generation in recent years has been the CNN decoder-encoder model often applied in voxel space. However this often scales very poorly with the resolution limiting the effectiveness of these models. Several sophisticated alternatives for decoding to 3D shapes have been proposed typically relying on alternative deep learning architectures. We show however in this work that standard benchmarks in 3D reconstruction can be tackled with a surprisingly simple approach: a linear decoder obtained by principal component analysis on the signed distance transform of the surface. This approach allows easily scaling to larger resolutions. We show in multiple experiments it is competitive with state of the art methods and also allows the decoder to be fine-tuned on the target task using a loss designed for SDF transforms, obtaining further gains.    ",
        "Introduction": "  INTRODUCTION In recent years there has been an increased interest in extending the successes of deep learning to problems requiring analysis and representation of 3D shapes. This includes many long standing tasks such as 3D shape reconstruction ( Wu et al., 2016a ;  Choy et al., 2016 ) from single or multiple views, as well as shape completion  Mescheder et al. (2018) . There are a number of applications of these methods in robotics, surgery, and augmented reality. A popular approach has been the use of CNN decoder-encoder architectures  Choy et al. (2016)  as popularized in applications like segmentation  Chen et al. (2017) . Here for example in the single view reconstruction 2D CNN will encode the 2-D image and a 3D CNN decoder model will produce the final representation in voxels. This however is ineffective in larger resolutions and does not make full use of the structure of the object. Similar problems arise in more general attempts to learn latent variable models of shapes ( Wu et al., 2016b ;  Gadelha et al., 2017 ). Here one may be interested in tasks such as unconditional generation and reconstruction. More recently authors have considered alternative representations of shapes to a standard 3D dis- cretized set of voxels( Choy et al., 2016 ;  Girdhar et al., 2016b ;  Wu et al., 2016a ;  Yan et al., 2016 ;  Zhu et al., 2017 ), one that can permit more efficient learning and generation. These include point clouds  Fan et al. (2017) , meshes( Wang et al., 2018 ;  Georgia Gkioxari, 2019 ), and signed distance transform based representations ( Michalkiewicz et al., 2019 ;  Park et al., 2019 ). To date there is not an agreed upon canonical 3-D shape representation for use with deep learning models. Further- more authors have considered alternative architectures aimed at dealing with this problem ( Richter & Roth, 2018 ;  Tatarchenko et al., 2017 ). However, in this work, we ask whether a simpler approach can yield strong results. Building on the recent use of the Signed Distance Function (SDF) in shape representation we demonstrate a simple latent shape representation that can be used in downstream tasks and easily decoded. More specifically, in this work, we consider a latent shape representation obtained by applying PCA on the SDF transformed shape. We show this leads to a latent shape representation that can be used directly in downstream tasks like 3D shape reconstruction from a single view and 3D shape completion from a point cloud. Our work a) reinforces the relevance of SDF as a representation for 3D deep learning b) demonstrates that a simple representation obtained by applying PCA on the SDF transform can lead to an effective latent shape representation. This representation allows for results competitive to state-of-the-art Under review as a conference paper at ICLR 2020 in standard benchmarks. Our work also suggests more complex benchmarks than those currently considered may be needed to push forward the study of learned 3D shape reconstruction. The paper is structured as follows. In Sec. 2 we discuss the related work. We outline the basic methods used in the experiments in Sec. 3. We show extensive quantitative and experimental results comparing the eigenSDF approach to existing methods in Sec. 4.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the properties of deep rectifier (ReLU) networks at initialization, specifically focusing on the He initialization scheme (He et al., 2015). We show that deep ReLU networks obey the activation norm equality and the gradient norm equality property in the forward and backward pass, respectively. We relax the assumptions made in previous papers and discuss the implications of our findings.",
        "Abstract": "It has been noted in existing literature that over-parameterization in ReLU networks generally improves performance. While there could be several factors involved behind this, we prove some desirable theoretical properties at initialization which may be enjoyed by ReLU networks. Specifically, it is known that He initialization in deep ReLU networks asymptotically preserves variance of activations in the forward pass and variance of gradients in the backward pass for infinitely wide networks, thus preserving the flow of information in both directions. Our paper goes beyond these results and shows novel properties that hold under He initialization: i) the norm of hidden activation of each layer is equal to the norm of the input, and, ii) the norm of weight gradient of each layer is equal to the product of norm of the input vector and the error at output layer. These results are derived using the PAC analysis framework, and hold true for finitely sized datasets such that the width of the ReLU network only needs to be larger than a certain finite lower bound. As we show, this lower bound depends on the depth of the network and the number of samples, and by the virtue of being a lower bound, over-parameterized ReLU networks are endowed with these desirable properties. For the aforementioned hidden activation norm property under He initialization, we further extend our theory and show that this property holds for a finite width network even when the number of data samples is infinite. Thus we overcome several limitations of existing papers, and show new properties of deep ReLU networks at initialization.",
        "Introduction": "  INTRODUCTION Deep rectifier (ReLU) networks are popular in deep learning due to their ease of training and state- of-the-art generalization. This success of deep rectifier networks can be partly attributed to good initialization strategies (for example  Glorot & Bengio (2010) ;  He et al. (2015) ). Essentially, good parameter initializations guarantee that there is no exploding or vanishing of information across hidden layers. These properties help gradient descent based optimization methods in navigating the complex non-linear loss landscape of deep networks by initializing them at a good starting point where training can begin. Such favorable properties promised by these initialization strategies are (in most cases) shown to hold true in asymptotic settings where the network width tends to infinity and/or under strict assumptions made about the distribution of the input data. A detailed account of these existing papers and a contrast between these papers and our work is discussed in section 2. Our paper relaxes the aforementioned assumptions made in previous papers. Further, we show novel properties that hold for deep ReLU networks at initialization when using the He initialization scheme ( He et al., 2015 ). Specifically, we show that deep ReLU networks obey the following properties in the forward (Eq. 1) back backward (Eq. 2) pass (see section 3 for notations), We refer to the above properties as as the the activation norm equality and the gradient norm equality property.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a framework for comparing the performance of classifiers - human, machine or other - on image classification tasks. The framework is based on the principle of computing and analysing minimal entropy positive inputs, which are inputs with minimal information with respect to yielding correct classification results. Experiments are conducted on a sample ILSVRC test-set for state-of-the-art deep neural-network (DNN) models and more than 500 human subjects. Results show that minimal entropy images are considerably smaller than original images for DNNs, and that the minimal-entropy positive images for humans are considerably smaller in the case of colour and resolution reductions. Furthermore, the precision of human classifiers on DNN minimal-entropy positive images was considerably better than the corresponding results for cross-classification among DNN models. These results provide insights into how different classifiers rely differently on different types of information, and suggest that humans are capable of performing (much) more laconic classification in the evaluated setting than state-of-the-art machine classifiers.",
        "Abstract": "We propose laconic classification as a novel way to understand and compare the performance of diverse image classifiers. The goal in this setting is to minimise the amount of information (aka. entropy) required in individual test images to maintain correct classification. Given a classifier and a test image, we compute an approximate minimal-entropy positive image for which the classifier provides a correct classification, becoming incorrect upon any further reduction. The notion of entropy offers a unifying metric that allows to combine and compare the effects of various types of reductions (e.g., crop, colour reduction, resolution reduction) on classification performance, in turn generalising similar methods explored in previous works. Proposing two complementary frameworks for computing the minimal-entropy positive images of both human and machine classifiers, in experiments over the ILSVRC test-set, we find that machine classifiers are more sensitive entropy-wise to reduced resolution (versus cropping or reduced colour for machines, as well as reduced resolution for humans), supporting recent results suggesting a texture bias in the ILSVRC-trained models used. We also find, in the evaluated setting, that humans classify the minimal-entropy positive images of machine models with higher precision than machines classify those of humans.",
        "Introduction": "  INTRODUCTION Deep neural networks now surpass human-level performance on a variety of specific tasks and met- rics relating to visual recognition. In a widely-used yardstick for human-level performance,  Rus- sakovsky et al. (2015)  estimated that an expert human with prior training can achieve a top-5 clas- sification error rate of 5.1% on a dataset of 1,500 ILSVRC images and 1,000 target classes. Shortly after,  He et al. (2015)  surpassed human-level performance on the same task achieving 4.9% top-5 er- ror with PReLU-net. Later works would further reduce this error rate, including ResNet (3.6%) ( He et al., 2016 ), Trimps-Soushen (3.0%) ( Shao et al., 2016 ), SeNet (2.3%) ( Hu et al., 2019 ), etc., with contemporary state-of-the-art models more than halving estimated human error for this specific task. Though such results represent landmark advances, by focusing on classification errors alone, they do not reveal the full story of relative machine performance for image classification. Works on adversarial examples ( Dalvi et al., 2004 ;  Szegedy et al., 2014 ;  Nguyen et al., 2015 ), for instance, establish that human and machine perception diverges greatly for specifically constructed images. Other works have presented bespoke experiments comparing human and machine performance be- yond classification errors, presenting evidence for a lack of robustness in the presence of noisy ( Rus- sakovsky et al., 2015 ;  Dodge & Karam, 2017 ; 2019) or incomplete information (Ullman et al., 2016;  Wick et al., 2016 ;  Linsley et al., 2017 ;  Ho-Phuoc, 2018 ;  Srivastava et al., 2019 ), a sensitivity to spa- tial  Fawzi & Frossard (2015) ;  Hénaff & Simoncelli (2016) ;  Xiao et al. (2018) ;  Engstrom et al. (2019)  or colour  Hosseini & Poovendran (2018) ;  Hosseini et al. (2018)  transformations, a lack of generali- sation ( Geirhos et al., 2018 ), a bias towards texture ( Geirhos et al., 2019 ), etc., in evaluated models. By applying specific transformations on test images prior to classification, these latter works provide insights into the specific differences in the types of information that humans and machines rely on to perform adequately at this task. These latter recent works suggest the need for an information-theoretic framework that generalises such issues: a framework within which the performance of classifiers - be they human, machine or other - can be compared and understood, allowing to quantify, in a more fine-grained manner, the Under review as a conference paper at ICLR 2020 type of information in the input on which a given classifier depends. While previous works address individual or multiple types of information reduction on input images in isolation, a more general framework should allow to combine and compare different types of reduction on test inputs. In this paper, we propose such a framework, based on the principle of computing and analysing minimal entropy positive inputs: inputs with minimal information with respect to yielding correct classification results. The notion of entropy intuitively generalises and allows to compare the rela- tive effects of different reductions on inputs - and their combinations - on classifier performance; such reductions may include, for example, cropping, downsampling and quantisation. The goal in this framework thus shifts from precise classification to laconic classification: providing a clas- sifier that minimises the entropy of input(s) required for correct classification. Being based on a continuous notion of entropy rather than a discrete notion of correct/incorrect, this latter goal thus presents a novel challenge beyond minimising classification error-a goal for which state-of-the-art approaches are close to achieving perfect results on datasets like ILSVRC. In fact, existing datasets - such as ILSRVC - can be straightforwardly leveraged for evaluating classifiers under this new goal. Models performing more laconic classification (we currently conjecture) should likewise perform more robustly in practical settings involving incomplete or noisy information capture. Though the framework we propose can be applied to any classifier for any classification task, herein we first instantiate the framework on the aforementioned problem of image classification. We con- sider three general operations for reducing the entropy of test images: crop, resolution reduction (downsampling) and colour reduction (quantisation). We then propose two methods for finding the minimal entropy positive images under these reductions for two different types of classifier. Given a pre-trained machine classifier - where classification can be separated from learning - an input test image, and a set of reduction operations, we use a known search algorithm ( Powell, 1964 ) and apply the given reduction functions to the input image to find (under certain assumptions) the lowest entropy image that the model classifies correctly such that applying any further reduction of entropy leads to incorrect classification. We apply the aforementioned framework to find the mini- mal entropy images from a sample ILSVRC test-set for state-of-the-art deep neural-network (DNN) models (GoogLeNet, SqueezeNet, ResNet50 and SeNetResNet50), with respect to the three afore- mentioned reduction operations and their combination. Our experiments show that minimal entropy images are considerably smaller than original images for DNNs; for example, we find that with only 2-6% of the information content of the original test-images (on average) the best performing machine model can still produce a correct classification of the reduced image. We then consider also human classifiers in order to compare their ability to perform laconic classifi- cation with DNN models; applying our framework for humans was not trivial since learning cannot be separated from classification (we cannot start with the full input test-image and reduce it since the human will remember the image) and an automatic optimization algorithm was not possible. To cope with this we designed an experiment reversing the optimization goal: starting from a void image, the human evaluator may add information incrementally until they believe that a confident classification of the displayed image is possible. We apply this framework with more than 500 human subjects through an online interface. Our results show that the minimal-entropy positive images for humans are considerably smaller in the case of colour and resolution reductions (53-62% the size of the corresponding size for the best performing machine model). Finally we cross-classified the minimal-entropy positive images among DNNs and humans: given classifier A and B, we presented A's minimal-entropy positive images to B and vice versa, com- puting classification precision. We show that the precision of human classifiers on DNN minimal- entropy positive images was considerably better (0.74 precision in the worst case) than the corre- sponding results for cross-classification among DNN models (0.02 precision in the worst case for the best model). Furthermore, we find that the DNN models on human images give precisions of 0.03-0.43, depending on the reductions used for the images and the model used for classification. Our results provide insights into how different classifiers - both machine and human - rely dif- ferently on different types of information, providing further evidence to support, for example, prior claims of the lack of robustness to incomplete information relative to humans in such models ( Dodge & Karam, 2017 ), a bias towards texture in ILSVRC-trained models ( Geirhos et al., 2019 ), and so forth. As a more general conclusion, we show that humans are capable of performing (much) more Under review as a conference paper at ICLR 2020 laconic classification in the evaluated setting than state-of-the-art machine classifiers. We conclude with open challenges regarding the laconic classification of images using DNNs.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel interpretable model based on Multitask Gaussian Processes and Attentive Neural Networks for the early prediction of sepsis. The model is evaluated on real-world medical data and is shown to have superior predictive performance and interpretability compared to previous methods. A gold standard for Sepsis-3 labelling is also provided, implemented on the MIMIC-III data set.",
        "Abstract": "With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty.",
        "Introduction": "  INTRODUCTION Every year, it is estimated that 31.5 million people worldwide contract sepsis. With a mortality rate of 17% in its benign state and 26% for its severe state ( Fleischmann et al., 2016 ), sepsis is one of the leading causes of hospital mortality ( Vincent et al., 2014 ), costing the healthcare system more than 16 billion dollars in the USA alone ( Angus et al., 2001 ). Studies demonstrated that early treatment has a significant positive effect on the survival rate ( Kumar et al., 2006 ;  Nguyen et al., 2007 ). In particular,  Castellanos-Ortega et al. (2010)  demonstrated that each hour delay in treating a patient results in a 7.6% increase in mortality. Current methods of screening, such as Modified Early Warning System (MEWS) and Systemic Inflammatory Response Syndrome (SIRS) have been criticised for their lack of specificity, leading to low accuracies and high false alarm rates. In 2015, the Third International Consensus Definitions for Sepsis ( Singer et al., 2016 ;  Seymour et al., 2016 ;  Shankar-Hari et al., 2016 ) committee worked towards incorporating medical and technological advances into an up-to-date definition of sepsis, providing scientists with widely acknowledged illness criteria. Together with the rise of Electronic Health Records (EHR), the scientific community is now armed with both the data and labelling techniques to experiment with novel prediction methods ( Islam et al., 2019 ;  Henry et al., 2015 ;  Ghosh et al., 2017 ;  Calvert et al., 2016 ; Desautels et al., 2016), which are already proving effective in increasing survival rate ( Shimabukuro et al., 2017 ) and promising in decreasing costs. New models developed so far either relied on some interpretable yet simple prediction methods, such as logistic regression ( Calvert et al., 2016 ) and decision tree based classifiers ( Mao et al., 2018 ;  Delahanty et al., 2019 ), or on effective yet black-box methods such as Recurrent Neural Networks ( Futoma et al., 2017b ). Moreover, the results achieved by different authors are rarely comparable: although most use the MIMIC-III data set, the disparities in labelling rules result in highly variable data sets (eg.  Raghu et al. (2018)  have 17,898 septic patients vs. 2,577 for  Desautels et al. (2016) ). This work presents an attempt at reconciling interpretability and predictive performance on the sepsis prediction task and makes the following contributions: • Gold standard for labelling. We provide a gold standard for Sepsis-3 labelling implemented on the MIMIC-III data set. • Novel interpretable model. We present an explainable and end-to-end trainable model based on Multitask Gaussian Processes and Attentive Neural Networks for the early prediction of sepsis. • Empirical evaluation. We assess our model on real-world medical data and report superior predictive performance and interpretability compared to previous methods. An overview of our proposed method is shown in  Figure 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a study on the investigation of neural joint models with the bandwidth-limited channel. The paper casts the problem of learning joint communication as a variational learning problem and empirically evaluates the gap between neural systems for joint and separate communication. The paper also designs standard channels such as the Gaussian and Binary channel as differentiable probabilistic nodes, and investigates two core design choices of the neural joint model and bandwidth-limited channel. The main contributions of this work include improved decoding speed and code length, improved transmission rate through learned prior models, and improved image reconstructions in the low bandwidth regime through the introduction of auxiliary latent variables in the decoding process.",
        "Abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.",
        "Introduction": "  INTRODUCTION The 21st century is often referred to as the information age. Information is being created, stored and sent at rates never before seen. To cope with this deluge of information, it is vital to design optimal communication systems. Such systems solve the problem of reliably transmitting information from sender to receiver given some form of information loss due to transmission errors (i.e. through a noisy channel). As the size of the transmitted messages goes to infinity for memory-less communication channels, the joint source-channel coding theorem ( Shannon, 1948 ) states that it is optimal to split the communication task into two sub-tasks: (i) removing redundant information from the message (source coding) and (ii) re-introducing some redundancy into the encoded message to allow for message reconstruction despite the channel information loss (channel coding). As a result, separate systems have been studied extensively in the literature and in fact are the standard way of coding for many scenarios. However, it is also well known that there are limits to the optimality of separate systems in practical settings. Most importantly for this work, limitations arise when we seek to encode finite length messages ( Kostina & Verdú, 2013 ). These limits lead to two consequences: (i) When there is a budget on transmission bits, source and channel coding errors need to be balanced for best reconstruction results. (ii) Decoding via maximum-likelihood principle becomes an NP-hard problem ( Berlekamp et al., 1978 ). Thus approximations need to be made that can lead to highly sub-optimal solutions ( Koetter & Vontobel, 2003 ;  Feldman et al., 2005 ;  Vontobel & Koetter, 2007 ). Recent work ( Choi et al., 2019 ;  Farsad et al., 2018 ), has thus looked at the problem of learning to jointly communicate. This includes systems that learn to do source and channel coding jointly from data. Practically this can be achieved by learning neural network encoders and decoders, where channels are simulated by adding noise to encoded messages. Several desirable properties of such systems were shown, including improvements in decoding speed and code length. Complementary to this body of work, we focus this study on the investigation of neural joint models with the bandwidth- limited channel. Specifically, we direct our experimentation on the bandwidth-limited channel due Under review as a conference paper at ICLR 2020 to it's ubiquity as a fundamental component in the real world communication systems. The main contributions of this work include: 1. We cast the problem of learning joint communication as a variational learning problem, parallel to other work ( Choi et al., 2019 ). 2. We justify the importance of jointly learned systems by empirically evaluating the gap between neural systems for joint and separate communication. 3. We design standard channels such as the Gaussian and Binary channel as differentiable proba- bilistic nodes, which serve as base for our design of the bandwidth-limited channel. 4. We investigate two core design choices of our neural joint model and bandwidth-limited channel: (i) how transmission rate can be improved through learned prior models (ii) how we may improve image reconstructions in the low bandwidth regime through the introduction of auxiliary latent variables in the decoding process.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel and general method for generating unrestricted adversarial inputs. The training procedure for generative adversarial networks (GANs) is modified so that the generator network is rewarded for producing data that are both realistic and deceive a fixed target network. This method is adaptive, efficient, and can easily be applied to any existing conditional GAN codebase and checkpoints, regardless of architecture, training procedure, or application domain. It is also scalable to ImageNet. The results demonstrate that this method is effective in generating unrestricted adversarial inputs that can fool a target network.",
        "Abstract": "Neural networks are vulnerable to adversarially-constructed perturbations of their inputs. Most research so far has considered perturbations of a fixed magnitude under some $l_p$ norm. Although studying these attacks is valuable, there has been increasing interest in the construction of—and robustness to—unrestricted attacks, which are not constrained to a small and rather artificial subset of all possible adversarial inputs. We introduce a novel algorithm for generating such unrestricted adversarial inputs which, unlike prior work, is adaptive: it is able to tune its attacks to the classifier being targeted. It also offers a 400–2,000× speedup over the existing state of the art. We demonstrate our approach by generating unrestricted adversarial inputs that fool classifiers robust to perturbation-based attacks. We also show that, by virtue of being adaptive and unrestricted, our attack is able to bypass adversarial training against it.",
        "Introduction": "  INTRODUCTION Despite their dramatic successes in other respects, neural networks are well-known to not be adver- sarially robust. Szegedy et al. (2014) discovered that neural networks are vulnerable to what they termed adversarial inputs: by adding carefully-chosen perturbations to correctly-classified inputs, the accuracy of any neural network could be almost arbitrarily decreased. Since then, the machine learning community has rightly focused a great deal of research effort on this phenomenon. Many early efforts to train more robust models initially appeared promising, but have since been shown to be vulnerable to new algorithms for constructing adversarial perturbations ( Xu et al., 2019 ;  Athalye et al., 2018 ). As a re- sult, more attention has been given to methods that provide formal guarantees about performance in the presence of adversarial perturbations ( Liu et al., 2019 ), with the state of the art now providing non-trivial guarantees for the MNIST test set ( Wong & Kolter, 2018 ;  Croce et al., 2018 ;  Wang et al., 2018 ). However, almost all of this work has focused exclusively on adversarial perturbations whose magnitude is constrained by an l p norm. There is a growing acknowledgement that this threat model is somewhat contrived: such examples are not a realistic security concern and also occupy a vanishingly small fraction of the set of potential adversarial inputs. Therefore, there is a burgeoning interest in adversarial attacks that are unrestricted, in the sense that they do not necessarily derive from a perturbation of a natural input ( Brown et al., 2018 ;  Song et al., 2018b ). The main contribution of this paper is a novel and general method to generate unrestricted adversarial inputs. In short, the training procedure for generative adversarial networks (GANs) is modified so that the generator network is rewarded for producing data that are both realistic and deceive a fixed target network. Our approach has four advantages over prior work: 1. Our method is adaptive in that it adjusts itself to best attack the specific network being targeted. For instance, adversarial training is ineffective against our approach. 2. Our method is efficient (offering a 400-2000× speedup over prior work). 3. Our method can easily be applied to any existing conditional GAN codebase and checkpoints, regardless of architecture, training procedure, or application domain. 4. Our method therefore demonstrably scales to ImageNet. Under review as a conference paper at ICLR 2020",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new method, Conditional Normalizing Flows (CNFs), for learning the likelihood of conditional distributions p Y |X (y|x). CNFs are used to condition the prior and the invertible mapping on the input x, and are applied to super-resolution and vessel segmentation tasks. The performance of CNFs is evaluated against factored baselines by comparing likelihood and application specific evaluation metrics.",
        "Abstract": "Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.",
        "Introduction": "  INTRODUCTION Learning conditional distributions p Y |X (y|x) is one of the oldest problems in machine learning. When the output y is high-dimensional this is a particularly challenging task, and the practitioner is left with many design choices. Do we factorize the conditional? If not, do we model correlations with, say, a conditional random field ( Prince, 2012 )? Do we use a unimodal distribution? How fat should the tails be? Do we use an explicit likelihood at all, or use implicit methods ( Mohamed & Rezende, 2015 ) such as a GAN ( Goodfellow et al., 2014 )? Do we quantize the output? Ideally, the practitioner should not have to make design choices at all, and the distribution should be learned from the data. In the field of density estimation normalizing flows (NFs) are a relatively new family of models ( Rezende & Mohamed, 2015 ). NFs model complicated high dimensional marginal distributions p Y (y) by transforming a simple base distribution or prior p Z (z) through a learnable, invertible mapping f φ and then applying the change of variables formula. NFs are efficient in inference and sampling, are able to learn inter-dimensional correlations and multi-modality, and they are exact likelihood models, amenable to gradient-based optimization. Flow-based generative models ( Dinh et al., 2016 ) are generally trained on the image space, and are in some cases computationally efficient in both the forward and inverse direction. These are advantageous over other likelihood based methods because i) sampling is efficient opposed to au- toregressive models ( Van Oord et al., 2016 ), and ii) flows admit exact likelihood optimization in contrast with variational autoencoders ( Kingma & Welling, 2014 ). Conditional random fields directly model correlations between pixels, and have been fused with deep learning ( Chen et al., 2016 ). However, they require the practitioner to choose which pixels have pairwise interactions. Another approach uses adversarial training ( Goodfellow et al., 2014 ). A downside is that the training procedure can be unstable and they are difficult to evaluate quantita- tively. We propose to learn the likelihood of conditional distributions with few modeling choices using Con- ditional Normalizing Flows (CNFs). CNFs can be harnessed for conditional distributions p Y |X (y|x) Under review as a conference paper at ICLR 2020 by conditioning the prior and the invertible mapping on the input x. In particular, we apply condi- tional flows to super-resolution ( Wang et al., 2018 ) and vessel segmentation ( Staal et al., 2004 ). We evaluate their performance gains on multivariate prediction tasks along side architecturally-matched factored baselines by comparing likelihood and application specific evaluation metrics.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Reinforcement Learning (RL) is a method for learning behavior policies by maximizing expected reward through interactions with an environment. Multiple Objective RL (MORL) seeks to learn a set of potential policies so that importance of objectives can be specified after training, creating more flexible, adaptable agents. The most common scalarization function for MORL is a linear combination in the form of a weight vector, however, many real world scalarization functions are non-linear. This paper explores scalarization functions that are interpretable and allow for non-linearities, providing more flexibility for specifying desired behavior.",
        "Abstract": "In the multi-objective reinforcement learning (MORL) paradigm, the relative importance of each environment objective is often unknown prior to training, so agents must learn to specialize their behavior to optimize different combinations of environment objectives that are specified post-training. These are typically linear combinations, so the agent is effectively parameterized by a weight vector that describes how to balance competing environment objectives. However, many real world behaviors require non-linear combinations of objectives. Additionally, the conversion between desired behavior and weightings is often unclear.\nIn this work, we explore the use of a language based on propositional logic with quantitative semantics--in place of weight vectors--for specifying non-linear behaviors in an interpretable way. We use a recurrent encoder to encode logical combinations of objectives, and train a MORL agent to generalize over these encodings. We test our agent in several grid worlds with various objectives and show that our agent can generalize to many never-before-seen specifications with performance comparable to single policy baseline agents. We also demonstrate our agent's ability to generate meaningful policies when presented with novel specifications and quickly specialize to novel specifications.",
        "Introduction": "  INTRODUCTION Reinforcement Learning (RL) is a method for learning behavior policies by maximizing expected reward through interactions with an environment. RL has grown in popularity as RL agents have excelled at increasingly complex tasks, including board games (Silver et al., 2016), video games (Mnih et al., 2015), robotic control (Haarnoja et al., 2018), and other high dimensional, complex tasks. RL continues to be a valued area of research as algorithms become more generalizable and sample efficient, making them more feasible for deployment in real world scenarios. Many RL tasks can be imagined in which multiple possibly conflicting objectives exist. The relative importance of each objective may not be known by the system designer prior to training, and-when it comes to real world deployment-it may be difficult or impossible to retrain agents as the priorities of objectives change over time. Rather than retrain agents for each prioritization, multiple objective RL (MORL) (Roijers et al., 2013) seeks to learn a set of potential policies so that importance of objectives can be specified after training, thus creating more flexible, adaptable agents. For example, a cleaning agent in a house environment may have several objectives such as dusting, sweeping, and vacuuming. The agent can learn to complete each of these objectives, but certain objectives may take priority over others. A user may specify that dusting is twice as important as sweeping and that vacuuming is not important at all. These priorities may change, and MORL allows agents to learn policies that can satisfy any prioritization of objectives. As part of MORL, a scalarization function is chosen to convert a multiple objective reward vector into a scalar. The most common scalarization function for MORL is a linear combination in the form of a weight vector. However, many real world scalarization functions are non-linear. Additionally, weight vectors are not ideal for specifying desired behavior. A user may need to experiment in order to find the weights that result in a desired behavior. Thus scalarization functions that are interpretable and allow for non-linearities are preferred.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the efficacy of Contrastive Predictive Coding (CPC) for data-efficient image recognition. We present a new implementation of CPC with improved linear classification accuracy, and demonstrate that deep networks trained on top of the resulting CPC representations can achieve test-time classification accuracy far above networks trained on raw pixels, outperforming all other unsupervised representation learning methods. We also assess the generality of CPC representations by transferring them to a new task and dataset, and find state-of-the-art performance.",
        "Abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.",
        "Introduction": "  INTRODUCTION Deep neural networks excel at perceptual tasks when labeled data are abundant, yet their per- formance degrades substantially when provided with limited supervision ( Fig. 1 , red). In con- trast, humans and animals can quickly learn about new classes of objects from few exam- ples ( Landau et al., 1988 ;  Markman, 1989 ). What accounts for this monumental difference in data-efficiency between biological and ma- chine vision? While highly-structured repre- sentations (e.g. as proposed by  Lake et al., 2015 ) may improve data-efficiency, it remains unclear how to program explicit structures that capture the enormous complexity of real visual scenes like those in ImageNet ( Russakovsky et al., 2015 ). An alternative hypothesis has proposed that intelligent systems need not be structured a priori, but can instead learn about the structure of the world in an unsupervised manner ( Barlow, 1989 ;  Hinton et al., 1999 ;  Le- Cun et al., 2015 ). Choosing an appropriate training objective is an open problem, but a promising guiding principle has emerged re- cently: good representations should make the spatio-temporal variability in natural signals more predictable. Indeed, human perceptual representations have been shown to linearize (or 'straighten') the temporal transformations found in natural videos, a property lacking from cur- rent supervised image recognition models ( Hénaff et al., 2019 ), and theories of both spatial and tem- poral predictability have succeeded in describing properties of early visual areas ( Rao & Ballard, Under review as a conference paper at ICLR 2020 1999 ;  Palmer et al., 2015 ). In this work, we hypothesize that spatially predictable representations may allow artificial systems to benefit from human-like data-efficiency. Contrastive Predictive Coding (CPC,  van den Oord et al., 2018 ) is an unsupervised objective which learns such predictable representations. CPC is a general technique that only requires in its definition that observations be ordered along e.g. temporal or spatial dimensions, and as such has been applied to a variety of different modalities including speech, natural language and images. This generality, combined with the strong performance of its representations in downstream linear classification tasks, makes CPC a promising candidate for investigating the efficacy of predictable representations for data-efficient image recognition. Our work makes the following contributions: • We revisit CPC in terms of its architecture and training methodology, and arrive at a new implementation of CPC with dramatically-improved ability to linearly separate im- age classes (+17% Top-1 ImageNet classification accuracy). • We then train deep networks on top of the resulting CPC representations using very few la- beled images (e.g. 1% of the ImageNet dataset), and demonstrate test-time classification ac- curacy far above networks trained on raw pixels (73% Top-5 accuracy, a 28% absolute im- provement), outperforming all other unsupervised representation learning methods (+15% Top-5 accuracy over the previous state-of-the-art ( Zhai et al., 2019 )). Surprisingly, this representation also surpasses supervised methods when given the entire ImageNet dataset (+1% Top-5 accuracy). • We isolate the contributions of different components of the final model to such downstream tasks. Interestingly, we find that linear classification accuracy is not always predictive of low-data classification accuracy, emphasizing the importance of this metric as a stand-alone benchmark for unsupervised learning. • Finally, we assess the generality of CPC representations by transferring them to a new task and dataset: object detection on PASCAL-VOC 2007. Consistent with the results from the previous section, we find CPC to give state-of-the-art performance in this setting.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on semi-supervised learning (SSL) for image classification, a recently very active research area. SSL is a transversal task for different domains including images, audio, time series, and text. Recent approaches in image classification primarily focus on exploiting the consistency in the predictions for the same sample under different perturbations (consistency regularization) or generating labels for the unlabeled data to guide the learning process (pseudo-labeling). Strategies such as a warm-up phase using labeled data, uncertainty weighting, adversarial attacks, or graph-consistency are used to deal with confirmation bias, also known as noise accumulation.",
        "Abstract": "Semi-supervised learning, i.e. jointly learning from labeled an unlabeled samples, is an active research topic due to its key role on relaxing human annotation constraints. In the context of image classification, recent advances to learn from unlabeled samples are mainly focused on consistency regularization methods that encourage invariant predictions for different  perturbations of unlabeled samples. We, conversely, propose to learn from unlabeled data by generating soft pseudo-labels using the network predictions. We show that a naive pseudo-labeling overfits to incorrect pseudo-labels due to the so-called confirmation bias and demonstrate that mixup augmentation and setting a minimum number of labeled samples per mini-batch are effective regularization techniques for reducing it. The proposed approach achieves state-of-the-art results in CIFAR-10/100 and Mini-ImageNet despite being much simpler than other state-of-the-art. These results demonstrate that pseudo-labeling can outperform consistency regularization methods, while the opposite was supposed in previous work. Code will be made available.",
        "Introduction": "  INTRODUCTION Convolutional neural networks (CNNs) have become the dominant approach in computer vision ( Lin et al., 2017 ;  Liu et al., 2018 ;  Kim et al., 2018 ;  Xie et al., 2018 ). To best exploit them, vast amounts of labeled data are required. Obtaining such labels, however, is not trivial, and the research community is exploring alternatives to alleviate this ( Li et al., 2017 ;  Oliver et al., 2018 ;  Liu et al., 2019 ). Knowledge transfer via deep domain adaptation ( Wang & Deng, 2018 ) is a popular alternative that seeks to learn transferable representations from source to target domains by embedding domain adaptation in the learning pipeline. Other approaches focus exclusively on learning useful repre- sentations from scratch in a target domain when annotation constraints are relaxed ( Oliver et al., 2018 ;  Arazo et al., 2019 ;  Gidaris et al., 2018 ). Semi-supervised learning (SSL) ( Oliver et al., 2018 ) focuses on scenarios with sparsely labeled data and extensive amounts of unlabeled data; learning with label noise ( Arazo et al., 2019 ) seeks robust learning when labels are obtained automatically and may not represent the image content; and self-supervised learning ( Gidaris et al., 2018 ) uses data supervision to learn from unlabeled data in a supervised manner. This paper focuses on SSL for image classification, a recently very active research area ( Li et al., 2019 ). SSL is a transversal task for different domains including images ( Oliver et al., 2018 ), audio (Zhang et al., 2016), time series ( González et al., 2018 ), and text ( Miyato et al., 2016 ). Recent approaches in image classification primarily focus on exploiting the consistency in the predictions for the same sample under different perturbations (consistency regularization) ( Sajjadi et al., 2016 ;  Li et al., 2019 ), while other approaches directly generate labels for the unlabeled data to guide the learning process (pseudo-labeling) ( Lee, 2013 ;  Iscen et al., 2019 ). These two alternatives differ importantly in the mechanism they use to exploit unlabeled samples. Consistency regularization and pseudo-labeling approaches apply different strategies such as a warm-up phase using labeled data ( Tarvainen & Valpola, 2017 ;  Iscen et al., 2019 ), uncertainty weighting ( Shi et al., 2018 ;  Li et al., 2019 ), adversarial attacks ( Miyato et al., 2018 ;  Qiao et al., 2018 ), or graph-consistency ( Luo et al., 2018 ;  Iscen et al., 2019 ). These strategies deal with confirmation bias ( Tarvainen & Valpola, 2017 ;  Li et al., 2019 ), also known as noise accumulation (Zhang et al., 2016). This bias stems from using incorrect predictions on unlabeled data for training in subsequent epochs and, thereby increasing confidence in incorrect predictions and producing a model that will tend to resist new changes.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper explores the use of active deep learning in the context of image classification. It proposes a novel approach of using all data, labeled or not, during model training at each active learning cycle, departing from the standard scenario in which unlabeled data are only used for inference and selection for annotation. The paper implements this idea using two principles: unsupervised feature learning and semi-supervised learning. Results show that unsupervised pre-training improves accuracy in many cases at little additional cost.",
        "Abstract": "Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class. ",
        "Introduction": "  INTRODUCTION Active learning ( Settles, 2009 ) is an important pillar of machine learning but it has not been explored much in the context of deep learning until recently ( Gal et al., 2017 ;  Beluch et al., 2018 ;  Wang et al., 2017 ;  Geifman & El-Yaniv, 2017 ;  Sener & Savarese, 2018 ). The standard active learning scenario focuses on training a model on few labeled examples alone, while unlabeled data are only used for acquisition, i.e., performing inference and selecting a subset for annotation. This is the opposite of what would normally work well when learning a deep model from scratch, i.e., training on a lot of data with some loss function that may need labels or not. At the same time, evidence is being accumulated that, when training powerful deep models, the difference in performance between acquisition strategies is small ( Gissin & Shalev-Shwartz, 2018 ;  Chitta et al., 2019 ;  Beluch et al., 2018 ). In this work, focusing on image classification, we revisit active deep learning with the seminal idea of using all data, whether labeled or not, during model training at each active learning cycle. This departs from the standard scenario in that unlabeled data are now directly contributing to the cost function being minimized and to subsequent parameter updates, rather than just being used to perform inference for acquisition, whereby parameters are fixed. We implement our idea using two principles: unsupervised feature learning and semi-supervised learning. While both are well recognized in deep learning in general, we argue that their value has been unexplored or underestimated in the context of deep active learning. Unsupervised feature learning or self-supervised learning is a very active area of research in deep learning, often taking the form of pre-training on artificial tasks with no human supervision for representation learning, followed by supervised fine-tuning on different target tasks like classification or object detection ( Doersch et al., 2015 ;  Wang & Gupta, 2015 ;  Gidaris et al., 2018 ;  Caron et al., 2018 ). To our knowledge, all deep active learning research so far considers training deep models from scratch. In this work, we perform unsupervised feature learning on all data once at the beginning of the active learning pipeline and use the resulting parameters to initialize the model at each active learning cycle. Relying on  Caron et al. (2018) , we show that such unsupervised pre-training improves accuracy in many cases at little additional cost.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to transfer learning in reinforcement learning (RL) by leveraging a Boolean algebra over the space of tasks and optimal value functions. We prove that there exists a homomorphism between the task and value function algebras, allowing an agent to solve any new task written as a Boolean expression without further learning. We illustrate our approach in a simple domain and demonstrate composition in high-dimensional video game environments, showing that an agent can leverage its existing skills to solve new tasks without further learning.",
        "Abstract": "We propose a framework for defining a Boolean algebra over the space of tasks. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains, including a high-dimensional video game environment requiring function approximation, where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks. ",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) has achieved recent success in a number of difficult, high-dimensional environments ( Mnih et al., 2015 ;  Levine et al., 2016 ;  Lillicrap et al., 2016 ;  Silver et al., 2017 ). However, these methods generally require millions of samples from the environment to learn op- timal behaviours, limiting their real-world applicability. A major challenge is thus in designing sample-efficient agents that can transfer their existing knowledge to solve new tasks quickly. This is particularly important for agents in a multitask or lifelong setting, since learning to solve complex tasks from scratch is typically infeasible. One approach to transfer is composition (Todorov, 2009), which allows an agent to leverage existing skills to build complex, novel behaviours. These newly-formed skills can then be used to solve or speed up learning in a new task. In this work, we focus on concurrent composition, where existing base skills are combined to produce new skills (Todorov, 2009;  Saxe et al., 2017 ;  Haarnoja et al., 2018 ;  Van Niekerk et al., 2019 ;  Hunt et al., 2019 ;  Peng et al., 2019 ). This differs from other forms of composition, such as options ( Sutton et al., 1999 ) and hierarchical RL ( Bacon et al., 2017 ), where actions and skills are chained in a temporal sequence. In this work, we define a Boolean algebra over the space of tasks and optimal value functions. This extends previous composition results to encompass all Boolean operators: conjunction, disjunction, and negation. We then prove that there exists a homomorphism between the task and value function algebras. Given a set of base tasks that have been previously solved by the agent, any new task written as a Boolean expression can immediately be solved without further learning, resulting in a zero-shot super-exponential explosion in the agent's abilities. We illustrate our approach in a simple domain, where an agent first learns to reach a number of rooms, after which it can then optimally solve any task expressible in the Boolean algebra. We then demonstrate composition in high-dimensional video game environments, where an agent first learns to collect different objects, and then compose these abilities to solve complex tasks immediately. Our results show that, even when function approximation is required, an agent can leverage its existing skills to solve new tasks without further learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel technique, data echoing, for reducing the amount of upstream computation needed to reach a competitive out-of-sample error rate on various datasets and model architectures. Data echoing is a simple, cheap, and effective method for reclaiming idle accelerator capacity by adding a stage to the training pipeline that repeats data from the previous stage. Results demonstrate that data echoing can provide a walltime speedup in practice, support a wide range of echoing factors, and benefit from additional shuffling after echoing. Countering expectations, data echoing reaches the same final error rate as well-tuned baselines.",
        "Abstract": "In the twilight of Moore's law, GPUs and other specialized hardware accelerators have dramatically sped up neural network training. However, earlier stages of the training pipeline, such as disk I/O and data preprocessing, do not run on accelerators. As accelerators continue to improve, these earlier stages will increasingly become the bottleneck. In this paper, we introduce “data echoing,” which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. We investigate the behavior of different data echoing algorithms on various workloads, for various amounts of echoing, and for various batch sizes. We find that in all settings, at least one data echoing algorithm can match the baseline's predictive performance using less upstream computation. We measured a factor of 3.25 decrease in wall-clock time for ResNet-50 on ImageNet when reading training data over a network.",
        "Introduction": "  INTRODUCTION Over the past decade, dramatic increases in neural network training speed have facilitated dramatic improvements in predictive performance by allowing researchers to train bigger models using larger datasets and to explore new ideas more rapidly. As Moore's law ends, general purpose processors are no longer rapidly becoming faster, but specialized hardware continues to drive significant speedups by optimizing for a narrower set of operations. For example, GPUs and TPUs 1 optimize for highly parallelizable matrix operations, which are core components of neural network training algorithms. However, neural network training requires more than just the operations that run well on accelerators - a training program may need to read and decompress training data, shuffle it, batch it, and even transform or augment it. These steps exercise multiple system components, including CPUs, disks, network bandwidth, and memory bandwidth. It is impractical to design specialized hardware for all these general operations that involve so many different components. Moreover, these operations are not simply executed once at the start of the training program. Since many of today's datasets are too large 2 to fit into an accelerator's memory or even the host machine's main memory, most large-scale neural network training systems stream over the training data, incrementally reading it from disk, pre-processing it in main memory, and copying successive batches of training examples to the accelerator, which runs the training algorithm. Therefore, each training step involves a mixture of operations that do and do not run on accelerators. There are workloads where the code running on accelerators consumes only a small portion of the overall wall time, and this scenario will only become more common if accelerator improvements continue to outpace improvements in CPUs. In order to speed up training in these cases, we must either (1) make the non-accelerator work faster, or (2) reduce the amount of non-accelerator work required to achieve the desired performance. Option (1) is appealing but requires substantial engineering labor or problem-specific techniques (e.g.  Ying et al., 2018 ;  Kumar et al., 2018 ). Adding more workers might be too expensive. Instead, we focus on option (2) and explore techniques for reducing the total amount of work spent reading and preparing inputs in the training pipeline.  Figure 1  shows the data processing and training pipeline for ResNet-50 ( He et al., 2016 ) on ImageNet, which is representative of many large-scale computer vision programs. First, the training program reads each image from disk, decompresses it into a 3 dimensional array of values, and pushes it into a shuffle buffer. The next stage of the pipeline samples images at random from the shuffle buffer to approximate shuffling the entire dataset, but with a fixed memory budget. The next stage performs pre-processing and data augmentation - each image is randomly cropped and resized to a 224 × 224 × 3 array, then randomly horizontally flipped, and finally has its colors randomly jittered. These random distortions help improve the generalization of vision models, and while these particular operations are specific to images, almost every deep learning pipeline performs some kind of pre-processing on its input data. Finally, images and labels are gathered into batches and sent to the accelerator to perform a step of minibatch stochastic gradient descent (SGD). For brevity, we will refer to the operation that updates the model's parameters for a given batch of training examples as the \"SGD step\" throughout the paper, even though variants of the basic SGD algorithm are also popular. Our technique applies equally well for any training algorithm that works on successive batches of training examples. To maximize throughput, the training program is often executed as a pipeline process, so that each stage in  Figure 1  operates in parallel from the other stages. Each stage might further employ multiple parallel worker threads or machines. If any of the stages upstream from the SGD step cannot process images at the same rate as the SGD step, the accelerator will be partly idle (see Figure 2a). This can happen for many reasons, including slow transfer from disk or cloud storage, time-consuming pre-processing operations, or inadequate tuning of the number of CPU threads dedicated to each stage of the pipeline. While it can be possible to improve training time by dedicating engineering effort to optimizing the input pipeline, such efforts are often time consuming and can distract from the practitioner's main goal of improving their model's predictive performance. Instead, we propose data echoing as a simple, cheap, and effective method for reclaiming idle accelerator capacity. Rather than waiting for more data to become available, we propose simply reusing data that is already available. We do this by adding a stage to the pipeline that repeats (or \"echoes\") data from the previous stage. Once a practitioner identifies the largest bottleneck in the training pipeline, they can insert an echoing stage after it to reclaim idle accelerator capacity (see Figure 2b). In this paper, we demonstrate that: 1. data echoing reduces the amount of upstream computation needed to reach a competitive out-of-sample error rate on various datasets and model architectures; 2. data echoing can provide a walltime speedup in practice; 3. data echoing can support a wide range of echoing factors; 4. the effectiveness of data echoing depends on the insertion point in the training pipeline; 5. data echoing can benefit from additional shuffling after echoing, but does not require it; and 6. countering expectations, data echoing reaches the same final error rate as well-tuned base- lines.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to address the problem of overconfident predictions in deep neural network (DNN) classifiers. We propose to explicitly train a classifier using out-of-distribution (OOD) samples generated by a generative adversarial network (GAN). We empirically show that, for effective OOD detection, the generated OOD samples should follow and be close to the low-density boundaries of in-distribution, and the proposed GAN training indeed tries to do that. We then train a multi-class softmax DNN classifier with in-distribution samples to minimize the standard cross-entropy loss and the generated OOD samples to minimize a KL loss that forces the classifier's predictive distribution to follow a uniform one. The resulting classifier is called a \"confident-classifier\" and can be used to classify a sample as being in or out-of distribution based on the maximum prediction probability or the entropy of the output.",
        "Abstract": "Discriminatively trained neural classifiers can be trusted, only when the input data comes from the training distribution (in-distribution).  Therefore, detecting out-of-distribution  (OOD)  samples is very important to avoid classification errors. In the context of OOD detection for image classification,  one of the recent approaches proposes training a classifier called “confident-classifier” by minimizing the standard cross-entropy loss on in-distribution samples and minimizing the KLdivergence between the predictive distribution of OOD samples in the low-density“boundary” of in-distribution and the uniform distribution (maximizing the entropy of the outputs).  Thus, the samples could be detected as OOD if they have low confidence or high entropy.  In this paper, we analyze this setting both theoretically and experimentally.  We also propose a novel algorithm to generate the“boundary” OOD samples to train a classifier with an explicit “reject” class for OOD samples.  We compare our approach against several recent classifier-based OOD detectors including the confident-classifiers on MNIST and Fashion-MNISTdatasets.  Overall the proposed approach consistently performs better than others across most of the experiments.",
        "Introduction": "  INTRODUCTION Discriminatively trained deep neural networks have achieved state of the art results in many clas- sification tasks such as speech recognition, image classification, and object detection. This has resulted in deployment of these models in real life applications where safety is paramount (e.g., autonomous driving). However, recent progress has shown that deep neural network (DNN) clas- sifiers make overconfident predictions even when the input does not belong to any of the known classes ( Nguyen et al. (2015) ). This follows from the design of DNN classifiers that are optimized over in-distribution data without the knowledge of OOD data. The resulting decision boundaries are typically \"unbounded/open\" as shown in Figure 1a resulting in over-generalization ( Spigler (2019) ,  Scheirer et al. (2012) ). There have been many approaches proposed to address this problem under the umbrella of OOD detection 1 .  Lee et al. (2018a)  propose to explicitly train a classifier using the OOD samples gener- ated by a GAN ( Goodfellow et al. (2014a) ). They empirically try to show that, for effective OOD detection, the generated OOD samples should follow and be close to the low-density boundaries of in-distribution, and the proposed GAN training indeed tries to do that. A multi-class softmax DNN classifier is trained with in-distribution samples to minimize the standard cross-entropy loss (minimizing the output entropy) and the generated OOD samples are trained to minimize a KL loss that forces the classifier's predictive distribution to follow a uniform one (maximizing the output entropy). The resulting classifier is called a \"confident-classifier\". One can then classify a sample as being in or out-of distribution based on the maximum prediction probability or the entropy of the output.  Sricharan & Srivastava (2018)  also follow a similar approach with slight modifications.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel weakly-supervised multi-instance visual learning framework, Multiple Instance Spatial Transformer (MIST), which allows for the use of top-K selection and end-to-end training. MIST is evaluated on three tasks: recovering the basis functions that created a given texture, classification of handwritten digits in cluttered scenes, and recognition of house numbers in real-world environments. Results show that MIST can reconstruct images as parts, as well as detect/classify instances without any location supervision.",
        "Abstract": "We propose a deep network that can be trained to tackle image reconstruction and classification problems that involve detection of multiple object instances, without any supervision regarding their whereabouts. The network learns to extract the most significant top-K patches, and feeds these patches to a task-specific network -- e.g., auto-encoder or classifier -- to solve a domain specific problem. The challenge in training such a network is the non-differentiable top-K selection process. To address this issue, we lift the training optimization problem by treating the result of top-K selection as a slack variable, resulting in a simple, yet effective, multi-stage training. Our method is able to learn to detect recurrent structures in the training dataset by learning to reconstruct images. It can also learn to localize structures when only knowledge on the occurrence of the object is provided, and in doing so it outperforms the state-of-the-art.",
        "Introduction": "  INTRODUCTION Finding and processing multiple instances of characteristic entities in a scene is core to many computer vision applications, including object detection ( Ren et al., 2015 ;  He et al., 2017 ;  Redmon & Farhadi, 2017 ), pedestrian detection ( Dollár et al., 2012 ;  Sewart & Andriluka, 2016 ;  Zhang et al., 2018a ), and keypoint localization ( Lowe, 2004 ;  Bay et al., 2008 ). In traditional vision pipelines, a common approach to localizing entities is to select the top-K responses in a heatmap and use their locations ( Lowe, 2004 ;  Bay et al., 2008 ;  Felzenszwalb et al., 2010 ). However, this type of approach does not provide a gradient with respect to the heatmap, and cannot be directly integrated into neural network-based systems. To overcome this challenge, previous work proposed to use grids ( Redmon et al., 2016 ;  He et al., 2017 ;  Detone et al., 2018 ) to simplify the formulation by isolating each instance ( Yi et al., 2016 ), or to optimize over multiple branches ( Ono et al., 2018 ). While effective, these approaches require additional supervision to localize instances, and do not generalize well outside their intended application domain. Other formulations, such as sequential attention ( Ba et al., 2015 ;  Gregor et al., 2015 ;  Eslami et al., 2015 ) and channel-wise approaches ( Zhang et al., 2018c ) are problematic to apply when the number of instances of the same object is large. Here, we introduce a novel way to approach this problem, which we term Multiple Instance Spatial Transformer, or MIST for brevity. As illustrated in  Figure 1  for the image synthesis task, given an image, we first compute a heatmap via a deep network whose local maxima correspond to locations of interest. From this heatmap, we gather the parameters of the top-K local maxima, and then extract the corresponding collection of image patches via an image sampling process. We process each patch independently with a task-specific network, and aggregate the network's output across patches. Training a pipeline that includes a non-differentiable selection/gather operation is non-trivial. To solve this problem we propose to lift the problem to a higher dimensional one by treating the parameters defining the interest points as slack variables, and introduce a hard constraint that they must correspond to the output that the heatmap network gives. This constraint is realized by introducing an auxiliary function that creates a heatmap given a set of interest point parameters. We then solve for the relaxed version of this problem, where the hard constraint is turned into a soft one, and the slack variables are also optimized within the training process. Critically, our training strategy allows the network to incorporate both non-maximum suppression and top-K selection. We evaluate the performance of our approach for 1 the problem of recovering the basis functions that created a given texture, Under review as a conference paper at ICLR 2020 2 classification of handwritten digits in cluttered scenes, and 3 recognition of house numbers in real-world environments. In summary, in this paper we: • introduce the MIST framework for weakly-supervised multi-instance visual learning; • propose an end-to-end training method that allows the use of top-K selection; • show that our framework can reconstruct images as parts, as well as detect/classify instances without any location supervision.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to deep neural networks (DNNs) for safety-critical systems. It proposes a statistical test to evaluate the realism of uncertainty mechanisms in regression tasks, and introduces a probabilistic U-Net-like FRRN semantic segmentation network to systematically assess the realism of two uncertainty mechanisms. The proposed approach to variational inference provides more realistic uncertainty estimates compared to existing approaches, and is suitable for use in safety-critical systems.",
        "Abstract": "Statistical models are inherently uncertain. Quantifying or at least upper-bounding their uncertainties is vital for safety-critical systems. While standard neural networks do not report this information, several approaches exist to integrate uncertainty estimates into them. Assessing the quality of these uncertainty estimates is not straightforward, as no direct ground truth labels are available. Instead, implicit statistical assessments are required. For regression, we propose to evaluate uncertainty realism---a strict quality criterion---with a Mahalanobis distance-based statistical test. An empirical evaluation reveals the need for uncertainty measures that are appropriate to upper-bound heavy-tailed empirical errors. Alongside, we transfer the variational U-Net classification architecture to standard supervised image-to-image tasks. It provides two uncertainty mechanisms and significantly improves uncertainty realism compared to a plain encoder-decoder model.",
        "Introduction": "  Introduction Having attracted great attention in both academia and digital economy, deep neural networks (DNNs) ( Goodfellow et al., 2016 ) are about to become vital components of safety-critical systems. Applications like early diagnoses of severe diseases and safe automated transport seem to be within reach ( Liu et al., 2014 ;  Bojarski et al., 2016 ). To actually deliver on these promises, the considerable potential of such safety-critical systems to harm humans and to cause severe damages has to be minimized. This fact comes with new challenges for the development of DNNs: next to the performance itself, further requirements such as low latency and high robustness gain in importance. Furthermore, safety-critical systems do not tolerate failures and hence have to be monitored and assessed at runtime to ensure safe functioning. One such mean of understanding the state of a software system is measuring the statistical uncertainty of the system module given the current input. Quantifying such uncertainties helps to make decisions especially in situations of partial availability of the relevant informa- tion. In particular in modular systems, subsequent modules should profit from the addition of knowledge about the uncertainty within the processing of the current input. Amongst others, Monte Carlo (MC) dropout and variational inference are promising ap- proaches to estimate the prediction uncertainty of DNNs (see section 2 for more details). These approaches try to estimate more realistically the statistical uncertainties of DNNs that go beyond computing dispersion metrics on the DNN's softmax output which is known to be rather easily fooled by adversarial perturbations ( Nguyen et al., 2015 ). For uncertainty estimates to be used in safety-critical systems, we require them to be realistic ( Horwood et al., 2014 ), i.e. we require these estimates to resemble the residuals (the fitting errors) of the neural network outcomes ( Figure 1 ). This poses a conceptual challenge as standard optimization schemes do not allow for direct training of realistic uncertainties. Therefore, high predictive performance and uncertainty realism might be largely unrelated to one another. It is desirable to achieve these two objectives at the same time. For regression, we put forward an evaluation scheme to test for uncertainty realism. For classification, we argue that existing assessment methods already (partly) satisfy these Under review as a conference paper at ICLR 2020 realism demands. Instead we propose a novel approach to variational inference that provides more realistic uncertainty estimates compared to existing approaches. In detail, our contribution is as follows: • We propose a statistical test that allows to evaluate the realism for uncertainty mechanisms in regression tasks. These test outcomes are empirically analyzed for a 4D regression task in the vision domain (object detection with SqueezeDet and MC dropout). Further analyses call for uncertainty measures that are appropriate to upper-bound heavy-tailed empirical errors. • We introduce a probabilistic U-Net-like FRRN semantic segmentation network and systematically assess the realism of the two uncertainty mechanisms it naturally provides. We find probabilistic FRRN to significantly improve uncertainty realism compared to (plain) FRRN.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel momentum decay rule for training deep neural networks (DNNs) which significantly surpasses the performance of both Adam and SGDM across a variety of datasets and networks. The proposed momentum decay rule is motivated by decaying the total contribution of a gradient to all future updates, with limited overhead and additional computation. Experiments are provided on various datasets, including MNIST, CIFAR-10, CIFAR-100, STL-10, Penn Treebank (PTB), and networks, including Convolutional Neural Networks (CNN) with Residual architecture (ResNet), Wide Residual architecture (Wide ResNet), Non-Residual architecture (VGG-16), Recurrent Neural Networks (RNN) with Long Short-Term Memory architecture (LSTM), Variational AutoEncoders (VAE), and the recent Noise Conditional Score Network (NCSN).",
        "Abstract": "Momentum is a simple and popular technique in deep learning for gradient-based optimizers. We propose a decaying momentum (Demon) rule, motivated by decaying the total contribution of a gradient to all future updates. Applying Demon to Adam leads to significantly improved training, notably competitive to momentum SGD with learning rate decay, even in settings in which adaptive methods are typically non-competitive. Similarly, applying Demon to momentum SGD rivals momentum SGD with learning rate decay, and in many cases leads to improved performance. Demon is trivial to implement and incurs limited extra computational overhead, compared to the vanilla counterparts. ",
        "Introduction": "  INTRODUCTION Deep Neural Networks (DNNs) have drastically advanced the state-of-the-art performance in many computer science applications, including computer vision ( Krizhevsky et al., 2012 ), ( He et al., 2016 ;  Ren et al., 2015 ), natural language processing ( Mikolov et al., 2013 ;  Bahdanau et al., 2014 ;  Gehring et al., 2017 ) and speech recognition (Sak et al., 2014;  Sercu et al., 2016 ). Yet, in the face of such significant developments, the age-old stochastic gradient descent (SGD), and the accelerated variant SGD with momentum (SGDM), algorithm remains one of the most, if not the most, popular method for training DNNs ( Sutskever et al., 2013 ;  Goodfellow et al., 2016 ;  Wilson et al., 2017 ). Adaptive methods ( Duchi et al., 2011 ; Zeiler, 2012;  Hinton et al., 2012 ;  Kingma & Ba, 2014 ;  Ma & Yarats, 2018 ) sought to simplify the training process, while providing similar performance. How- ever, while they are often used by practitioners, there are cases where their use leads to a performance gap ( Wilson et al., 2017 ; Shah et al., 2018). At the same time, much of the state-of-the-art perfor- mance on highly contested benchmarks-such as the image classification dataset ImageNet-have been produced with SGDM ( Krizhevsky et al., 2012 ;  He et al., 2016 ;  Xie et al., 2017 ; Zagoruyko & Komodakis, 2016;  Huang et al., 2017 ;  Ren et al., 2015 ;  Howard et al., 2017 ). Nevertheless, a key factor in any algorithmic success still lies in hyperparameter tuning. For exam- ple, in the literature above, they obtain such performance with a well-tuned SGD with momentum and a learning rate decay schedule, or with a proper hyperparameter tuning in adaptive methods. Slight changes in learning rate, learning rate decay, momentum, and weight decay (amongst others) can drastically alter performance. Hyperparameter tuning is arguably one of the most time consum- ing parts of training DNNs, and researchers often resort to a costly grid search. Thus, finding new and simple hyper-parameter tuning routines that boost the performance of state of the art algorithms is of ultimate importance and one of the most pressing problems in machine learning. The focus of this work is on the momentum parameter and how we can boost the performance of training methods with a simple technique. Momentum helps speed up learning in directions of low curvature, without becoming unstable in directions of high curvature. Minimizing the objective function L(·), the simplest and most common momentum method, SGDM, is given by the following recursion for variable vector θ t ∈ R p : The coefficient β-traditionally, selected constant in [0, 1]-controls how quickly the momentum decays, g t represents a stochastic gradient, usually E[g t ] = ∇L(θ t ), and η > 0 is the step size. But how do we select β? The most prominent choice among practitioners is β = 0.9. This is supported by recent works that prescribe it ( Chen et al., 2016 ;  Kingma & Ba, 2014 ;  Hinton et al., Under review as a conference paper at ICLR 2020 2012 ;  Reddi et al., 2019 ), and by the fact that most common softwares, such as PyTorch ( Paszke et al., 2017 ), declare β = 0.9 as the default value in their optimizer implementations. However, there is no indication that this choice is universally well-behaved. There are papers that attempt to tune the momentum parameter. Under an asynchronous distributed setting, ( Mitliagkas et al., 2016 ) observe that running SGD asynchronously is similar to adding a momentum-like term to SGD; they also provide experimental evidence that naively setting β = 0.9 would result in a momentum \"overdose\", leading to suboptimal performance. As another example, YellowFin ( Zhang & Mitliagkas, 2017 ) is a learning rate and momentum adaptive method for both the synchronous and asynchronous setting, motivated by a quadratic model analysis and some ro- bustness insights. The main message of that work is that, like η, momentum acceleration needs to be carefully selected based on properties of the objective, the data, and the underlying computational resources. Finally, moving from classical DNN settings towards generative adversarial networks (GANs), the proposed momentum values tend to decrease from β = 0.9 ( Mirza & Osindero, 2014 ;  Radford et al., 2015 ;  Arjovsky et al., 2017 ), taking even negative values ( Gidel et al., 2018 ). In this paper, we introduce a novel momentum decay rule which significantly surpasses the perfor- mance of both Adam and SGDM (as they are used currently), in addition to other state-of-the-art adaptive learning rate and adaptive momentum methods, across a variety of datasets and networks. In particular, our findings can be summarized as follows: i) We propose a new momentum decay rule, motivated by decaying the total contribution of a gradient to all future updates, with limited overhead and additional computation. ii) Using the momentum decay rule with Adam, we observe large performance gains-relative to vanilla Adam-where the network continues to learn for far longer after Adam begins to plateau, and suggest that the momentum decay rule should be used as default for this method. iii) We observe comparative performance for SGDM between momentum decay and learning rate decay; an interesting result given the unparalleled effectiveness of learning rate decay schedule. Experiments are provided on various datasets, including MNIST, CIFAR-10, CIFAR-100, STL-10, Penn Treebank (PTB), and networks, including Convolutional Neural Networks (CNN) with Resid- ual architecture (ResNet) ( He et al., 2016 ), Wide Residual architecture (Wide ResNet) (Zagoruyko & Komodakis, 2016), Non-Residual architecture (VGG-16) ( Simonyan & Zisserman, 2014 ), Recur- rent Neural Networks (RNN) with Long Short-Term Memory architecture (LSTM) ( Hochreiter & Schmidhuber, 1997 ), Variational AutoEncoders (VAE) ( Kingma & Welling, 2015 ), and the recent Noise Conditional Score Network (NCSN) (Song & Ermon, 2019).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper considers the sign gradient descent (signGD) method for unconstrained, continuous optimization problems, min x∈R d f (x), with a differentiable and lower-bounded objective f : R d → R. SignGD is attractive in distributed optimization due to its reduced communication cost, and is closely related to the popular Adam method. This paper studies signGD and aims to gain a better understanding of the Adam method.",
        "Abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.",
        "Introduction": "  INTRODUCTION We consider an unconstrained, continuous optimization problem, min x∈R d f (x), with a differen- tiable and lower-bounded objective f : R d → R. The prototypical first-order optimization algorithm to solve this problem is gradient descent (GD), which iteratively updates x t+1 = x t −α t ∇f t , where f t := f (x t ). Several recent works have considered the sign gradient descent (signGD) method, x t+1 = x t − α t sign(∇f t ) , (1) where the application of the sign is to be understood elementwise. This method is attractive in distributed optimization where it conveniently reduces the communication cost to a single bit per gradient coordinate (e.g.,  Seide et al., 2014 ;  Karimireddy et al., 2019 ), but its interest extends beyond that.  Balles & Hennig (2018)  and  Bernstein et al. (2018)  point out that sign gradient descent is closely related to the popular Adam method ( Kingma & Ba, 2015 ) and demonstrate that it often achieves similar practical performance on deep learning tasks. Studying sign gradient descent can thus be seen as a step towards a better understanding of the ubiquitous Adam method.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an end-to-end learning framework for learning representations of signals or images from irregularly-sampled observation datasets. The proposed framework jointly embeds an energy form and an associated interpolation scheme, and is demonstrated to be relevant for different data types, such as time series, images and image sequences, with potentially high missing data rates. The energy form is based on a neural-network architecture, and includes Markovian priors embedded in CNNs. Numerical experiments are reported to demonstrate the effectiveness of the proposed framework.",
        "Abstract": "For numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often irregular space-time sampling patterns and large missing data rates. These sampling properties is a critical issue to apply state-of-the-art learning-based (e.g., auto-encoders, CNNs,...) to fully benefit from the available large-scale observations and reach breakthroughs in the reconstruction and identification of processes of interest. In this paper, we address the end-to-end learning of representations of signals, images and image sequences from irregularly-sampled data, {\\em i.e.} when the training data involved missing data. From an analogy to Bayesian formulation, we consider energy-based representations. Two energy forms are investigated: one derived from auto-encoders and one relating to Gibbs energies. The learning stage of these energy-based representations (or priors) involve a joint interpolation issue, which resorts to solving an energy minimization problem under observation constraints. Using a neural-network-based implementation of the considered energy forms, we can state an end-to-end learning scheme from irregularly-sampled data. We demonstrate the relevance of the proposed representations for different case-studies: namely, multivariate time series, 2{\\sc } images and image sequences.",
        "Introduction": "  INTRODUCTION In numerous application domains, the available observation datasets do not involve gap-free and regularly-gridded signals or images. The irregular-sampling may result both from the characteristics of the sensors and sampling strategy, e.g. considered orbits and swaths in spacebone earth observa- tion and astrophysics, sampling schemes in medical imaging, as well as environmental conditions which may affect the sensor, e.g. atmospheric conditions and clouds for earth observation. A rich literature exists on interpolation for irregularly-sampled signals and images (also referred to as inpainting in image processing ( 4 )). A classic framework states the interpolation issue as the miminisation of an energy, which may be interpreted in a Bayesian framework. A variety of en- ergy forms, including Markovian priors ( 12 ), patch-based priors ( 20 ), gradient norms in variational and/or PDE-based formulations ( 4 ), Gaussian priors () as well as dynamical priors in fluid dynamics ( 3 ). The later relates to optimal interpolation and kriging ( 8 ), which is among the state-of-the-art and operational schemes in geoscience ( 10 ). Optimal schemes classically involve the inference of the considered covariance-based priors from irregularly-sampled data. This may however be at the expense of Gaussianity and linearity assumptions, which do not often apply for real signals and im- ages. For the other types of energy forms, their parameterization are generally set a priori and not learnt from the data. Regarding more particularly data-driven and learning-based approaches, most previous works ( 2 ;  11 ;  20 ) have addressed the learning of interpolation schemes under the assump- tion that a representative gap-free dataset is available. This gap-free dataset may be the image itself ( 9 ;  20 ;  18 ). For numerous application domains, as mentionned above, this assumption cannot be fulfilled. Regarding recent advances in learning-based schemes, a variety of deep learning models, e.g. ( 7 ;  16 ;  24 ;  23 ), have been proposed. Most of these works focus on learning an interpolator. One may however expect to learn not only an interpolator but also some representation of considered data, which may be of interest for other applications. In this respect, RBM models (Restricted Boltzmann Under review as a conference paper at ICLR 2020 Machines) ( 22 ;  6 ) are particularly appealing at the expense however of computationally-expensive MCMC schemes. In this paper, we aim to learn representations of signals or images from irregularly-sampled obser- vation datasets. Our contribution is three-fold: • an end-to-end learning of energy-based representations from irregularly-sampled training data. Based on a neural-network architecture, it jointly embeds the considered energy form and an associated interpolation scheme. • besides classic auto-encoder representations, we introduce NN-based Gibbs-Energy repre- sentations, which relate to Markovian priors embedded in CNNs. • the demonstration of the relevance of the proposed end-to-end learning framework for dif- ferent data types, namely time series, images and image sequences, with possibly very high missing data rates. The remainder is organized as follows. Section 2 formally states the considered issue. We introduce the proposed end-to-end learning scheme in Section 3. We report numerical experiments in Section 4 and discuss our contribution with respect to related work in Section ??.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of model-based methods to increase the data efficiency of exploration in autonomous robots. We propose a method called Planning for Policy Search (PPS) which combines planning and policy search to reduce the number of samples required to learn a good policy. We evaluate the data generated by PPS compared to those from Deep-Reinforcement Learning (D-RL) methods, and investigate whether PPS is less susceptible to local optima than D-RL methods. We also analyze whether the data collected by PPS can be reused more easily. Our results show that PPS is able to cover a larger area of the state space and is less susceptible to local optima than D-RL methods.",
        "Abstract": "Most Deep Reinforcement Learning methods perform local search and\ntherefore are prone to get stuck on non-optimal\nsolutions. Furthermore, in simulation based training, such as\ndomain-randomized simulation training, the availability of a simulation\nmodel is not exploited, which potentially decreases\nefficiency. To overcome issues of local search and exploit\naccess to simulation models, we propose the use of kino-dynamic\nplanning methods as part of a model-based reinforcement learning\nmethod and to learn in an off-policy fashion from solved planning\ninstances. We show that, even on a simple toy domain, D-RL\nmethods (DDPG, PPO, SAC) are not immune to local optima and\nrequire additional exploration mechanisms. We show that our\nplanning method exhibits a better state space coverage, collects\ndata that allows for better policies than D-RL methods without\nadditional exploration mechanisms and that starting from the\nplanner data and performing additional training results in as\ngood as or better policies than vanilla D-RL methods, while also\ncreating data that is more fit for re-use in modified tasks.\n",
        "Introduction": "  INTRODUCTION Robots in human-centric environments are confronted with less structured, more varied and more quickly changing situations than in typical automated manufacturing environments. Research in autonomous robots adresses these challenges using modern machine learning methods. However, learning and trying out actions directly on a real robot is time-consuming and potentially dangerous to the environment as well as to the robot. In contrast, physically-based simulation provides the benefit of faster, cheaper, and safer ways for robot learning. If simulation models are available, they can be used by sampling-based planning methods that are able to directly plan robot behaviour using these models. However, the time required to perform planning can make this intractable for execution. Finding policies that directly map from the current state to the next applicable action eliminates the need for planning. While Deep-Reinforcement Learning (D-RL) has shown promising results, for example those by  OpenAI et al. (2018) , D-RL training can be tedious and resource demanding.  Plappert et al. (2017)  report problems on the HalfCheetah environment where the algorithms con- verge to a local optimum corresponding to the cheetah wiggling on its back. They alleviated this problem by a different exploration scheme. In preliminary experiments (not included in this paper) we found similar problems: D-RL algo- rithms were not able to learn a pushing task with a simulated 7-DoF robot arm. The algorithms we used were Deep Deterministic Policy Gradient (DDPG)  Lillicrap et al. (2015)  and Proximal Policy Gradient (PPO)  Schulman et al. (2017)  (from OpenAI Baselines by  Dhariwal et al. (2017) ). The algorithms were also not reaching relevant parts of the state space. Consequently, and in line with the findings of  Plappert et al. (2017)  we assume that part of the problem of failing to learn good policies is related to insufficient exploration. To remedy this problem, one might increase search time while keeping exploration noise high, or use more principled exploration. While in- creasing search time will in the limit also yield acceptable solutions, directed exploration appears more promising to find good solutions more reliably and in less time. We thus focus on the latter approach, as covering a more diverse area of the state space increases the chances of finding an optimal solution, and moving away from random or exhaustive search reduces the number of samples required to learn a good policy. Model-based methods can use their models of the task in an efficient way to plan over multiple steps and explore the state space in a more directed way. Given an accurate model, optimal policies can be produced without interacting with the world and thus with fewer samples ( Hester & Stone, 2012 ). In particular, Rapidly Exploring Random Tree (RRT) are planning methods that focus on maximizing state-space exploration. We propose to take advantage of the benefits the aforementioned planning methods provide while tackling the problem of planning time by synthesizing the planning results into a policy. This essen- tially makes the proposed method a model-based method ( Sutton & Barto, 2018 ). We will refer to this method as Planning for Policy Search (PPS). This is of particular interest in domain-randomized training, where simulation models are always available, to increase the data efficiency of exploration. Here we investigate a preliminary version of this method that combines planning and policy search but does not perform randomizations yet. In particular, we investigate the following questions: Q1 How do the data generated by RRT compare to those from D-RL methods? Do they cover a larger area of the state space? Do the reward distributions differ? Q2 Are PPS methods less susceptible to local optima than D-RL methods? Q3 Can the data collected by PPS be reused more easily? The experimental setup used to investigate these questions is described in  Figure 1  . In a simulated environment, the planner and reinforcement learning agent are run - each separately - to generate environment interactions. In the case of the reinforcement learning agent a policy is learned, and its return is evaluated (Q2, Sec. 4.2 ). In both cases, the collected data are stored as a dataset. In a second step, these datasets are analyzed with respect to their state-space coverage (Q1, Sec. 4.1 ). Then the datasets are used to train an RL agent in an off-policy fashion. The returns of this agent's policy are again evaluated (Q1, Sec. 4.1 ). In a further experiment an agent is trained partially from these datasets and partially from experience it generates (Q3, Sec. 4.3 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a deep neural network architecture designed to estimate the probability of passing events in soccer from low-level tracking-data input and weak labeling of each event's success. The network is able to estimate the probability of pass success at any location on the field while providing a visual representation of these probabilities. Results from experiments on a broad set of 208,489 passing events from professional soccer matches show that the model is able to properly estimate the complete probability map and achieve considerably better results for single pass probability estimation than baseline estimation, linear and non-linear models built on top of handcrafted features.",
        "Abstract": "We propose a fully convolutional network architecture that is able to estimate a full surface of pass probabilities from single-location labels derived from high frequency spatio-temporal data of professional soccer matches. The network is able to perform remarkably well from low-level inputs by learning a feature hierarchy that produces predictions at different sampling levels that are merged together to preserve  both coarse and fine detail. Our approach presents an extreme case of weakly supervised learning where there is just a single pixel correspondence between ground-truth outcomes and the predicted probability map. By providing not just an accurate evaluation of observed events but also a visual interpretation of the results of other potential actions, our approach opens the door for spatio-temporal decision-making analysis, an as-yet little-explored area in sports. Our proposed deep learning architecture can be easily adapted to solve many other related problems in sports analytics; we demonstrate this by extending the network to learn to estimate pass-selection likelihood.",
        "Introduction": "  INTRODUCTION Sports analytics is a fast-growing research field with a strong focus on data-driven performance analysis from professional athletes and teams. Soccer, as well as many other team-sports, has recently benefited from the availability of high-frequency tracking data of both player and ball locations, facilitating the development of fine-grained spatio-temporal performance metrics ( Rein & Memmert, 2016 ;  Stein et al., 2017 ). One of the main goals of performance analysis is to answer specific questions from soccer coaches, but in order to do so we require models to be robust enough to capture the nuances of a complex sport, and be highly interpretable so findings can be communicated effectively. In other words, we need models to be both accurate and also translatable to soccer coaches and game analysts in visual terms. The vast majority of research in soccer analytics addresses questions related to either the most commonly observed events during a match, such as goals, shots and passes or the effects of players' movements and match dynamics ( Gudmundsson & Horton, 2017 ). Most modeling approaches share one or more common issues, such as: heavy use of handcrafted features, no visual interpretability, and coarse representations that ignore meaningful spatial relationships. While many of these methods are able to provide valid insights into specific problems, in terms of model design we still lack a comprehensive approach that can learn from lower-level input, exploit spatial relationships on any location (beyond location of the origin and destination of events), and provide not only accurate evaluations of observed events at their given location but also an estimation of unobserved events at any other location on the field. In this work we present a deep neural network architecture designed to estimate the probability of passing events in soccer from low-level tracking-data input and weak labeling of each event's success. The network is able to estimate the probability of pass success at any location on the field while providing a visual representation of these probabilities. Beyond pass probabilities, such an architecture can be adapted to many other problems such as pass selection, pass risk and reward Under review as a conference paper at ICLR 2020 evaluation, expected possession value estimation, and player movement evaluation, among many others. The presented architecture was inspired by recently developed fully convolutional networks that have been proven to be successful for image segmentation ( Long et al., 2015 ;  Yu & Koltun, 2015 ;  Pathak et al., 2015 ;  Jing & Tian, 2019 ). Our problem is similar in many ways to that of image segmentation, where the objective is to provide a pixel-level classification of objects in the image. Image segmentation is usually addressed as a weakly supervised learning problem where only single labels of the objects in the image are provided, instead of pixel-wise labeling. In the case of pass probability, as well as in most of the other mentioned applications in soccer, only a single label is available for each pass to indicate whether it was successful or not, as well as the location of the origin and destination of passes. However, unlike image segmentation where an object label is typically associated with multiple pixels in the image, here the label only corresponds to a single location in the original input. This set up presents an extreme case of weakly supervised learning. We use a deep jet-inspired nonlinear feature hierarchy ( Long et al., 2015 ) to combine coarse and fine layers, through convolutional fusion layers. One of the main contributions of this paper is the computation of the network loss by single-pixel masking, according to the destination location associated to each pass event. We show that from backpropagation of the loss between a single-location prediction of the full output matrix and the known outcome of the event we can learn complex spatial features and successfully estimate the pass probability map for any location on the field. We compare our model with a baseline estimation of pass probability, as well as with linear and non-linear models built on top of handcrafted features, and achieve considerably better results for single pass probability estimation, while also properly estimating the complete probability map. In the following sections we describe related work, provide a detailed explanation of the architecture and design considerations of the model, and present experimental results on a broad set of 208, 489 passing events from professional soccer matches.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes an end-to-end differentiable neural network approach for learning and utilizing the notion of a variable, which can be used to lift examples into invariants used by the network to perform reasoning tasks. The proposed architecture is capable of learning and using variables by lifting a given example through soft unification, and is evaluated on four datasets. The results and analysis of the learned invariants that capture the underlying patterns present in the tasks are presented.",
        "Abstract": "Human reasoning involves recognising common underlying principles across many examples by utilising variables. The by-products of such reasoning are invariants that capture patterns across examples such as \"if someone went somewhere then they are there\" without mentioning specific people or places. Humans learn what variables are and how to use them at a young age, and the question this paper addresses is whether machines can also learn and use variables solely from examples without requiring human pre-engineering. We propose Unification Networks that incorporate soft unification into neural networks to learn variables and by doing so lift examples into invariants that can then be used to solve a given task. We evaluate our approach on four datasets to demonstrate that learning invariants captures patterns in the data and can improve performance over baselines.",
        "Introduction": "  INTRODUCTION Humans have the ability to process symbolic knowledge and maintain symbolic thought ( Unger & Deacon, 1998 ). When reasoning, humans do not require combinatorial enumeration of exam- ples but instead utilise invariant patterns with placeholders replacing specific entities. Symbolic cognitive models ( Lewis, 1999 ) embrace this perspective with the human mind seen as an informa- tion processing system operating on formal symbols such as reading a stream of tokens in natural language. The language of thought hypothesis ( Morton & Fodor, 1978 ) frames human thought as a structural construct with varying sub-components such as \"X went to Y\". By recognising what varies across examples, humans are capable of lifting examples into invariant principles that account for other instances. This symbolic thought with variables is learned at a young age through symbolic play ( Piaget, 2001 ). For instance a child learns that a sword can be substituted with a stick ( Frost et al., 2004 ) and engage in pretend play. Although variables are inherent in models of computation and symbolic formalisms, as in first-order logic ( Russell & Norvig, 2016 ), they are pre-engineered and used to solve specific tasks by means of unification or assignments that bound variables to given values. However, when learning from data only, being able to recognise when and which symbols should take on dif- ferent values, i.e. symbols that can act as variables, is crucial for lifting examples into general principles that are invariant across multiple instances.  Figure 1  shows the invariant learned by our approach: if someone is the same thing as someone else then they have the same colour. With this invariant, our approach can solve all of the training and test examples in task 16 of the bAbI dataset ( Weston et al., 2016 ). In this paper we address the question of whether a machine can learn and use the notion of a variable, i.e. a symbol that can take on different values. For instance, given an example of the form \"bernhard is a frog\" the machine would learn that the token \"bernhard\" could be someone else and the token \"frog\" could be something else. If we consider unification a selection of the most appropriate value for a variable given a choice of values, we can reframe it as a form of attention. Attention models ( Bahdanau et al., 2015 ;  Luong et al., 2015 ;  Chaudhari et al., 2019 ) allow neural networks to focus, attend to certain parts of the input often for the purpose of selecting a relevant portion. Since attention mechanisms are also differentiable they are often jointly Under review as a conference paper at ICLR 2020 learned within a task. This perspective motivates our idea of a unification mechanism that utilises attention and is therefore fully differentiable which we refer to as soft unification. Hence, we propose an end-to-end differentiable neural network approach for learning and utilising the notion of a variable that in return can lift examples into invariants used by the network to perform reasoning tasks. Specifically, we (i) propose a novel architecture capable of learning and using variables by lifting a given example through soft unification, (ii) present the empirical results of our approach on four datasets and (iii) analyse the learned invariants that capture the underlying patterns present in the tasks. Our implementation using Chainer ( Tokui et al., 2015 ) is publicly available at [link removed](anonymous link provided with submission).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the use of model uncertainty in predictive medicine, focusing on the application of deep neural networks combined with advanced model uncertainty methods. We discuss how model uncertainty can directly impact clinical care by answering questions related to population-level metric performance, calibration of predictions, feature values, and optimal decisions. We also discuss the implications of model uncertainty in terms of patient subgroups, such as ethnicity, gender, age, and length of stay.",
        "Abstract": "In medicine, both ethical and monetary costs of incorrect predictions can be significant, and the complexity of the problems often necessitates increasingly complex models. Recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. In light of this, we investigate the role of model uncertainty methods in the medical domain. Using RNN ensembles and various Bayesian RNNs, we show that population-level metrics, such as AUC-PR, AUC-ROC, log-likelihood, and calibration error, do not capture model uncertainty. Meanwhile, the presence of significant variability in patient-specific predictions and optimal decisions motivates the need for capturing model uncertainty. Understanding the uncertainty for individual patients is an area with clear clinical impact, such as determining when a model decision is likely to be brittle. We further show that RNNs with only Bayesian embeddings can be a more efficient way to capture model uncertainty compared to ensembles, and we analyze how model uncertainty is impacted across individual input features and patient subgroups.",
        "Introduction": "  INTRODUCTION Machine learning has found great and increasing levels of success in the last several years on many well-known benchmark datasets. This has led to a mounting interest in non-traditional problems and domains, each of which bring their own requirements. In medicine specifically, individualized predictions are of great importance to the field ( Council et al., 2011 ), and there can be severe costs for incorrect predictions and decisions due to the risk to human life and the associated ethical concerns ( Gillon, 1994 ). Existing state-of-the-art approaches using deep neural networks in medicine often make use of either a single model or an average over a small ensemble of models, focusing on improving the accuracy of probabilistic predictions ( Harutyunyan et al., 2017 ;  Rajkomar et al., 2018b ;  Xu et al., 2018 ;  Choi et al., 2018 ). These works, while focusing on capturing the data uncertainty, do not address the model uncertainty that is inherent in fitting deep neural networks. For example, when predicting patient mortality in an ICU setting, existing approaches might be able to achieve high AUC-ROC, but will be unable to differentiate between patients for whom the model is certain about its probabilistic prediction, and those for whom the model is fairly uncertain. In this paper, we examine the use of model uncertainty specifically in the context of predictive medicine. Model uncertainty has made many methodological advances in recent years-including reparameterization-based variational Bayesian neural networks ( Blundell et al., 2015 ;  Kucukelbir et al., 2017 ;  Louizos & Welling, 2017 ), Monte Carlo dropout ( Gal & Ghahramani, 2016 ), ensembles ( Lakshminarayanan et al., 2017 ), and function priors ( Hafner et al., 2018 ;  Garnelo et al., 2018 ;  Malinin & Gales, 2018 ). Deep neural networks combined with advanced model uncertainty methods can directly impact clinical care by answering several questions that naturally occur in predictive medicine: • How do the realized functions in any of the approaches, such as individual models in the ensemble approach, compare in terms of population-level metric performance such as AUC-PR, AUC-ROC, or log-likelihood? • If and how does model uncertainty assist in calibrating predictions? Under review as a conference paper at ICLR 2020 • How does model uncertainty change across different patient subgroups, in terms of ethnicity, gender, age, or length of stay? • How do various feature values contribute towards model uncertainty? • How does model uncertainty affect optimal decisions made under a given clinically-relevant cost function?",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel sparse discriminative Gaussian mixture (SDGM) model for supervised classification. The SDGM is a GMM-based discriminative model that is trained by sparse Bayesian learning, which improves the generalization capability by obtaining a sparse solution and determines the number of components automatically by removing redundant components. Furthermore, the SDGM can be embedded into neural networks (NNs) such as convolutional NNs and trained in an end-to-end manner with an NN. The paper demonstrates that the SDGM can show superior performance than the fully connected layer with a softmax function via an end-to-end learning with an NN on the image recognition task.",
        "Abstract": "In probabilistic classification, a discriminative model based on Gaussian mixture exhibits flexible fitting capability. Nevertheless, it is difficult to determine the number of components. We propose a sparse classifier based on a discriminative Gaussian mixture model (GMM), which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and automatically determines the number of components by removing redundant components. The SDGM can be embedded into neural networks (NNs) such as convolutional NNs and can be trained in an end-to-end manner. Experimental results indicated that the proposed method prevented overfitting by obtaining sparsity. Furthermore, we demonstrated that the proposed method outperformed a fully connected layer with the softmax function in certain cases when it was used as the last layer of a deep NN.",
        "Introduction": "  INTRODUCTION In supervised classification, probabilistic classification is an approach that assigns a class label c to an input sample x by estimating the posterior probability P (c|x). This approach is primarily categorized into two types of models: discriminative model and generative model. The former optimizes the posterior distribution P (c|x) directly on a training set, whereas the latter finds the class conditional distribution P (x|c) and class prior P (c) and subsequently derives the posterior distribution P (c|x) using Bayes' rule. The discriminative model and generative model are mutually related ( Lasserre et al., 2006 ;  Minka, 2005 ). According to  Lasserre et al. (2006) , the only difference between these models is their sta- tistical parameter constraints. Therefore, given a certain generative model, we can derive a corre- sponding discriminative model. For example, the discriminative model corresponding to a unimodal Gaussian distribution is logistic regression (see Appendix A for derivation). Several discriminative models corresponding to the Gaussian mixture model (GMM) have been proposed ( Axelrod et al., 2006 ;  Bahl et al., 1996 ;  Klautau et al., 2003 ;  Tsai & Chang, 2002 ;  Tsuji et al., 1999 ;  Tüske et al., 2015 ;  Wang, 2007 ). They indicate more flexible fitting capability than the generative GMM and have been applied successfully in fields such as speech recognition ( Axelrod et al., 2006 ;  Tüske et al., 2015 ;  Wang, 2007 ). The problem to address in mixture models such as the GMM is the determination of the number of components M . Classically, Akaike's information criterion and the Bayesian information criterion have been used; nevertheless, they require a considerable computational cost because a likelihood must be calculated for every candidate component number. In the generative GMM, methods that optimize M during learning exist ( Crouse et al., 2011 ; Štepánová & Vavrečka, 2018 ). However, in a discriminative GMM, a method to optimize M simultaneously during learning has not been clearly formulated. In this paper, we propose a novel GMM having two important properties: sparsity and discriminabil- ity, which is named sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained by sparse Bayesian learning. This learning algorithm improves the generalization capability by obtaining a sparse solution and determines the number of components automatically by removing redundant components. Furthermore, the SDGM can be embedded into Under review as a conference paper at ICLR 2020 neural networks (NNs) such as convolutional NNs and trained in an end-to-end manner with an NN. To the authors best knowledge, there is no GMM that has both of sparsity and discriminability. The contributions of this study are as follows: • We propose a novel sparse classifier based on a discriminative GMM. The proposed SDGM has both sparsity and discriminability, and determines the number of components automat- ically. The SDGM can be considered as the theoretical extension of the discriminative GMM and the relevance vector machine (RVM) ( Tipping, 2001 ). • This study attempts to connect both fields of probabilistic models and NNs. From the equivalence of a discriminative model based on Gaussian distribution to a fully connected layer, we demonstrate that the SDGM can be used as a module of a deep NN. We also show that the SDGM can show superior performance than the fully connected layer with a softmax function via an end-to-end learning with an NN on the image recognition task.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes an improved evaluation procedure, SABER (Standardized Atari BEnchmark for Reinforcement learning), for evaluating the performance of different deep reinforcement learning (DRL) algorithms on the Atari Learning Environment (ALE). SABER is benchmarked on the world records human baseline and current state-of-the-art DRL algorithm Rainbow, and a new state-of-the-art agent, Rainbow-IQN, is introduced and benchmarked. Results show that the Atari benchmark is a hard task for current general algorithms, and Rainbow-IQN provides an improvement range for future comparisons. An open-source implementation of Rainbow and Rainbow-IQN is provided for reproducibility.",
        "Abstract": "Consistent and reproducible evaluation of Deep Reinforcement Learning (DRL) is not straightforward. In the Arcade Learning Environment (ALE), small changes in environment parameters such as stochasticity or the maximum allowed play time can lead to very different performance. In this work, we discuss the difficulties of comparing different agents trained on ALE. In order to take a step further towards reproducible and comparable DRL, we introduce SABER, a Standardized Atari BEnchmark for general Reinforcement learning algorithms. Our methodology extends previous recommendations and contains a complete set of environment parameters as well as train and test procedures. We then use SABER to evaluate the current state of the art, Rainbow. Furthermore, we introduce a human world records baseline, and argue that previous claims of expert or superhuman performance of DRL might not be accurate. Finally, we propose Rainbow-IQN by extending Rainbow with Implicit Quantile Networks (IQN) leading to new state-of-the-art performance. Source code is available for reproducibility.",
        "Introduction": "  INTRODUCTION Human intelligence is able to solve many tasks of different natures. In pursuit of generality in artificial intelligence, video games have become an important testing ground: they require a wide set of skills such as perception, exploration and control. Reinforcement Learning (RL) is at the forefront of this development, especially when combined with deep neural networks in DRL. One of the first general approaches reaching reasonable performance on many Atari games while using the exact same hyper-parameters and neural network architecture was Deep Q-Network (DQN) ( Mnih et al., 2015 ), a value based DRL algorithm which directly takes the raw image as input. This success sparked a lot of research aiming to create better, faster and more stable general algorithms. The ALE ( Bellemare et al., 2013 ), featuring more than 60 Atari games (see  Figure 1 ), is heavily used in this context. It provides many different tasks ranging from simple paddle control in the ball game Pong to complex labyrinth exploration in Montezuma's Revenge which remains unsolved by general algorithms up to today. In this work, we first discuss current issues in the evaluation procedure of different DRL algorithms on ALE and their impact. We then propose an improved evaluation procedure, extending the rec- ommendations of  Machado et al. (2018) , named SABER : a Standardized Atari BEnchmark for Under review as a conference paper at ICLR 2020 Reinforcement learning. We suggest benchmarking on the world records human baseline and show that RL algorithms are in fact far from solving most of the Atari games. As an illustration of SABER, current state-of-the-art DRL algorithm Rainbow ( Hessel et al., 2018 ) is benchmarked. Finally, we introduce and benchmark on SABER a new state-of-the-art agent: a distributable combination of Rainbow and Implicit Quantiles Network (IQN) ( Dabney et al., 2018 ). The main contributions of this work are : • The proposal, description and justification of the SABER benchmark. • Introduction of a world records human baseline. We argue it is more representative of the human level than the one used in most of previous works. With this metric, we show that the Atari benchmark is in fact a hard task for current general algorithm. • A SABER compliant evaluation of current state-of-the art agent Rainbow. • A new state-of-the-art agent on Atari, Rainbow-IQN, with a comparison on SABER to Rainbow, to give an improvement range for future comparisons. • For reproducibility sake, an open-source implementation of Rainbow, Rainbow-IQN, dis- tributed following the idea from  Horgan et al. (2018) .",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates why deep ensembles trained with random initialization work so well in practice. We analyze the loss landscape of a deep neural network trained on a classification task and show that the functions sampled from different randomly initialized trajectories tend to be very diverse, while functions sampled along a single training trajectory or subspace thereof tend to be very similar in predictions. We also find that solution modes are connected in the loss landscape but they are distinct in the space of predictions, and low-loss tunnels create functions with near-identical low values of loss along the path, however these functions tend to be very different in function space.",
        "Abstract": "Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty  and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. We demonstrate that while low-loss connectors between modes exist, they are not connected in the space of predictions. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods.",
        "Introduction": "  INTRODUCTION Consider a typical classification problem, where x n ∈ R D denotes the D-dimensional features and y n ∈ [1, . . . , K] denotes the class label. Assume we have a parametric model p(y|x, θ) for the conditional distribution where θ denotes weights and biases of a neural network, and p(θ) is a prior distribution over parameters. The Bayesian posterior over parameters is given by Computing the exact posterior distribution over θ is computationally expensive (if not impossible) when p(y n |x n , θ) is a deep neural network. A variety of approximations have been developed for Bayesian neural networks, including Laplace approximation ( MacKay, 1992 ), Markov chain Monte Carlo methods ( Neal, 1996 ;  Welling & Teh, 2011 ;  Springenberg et al., 2016 ), variational Bayesian methods ( Graves, 2011 ;  Blundell et al., 2015 ;  Louizos & Welling, 2017 ;  Wen et al., 2018 ) and Monte-Carlo dropout ( Gal & Ghahramani, 2016 ;  Srivastava et al., 2014 ). While computing the posterior is challenging, it is usually easy to perform maximum-a-posteriori (MAP) estimation, which corresponds to a mode of the posterior. The MAP solution can be written as the minimizer of the following loss (negative log likelihood + negative log prior): The MAP solution is computationally efficient, but only gives a point estimate and not a distribution over parameters. Deep ensembles, proposed by  Lakshminarayanan et al. (2017) , train an ensemble Under review as a conference paper at ICLR 2020 of neural networks by initializing at M different values and repeating the minimization multiple times which could lead to M different solutions, if the loss is non-convex. ( Lakshminarayanan et al. (2017)  found adversarial training provides additional benefits in some of their experiments, but we will ignore adversarial training and focus only on ensembles with random initialization in this paper.) Given finite training data, many parameter values could equally well explain the observations, and capturing these diverse solutions is crucial for quantifying epistemic uncertainty ( Kendall & Gal, 2017 ). Bayesian neural networks learn a distribution over weights, and a good posterior approximation should be able to learn multi-modal posterior distributions in theory. Deep ensembles were inspired by the bootstrap ( Breiman, 1996 ), which has nice theoretical properties. However, it has been empirically observed by  Lakshminarayanan et al. (2017) ;  Lee et al. (2015)  that training individual networks with just random initialization is sufficient in practice and using the bootstrap even hurts performance in some cases (e.g. for small ensemble sizes). Furthermore,  Ovadia et al. (2019)  and  Gustafsson et al. (2019)  independently benchmarked existing methods for uncertainty quantification on a variety of datasets and architectures, and observed that ensembles tend to outperform approximate Bayesian neural networks in terms of both accuracy and uncertainty, particularly under dataset shift. These empirical observations raise an important question: Why do ensem- bles trained with just random initial- ization work so well in practice? One possible hypothesis is that ensembles tend to sample from different modes 1 in function space, whereas variational Bayesian methods (which minimize D KL (q(θ)|p(θ|{x n , y n } N n=1 )) might fail to explore multiple modes even though they are effective at captur- ing uncertainty within a single mode. See  Figure 1  for a cartoon illustration. Note that while the MAP solution is a local minima for the training loss by definition, it may not necessarily be a local minima for the validation loss. Recent work on understanding loss landscapes ( Fort & Jastrzebski, 2019 ;  Draxler et al., 2018 ;  Garipov et al., 2018 ) allows us to investigate this hypothesis. Note that prior work on loss landscapes has focused on mode-connectivity and low-loss tunnels, but has not explicitly focused on how diverse the functions from different modes are, beyond an initial exploration in  Fort & Jastrzebski (2019) . Our findings show that: • The functions sampled along a single training trajectory or subspace thereof (e.g. diagonal Gaussian, low-rank Gaussian and Dropout subspaces) tend to be very similar in predictions (while potential far away in the weight space), whereas functions sampled from different randomly initialized trajectories tend to be very diverse. • Solution modes are connected in the loss landscape but they are distinct in the space of predictions. Low-loss tunnels create functions with near-identical low values of loss along the path, however these functions tend to be very different in function space, changing significantly in the middle of the tunnel.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents MGLM, a multichannel generative modeling framework for modeling the joint distribution p(x1,...,xk) over k channels. MGLM is capable of both unconditional generation and conditional generation, such as machine translation. We demonstrate the effectiveness of MGLM on the Multi30K machine translation task, consisting of four languages: English, French, Czech, and German. We show that MGLM is competitive in BLEU, and has significant advantages in inference time and model memory savings. We also analyze the Quality-Diversity tradeoff from sampling MGLM and prior work.",
        "Abstract": "A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning but through two separate channels corresponding to their languages. In this work, we present Multichannel Generative Language Models (MGLM), which models the joint distribution over multiple channels, and all its decompositions using a single neural network. MGLM can be trained by feeding it k way parallel-data, bilingual data, or monolingual data across pre-determined channels. MGLM is capable of both conditional generation and unconditional sampling. For conditional generation, the model is given a fully observed channel, and generates the k-1 channels in parallel. In the case of machine translation, this is akin to giving it one source, and the model generates k-1 targets. MGLM can also do partial conditional sampling, where the channels are seeded with prespecified words, and the model is asked to infill the rest. Finally, we can sample from MGLM unconditionally over all k channels. Our experiments on the Multi30K dataset containing English, French, Czech, and German languages suggest that the multitask training with the joint objective leads to improvements in bilingual translations. We provide a quantitative analysis of the quality-diversity trade-offs for different variants of the multichannel model for conditional generation, and a measurement of self-consistency during unconditional generation. We provide qualitative examples for parallel greedy decoding across languages and sampling from the joint distribution of the 4 languages.",
        "Introduction": "  INTRODUCTION A natural way to consider two parallel sentences in different languages is that each language is ex- pressing the same underlying meaning under a different viewpoint. Each language can be thought of as a transformation that maps an underlying concept into a view that we collectively agree is determined as 'English' or 'French'. Similarly, an image of a cat and the word 'cat' are expressing two views of the same underlying concept. In this case, the image corresponds to a high bandwidth channel and the word 'cat' to a low bandwidth channel. This way of conceptualizing parallel view- points naturally leads to the formulation of a fully generative model over each instance, where the transformation corresponds to a particular generation of the underlying view. We define each of these views as a channel. As a concrete example, given a parallel corpus of English and French sen- tences, English and French become two channels and the corresponding generative model becomes p(English, French). One key advantage to this formulation is that single model can be trained that can capture the full expressivity of the underlying concept, allowing us to compute conditionals and marginals along with the joint. In the case of parallel sentences, the conditionals correspond to translations from one channel to another while the marginals correspond to standard monolingual language models. In this work, we present a general framework for modeling the joint distribution p(x 1 , ..., x k ) over k channels. Our framework marginalizes over all possible factorizations of the joint distribution. Sub- sequently, this allows our framework to perform, 1) unconditional generation and 2) conditional gen- eration. We harness existing recent work on insertion-based methods that utilize semi-autoregressive models that are permutation-invariant to the joint factorization. Specifically, we show a proof-of-concept multichannel modeling by extending KERMIT ( Chan et al., 2019 ) to model the joint distribution over multiple sequence channels. Specifically, we train KERMIT on the Multi30K ( Elliott et al., 2016 ) machine translation task, consisting of four lan- Under review as a conference paper at ICLR 2020 guages: English (EN), French (FR), Czech (CS), and German (DE). One advantage of multilingual KERMIT is during inference, we can generate translation for a single target language, or generate translations for k − 1 languages in parallel in logarithmic time in the token length per language. We illustrate qualitative examples for parallel greedy decoding across languages and sampling from the joint distribution of the 4 languages. The key contributions in this work are: 1. We present MGLM, a multichannel generative modeling framework. MGLM models the joint distribution p(x 1 , . . . , x k ) over k channels. 2. We demonstrate both conditional generation (i.e., machine translation) and unconditional sampling from MGLM. 3. In the case of conditional generation over multiple languages, we show that not only we are competitive in BLEU, but also with significant advantages in inference time and model memory savings. 4. We analyze the Quality-Diversity tradeoff from sampling MGLM and prior work. We highlight that while we focus on languages as a specific instantiation of a channel, our framework can generalize to any arbitrary specification, such as other types of languages or other modalities.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the behavior of stochastic gradient descent (SGD) on high-dimensional neural network models, and how neural net architecture design affects training performance. It provides theoretical and empirical evidence to establish direct relationships between layer width, network depth, problem dimensionality, and SGD dynamics on overparameterized networks. It also examines the effects of various innovations such as careful initialization strategies, residual connections, and normalization schemes on training performance.",
        "Abstract": "The goal of this paper is to study why typical neural networks train so fast, and how neural network architecture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this.  When confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slowing down convergence. But when gradient confusion is low, data samples interact harmoniously, and training proceeds quickly. Through novel theoretical and experimental results, we show how the neural net architecture affects gradient confusion, and thus the efficiency of training. We show that increasing the width of neural networks leads to lower gradient confusion, and thus easier model training. On the other hand, increasing the depth of neural networks has the opposite effect. Finally, we observe empirically that techniques like batch normalization and skip connections reduce gradient confusion, which helps reduce the training burden of very deep networks.",
        "Introduction": "  INTRODUCTION Stochastic gradient descent (SGD) (Robbins & Monro, 1951) and its variants with momentum (Sutskever et al., 2013) have become the standard optimization routine for neural networks due to their fast convergence and good generalization properties (Wilson et al., 2017; Keskar & Socher, 2017; Sutskever et al., 2013). Yet the behavior of SGD on high-dimensional neural network models still eludes full theoretical understanding, both in terms of its convergence and generalization properties. In this paper, we study why SGD is so efficient at converging to low loss values on most standard neural networks, and how neural net architecture design affects training performance. Classical stochastic optimization theory predicts that the learning rate of SGD needs to decrease over time for convergence to be guaranteed to the minimizer of a convex function (Shamir & Zhang, 2013; Bertsekas, 2011). For strongly convex functions for example, such results show that a decreasing learning rate schedule of O(1/k) is required to guarantee convergence to within -accuracy of the minimizer in O(1/ ) iterations, where k denotes the iteration number. Such decay schemes, however, typically lead to poor performance on standard neural network problems. Neural networks operate in a regime where the number of parameters is much larger than the number of training data. In this regime, SGD seems to converge quickly with constant learning rates. Most neural net practitioners use a constant learning rate for the majority of training, with exponentially decaying learning rate schedules at the end, without seeing the method stall (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016; Zagoruyko & Komodakis, 2016). With constant learning rates, theoretical guarantees show that SGD converges quickly to a neighborhood of the minimizer, but then reaches a noise floor beyond which it stops converging; this noise floor depends on the learning rate and the variance of the gradients (Moulines & Bach, 2011; Needell et al., 2014). Some more recent results have shown that when models can fit the data completely while being strongly convex, convergence without a noise floor is possible without decaying the learning rate (Schmidt & Roux, 2013; Ma et al., 2017; Bassily et al., 2018; Vaswani et al., 2018). While these results do give important insights, they do not fully explain the dynamics of SGD on neural nets, and how they relate to overparameterization. Training performance is also highly affected by the neural network architecture. It is common knowledge among neural network practitioners that deeper networks train slower (Bengio et al., 1994; Glorot & Bengio, 2010). This has led to several innovations over the years to get deeper networks to train more easily, such as careful initialization strategies (Glorot & Bengio, 2010; He et al., 2015; Zhang et al., 2019), residual connections (He et al., Under review as a conference paper at ICLR 2020 2016), and various normalization schemes like batch normalization (Ioffe & Szegedy, 2015) and weight normalization (Salimans & Kingma, 2016). Furthermore, there is ample evidence to indicate that wider networks are easier to train (Zagoruyko & Komodakis, 2016; Nguyen & Hein, 2017; Lee et al., 2019), and recent theoretical results suggest that the dynamics of SGD simplify considerably for very wide networks (Jacot et al., 2018; Lee et al., 2019). Several prior works have investigated the difficulties of training deep networks (Glorot & Bengio, 2010; Balduzzi et al., 2017), and the benefits of width (Nguyen & Hein, 2017; Lee et al., 2019; Du et al., 2018; Allen-Zhu et al., 2018). This work advances the existing literature by identifying and analyzing a condition that enables us to theoretically and empirically establish novel direct relationships between layer width, network depth, problem dimensionality, and SGD dynamics on overparameterized networks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a novel approach to graph representation learning, which uses deep learning to discern one observation from another and determine how similar or dissimilar they are. It highlights the difficulty of comparing graphs, which does not only consist of looking for similar elements (nodes) but also structural similarities between the substructures within. The paper discusses existing methods utilizing kernel methods and more recently, deep learning approaches for graph representation learning. It proposes a new approach to graph representation learning which uses deep learning to learn node representations and pooling operations to aggregate node representations into a single representation for the entire graph.",
        "Abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ",
        "Introduction": "  INTRODUCTION A fundamental prerequisite for machine learning algorithms to learn about input data is the abil- ity to discern one observation from another. Even more powerful is the ability to determine how similar or dissimilar such observations are from one another to make more detailed associations. For observations represented in Euclidean space with feature vectors the concept of similarity be- tween observations is intuitive as it may be computed as distance using Euclidean distance or cosine similarity formulas. Unfortunately, for observations represented as graphs defining the notion of similarity, and even more so designing methods for computing similarity has been a long ongoing challenge in maths and computer science. This is partly because assessing comparability of graphs does not only consist of looking for similar elements (nodes) but also structural similarities between the substructures within. An obvious real world example on the importance of this distinction can be found in chemistry where molecules called isomers exhibit identical chemical formulas but different structural properties which induce different behaviours and traits (Petrucci et al., 2017). The diffi- culty of comparing graphs is highlighted by the graph isomorphism test (Garey & Johnson, 1990), which despite its complexity only gives a binary evaluation of the structural equivalence between two graphs which is insufficient for demands of fine grained machine learning tasks. Consequently, the graph learning domain predominantly features kernel methods which approximate the comparability of graphs using invariants or substructures such as nodes, subgraphs and random walks within the graphs (Vishwanathan et al., 2010). Whilst they are powerful and intuitive, such kernels are dependent and often tied to certain methods such as support vector machines (SVM) to perform learning tasks (Yanardag & Vishwanathan, 2015). Hence, existing methods utilizing kernel methods are often unable to handle other downstream learning tasks such as regression or clustering without significant revision. More recently, deep learning approaches for graph representation learning have gained significant research activity with the successful interpretation of graph convolutional methods for learning node representations (Kipf & Welling, 2017; Scarselli et al., 2009; Veličković et al., 2018). Representa- tions at the graph level are then constructed through application of different pooling operations which aggregate node representations into a single representation for the entire graph (Ying et al., 2018; Goyal & Ferrara, 2018).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines how the linguistic structure of labels influences the visual representations learned by convolutional neural network (CNN) models. We manipulated the labels used to supervise the training of CNN models, each having the same architecture and given identical visual inputs. We then compared visual representations learned by these models and predicted human similarity judgement using an Odd-one-out task. Our study suggests that the semantic structure of labels and datasets should be carefully constructed if the goal is to build vision models that learn visual features representations having the potential for human-like generalization.",
        "Abstract": "We investigated the changes in visual representations learnt by CNNs when using different linguistic labels (e.g., trained with basic-level labels only, superordinate-level only, or both at the same time) and how they compare to human behavior when asked to select which of three images is most different. We compared CNNs with identical architecture and input, differing only in what labels were used to supervise the training. The results showed that in the absence of labels, the models learn very little categorical structure that is often assumed to be in the input. Models trained with superordinate labels (vehicle, tool, etc.) are most helpful in allowing the models to match human categorization, implying that human representations used in odd-one-out tasks are highly modulated by semantic information not obviously present in the visual input.",
        "Introduction": "  INTRODUCTION A critical distinction between human category learning and machine category learning is that only humans have a language. A language means that human learning is not limited to a one-to-one correspondence between a visual input and a category label. Indeed, the users of a language are known to actively seek out categorical relationships between objects and use these relationships in making perceptual similarity judgments and in controlling behavior ( Hays, 2000 ;  Lupyan & Lewis, 2017 ). A premise of our work is that a language provides a semantic structure to labels, and that this structure contributes to the superior efficiency and flexibility of human vision compared to any artificial systems ( Pinto et al., 2010 ). Of course, the computer vision literature on zero-shot and few- shot learning has also made good progress in leveraging semantic information (e.g., image captions, attribute labels, relational information) to increase the generalizability of a model's performance ( Lampert et al., 2013 ;  Sung et al., 2018 ;  Lei Ba et al., 2015 ). Still, this performance pales in comparison to the human ability for classification, where zero-shot and few-shot learning is the norm, and efficiently-acquired category knowledge is easily generalized to new exemplars ( Ashby & Maddox, 2005 ; Ashby & Ell, 2001). One reason why machine learning lags behind human performance may be because of a failure to fully consider the semantic structure of the ground-truth labels used for training, which can be heavily biased by basic or subordinate- level categories. This might result in models learning visual feature representations that may not be best for generalization to new, higher-level categories. For example, ImageNet ( Deng et al., 2009 ) contains 120 different dog categories, making the models that are trained using these labels dog experts, creating an interesting but highly atypical semantic structure. Here we study how the linguistic structure of labels influences what is learned by models trained on the same visual inputs. Specifically, we manipulated the labels used to supervise the training of CNN models, each having the same architecture and given identical visual inputs. For example, some of these models were trained with basic-level labels only, some with only superordinate-level labels, and some with both. We then compare visual representations learned by these models, and predict human similarity judgement that we collected using an Odd-one-out task where people had to select which of three object images was the most different. With this dataset, and using categorical representations extracted from our trained models, we could predict human similarity decisions with Under review as a conference paper at ICLR 2020 up to 74% accuracy, which gives us some understanding of the labels needed to produce human-like representations. Our study also broadly benefits both computer vision and behavioral science (e.g., psychology, neuroscience) by suggesting that the semantic structure of labels and datasets should be carefully constructed if the goal is to build vision models that learn visual features representations having the potential for human-like generalization. For behavioral science, this research provides a useful computational framework for understanding the effect of training labels on the human learn- ing of category relationships in the context of thousands of naturalistic images of objects.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a refinement of the universal approximation theorems for PointNet and DeepSets architectures, which allow for the direct processing of point clouds within a deep learning framework. The paper provides three main results which show that PointNet and DeepSets architectures can uniformly approximate real-valued functions that are uniformly continuous with respect to the Hausdorff and Wasserstein metrics, and that only the constant functions can be uniformly approximated by both architectures. Additionally, the paper provides explicit error lower bounds and adversarial examples to show that even when limited to point clouds of size k, PointNet cannot uniformly approximate center-of-mass.",
        "Abstract": "In this paper we prove new universal approximation theorems for deep learning on point clouds that do not assume fixed cardinality. We do this by first generalizing the classical universal approximation theorem to general compact Hausdorff spaces and then applying this to the permutation-invariant architectures presented in 'PointNet' (Qi et al) and 'Deep Sets' (Zaheer et al). Moreover, though both architectures operate on the same domain, we show that the constant functions are the only functions they can mutually uniformly approximate. In particular, DeepSets architectures cannot uniformly approximate the diameter function but can uniformly approximate the center of mass function but it is the other way around for PointNet. ",
        "Introduction": "  INTRODUCTION Recently, architectures proposed in PointNet ( Qi et al., 2017 ) and Deep Sets ( Zaheer et al., 2017 ) have allowed for the direct processing of point clouds within a deep learning framework. These methods produce outputs that are permutation-invariant with respect to the member points and work for point clouds of arbitrarily large cardinality. A common source of such data is LIDAR mea- surements from autonomous vehicles.  Zaheer et al. (2017)  also presents a permutation-equivariant architecture which we do not discuss here. Each of of these works provide their own universal approximation theorem (UAT) to support the empirical success of their architectures. However, both results assume the cardinality of the point cloud is fixed to some size n. In this work we refine these results, remove the cardinality limitation, use weaker architecture assumptions, and arrive at three main results which can be summarized roughly as follows (assuming unrestricted finite cardinality for the input point clouds): 1) PointNet (DeepSets) architectures can uniformly approximate real-valued functions that are uniformly continuous with respect to the Hausdorff (Wasserstein) metric and nothing else (Theorem 3.4). 2) Only the constant functions can be uniformly approximated by both architectures. In partic- ular, PointNet architectures can uniformly approximate the diameter function but DeepSets architectures cannot. Conversely, DeepSets architectures can uniformly approximate the center-of-mass function but PointNet architectures cannot (Theorem 4.1). 3) We prove explicit error lower bounds and produce adversarial examples to show that even when limited to point clouds of size k, PointNet cannot uniformly approximate center-of- mass (Theorem 4.2). To do this we extend the many universal approximation results for feed-forward networks ( Cybenko, 1989 ;  Hornik et al., 1989 ;  Leshno et al., 1993 ;  Stinchcombe, 1999 ) to the abstract setting of general compact Hausdorff spaces. We then find appropriate compact metric spaces over which PointNet and DeepSets architectures can be easily analyzed and then finally we observe the resulting conse- quences in the original setting of interest, i.e. point clouds.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel multi-level Hierarchical Reinforcement Learning (HRL) architecture for navigation tasks that require planning over long horizons with sparse rewards. The architecture decouples the major roles in a complex navigation task, namely planning and low-level control, and enables generalization beyond training conditions and environments. The paper also demonstrates the benefits of functional decomposition via transfer of individual layers between different agents.",
        "Abstract": "Solving long-horizon sequential decision making tasks in environments with sparse rewards is a longstanding problem in reinforcement learning (RL) research. Hierarchical Reinforcement Learning (HRL) has held the promise to enhance the capabilities of RL agents via operation on different levels of temporal abstraction. Despite the success of recent works in dealing with inherent nonstationarity and sample complexity, it remains difficult to generalize to unseen environments and to transfer different layers of the policy to other agents. In this paper, we propose a novel HRL architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), which allows decomposition of the hierarchical layers into independent subtasks, yet allows for joint training of all layers in end-to-end manner. The main insight is to combine a control policy on a lower level with an image-based planning policy on a higher level. We evaluate our method on various complex continuous control tasks for navigation, demonstrating that generalization across environments and transfer of higher level policies can be achieved. See videos https://sites.google.com/view/hide-rl",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) has been succesfully applied to sequential-decision making tasks, such as learning how to play video games in Atari ( Mnih et al., 2013 ), mastering the game of Go ( Silver et al., 2017 ) or continuous control in robotics ( Lillicrap et al., 2015 ;  Levine et al., 2015 ;  Schulman et al., 2017 ). However, despite the success of RL agents in learning control policies for myopic tasks, such as reaching a nearby target, they lack the ability to effectively reason over extended hori- zons. In this paper, we consider continuous control tasks that require planning over long horizons in navigation environments with sparse rewards. The task becomes particularly challenging with sparse and delayed rewards since an agent needs to infer which actions caused the reward in a do- main where most samples give no signal at all. Common techniques to mitigate the issue of sparse rewards include learning from demonstrations ( Schaal, 1999 ;  Peng et al., 2018 ) or using enhanced exploration strategies ( Bellemare et al., 2016 ;  Pathak et al., 2017 ;  Andrychowicz et al., 2017 ). Hierarchical Reinforcement Learning (HRL) has been proposed in part to solve such tasks. Typi- cally, a sequential decision making task is split into several simpler subtasks of different temporal and functional abstraction levels ( Sutton et al., 1999 ;  Andre & Russell, 2002 ). Although the hierar- chies would ideally be learned in parallel, most methods resort to curriculum learning ( Frans et al., 2017 ;  Florensa et al., 2017 ;  Bacon et al., 2016 ;  Vezhnevets et al., 2017 ). Recent goal-conditioned hi- erarchical architectures have successfully trained policies jointly via off-policy learning ( Levy et al., 2019 ;  Nachum et al., 2018 ; 2019). However, these methods often do not generalize to unseen envi- ronments as we show in Section 5.1. We argue that this is due to a lack of true separation of planning and low-level control across the hierarchy. In this paper, we consider two main problems, namely functional decomposition of HRL architectures in navigation-based domains and generalization of RL agents to unseen environments ( figure 1 ). To address these issues, we propose a novel multi-level HRL architecture that enables both func- tional decomposition and temporal abstraction. We introduce a 3-level hierarchy that decouples the major roles in a complex navigation task, namely planning and low-level control. The benefit of a modular design is twofold. First, layers have access to only task-relevant information for a pre- defined task, which significantly improves the generalization ability of the overall policy. Hence, this enables policies learned on a single task to solve randomly configured environments. Second, Under review as a conference paper at ICLR 2020 the planning and control layers are modular and thus allow for composition of cross-agent architec- tures. We empirically show that the planning layer of the hierarchy can be transferred successfully to new agents. During training we provide global environment information only to the planning layer, whereas the full internal state of the agent is only accessible by the control layer. The actions of the top and middle layers are in the form of displacement in space. Similarly, the goals of the middle and lowest layers are relative to the current position. This prevents the policies from overfitting to the global position in an environment and hence encourages generalization to new environments. In our framework (see  figure 2 ), the planner (i.e., the highest level policy π 2 ) learns to find a trajec- tory leading the agent to the goal. Specifically, we learn a value map of the environment by means of a value propagation network ( Nardelli et al., 2019 ). To prevent the policy from issuing too ambitious subgoals, an attention network estimates the range of the lower level policy π 0 (i.e., the agent). This attention mask also ensures that the planning considers the agent performance. The action of π 2 is the position which maximizes the masked value map, which serves as goal input to the policy π 1 . The middle layer implements an interface between the upper planner and lower control layer, which refines the coarser subgoals into shorter and reachable targets for the agent. The middle layer is crucial in functionally decoupling the abstract task of planning (π 2 ) from agent specific continuous control. The lowest layer learns a control policy π 0 to steer the agent to intermediate goals. While the policies are functionally decoupled, they are trained together and must learn to cooperate. In this work, we focus on solving long-horizon tasks with sparse rewards in complex continuous navigation domains. We first show in a maze environment that generalization causes challenges for state-of-the-art approaches. We then demonstrate that training with the same environment configura- tion (i.e., fixed start and goal positions) can generalize to randomly configured environments. Lastly, we show the benefits of functional decomposition via transfer of individual layers between different agents. In particular, we train our method with a simple 2DoF ball agent in a maze environment to learn the planning layer which is later used to steer a more complex agent. The results indicate that the proposed decomposition of policy layers is effective and can generalize to unseen environments. In summary our main contributions include: • A novel multi-layer HRL architecture that allows functional decomposition and temporal abstraction for navigation tasks. • This architecture enables generalization beyond training conditions and environments. • Functional decomposition that allows transfer of individual layers across different agents.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel OOD-detection algorithm based on Shannon's entropy-based definition of typicality. The algorithm is formulated as a statistical hypothesis test and employs a bootstrap procedure to set the OOD-decision threshold. Experiments demonstrate that the detection procedure succeeds in many of the challenging cases presented by Nalisnick et al. (2019). Additionally, failure modes are discussed to inspire future work.",
        "Abstract": "Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data [Nalisnick et al., 2019; Choi et al., 2019].  We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density.  In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed [Bishop, 1994].  To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods.  The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated.  We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. [2019].",
        "Introduction": "  INTRODUCTION Recent work (Nalisnick et al., 2019;  Choi et al., 2019 ;  Shafaei et al., 2018 ) showed that a variety of deep generative models fail to distinguish training from out-of-distribution (OOD) data according to the model likelihood. This phenomenon occurs not only when the data sets are similar but also when they have dramatically different underlying semantics. For instance, Glow ( Kingma & Dhariwal, 2018 ), a state-of-the-art normalizing flow, trained on CIFAR-10 will assign a higher likelihood to SVHN than to its CIFAR-10 training data (Nalisnick et al., 2019;  Choi et al., 2019 ). This result is surprising since CIFAR-10 contains images of frogs, horses, ships, trucks, etc. and SVHN contains house numbers. A human would be very unlikely to confuse the two sets. These findings are also troubling from an algorithmic standpoint since higher OOD likelihoods break previously proposed methods for classifier validation ( Bishop, 1994 ) and anomaly detection ( Pimentel et al., 2014 ). We conjecture that these high OOD likelihoods are evidence of the phenomenon of typicality. 1 Due to concentration of measure, a generative model will draw samples from its typical set ( Cover & Thomas, 2012 ), a subset of the model's full support. However, the typical set may not necessarily intersect with regions of high probability density. For example, consider a d-dimensional isotropic Gaussian. Its highest density region is at its mode (the mean) but the typical set resides at a distance of √ d from the mode ( Vershynin, 2018 ). Thus a point near the mode will have high likelihood while being extremely unlikely to be sampled from the model. We believe that deep generative models exhibit a similar phenomenon since, to return to the CIFAR-10 vs SVHN example, Nalisnick et al. (2019) showed that sampling from the model trained on CIFAR-10 never generates SVHN-looking images despite SVHN having higher likelihood. Based on this insight, we propose that OOD detection should be done by checking if an input resides in the model's typical set, not just in a region of high density. Unfortunately it is impossible to analytically derive the regions of typicality for the vast majority of deep generative models. To define a widely applicable and scalable OOD-detection algorithm, we formulate  Shannon (1948) 's entropy-based definition of typicality into a statistical hypothesis test. To ensure that the test is robust Under review as a conference paper at ICLR 2020 (a) Gaussian Example (b) Illustration (c) Simulation even in the low-data regime, we employ a bootstrap procedure ( Efron & Tibshirani, 1994 ) to set the OOD-decision threshold. In the experiments, we demonstrate that our detection procedure succeeds in many of the challenging cases presented by Nalisnick et al. (2019). In addition to these successes, we also discuss failure modes that reveal drastic variability in OOD detection for the same data set pairs under different generative models. We highlight these cases to inspire future work.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes two modifications to the standard gating mechanism of recurrent models to address the vanishing gradient problem. The first modification is uniform gate initialization, which initializes the activations of the gates from a distribution that captures a wider spread of dependency lengths. The second modification is the UR-gating mechanism, which uses an auxiliary refine gate to modulate a main gate, allowing it to have a wider range of activations without gradients vanishing as quickly. These modifications can be applied to any gate and have minimal to no overhead in terms of speed, memory, code complexity, and (hyper-)parameters.",
        "Abstract": "In this work, we revisit the gating mechanisms widely used in various recurrent and feedforward networks such as LSTMs, GRUs, or highway networks. These gates are meant to control information flow, allowing gradients to better propagate back in time for recurrent models. However, to propagate gradients over very long temporal windows, they need to operate close to their saturation regime. We propose two independent and synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyper-parameters, and are aimed at improving learnability of the gates when they are close to saturation. Our proposals are theoretically justified, and we show a generic framework that encompasses other recently proposed gating mechanisms such as chrono-initialization and master gates . We perform systematic analyses and ablation studies on the proposed improvements and evaluate our method on a wide range of applications including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning.  Empirically, our proposed gating mechanisms robustly increase the performance of recurrent models such as LSTMs, especially on tasks requiring long temporal dependencies.",
        "Introduction": "  INTRODUCTION Recurrent neural networks (RNNs) have become a standard machine learning tool for learning from sequential data. However, RNNs are prone to the vanishing gradient problem, which occurs when the gradients of the recurrent weights become vanishingly small as they get backpropagated through time (Hochreiter et al., 2001). A common approach to alleviate the vanishing gradient problem is to use gating mechanisms, leading to models such as the long short term memory (Hochreiter & Schmidhuber, 1997, LSTM) and gated recurrent units (Chung et al., 2014, GRUs). These gated RNNs have been very suc- cessful in several different application areas such as in reinforcement learning (Kapturowski et al., 2018; Espeholt et al., 2018) and natural language processing (Bahdanau et al., 2014; Kočiskỳ et al., 2018). At every time step, gated recurrent models form a weighted combination of the history summarized by the previous state, and (a function of) the incoming inputs, to create the next state. The values of the gates, which are the coefficients of the combination, control the length of temporal dependencies that can be addressed. This weighted update can be seen as an additive or residual connection on the recurrent state, which helps signals propagate through time without vanishing. However, the gates themselves are prone to a saturating property which can also hamper gradient-based learning. This is particularly troublesome for RNNs, where carrying information for very long time delays requires gates to be very close to their saturated states. We formulate and address two particular problems that arise with the standard gating mechanism of re- current models. First, typical initialization of the gates is relatively concentrated. This restricts the range of timescales the model can address, as the timescale of a particular unit is dictated by its gates. Our first proposal, which we call uniform gate initialization (Section 2.2), addresses this by directly initializing the activations of these gates from a distribution that captures a wider spread of dependency lengths. Second, learning when gates are in their saturation regime is difficult because of vanishing gradients through the gates. We derive a modification that uses an auxiliary refine gate to modulate a main gate, which allows it to have a wider range of activations without gradients vanishing as quickly. Combining these two independent modifications yields our main proposal, which we call the UR- gating mechanism. These changes can be applied to any gate (i.e. bounded parametrized function) and have minimal to no overhead in terms of speed, memory, code complexity, and (hyper-)parameters.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces predictive clustering, a data-driven framework that combines prediction with clustering to partition patients with time-series observations into a set of clusters. This approach allows clinicians to anticipate patients' prognoses by comparing \"similar\" patients in order to design treatment guidelines that are tailored to homogeneous patient subgroups. Predictive clustering optimizes the cluster assignments such that patients in a cluster share similar future outcomes to provide a prognostic value.",
        "Abstract": "Due to the wider availability of modern electronic health records (EHR), patient care data is often being stored in the form of time-series. Clustering such time-series data is crucial for patient phenotyping, anticipating patients’ prognoses by identifying “similar” patients, and designing treatment guidelines that are tailored to homogeneous patient subgroups. In this paper, we develop a deep learning approach for clustering time-series data, where each cluster comprises patients who share similar future outcomes of interest (e.g., adverse events, the onset of comorbidities, etc.). The clustering is carried out by using our novel loss functions that encourage each cluster to have homogeneous future outcomes. We adopt actor-critic models to allow “back-propagation” through the sampling process that is required for assigning clusters to time-series inputs. Experiments on two real-world datasets show that our model achieves superior clustering performance over state-of-the-art benchmarks and identifies meaningful clusters that can be translated into actionable information for clinical decision-making.",
        "Introduction": "  INTRODUCTION Chronic diseases - such as cystic fibrosis, dementia, and diabetes - are heterogeneous in nature, with widely differing outcomes even in narrow patient subgroups. Disease progression manifests through a broad spectrum of clinical factors, collected as a sequence of measurements over time in electronic health records (EHR), which gives a rise to complex progression patterns among patients (Samal et al., 2011). For example, cystic fibrosis evolves slowly, allowing for the development of related comorbidities and bacterial infections, and creating distinct behaviors/responses to therapeu- tic interventions, which in turn makes the survival and quality of life substantially different (Ramos et al., 2017). Identifying patient subgroups with similar progression patterns can be advantageous for understanding such heterogeneous underlying diseases. This allows clinicians to anticipate pa- tients' prognoses by comparing \"similar\" patients in order to design treatment guidelines that are tailored to homogeneous patient subgroups (Zhang et al., 2019). Temporal clustering has been recently used as a data-driven framework to partition patients with time-series observations into a set of clusters (i.e., into subgroups of patients). Recent research has typically focused on either finding fixed-length and low-dimensional representations (Zhang et al., 2019; Rusanov et al., 2016) or on modifying the similarity measure (Giannoula et al., 2018; Lu- ong and Chandola, 2017) both in an attempt to apply the conventional clustering algorithms (e.g., K-means (Lloyd, 1982)) to time-series observations. However, clusters identified from these ap- proaches these approaches are purely unsupervised - they do not account for each patient's observed outcome (e.g., adverse events, the onset of comorbidities, etc.) - which leads to heterogeneous clus- ters if the clinical presentation of the disease differs even for similar patients. Thus, a common prognosis in each cluster remains unknown which can mystify the understanding of the underlying disease progression (Boudier et al., 2019). For instance, patients who appear to have similar time- series observations may develop different sets of comorbidities in the future which, in turn, require different treatment guidelines to reduce such risks (Wami et al., 2013). To overcome this limitation, we focus on predictive clustering (Blockeel et al., 2017) which combines prediction with clustering. Therefore, the cluster assignments are optimized such that patients in a cluster share similar future outcomes to provide a prognostic value.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a method for learning reusable options for a set of related tasks with minimal information provided by the user. The proposed method seeks to learn a set of options that minimize the expected number of decisions needed to represent trajectories generated from the (near)-optimal policies learned by the agent, while also maximizing the probability of generating those trajectories. This approach can turn problems that are prohibitively expensive to solve into relatively simple problems, and has potential applications in robotics and industry.",
        "Abstract": "Reinforcement learning (RL) has become an increasingly active area of research in recent years. Although there are many algorithms that allow an agent to solve tasks efficiently, they often ignore the possibility that prior experience related to the task at hand might be available. For many practical applications, it might be unfeasible for an agent to learn how to solve a task from scratch, given that it is generally a computationally expensive process; however, prior experience could be leveraged to make these problems tractable in practice. In this paper, we propose a framework for exploiting existing experience by learning reusable options. We show that after an agent learns policies for solving a small number of problems, we are able to use the trajectories generated from those policies to learn reusable options that allow an agent to quickly learn how to solve novel and related problems.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) techniques have experienced much of their success in simulated en- vironments, such as video games ( Mnih et al., 2015 ) or board games ( Silver et al., 2016 ; Tesauro, 1995). One of the main reasons why RL has worked so well in these applications is that we are able simulate millions of interactions with the environment in a relatively short period of time. In many real world applications, however, where the agent interacts with the physical world, it might not be easy to generate such a large number of interactions. The time and cost associated with training such systems could render RL an unfeasible approach for training in large scale. As a concrete example, consider training a large number of humanoid robots (agents) to move quickly, as in the Robocup competition ( Farchy et al., 2013 ). Although the agents have similar dynamics, subtle variations mean that a single policy shared across all agents would not be an ef- fective solution. Furthermore, learning a policy from scratch for each agent is too data-inefficient to be practical. As shown by  Farchy et al. (2013) , this type of problem can be addressed by leveraging the experience obtained from solving a related task (e.g., walking) to quickly learn a policy for each individual agent that is tailored to a new task (e.g., running). These situations also occurs in industry, such as robots tasked with sorting items in fulfillment centers. A simple approach, like using PD controllers, would fail to adapt to the forces generated from picking up objects with different weight distributions, causing the arm to drop the objects. RL is able to mitigate this problem by learning a policy for each arm that is able to make corrections quickly, which is tailored to the robot's dynam- ics. However, training a new policy for each agent would be far too costly to be a practical solution. In these scenarios, it is possible to use a small number of policies learned a subset of the agents, and then leverage the experience obtained from learning those policies to allow the remaining agents to quickly learn their corresponding policies. This approach can turn problems that are prohibitively expensive to solve into relatively simple problems. To make use of prior experience and improve learning on new related problems in RL, several lines of work, which are complementary to each other, have been proposed and are actively being studied. Transfer learning ( Taylor & Stone, 2009 ) refers to the problem of adapting information acquired while solving one task to another. One might consider learning a mapping function that allows for a policy learned in one task to be used in a different task ( Ammar et al., 2015 ) or simply learn a mapping of the value function learned in one task to another ( Taylor et al., 2007 ). These techniques can be quite effective, but are also limited in that they consider mapping information from one source task to another target task. Another approach to reusing prior knowledge is through meta learning Under review as a conference paper at ICLR 2020 or learning to learn ( Schmidhuber, 1995 ;  Schmidhuber et al., 1998 ). In the context of RL, the goal under this framework for an agent to be exposed to a number of tasks where it can learn some general behavior that generalizes to new tasks ( Finn et al., 2017 ). One last technique to leverage prior experience, and the one this paper focuses on, is through tem- porally extended actions or temporal abstractions ( McGovern & Sutton, 1998 ;  Sutton et al., 1999 ). While in the standard RL framework the agent has access to a set of primitive actions (i.e., actions that last for one time step), temporally extended actions allow an agent to execute actions that last for several time-steps. They introduce a bias in the behavior of the agent which, if appropriate for the problem at hand, results in dramatic improvements in how quickly the agent learns to solve a new task. A popular representation for temporally extended actions is the options framework ( Sutton & Precup, 1998 ;  Sutton et al., 1999 ) (formally introduced in the next section), which is the focus of this work. It has been shown that options learned in a specific task or set of tasks, can be reused to improve learning on new tasks ( Machado et al., 2017 ;  Bacon et al., 2017 ); however, this often requires knowledge from the user about which options or how many options are appropriate for the type of problems the agent will face. In this paper, we propose learning reusable options for a set of related tasks with minimal infor- mation provided by the user. Throughout this paper, we refer as (near)-optimal policies to those policies that were learned to solve a particular task, but are not strictly speaking optimal. We con- sider the scenario where the agent must solve a large numbers of tasks and show that after learning a (near)-optimal policy for a small number of problems, we can learn an appropriate number of options that facilitates learning in a remaining set of tasks. To do so, we propose learning a set of options that minimize the expected number of decisions needed to represent trajectories generated from the (near)-optimal policies learned by the agent, while also maximizing the probability of gen- erating those trajectories. Unlike techniques that learn options to rach bottleneck states ( McGovern & Barto, 2001 ) or states deemed of high value ( Machado et al., 2017 ), our method seeks to learn options that are able to generate trajectories known to perform well. This does not necessarily lead to learn options that reach states one might consider \"interesting\".",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Composite Q-learning, MVE-TD3 and TD3(∆), three model-free Temporal-Difference algorithms that leverage multi-step returns to increase data-efficiency in deep Q-learning. These algorithms are evaluated in the tabular case and for three simulated robot tasks, showing an increase in data-efficiency compared to standard deep Q-learning.",
        "Abstract": "In the past few years, off-policy reinforcement learning methods have shown promising results in their application for robot control. Deep Q-learning, however, still suffers from poor data-efficiency which is limiting with regard to real-world applications. We follow the idea of multi-step TD-learning to enhance data-efficiency while remaining off-policy by proposing two novel Temporal-Difference formulations: (1) Truncated Q-functions which represent the return for the first n steps of a policy rollout and (2) Shifted Q-functions, acting as the farsighted return after this truncated rollout. We prove that the combination of these short- and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. We show the efficacy of Composite Q-learning in the tabular case and compare our approach in the function-approximation setting with TD3, Model-based Value Expansion and TD3(Delta), which we introduce as an off-policy variant of TD(Delta). We show on three simulated robot tasks that Composite TD3 outperforms TD3 as well as state-of-the-art off-policy multi-step approaches in terms of data-efficiency.",
        "Introduction": "  INTRODUCTION In recent years, Q-learning (Watkins and Dayan, 1992) has achieved major successes in a broad range of areas by employing deep neural networks (Mnih et al., 2015; Silver et al., 2018; Lillicrap et al., 2016), including environments of higher complexity (Riedmiller et al., 2018) and even in first real world applications (Haarnoja et al., 2019). Due to its off-policy update, Q-learning can leverage transitions collected by any policy which makes it more data-efficient compared to on- policy methods. Deep Q-learning, however, still has a very high demand for data samples which is limiting with regard to robot applications. One reason for the low data-efficiency is the long temporal horizon the reward signal has to propa- gate through. Data-efficiency of on-policy Temporal-Difference methods can be enhanced by the use of n-step returns, where a Monte Carlo rollout of length n is combined with a bootstrap of the value function. To employ n-step returns in an off-policy setting, subtrajectories of the exploratory policy have to be stored. These stored multi-step returns, however, will differ from the true value of the target-policy. In order to benefit from n-step data, the replay buffer has to be restricted in size or n has to be set to a small value to keep the samples close to the target-policy (Barth-Maron et al., 2018; Hessel et al., 2018). To avoid these problems, a dynamics model can be used for imaginary roll- outs, the so-called Model-based Value Expansion (MVE) (Feinberg et al., 2018). Alternatively, the full return can be composed of value functions with increasing discount, an approach called TD(∆) (Romoff et al., 2019). In this work, we define a model-free Temporal-Difference formulation which follows the idea of multi-step learning while remaining off-policy. Our contributions are threefold. First, we introduce the Composite Q-learning algorithm. For its formulation, we define Truncated Q-functions, representing the return for the first n steps of a target-policy rollout w.r.t. the full action-value. In addition, we introduce Shifted Q-functions which represent the farsighted return after this truncated rollout. Both are then combined in a mutual re- cursive definition of the Q-function for the final algorithm. Second, we evaluate MVE within TD3, leading to MVE-TD3. And third, we introduce TD3(∆), an extension of TD(∆) to deep Q-learning. We discuss related work in Section 2, describe the theoretical background in Section 3 and define Composite Q-learning, MVE-TD3 and TD3(∆) in Section 4. By breaking down the long-term re- turn into a composition of several short-term predictions, our method increases data-efficency which Under review as a conference paper at ICLR 2020 we show in the tabular case and for three simulated robot tasks in Section 5. We then conclude in Section 6.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the implicit bias induced from using different learning rates and batch sizes when minimizing the cross-entropy loss with SGD. A connection is drawn between a particular measure of flatness and margin based loss functions. A new loss function (Gcdf loss) is proposed that can be interpreted as a measure of flatness for the 0 − 1 loss. Experiments on CIFAR10 and MNIST show that larger learning rates and smaller batch sizes are better at implicitly minimizing the cross-entropy loss with larger temperature parameter, the hinge loss with larger margin parameter and the Gcdf loss with larger standard deviation parameter.",
        "Abstract": "Understanding the implicit bias of optimization algorithms is important in order to improve generalization of neural networks. One approach to try to exploit such understanding would be to then make the bias explicit in the loss function.  Conversely, an interesting approach to gain more insights into the implicit bias could be to study how different loss functions  are being implicitly minimized when training the network. In this work, we concentrate our study on the inductive bias occurring when minimizing the cross-entropy loss with different batch sizes and learning rates.  We investigate how three loss functions are being implicitly minimized during training. These three loss functions are the Hinge loss with different margins, the cross-entropy loss with different temperatures and a newly introduced Gcdf loss with different standard deviations. This  Gcdf loss establishes a connection between a sharpness measure for the 0−1 loss and margin based loss functions. We find that a common behavior is emerging for all the loss functions considered.",
        "Introduction": "  INTRODUCTION In the last few years, deep learning has succeeded in establishing state of the art performances in a wide variety of tasks in fields like computer vision, natural language processing and bioinformatics ( LeCun et al., 2015 ). Understanding when and how these networks generalize better is important to keep improving their performance. Many works starting mainly from  Neyshabur et al. (2015) ,  Zhang et al. (2017)  and  Keskar et al. (2017)  hint to a rich interplay between regularization and the optimization process of learning the weights of the network. The idea is that a form of inductive bias can be realized implicitly by the optimization algorithm. In this paper, we investigate the implicit bias induced from using different learning rates and batch sizes when minimizing the cross-entropy loss with SGD. A common theory is that more noise in the gradient bias the solution toward flatter minima ( Keskar et al., 2017 ). We draw a connection between a particular measure of flatness and margin based loss functions 1 . Our contributions are the following: 1. A new loss function (Gcdf loss) that can be interpreted as a measure of flatness for the 0 − 1 loss (for the top layer's weights of the network). 2. A methodology consisting in tracking alternative loss functions during training and compar- ing them for a given training loss value to try to uncover implicit biases in the optimization algorithm applied to varying the learning rate and batch size in SGD. 3. Experimental results on CIFAR10 and MNIST showing that larger learning rates and smaller batch sizes are better at implicitly minimizing the cross-entropy loss with larger temperature parameter, the hinge loss with larger margin parameter and the Gcdf loss with larger standard deviation parameter. At the opposite, smaller learning rates and larger batch sizes are better at implicitly minimizing the cross-entropy loss, the hinge loss and the Gcdf loss with smaller values of their respective parameter.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the reliability of confidence in machine learning models and the mathematical formulation of calibration. It discusses the Expected Calibration Error (ECE) metric, which has been used to evaluate the calibration of deep neural networks, and identifies pathologies with this metric. It then proposes alternative metrics, Static Calibration Error and Adaptive Calibration, which should be used instead of ECE to evaluate the calibration of models. This paper is expected to change the decisions of anyone attempting to train well-calibrated classification models.",
        "Abstract": "Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. We propose two new measures for calibration, the Static Calibration Error (SCE) and Adaptive Calibration Error (ACE). These measures take into account every prediction made by a model, in contrast to the popular Expected Calibration Error.",
        "Introduction": "  INTRODUCTION The reliability of a machine learning model's confidence in its predictions is critical for high risk applications, such as deciding whether to trust a medical diagnosis prediction ( Crowson et al., 2016 ;  Jiang et al., 2011 ;  Raghu et al., 2018 ). One mathematical formulation of the reliability of confidence is calibration ( Murphy & Epstein, 1967 ;  Dawid, 1982 ). Intuitively, for class predictions, calibration means that if a model assigns a class with 90% probability, that class should appear 90% of the time. Calibration is not directly measured by proper scoring rules like negative log likelihood or the quadratic loss. The reason that we need to assess the quality of calibrated probabilities is that presently, we optimize against a proper scoring rule and our models often yield uncalibrated probabilities. Recent work proposed Expected Calibration Error (ECE;  Naeini et al., 2015 ), a measure of calibration error which has lead to a surge of works developing methods for calibrated deep neural networks (e.g.,  Guo et al., 2017 ;  Kuleshov et al., 2018 ). In this paper, we show that ECE has numerous pathologies, and that recent calibration methods, which have been shown to successfully recalibrate models according to ECE, cannot be properly evaluated via ECE. Calibration (and uncertainty quantification generally) is critical in autonomous vehicles, the explo- ration phase of may algorithms, medical applications, and many more safety-critical applications of machine learning. A suite of recent papers (e.g.,  Lee et al., 2017 ;  Vaicenavicius et al., 2019 ;  Thulasidasan et al., 2019 ;  Kumar et al., 2018 ;  Guo et al., 2017 ;  Seo et al., 2019 ) use ECE to validate their models' calibration. We identify concerns with that methodology. As this metric has become the default choice for measuring calibration in industry and for researchers, we expect our criticism to change the decisions of anyone attempting to train well-calibrated classification models. We recommend that rather than using Expected Calibration Error, practitioners use Static Calibratinon Error or Adaptive Calibration.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a Mirror Descent (MD) framework for Neural Network (NN) quantization, which is a technique for network compression. The MD framework is formulated as a constrained optimization problem and is converted into an unconstrained problem by introducing auxiliary variables. The paper discusses a numerically stable implementation of MD and evaluates the merits of the MD variants on CIFAR-10/100 and TinyImageNet classification datasets. The results show that the quantized networks obtained by the MD variants yield accuracies very close to the floating-point counterparts while outperforming directly comparable baselines.",
        "Abstract": "Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. NN quantization is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework (Bubeck (2015)) for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we discuss a numerically stable implementation of MD by storing an additional set of auxiliary dual variables (continuous). This update is strikingly analogous to the popular Straight Through Estimator (STE) based method which is typically viewed as a “trick” to avoid vanishing gradients issue but here we show that it is an implementation method for MD for certain projections. Our experiments on standard classification datasets (CIFAR-10/100, TinyImageNet) with convolutional and residual architectures show that our MD variants obtain fully-quantized networks with accuracies very close to the floating-point networks.",
        "Introduction": "  INTRODUCTION Despite the success of deep neural networks in various domains, their excessive computational and memory requirements limit their practical usability for real-time applications or in resource-limited devices. Quantization is a prominent technique for network compression, where the objective is to learn a network while restricting the parameters to take values from a small discrete set (usually binary). This leads to a dramatic reduction in memory (a factor of 32 for binary quantization) and inference time - as it enables specialized implementation using bit operations. Neural Network (NN) quantization is usually formulated as a constrained optimization problem min x∈X f (x), where f (·) denotes the loss function by abstracting out the dependency on the dataset and X ⊂ IR r denotes the set of all possible quantized solutions. Majority of the works in the literature ( Hubara et al. (2017) ;  Yin et al. (2018) ; Ajanthan et al. (2019)) convert this into an unconstrained problem by introducing auxiliary variables (x) and optimize via (stochastic) gradient descent. Specifically, the objective and the update step take the following form: miñ x∈IR r f (P (x)) ,x k+1 =x k − η ∇xf (P (x))|x =x k , (1) where P : IR r → X is a mapping from the unconstrained space to the quantized space (sometimes called projection) and η > 0 is the learning rate. In cases where the mapping P is not differentiable, a suitable approximation is employed ( Hubara et al. (2017) ). In this work, by noting that the well-known Mirror Descent (MD) algorithm, widely used for online convex optimization ( Bubeck (2015) ), provides a theoretical framework to perform gradient descent in the unconstrained space (dual space, IR r ) with gradients computed in the quantized space (primal space, X ), we introduce an MD framework for NN quantization. In essence, MD extends gradient de- scent to non-Euclidean spaces where Euclidean projection is replaced with a more general projection defined based on the associated distance metric. Briefly, the key ingredient of MD is a concept called mirror map which defines both the mapping between primal and dual spaces and the exact form of Under review as a conference paper at ICLR 2020 the projection. Specifically, in this work, by observing P in Eq. (1) as a mapping from dual space to the primal space, we analytically derive corresponding mirror maps under certain conditions on P . This enables us to derive different variants of the MD algorithm useful for NN quantization. Furthermore, as MD is often found to be numerically unstable ( Hsieh et al. (2018) ), we discuss a numerically stable implementation of MD by storing an additional set of auxiliary variables similar to the existing methods. As will be shown later, this update is strikingly analogous to the popular Straight Through Estimator (STE) based gradient method ( Hubara et al. (2017) ;  Bai et al. (2019) ) which is typically viewed as a \"trick\" to avoid vanishing gradients issue but here we show that it is an implementation method for MD under certain conditions on the mapping P . We believe this connection sheds some light on the practical effectiveness of STE. We evaluate the merits of our MD variants on CIFAR-10/100 and TinyImageNet classification datasets with convolutional and residual architectures. Our experiments show that the quantized networks obtained by the MD variants yield accuracies very close to the floating-point counterparts while outperforming directly comparable baselines. Finally, we would like to emphasize that even though our formulation does not necessarily extend the theory of MD, we believe showing MD as a suitable framework for NN quantization with superior empirical performance opens up new ways of designing MD-inspired update rules for NNs.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to machine translation by combining Context Based Machine Translation (CBMT) and Neural Machine Translation (NMT). CBMT is a phrase-based machine translation approach that relies on contextual occurrence of phrases, while NMT is a more fluent and accurate approach that learns the pattern of human translation using human translated parallel corpus. The combination of CBMT and NMT is proposed to address the problem of parallel corpus scarcity between language pairs, as well as to produce more fluent and accurate translations. The results of this research show that this approach utilizes the strengths of each method to achieve a significant translation performance improvement over simple NMT.",
        "Abstract": "The current approaches for machine translation usually require large set of parallel corpus in order to achieve fluency like in the case of neural machine translation (NMT), statistical machine translation (SMT) and example-based machine translation (EBMT). The context awareness of phrase-based machine translation (PBMT) approaches is also questionable. This research develops a system that translates English text to Amharic text using a combination of context based machine translation (CBMT) and a recurrent neural network machine translation (RNNMT). We built a bilingual dictionary for the CBMT system to use along with a large target corpus. The RNNMT model has then been provided with the output of the CBMT and a parallel corpus for training. Our combinational approach on English-Amharic language pair yields a performance improvement over the simple neural machine translation (NMT).",
        "Introduction": "  INTRODUCTION Context based machine translation (CBMT) is a phrase-based machine translation (PBMT) approach proposed by  Miller et al. (2006) . Unlike most PBMT approaches that rely on statistical occurrence of the phrases, CBMT works on the contextual occurrence of the phrases. CBMT uses bilingual dictionary as its main translator and produces phrases to be flooded into a large target corpus. The CBMT approach addresses the problem of parallel corpus scarcity between language pairs. The parallel corpus set for English-Amharic language pair, for instance, composes of the Bible, the Ethiopian constitution and international documents. These sources use words specific to their do- main and overlook phrases and words used by novels, news and similar literary documents. The CBMT uses synonyms of words in place of rare words and rely on large target corpus and a bilin- gual dictionary to help with data scarcity( Miller et al., 2006 ). It is not dependent on large parallel corpus like most PBMT such as the statistical machine translation (SMT)( Brown et al., 1990 ) and the example-based machine translation EBMT( Gangadharaiah, 2011 ). The CBMT, however, fails in fluently translating texts compared to the neural machine translation (NMT). The NMT learns the pattern of humans' translation using human translated parallel corpus. Its trans- lations are more fluent and accurate than all the rest so far when evaluated individually ( Popovic, 2017 ). However, NMT struggles to translate properly rare words and words not commonly used( Wu et al., 2016 ). In addition, NMT requires large parallel corpus for training. The aim of this research is to build a system by combining the CBMT with the NMT for English to Amharic translation. The combination of PBMT and NMT is the future and most promising than the individual approaches themselves ( Popovic, 2017 ). CBMT's ability to address rare words and the NMT's ability to produce fluent translation along with their context awareness makes them complementary couple. The combination is done by providing the NMT with two inputs, one from the source language and the other from the output of the CBMT to produces the final target sentence. In this paper, we show that this approach utilizes the strength of each method to achieve a significant translation perfor- mance improvement over simple NMT. The improvement is mostly dependent on the performance of the CBMT and mostly on the bilingual dictionary of the CBMT.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel Dual-Attention model for multimodal multitask learning, which learns task-invariant disentangled visual and textual representations and explicitly aligns them with each other. The model is evaluated on two tasks: Semantic Goal Navigation and Embodied Question Answering. Results show an absolute improvement of 43-61% on instructions and 5-26% for questions over baselines. The model is also shown to be able to transfer to tasks involving unseen concepts and handle relational tasks involving negation and spatial relationships.",
        "Abstract": "Visually-grounded embodied language learning models have recently shown to be effective at learning multiple multimodal tasks such as following navigational instructions and answering questions. In this paper, we address two key limitations of these models, (a) the inability to transfer the grounded knowledge across different tasks and (b) the inability to transfer to new words and concepts not seen during training using only a few examples. We propose a multitask model which facilitates knowledge transfer across tasks by disentangling the knowledge of words and visual attributes in the intermediate representations. We create scenarios and datasets to quantify cross-task knowledge transfer and show that the proposed model outperforms a range of baselines in simulated 3D environments. We also show that this disentanglement of representations makes our model modular and interpretable which allows for transfer to instructions containing new concepts.",
        "Introduction": "  INTRODUCTION Humans learn language by interacting with a dynamic perceptual environment, grounding words into visual entities and motor actions ( Smith and Gasser, 2005 ;  Barsalou, 2008 ). In recent years, there has been an increased focus on training embodied agents capable of visually-grounded language learning. These include multimodal tasks involving one-way communication, such as mapping navigational instructions to actions ( MacMahon et al., 2006 ;  Chen and Mooney, 2011 ; Artzi and Zettlemoyer, 2013;  Mei et al., 2016 ;  Misra et al., 2018 ); and tasks involving two-way communication such as embodied question answering ( Gordon et al., 2018 ;  Das et al., 2018 ) and embodied dialogue ( de Vries et al., 2018 ). Other studies have shown that grounded semantic goal navigation agents can be effective at exploiting the compositionality of language to generalize to unseen instructions with an unseen composition of semantic attributes ( Hermann et al., 2017 ;  Chaplot et al., 2018 ), or an unseen composition of steps in a multi-step instruction ( Oh et al., 2017 ). However, current grounded language learning models have certain limitations. Firstly, these models are typically trained only for a single multimodal task and lack the ability to transfer grounded knowledge of 'concepts' * across tasks. For example, if an agent learns to follow the instruction 'Go to the red torch' and answer the question 'What color is the pillar?', then ideally it should also understand 'Go to the red pillar' and 'What color is the torch?' without additional training. Training multitask grounded-language models can also improve training sample efficiency, as these multimodal tasks share many common learning challenges including perception, grounding, and navigation. The second limitation is the inability of trained models to quickly transfer to tasks involving unseen concepts. For example, consider a household instruction-following robot trained on an existing set of objects. We would like the robot to follow instructions involving a new object 'lamp' that has been added to the house. Existing models would need to be trained with the new object, which typically requires thousands of samples and can also lead to catastrophic forgetting of known objects. Even if the models were given some labeled samples to detect the new objects, they would require additional training to learn to combine existing grounded knowledge with the new concept (e.g., 'blue lamp' if 'blue' is already known). In this paper, we train a multimodal multitask learning model for two tasks: Semantic Goal Navigation, where the agent is given a language instruction to navigate to a goal location, and Embodied Question Answering, where the agent is asked a question and it can navigate in the environment to gather information to answer the question (see  Figure 5 ). We make the following contributions in this paper: First, we define a cross-task knowledge transfer evaluation cri- terion to test the ability of multimodal multi-task models to transfer knowledge of concepts across tasks. We show that several prior single-task models, when trained on both tasks, fail to achieve cross-task knowledge transfer. This is because the visual grounding of words is often implicitly learned as a by-product of end-to-end training of the underlying task, which leads to the entanglement of knowledge of concepts in the learnt representations. We propose a novel Dual-Attention model which learns task-invariant disentangled visual and tex- tual representations and explicitly aligns them with each other. We create datasets and simulation scenarios for testing cross- task knowledge transfer and show an absolute improvement of 43-61% on instructions and 5-26% for questions over baselines. Second, the disentanglement and explicit alignment of representations makes our model modular and interpretable. We show that this allows us to transfer the model to handle instructions involving unseen concepts by incorporating the output of object detectors. We also show that our model is able to combine the knowledge of existing concepts with a new concept without any additional policy training. Finally, we show that the modularity and interpretability of our model also allow us to use trainable neural modules ( Andreas et al., 2016 ) to handle relational tasks involving negation and spatial relationships and also tackle relational instructions involving new concepts.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents results from an embodied agent learning an internal representation of its sensory input by interacting with a virtual 3D world. The framework of learning by interacting with the world produced a meaningful and action-oriented internal representation of the agents observations, even though no semantic labels were provided. This highlights the importance of embodied cognition and suggests that in order to teach an artificial agent a true understanding of its (simulated) world it needs to be able to interact with the world.",
        "Abstract": "How do humans acquire a meaningful understanding of the world with little to no supervision or semantic labels provided by the environment? Here we investigate embodiment and a closed loop between action and perception as one key component in this process. We take a close look at the representations learned by a deep reinforcement learning agent that is trained with visual and vector observations collected in a 3D environment with sparse rewards. We show that this agent learns semantically meaningful and stable representations of its environment without receiving any semantic labels. Our results show that the agent learns to represent the action relevant information extracted from pixel input in a wide variety of sparse activation patterns. The quality of the representations learned shows the strength of embodied learning and its advantages over fully supervised approaches with regards to robustness and generalizability.",
        "Introduction": "  INTRODUCTION When the way supervised neural networks learn is compared to the way humans learn one can easily make out some major differences. Two of those differences are supervision and embodiment. Taking the example of object recognition from visual observations, a neural network will be presented with thousands of images of the object in question, each of them accompanied by a class label. A toddler in comparison will also collect many observations of the object of interest, however, will do so by interacting with the object, looking at it from different perspectives by moving the head or even moving the object ( Bambach et al., 2018 ). This law-governed change in observations conditioned on the movements of the toddler emphasises the importance of embodied cognition ( Engel et al., 2013 ). It will make it possible to recognize the object as a distinct entity, separate from its surroundings and learn a general concept of it. This allows it to robustly recognize the object again even when seen from new perspectives or under different lighting conditions ( Smith & Slone, 2017 ). When the toddler is now told the name of the object, an almost instantaneous association between label and object can be made without the need of thousands of labeled examples ( Samuelson & Smith, 2005 ). This therefore makes a very efficient strategy for learning stable representations of objects. Fully supervised neural networks have been shown to suffer from shortcomings that humans usually do not exhibit.  Szegedy et al. (2013)  showed how very small perturbations to an image, undetectable to the human eye, can drastically change the classification accuracy of a neural network. Even simply holding such adversarial examples in front of a camera ( Kurakin et al., 2016 ) or specific natural images ( Hendrycks et al., 2019 ) can have this effect. The networks seem to possess an over-reliance on local image features such as texture and do not consider global features such as the overall shape and outline of an object ( Baker et al., 2018 ). Considering the training circumstances, this effect is unsurprising. The networks are expected to learn the concept of objects solely from pixel values. Without being able to interact with objects or even just looking at them from slightly different perspectives, it is very difficult to figure out basic knowledge such as object and background relationships. We expect that an active exploration of the world would make it possible to learn a more general and robust concept of objects. Already in 2001  O'Regan & No (2001)  argued that even though it is clear that action requires per- ception, this relation also reverses. Perception and the understanding of what is perceived requires action ( Noë, 2005 ). According to  O'Regan & Noë (2001) , \"experience is not something that hap- pens in us but is something we do\" (p.99). They argue that an important part of perception is to learn Under review as a conference paper at ICLR 2020 how actions affect sensations. These sensory motor contingencies help us make sense of our per- ceptions, predict them and efficiently sample the environment for information ( Engel et al., 2013 ). In humans, perception is hugely influenced by how we interact with the world ( Witt, 2011 ). Goals and the expected cost to perform actions to achieve a goal influence our perception of physical entities ( Proffitt, 2006 ). Also more abstract processes such as language comprehension are linked to action systems in the brain ( Pulvermüller & Fadiga, 2010 ). We therefore postulate that in order to teach an artificial agent a true understanding of its (simulated) world it needs to be able to interact with the world. This paper will present results from an embodied agent acting in a virtual 3D world and learning an internal representation of its sensory input. The framework of learning by interacting with the world produced a meaningful and action-oriented internal representation of the agents observations, even though no semantic labels were provided.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a model-free deep reinforcement learning (DRL) implementation of the multi-step greedy policy iteration (PI) and value iteration (VI) algorithms proposed by Efroni et al. (2018a). These algorithms iteratively solve γκ-discounted decision problems, whose reward has been shaped by the solution of the decision problem at the previous iteration. Experiments illustrate the performance of model-free algorithms can be improved by using them as solvers of multi-step greedy PI and VI schemes, as well as emphasize important implementation details while doing so.",
        "Abstract": "Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL) and in the case when a model of the environment is available (e.g., in the game of Go). In this work, we explore the benefits of multi-step greedy policies in model-free RL when employed in the framework of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms iteratively solve short-horizon decision problems and converge to the optimal solution of the original one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free algorithms which are instances of the multi-step DP framework. As model-free algorithms are prone to instabilities w.r.t. the decision problem horizon, this simple approach can help in mitigating these instabilities and results in an improved model-free algorithms. We test this approach and show results on both discrete and continuous control problems.",
        "Introduction": "  INTRODUCTION The field of Reinforcement learning (RL) span a wide variety of algorithms for solving decision- making problems through repeated interaction with the environment. By incorporating deep neural networks into RL algorithms, the field of RL has recently witnessed remarkable empirical success (e.g.,  Mnih et al. 2015 ;  Lillicrap et al. 2015 ;  Levine et al. 2016 ;  Silver et al. 2017 ). Much of this success had been achieved by model-free RL algorithms, such as Q-learning and policy gradient. These algorithms are known to suffer from high variance in their estimations ( Greensmith et al., 2004 ) and to have difficulties handling function approximation (e.g.,  Thrun & Schwartz 1993 ;  Baird 1995 ;  Van Hasselt et al. 2016 ;  Lu et al. 2018 ). These problems are intensified in decision problems with long horizon, i.e., when the discount factor, γ, is large. Although using smaller values of γ addresses the γ-dependent issues and leads to more stable algorithms ( Petrik & Scherrer, 2009 ;  Jiang et al., 2015 ), it comes with a cost, as the algorithm may return a biased solution, i.e., it may not converge to an optimal solution of the original decision problem (the one with large value of γ).  Efroni et al. (2018a)  recently proposed another approach to mitigate the γ-dependant instabilities in RL in which they study a multi-step greedy versions of the well-known dynamic programming (DP) algorithms policy iteration (PI) and value iteration (VI) ( Bertsekas & Tsitsiklis, 1996 ).  Efroni et al. (2018a)  also proposed an alternative formulation of the multi-step greedy policy, called κ-greedy policy, and studied the convergence of the resulted PI and VI algorithms: κ-PI and κ-VI. These two algorithms iteratively solve γκ-discounted decision problems, whose reward has been shaped by the solution of the decision problem at the previous iteration. Unlike the biased solution obtained by solving the decision problem with a smaller value of γ, by iteratively solving decision problems with a smaller γκ horizon, the κ-PI and κ-VI algorithms could converge to an optimal policy of the original decision problem. In this work, we derive and empirically validate model-free deep RL (DRL) implementations of κ-PI and κ-VI. In these implementations, we use DQN ( Mnih et al., 2015 ) and TRPO ( Schulman et al., 2015 ) for (approximately) solving γκ-discounted decision problems (with shaped reward), which is the main component of the κ-PI and κ-VI algorithms. The experiments illustrate the performance of model-free algorithms can be improved by using them as solvers of multi-step greedy PI and VI schemes, as well as emphasize important implementation details while doing so.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a method for enabling meta-learning in task-unsegmented settings, operating directly on time series in which the latent task undergoes discrete, unobserved switches. The method integrates a Bayesian changepoint estimation scheme with existing meta-learning approaches, allowing the algorithm to reason about whether or not the task has changed in a time series. This enables a standard meta-learning algorithm, which is designed for the task segmented setting, to be both trained and tested directly on time series data without the need for task segmentation.",
        "Abstract": "Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.",
        "Introduction": "  INTRODUCTION Meta-learning methods have recently shown promise as an effective strategy for enabling efficient few-shot learning in complex domains from image classification to nonlinear regression (Finn et al., 2017; Snell et al., 2017). These methods leverage an offline meta-training phase, in which they use data from a distribution of tasks to optimize learning performance on new tasks. These algo- rithms have focused on settings with task segmentation, where the learning agent knows when tasks change. At meta-train time, these algorithms assume access to a meta-dataset of datasets from indi- vidual tasks, and at meta-test time, the learner is evaluated on a single task. However, there are many applications where task segmentation is unavailable, which have thus far been under-addressed in the meta-learning literature. For example, consider a robot which must learn to adapt to a chang- ing environment. The robot may switch from one environment to another during the course of deployment, and these task switches may not be directly observed. Furthermore, using an existing time series from interaction to craft a meta-dataset may require a difficult or expensive process of detecting switches in task. In this work, we aim to enable meta-learning in task-unsegmented settings, operating directly on time series in which the latent task undergoes discrete, unobserved switches, rather than requiring a pre-segmented meta-dataset. Equivalently, this problem can be viewed from the perspective of continual learning, in that we apply the meta-learning approach to the standard online learning problem statement wherein an agent must sequentially make predictions and learn with a potentially varying latent data generating process. To accomplish this, we integrate a Bayesian changepoint estimation scheme with existing meta-learning approaches, allowing the algorithm to reason about whether or not the task has changed in a time series. Thus, we enable a standard meta-learning algorithm, which is designed for the task segmented setting, to be both trained and tested directly on time series data without the need for task segmentation.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel neural-linear bandit algorithm that uses Thompson Sampling on top of the last layer of a deep neural network. The algorithm is resilient to catastrophic forgetting due to limited memory by adjusting the moments of the likelihood of the reward estimation conditioned on new features to match the likelihood conditioned on old features. Simulation results on several real-world and simulated data sets, including classification and regression, using Multi-Layered Perceptrons (MLPs) and a sentiment analysis data set using a Convolution Neural Network (CNNs) demonstrate that the algorithm improves performance when memory is limited.",
        "Abstract": "We study neural-linear bandits for solving problems where both exploration and representation learning play an important role. Neural-linear bandits leverage the representation power of deep neural networks and combine it with efficient exploration mechanisms, designed for linear contextual bandits, on top of the last hidden layer. Since the representation is being optimized during learning, information regarding exploration with \"old\" features is lost. Here, we propose the first limited memory neural-linear bandit that is resilient to this catastrophic forgetting phenomenon. We perform simulations on a variety of real-world problems, including regression, classification, and sentiment analysis, and observe that our algorithm achieves superior performance and shows resilience to catastrophic forgetting. ",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) can learn representations of data with multiple levels of abstraction and have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics ( LeCun et al., 2015 ;  Goodfellow et al., 2016 ). Using DNNs for function approximation in reinforcement learning (RL) enables the agent to generalize across states without domain-specific knowledge, and learn rich domain representations from raw, high-dimensional inputs ( Mnih et al., 2015 ;  Silver et al., 2016 ). Nevertheless, the question of how to perform efficient exploration during the representation learning phase is still an open problem. The -greedy policy ( Langford & Zhang, 2008 ) is simple to implement and widely used in practice ( Mnih et al., 2015 ). However, it is statistically suboptimal. Optimism in the Face of Uncertainty ( Abbasi-Yadkori et al., 2011 ;  Auer, 2002 , OFU), and Thompson Sampling ( Thompson, 1933 ;  Agrawal & Goyal, 2013 , TS) use confidence sets to balance exploitation and exploration. For DNNs, such confidence sets may not be accurate enough to allow efficient exploration. For example, using dropout as a posterior approximation for exploration does not concentrate with observed data ( Osband et al., 2018 ) and was shown empirically to be insufficient ( Riquelme et al., 2018 ). Alternatively, pseudo-counts, a generalization of the number of visits, were used as an exploration bonus ( Bellemare et al., 2016 ;  Pathak et al., 2017 ). Inspired by tabular RL, these ideas ignore the uncertainty in the value function approximation in each context. As a result, they may lead to inefficient confidence sets ( Osband et al., 2018 ). Linear models, on the other hand, are considered more stable and provide accurate uncertainty estimates but require substantial feature engineering to achieve good results. Additionally, they are known to work in practice only with \"medium-sized\" inputs (with around 1, 000 features) due to numerical issues. A natural attempt at getting the best of both worlds is to learn a linear exploration policy on top of the last hidden layer of a DNN, which we term the neural-linear approach. In RL, this approach was shown to refine the performance of DQNs ( Levine et al., 2017 ) and improve exploration when combined with TS ( Azizzadenesheli et al., 2018 ) and OFU ( O'Donoghue et al., 2018 ;  Zahavy et al., 2018a ). For contextual bandits,  Riquelme et al. (2018)  showed that neural-linear TS achieves superior performance on multiple data sets. A practical challenge for neural-linear bandits is that the representation (the activations of the last hidden layer) change after every optimization step, while the features are assumed to be fixed over time when used by linear contextual bandits.  Riquelme et al. (2018)  tackled this problem by storing the entire data set in a memory buffer and computing new features for all the data after each DNN learning phase. The authors also experimented with a bounded memory buffer, but observed a significant decrease in performance due to catastrophic forgetting ( Kirkpatrick et al., 2017 ), i.e., a loss of information from previous experience. In this work, we propose a neural-linear bandit that uses TS on top of the last layer of a DNN ( Fig. 1 ) 1 . Key to our approach is a novel method to compute priors whenever the DNN features change that makes our algorithm resilient to catastrophic forgetting. Specifically, we adjust the moments of the likelihood of the reward estimation conditioned on new features to match the likelihood conditioned on old features. We achieve this by solving a semi-definite program ( Vandenberghe & Boyd, 1996 , SDP) to approximate the covariance and using the weights of the last layer as prior to the mean. We present simulation results on several real-world and simulated data sets, including classification and regression, using Multi-Layered Perceptrons (MLPs). Our findings suggest that using our method to approximate priors improves performance when memory is limited. Finally, we demonstrate that our neural-linear bandit performs well in a sentiment analysis data set where the input is given in natural language (of size R 8k ) and we use a Convolution Neural Network (CNNs). In this regime, it is not feasible to use a linear method due to computational problems. To the best of our knowledge, this is the first neural-linear algorithm that is resilient to catastrophic forgetting due to limited memory.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an empirical criticism of BLEU and ROUGE, two of the most commonly used metrics for evaluating similarity between candidate and reference texts in machine translation and summarization. It proposes JAUNE, a set of criteria for a sound evaluation metric, and suggests ways to use recent advances in NLP to design data-driven metrics that address the weaknesses of BLEU and ROUGE while scoring high on the criteria.",
        "Abstract": "We review the limitations of BLEU and ROUGE -- the most popular metrics used to assess reference summaries against hypothesis summaries, and introduce JAUNE:  a set of criteria for what a good metric should behave like and propose concrete ways to use recent Transformers-based Language Models to assess reference summaries against hypothesis summaries.\n\n",
        "Introduction": "  INTRODUCTION Evaluation metrics play a central role in the machine learning community. They direct research efforts and define the state of the art models. In machine translation and summarization, the two most common metrics used for evaluating similarity between candidate and reference texts are BLEU ( Papineni et al., 2002 ) and ROUGE ( Lin, 2004 ). Both approaches rely on counting the matching n-grams in the candidate text to n-grams in the reference text. BLEU is precision focused while ROUGE is recall focused. These metrics have posed serious limitations and have already been criticized by the academic com- munity ( Reiter, 2018 ) ( Callison-Burch et al., 2006 ) ( Sulem et al., 2018 ) ( Novikova et al., 2017 ). In this work, we formulate an empirical criticism of BLEU and ROUGE, establish JAUNE: a set of criteria that a sound evaluation metric should pass. Furthermore we propose concrete ways to use recent advances in NLP to design data-driven metrics addressing the weaknesses found in BLEU and ROUGE while scoring high on the criteria for a sound evaluation metric.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a multi-class and multi-instance segmentation approach that is split into three tasks: detection, separation and segmentation. This semi-supervised learning approach is trained with \"lazy\" labels, which are coarse annotations of class instances together with only a few pixel-wise annotated images. A single deep neural network is used to handle the three tasks, which are jointly optimized. The proposed approach is evaluated on two applications, namely for the segmentation of SEM images of food microstructure and stained histology images of glandular tissues. The results show that the proposed approach outperforms state-of-the-art weakly supervised methods.",
        "Abstract": "The need for labour intensive pixel-wise annotation is a major limitation of many fully supervised learning methods for image segmentation.   In this paper,  we propose a deep convolutional neural network for multi-class segmentation that circumvents this problem by being trainable on coarse data labels combined with only a very small number of images with pixel-wise annotations.  We call this new labelling strategy ‘lazy’ labels.  Image segmentation is then stratified into three connected tasks: rough detection of class instances, separation of wrongly connected objects without a clear boundary, and pixel-wise segmentation to find the accurate boundaries of each object. These problems are integrated into a multi-task learning framework and the model is trained end-to-end in a semi-supervised fashion. The method is demonstrated on two segmentation datasets, including food microscopy images and histology images of tissues respectively. We show that the model gives accurate segmentation results even if exact boundary labels are missing for a majority of the annotated data.  This allows more flexibility and efficiency for training deep neural networks that are data hungry in a practical setting where manual annotation is expensive, by collecting more lazy (rough) annotations than precisely segmented images. ",
        "Introduction": "  INTRODUCTION Image segmentation has been an active research field in the past decades. Deep learning approaches play an increasingly important role and have become state-of-the-art in various segmentation tasks (Huang et al., 2018; Khoreva et al., 2017; Tsutsui et al., 2018; Ghosh et al., 2018; Litjens et al., 2017). Though fully supervised segmentation neural networks have a shown great success, one of their most challenging issues is the need for pixel-level annotations to train them. Obtaining such annotations usually requires a great amount of manual work and is therefore expensive. In this paper, we propose a multi-class and multi-instance segmentation approach that we split into three relevant tasks: detection, separation and segmentation (cf.  Figure 1 ). Doing so, we obtain a semi-supervised learning approach that is trained with so-called \"lazy\" labels, that is a lot of coarse annotations of class instances together with only a few pixel-wise annotated images that can be obtained from the coarse labels in a semi-automated way. In the following, we will refer to weak (resp. strong) annotations for coarse (resp. accurate) labels and denote them as WL and SL. Task 1 detects and classifies each object and roughly determines its region through an under- segmentation mask. Instance counting can be obtained as a by-product of this task. As the main objective is instance detection, exact labels for the whole object or its boundary are not necessary at this stage. We use instead weakly annotated images in which a rough region inside each object is marked, cf. the most top left part of  Figure 1 . For segmentation problems with a dense population of instances, such as the food components (see e.g.,  Figure 1 ), cells (Guerrero-Pena et al., 2018; Ronneberger et al., 2015), glandular tissue, or people in a crowd (Wang et al., 2018b), separating objects sharing a common boundary is a well known challenge. We can optionally perform a second task (Task 2) that focuses on the separation of instances that are connected without a clear boundary dividing them. Also for this task we rely on WL to reduce the burden of manual annotations: touching interfaces are specified with rough scribbles, cf. top left part of  Figure 1 . Task 3 finally tackles the pixel-wise classification of the instances. It requires strong annotations that are accurate up to the boundaries of the objects. Thanks to the information brought by the weak annotations, we here just Under review as a conference paper at ICLR 2020 need a very small set of accurate segmentation masks, cf. bottom left part of  Figure 1 . To that end, we propose to refine some of the coarse labels resulting from task 1 using a semi-automatic segmentation method which requires additional manual intervention. The three tasks are handled by a single deep neural network and are jointly optimized. The network architecture is inspired by the widely used segmentation network named U-net (Ronneberger et al., 2015). With all three tasks sharing the same contracting path, we introduce a new multi-task block for the expansive path. The network has three outputs and is fed with a combination of WL and SL described above. Since weakly and strongly annotated training data is shared between the tasks, part of the annotations are missing, especially for task 3. To accommodate for this we introduce a weighted loss function over the samples. Accurate segmentation labels for training are usually not easy to obtain, however, with our approach we demonstrate that exact labels for the whole training set are not needed for good segmentation learning performance. We evaluate the performance of the proposed approach on two applications, namely for the segmenta- tion of SEM images of food microstructure and stained histology images of glandular tissues. In summary, our contributions are as follows. (1). We propose a decomposition of the segmentation problems into three tasks and a corresponding user friendly labeling strategy. (2). We develop a multi-task learning framework that learns directly from the manual labels and is trained end-to-end. Our approach outperforms state-of-the-art weakly supervised methods such as Khoreva et al. (2017) (3). The network predicts segmentation mask as well as the object inner regions and touching object interfaces. Touching objects are effectively disconnected as a side product.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a reinforcement learning algorithm that uses physical derivatives to learn a map from the directions in which policy parameters change to the directions in which every state of the trajectory changes. The training phase consists of physically calculating directional derivatives by the finite difference after applying perturbed versions of a nominal policy. The test phase uses these directional derivatives to compute derivatives along unseen directions. The algorithm is based on the assumption that the trajectories of physical systems live on an intrinsic low-dimensional manifold and change slowly with the small changes in the parameters of the system.",
        "Abstract": "Model-free and model-based reinforcement learning are two ends of a spectrum. Learning a good policy without a dynamic model can be prohibitively expensive. Learning the dynamic model of a system can reduce the cost of learning the policy, but it can also introduce bias if it is not accurate. We propose a middle ground where instead of the transition model, the sensitivity of the trajectories with respect to the perturbation (shaking) of the parameters is learned. This allows us to predict the local behavior of the physical system around a set of nominal policies without knowing the actual model. We assay our method on a custom-built physical robot in extensive experiments and show the feasibility of the approach in practice. We investigate potential challenges when applying our method to physical systems and propose solutions to each of them.",
        "Introduction": "  INTRODUCTION Traditional reinforcement learning crucially relies on reward(Sutton & Barto, 2018). However, re- ward binds the agent to a certain task for which the reward represents success. Aligned with the recent surge of interest in unsupervised methods in reinforcement learning (Baranes & Oudeyer, 2013; Bellemare et al., 2016; Gregor et al., 2016; Hausman et al., 2018; Houthooft et al., 2016) and previously proposed ideas (Schmidhuber, 1991a; 2010), we argue that there exist properties of a dy- namical system which are not tied to any particular task, yet highly useful, and their knowledge can help solve other tasks more efficiently. This work focuses on the sensitivity of the produced trajecto- ries of the system with respect to the policy so called Physical Derivatives. The term physical comes from the fact that it uses the physics of the system rather than any idealized model. We learn a map from the directions in which policy parameters change to the directions in which every state of the trajectory changes. In general, our algorithm learns the Jacobian matrix of the system at every time step through the trajectory. The training phase consists of physically calculating directional deriva- tives by the finite difference after applying perturbed versions of a nominal policy (a.k.a. controller). Perturbing the parameters of the controller is the reason for naming our method shaking. The test phase uses these directional derivatives to compute derivatives along unseen directions. Due to the difficulty of computing the Jacobian matrix by the finite difference in higher dimensions, we use random controllers joint with probabilistic learning methods to obtain a robust estimate of the Jaco- bian matrix at each instant of time along a trajectory. We are capable of this generalization to unseen perturbations because the trajectories of physical systems live on an intrinsic low-dimensional man- ifold and change slowly with the small changes in the parameters of the system (Koopman, 1931). This assumption holds as long as the system is not chaotic or close to a bifurcation condition (Khalil, 2002).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an implementation of a Keyword Spotting model to mine local community radio content in a low-resource language in Uganda. The model is evaluated on the Luganda language, which is spoken and used in many agricultural communities in Uganda. The model is designed to detect and retrieve a series of words from a database of audio streams, and can be developed without sufficient labelled data. The results of this model could provide early warning to farmers and agricultural research bodies about crop diseases and pests, and enable real-time actionable surveillance data.",
        "Abstract": "In societies with well developed internet infrastructure, social media is the leading medium of communication for various social issues especially for breaking news situations. In rural Uganda however, public community radio is still a dominant means for news dissemination. Community radio gives audience to the general public especially to individuals living in rural areas, and thus plays an important role in giving a voice to those living in the broadcast area. It is an avenue for participatory communication and a tool relevant in both economic and social development.This is supported by the rise to ubiquity of mobile phones providing access to phone-in or text-in talk shows. In this paper, we describe an approach to analysing the readily available community radio data with machine learning-based speech keyword spotting techniques. We identify the keywords of interest related to agriculture and build models to automatically identify these keywords from audio streams. Our contribution through these techniques is a cost-efficient and effective way to monitor food security concerns particularly in rural areas. Through keyword spotting and radio talk show analysis, issues such as crop diseases, pests, drought and famine can be captured and fed into an early warning system for stakeholders and policy makers.",
        "Introduction": "  INTRODUCTION Ensuring a functional and near real-time system of surveillance for crop diseases and pests is of critical importance to sustaining the livelihoods of smallholder farmers in sub-Saharan Africa ( Mutembesa et al., 2018 ). Disease and pest surveillance systems have to be put in place to provide early warning to the farmers and the relevant agricultural research bodies. Usually, when a crop disease or pest is reported in a given area, experts from the respective research institutes take time to reach the reported location to carry out investigations. This usually involves inspection of the crops at specific intervals (of about 10 km) along the more accessible main roads, covering only small proportions of the areas of interest in major districts ( Mutembesa et al., 2018 ). Because the surveillance teams have to work within limited budgets, the surveys and the results from the surveys may be delayed or fewer regions may be sampled in a particular year. As the health experts provide an annual snapshot of the health of crops across the country they are limited in their ability to provide real-time actionable surveillance data. In many cases, the farmers never get to know the disease that has attacked their crops for weeks or even months. In many areas in Uganda, the vast majority of the affected people will use social media to communicate their concerns in their local communities. This social media is not Facebook or Twitter, its the local community radio stations existing in almost each village in sub-Saharan Africa ( Saeb et al., 2017 ). These rural radio stations give farmers an opportunity to interact with each other and also with the relevant agricultural authorities such as extension workers and agricultural experts. This can usually be through a number of formats like phone-in programs, live talk shows (Nakabugu, 2001). Specifically for some of the radio stations, they have targeted agricultural talk shows that can host an expert from an agricultural research institute who can aim at providing specific information for example: about crop disease and pest management. Keyword Spotting systems (KWS) is a classification task that aims at detection and retrieving of a series of words from a database of audio streams. The advantage of using a KWS is that unlike full automatic speech recognition systems, they can be developed without sufficient labelled data. This Under review as a conference paper at ICLR 2020 is common especially in low resourced languages  Menon et al. (2019) . In this paper, we discuss an implementation of a Keyword Spotting model that we use to mine local community radio content using specific keywords for a low resourced language in Uganda. We evaluate our approach on the Luganda language which is a low-resource language that is currently spoken and used in many of the Agricultural communities in Uganda.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a method for automatic image segmentation that incorporates shape and image priors to mitigate boundary ambiguities. The Level Set method for image segmentation evolves an initial contour of an object-of-interest along the normal direction with a forcing function. This method is important for accurate labeling of pixels in close proximity to inter-class boundaries, which is a challenging task in image segmentation.",
        "Abstract": "We propose a novel approach for image segmentation that combines Neural Ordinary Differential Equations (NODEs) and the Level Set method.  Our approach parametrizes the evolution of an initial contour with a NODE that implicitly learns from data a speed function describing the evolution.  In addition, for cases where an initial contour is not available and to alleviate the need for careful choice or design of contour embedding functions, we propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space. We evaluate our methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial contours provided by deep learning models while using a fraction of their number of parameters, our approach achieves F scores that are higher than several state-of-the-art deep learning algorithms",
        "Introduction": "  INTRODUCTION Image segmentation is the task of delineating pixels belonging to semantic labels. The ability to automatically segment objects is important because accurate labeling is expensive and hard ( Vit- tayakorn & Hays, 2011 ;  Zhang et al., 2018 ). Automatic image segmentation can have large impact in many domains, e.g. obstacle avoidance in autonomous driving and treatment planning in medical imaging. Accurate classification of pixels in close proximity to inter-class boundaries remains a challenging task in image segmentation. Object boundaries can have high curvature contours or weak pixel intensity that complicate separating the object from surrounding ones. In deep CNNs ( Simonyan & Zisserman, 2014 ;  Zeiler & Fergus, 2014 ;  Szegedy et al., 2015 ;  He et al., 2016 ;  Chen et al., 2017 ), the object-of-interest and surrounding competing objects can provide equal context to a receptive field of a boundary pixel, which can make accurate classification difficult. Humans also find it difficult to accurately label pixels near object boundaries. Level Set methods ( Zhao et al., 1996 ;  Brox et al., 2006 ) and Active Shapes ( Paragios & Deriche, 2000 ;  Chan & Vese, 2001 ) have been proposed to incorporate shape and image priors to mitigate boundary ambiguities ( Tsai et al., 2003 ;  Rousson & Paragios, 2002 ). The Level Set method for image segmentation evolves an initial contour of an object-of-interest along the normal direction with a forcing function. A contour is represented by an embedding function, typically a signed distance function, and its evolution amounts to solving a differential equation ( Osher & Sethian, 1988 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces an unsupervised reinforcement learning (RL) algorithm that autonomously explores the environment and iteratively distills this experience into general-purpose policies that can accomplish user-specified tasks at test time. The exploration objective is to maximize the entropy of the learned policy's visited state distribution, which should approach a uniform distribution over valid states. However, this objective is not enough to develop principled unsupervised RL algorithms that result in useful policies, and a mechanism is needed to control the resulting policy to achieve new tasks at test-time.",
        "Abstract": "Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. Skew-Fit enables self-supervised agents to autonomously choose and practice reaching diverse goals. We show that, under certain regularity conditions, our method converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) provides an appealing formalism for automated learning of behavioral skills, but separately learning every potentially useful skill becomes prohibitively time consuming, both in terms of the experience required for the agent and the effort required for the user to design reward functions for each behavior. What if we could instead design an unsupervised RL algorithm that automatically explores the environment and iteratively distills this experience into general-purpose policies that can accomplish new user-specified tasks at test time? For an agent to learn autonomously, it needs an explo- ration objective. In the absence of any prior knowledge about which states are more useful, an effective exploration scheme is one that visits as many states as possible, allowing a policy to au- tonomously prepare for user-specified task that it might see at test time. This objective has been formalized as maximizing the entropy of the learned policy's visited state distribution 1 H(S) ( Hazan et al., 2018a ), since a policy that maximizes this objective should approach a uniform distribution over valid states. Unfortunately, directly optimizing H(S) requires an accurate model of the policy and environment ( Hazan et al., 2018a ). Moreover, even if this optimization were tractable, another short-coming of this objective is that the resulting policy cannot be used to solve new tasks: it only knows how to maximize state entropy. In other words, to develop principled unsupervised RL algorithms that result in useful policies, maximizing H(S) is not enough. We need a mechanism that allows us to control the resulting policy to achieve new tasks at test-time.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces MelNet, a generative model for audio which captures longer-range dependencies than existing end-to-end models by modelling 2D time-frequency representations such as spectrograms. MelNet couples a fine-grained autoregressive model and a multiscale generation procedure to jointly capture local and global structure, and is broadly applicable to a variety of audio generation tasks, including unconditional speech and music generation. Additionally, MelNet is able to model highly multimodal data such as multi-speaker and multilingual speech.",
        "Abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.",
        "Introduction": "  INTRODUCTION Audio waveforms have complex structure at drastically varying timescales, which presents a challenge for generative models. Local structure must be captured to produce high-fidelity audio, while long- range dependencies spanning tens of thousands of timesteps must be captured to generate audio which is globally consistent. Existing generative models of waveforms such as WaveNet ( van den Oord et al., 2016a ) and SampleRNN ( Mehri et al., 2016 ) are well-adapted to model local dependencies, but as these models typically only backpropagate through a fraction of a second, they are unable to capture high-level structure that emerges on the scale of several seconds. We introduce a generative model for audio which captures longer-range dependencies than existing end-to-end models. We primarily achieve this by modelling 2D time-frequency representations such as spectrograms rather than 1D time-domain waveforms ( Figure 1 ). The temporal axis of a spectrogram is orders of magnitude more compact than that of a waveform, meaning dependencies that span tens of thousands of timesteps in waveforms only span hundreds of timesteps in spectrograms. In practice, this enables our spectrogram models to generate unconditional speech and music samples with consistency over multiple seconds whereas time-domain models must be conditioned on intermediate features to capture structure at similar timescales. Modelling spectrograms can simplify the task of capturing global structure, but can weaken a model's ability to capture local characteristics that correlate with audio fidelity. Producing high-fidelity audio has been challenging for existing spectrogram models, which we attribute to the lossy nature of spectrograms and oversmoothing artifacts which result from insufficiently expressive models. To reduce information loss, we model high-resolution spectrograms which have the same dimensionality as their corresponding time-domain signals. To limit oversmoothing, we use a highly expressive autoregressive model which factorizes the distribution over both the time and frequency dimensions. Modelling both fine-grained details and high-level structure in high-dimensional distributions is known to be challenging for autoregressive models. To capture both local and global structure in spectrograms with hundreds of thousands of dimensions, we employ a multiscale approach which generates spectrograms in a coarse-to-fine manner. A low-resolution, subsampled spectrogram that captures high-level structure is generated initially, followed by an iterative upsampling procedure that adds high-resolution details. Combining these representational and modelling techniques yields a highly expressive and broadly applicable generative model of audio. Our contributions are are as follows: • We introduce MelNet, a generative model for spectrograms which couples a fine-grained autoregressive model and a multiscale generation procedure to jointly capture local and global structure. • We show that MelNet is able to model longer-range dependencies than existing time-domain models. Additionally, we include an ablation to demonstrate that multiscale modelling is essential for modelling long-range dependencies. • We demonstrate that MelNet is broadly applicable to a variety of audio generation tasks, including unconditional speech and music generation. Furthermore, MelNet is able to model highly multimodal data such as multi-speaker and multilingual speech.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel adversarial defense technique that relies on a latent randomized tensor parametrization of each layer in a deep neural network. This technique is evaluated against various adversarial attacks and is shown to consistently and significantly improve over the current state-of-the-art, especially when combined with adversarial training. The method is verified to work across domains, by experimenting on both image-based and audio-based classification.",
        "Abstract": "As deep neural networks become widely adopted for solving most problems in computer vision and audio-understanding, there are rising concerns about their potential vulnerability. In particular, they are very sensitive to adversarial attacks, which manipulate the input to alter models' predictions. Despite large bodies of work to address this issue, the problem remains open.  In this paper, we propose defensive tensorization, a novel adversarial defense technique that leverages a latent high order factorization of the network. Randomization is applied in the latent subspace, therefore resulting in dense reconstructed weights, without the sparsity or perturbations typically induced by the randomization.\nOur approach can be easily integrated with any arbitrary neural architecture and combined with techniques like adversarial training. We empirically demonstrate the effectiveness of our approach on standard image classification benchmarks. We further validate the generalizability of our approach across domains and low-precision architectures by considering an audio classification task and binary networks. In all cases, we demonstrate superior performance compared to prior works in the target scenario.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) are powerful predictive models that achieve impressive accuracy across a wide range of artificial intelligence tasks, including image classification ( He et al., 2016 ;  Krizhevsky & Hinton, 2009 ) and speech recognition ( Amodei et al., 2016a ;  Graves et al., 2013 ;  Graves & Jaitly, 2014 ). The popularity of DNNs in production-ready systems has raised a serious security concern as DNNs are found to be susceptible to a wide range of adversarial attacks ( Madry et al., 2017 ;  Akhtar & Mian, 2018 ;  Dong et al., 2018 ;  Huang et al., 2017 ;  Goodfellow et al., 2014 ;  Kurakin et al., 2016 ), where small and imperceptible perturbations of the input data lead to incorrect predictions by the networks. These shortcomings pose an obstacle in wide-scale adoption of DNNs and expose an inherent weakness in their reliability. This is especially important when such models become part of security and safety related solutions ( Amodei et al., 2016b ). This susceptibility of DNNs to adversarial perturbations has led to a large volume of work that attempts to design robust networks ( Dhillon et al., 2018 ;  Lin et al., 2019 ;  Samangouei et al., 2018 ;  Song et al., 2017 ;  Guo et al., 2017 ). However, advances in designing robust DNNs have been followed with stronger perturbation schemes that defeat such defences ( Athalye et al., 2018 ). Most defenses that rely on randomization, either apply randomized transformations to the input, e.g. ( Xie et al., 2018 ), or randomization applied within the network, e.g., on the activations ( Dhillon et al., 2018 ) or on the weights directly ( Wang et al., 2018 ). However, all these approaches typically introduce artificats (e.g. sparsity in the weights or activations) and can be defeated by carefully crafted attacks ( Athalye et al., 2018 ). In addition, it is preferable to not to rely on modifications of the inputs but have a network that is inherently robust against attacks. In this paper, we take a different approach to randomization. We first parametrize the network using tensor factorization, effectively introducing a latent subspace spanning the weights. We then apply randomization in that latent subspace, enabling us to create models that are robust to adversarial attacks, without modifying directly the weights, activations or inputs. In summary, we make the following contributions: • We propose a novel adversarial defence technique that relies on a latent randomized tensor parametrization of each layer in the network and can be seamlessly integrated within any net- work architecture. Under review as a conference paper at ICLR 2020 • We thoroughly evaluate the robustness of our method against various adversarial attacks and show that it consistently and significantly improves over the current state-of-the-art especially when combined with adversarial training. • We show that our method successfully hardens the models against these attacks for both quantized and real-valued nets. • We verify that our strategy works across domains, by experimenting on both image-based and audio-based classification.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on flagging out-of-distribution (OOD) examples in tasks such as image classification, text classification, and speech recognition. Recent approaches to this problem have shown that looking at the maximal softmax value is insufficient. This paper introduces an extension of Gram matrices to characterize activity patterns by feature correlations, allowing for state-of-the-art (SOTA) detection rates on OOD examples.",
        "Abstract": "When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and class predicted. We find that characterizing activity patterns by feature correlations and identifying anomalies in pairwise feature correlation values can yield high OOD detection rates. We identify anomalies in the pairwise feature correlations by simply comparing each pairwise correlation value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and does not require access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. The method is applicable across a variety of architectures and vision datasets and generally performs better than or equal to state-of-the-art OOD detection methods, including those that do assume access to OOD examples.",
        "Introduction": "  INTRODUCTION Even when deep neural networks (DNNs) achieve impressive accuracy on challenging tasks, they do not always visibly falter on misclassified examples: in those cases they can often make predictions that are both very confident and completely incorrect. Yet, predictive uncertainty is essential in real-world contexts tolerating minimal error margins such as autonomous vehicle control and medical, financial and legal fields. In this work, we focus on flagging test examples that do not contain any of the classes modeled in the train distribution. Such examples are often referred to as being out-of-distribution (OOD), and while their existence has been well-known for some time, the challenges of identifying them and a baseline method to do so in a variety of tasks such as image classification, text classification, and speech recognition were presented by  Hendrycks and Gimpel (2017) . Recently,  Nalisnick et al. (2019a)  identified a similar problem with generative models: they demonstrate that flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. They report similar findings across several other pairs of popular image datasets. While we might expect neural networks to respond differently to OOD examples than to in-distribution (ID) examples, exactly where and how to find these differences in activity patterns is not at all clear.  Hendrycks and Gimpel (2017)  and others ( Nguyen et al., 2015 ;  Yu et al., 2011 ) showed that looking at the maximal softmax value is insufficient. In Section 2 we describe some other recent approaches to this problem. In this work, we find that characterizing activity patterns by feature correlations- computed with an extension of Gram matrices that we introduce-lets us quantify anomalies to allow state-of-the-art (SOTA) detection rates on OOD examples.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the learning curves of kernel methods applied to MNIST and CIFAR10 datasets. We find that the generalization error decays as a power law with an exponent β that does not depend on the data dimension. We propose a framework for regression in which the target function is assumed to be a Gaussian random field of zero mean with translation-invariant isotropic covariance. We observe that β is controlled by the high-frequency scaling of both the Teacher and Student kernels. We offer an interpretation that kernel methods are performing a local interpolation whose quality depends on the distance between adjacent data points. We also find that real datasets actually live on lower-dimensional manifolds, which explains the observed values for β. This analogy with Gaussian fields allows one to associate a smoothness index to any dataset once β and the effective dimension are measured.",
        "Abstract": "How many training data are needed to learn a supervised task? It is often observed that the generalization error decreases  as $n^{-\\beta}$ where $n$ is the number of training examples and $\\beta$  an exponent that  depends on both data and algorithm. In this work we measure  $\\beta$  when applying kernel methods to real datasets. For MNIST we find $\\beta\\approx 0.4$ and for CIFAR10 $\\beta\\approx 0.1$. Remarkably, $\\beta$ is the same for  regression and classification tasks, and for Gaussian or Laplace kernels. To rationalize the existence of non-trivial exponents that can be independent of the specific kernel used, we introduce the Teacher-Student framework for kernels. In this scheme, a Teacher generates data according to a Gaussian random field, and a Student learns  them via kernel regression. With a simplifying assumption --- namely that the data are sampled from a regular lattice --- we derive analytically $\\beta$  for translation invariant kernels, using previous results from the kriging literature.  Provided that the Student is not too sensitive to high frequencies, $\\beta$ depends only on the training data and their dimension. We confirm numerically that these predictions hold when the training points are  sampled  at random on a hypersphere. Overall, our results quantify how smooth Gaussian data should be to avoid the curse of dimensionality, and indicate that for kernel learning the relevant dimension of the data  should be defined in terms of how the distance between  nearest data points depends on $n$. With this definition one obtains reasonable effective smoothness estimates for MNIST and CIFAR10.",
        "Introduction": "  INTRODUCTION In supervised learning machines learn from a finite collection of n training data, and their generaliza- tion error is then evaluated on unseen data drawn from the same distribution. How many data are needed to learn a task is characterized by the learning curve relating generalization error to n. In various cases, the generalization error decays as a power law n −β , with an exponent β that depends on both the data and the algorithm. In ( Hestness et al., 2017 ) β is reported for state-of-the-art (SOTA) deep neural networks for various tasks: in for neural-machine translation β ≈ 0.3-0.36 (for fixed model size) or β ≈ 0.13 (for best-fit models at any n); language modeling shows β ≈ 0.06-0.09; in speech recognition β ≈ 0.3; SOTA models for image classification (on ImageNet) have exponents β ≈ 0.3-0.5. Currently there is no available theory of deep learning to rationalize these observations. Recently it was shown that for a proper initialization of the weights, deep learning in the infinite-width limit ( Jacot et al., 2018 ) converges to kernel learning. Moreover, it is nowadays part of the lore that there exist kernels whose performance is nearly comparable to deep networks (Bruna and Mallat, 2013; Arora et al., 2019), at least for some tasks. It is thus of great interest to understand the learning curves of kernels. For regression, if the target function being learned is simply assumed to be Lips- chitz, then the best guarantee is β = 1 /d ( Luxburg and Bousquet, 2004 ;  Bach, 2017 ) where d is the data dimension. Thus for large d, β is very small: learning is completely inefficient, a phenomenon referred to as the curse of dimensionality. As a result, various works on kernel regression make the much stronger assumption that the training points are sampled from a target function that belongs to the reproducing kernel Hilbert space (RKHS) of the kernel (see for example ( Smola et al., 1998 )). With this assumption β does not depend on d (for instance in ( Rudi and Rosasco, 2017 ) β = 1/2 is guaranteed). Yet, RKHS is a very strong assumption which requires the smoothness of the target Under review as a conference paper at ICLR 2020 function to increase with d ( Bach, 2017 ) (see more on this point below), which may not be realistic in large dimensions. In this work we compute β empirically for kernel methods applied on MNIST and CIFAR10 datasets. We find β MNIST ≈ 0.4 and β CIFAR10 ≈ 0.1 respectively. Quite remarkably, we observe essentially the same exponents for regression and classification tasks, using either a Gaussian or a Laplace kernel. Thus the exponents are not as small as 1 /d (d = 784 for MNIST, d = 3072 for CIFAR10), but neither are they 1 /2 as one would expect under the RKHS assumption. These facts call for frameworks in which assumptions on the smoothness of the data can be intermediary between Lipschitz and RKHS. Here we propose such a framework for regression, in which the target function is assumed to be a Gaussian random field of zero mean with translation-invariant isotropic covariance K T (x). The data can equivalently be thought as being synthesized by a \"Teacher\" kernel K T (x). Learning is performed with a \"Student\" kernel K S (x) that minimizes the mean-square error. In general K T (x) = K S (x). In this set-up learning is very similar to a technique referred to as kriging, or Gaussian process regression, originally developed in the geostatistics community ( Matheron, 1963 ;  Stein, 1999b ). To quantify learning, we first perform numerical experiments for data points distributed uniformly at random on a hypersphere of varying dimension d, focusing on a Laplace kernel for the Student, and considering a Laplace or Gaussian kernel for the Teacher. We observe that in both cases β(d) is a decreasing function. To derive β(d) we consider the simplified situation where the Gaussian random field is sampled at training points lying on a regular lattice. Building on the kriging literature ( Stein, 1999b ), we show that β is controlled by the high-frequency scaling of both the Teacher and Student kernels: assuming that the Fourier transforms of the kernels decay asK Importantly (i) Eq. (1) leads to a prediction for β(d) that accurately matches our numerical study for random training data points, leading to the conjecture that Eq. (1) holds in that case as well. We offer the following interpretation: ultimately, kernel methods are performing a local interpolation whose quality depends on the distance δ(n) between adjacent data points. δ(n) is asymptotically similar for random data or data sitting on a lattice. (ii) If the kernel K S is not too sensitive to high-frequencies, then learning is optimal as far as scaling is concerned and β = (α T − d)/d. We will argue that the smoothness index s ≡ [(α T − d)/2] characterizes the number of derivatives of the target function that are continuous. We thus recover the curse of dimensionality: s needs to be of order d to have non-vanishing β in large dimensions. Point (ii) leads to an apparent paradox: β is significant for MNIST and CIFAR10, for which d is a priori very large, leading to a smoothness value s in the hundreds in both cases, which appears unrealistic. The paradox is resolved by considering that real datasets actually live on lower-dimensional manifolds. As far as kernel learning is concerned, our findings support that the correct definition of dimension should be based on how the nearest-neighbors distance δ(n) scales with n: δ(n) ∼ n − 1 /d eff . Direct measurements of δ(n) support that MNIST and CIFAR10 live on manifolds of lower dimensions d eff MNIST ≈ 15 and d eff CIFAR10 ≈ 35. Considering the effective dimensions that we find, the observed values for β would be obtained for Gaussian fields of smoothness s MNIST ≈ 3 and s CIFAR10 ≈ 1, values that appear intuitively more reasonable. More generally this analogy with Gaussian fields allows one to associate a smoothness index s to any dataset once β and d eff are measured, which may turn out to be a useful characterization of data complexity in the future.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a non-linear latent variable model for modeling independent but not identically distributed data, with an inference procedure that encourages high mutual information between the latent representation and the data within a population, and low mutual information between the latent representation and the data across populations. Experiments demonstrate that the learned representations are useful for a diverse set of applications including image denoising, unsupervised sub-group discovery, and continual learning.",
        "Abstract": "A significant body of recent work has examined variational autoencoders as a powerful approach for tasks which involve modeling the distribution of complex data such as images and text. In this work, we present a framework for modeling multiple data sets which come from differing distributions but which share some common latent structure. By incorporating architectural constraints and using a mutual information regularized form of the variational objective, our method successfully models differing data populations while explicitly encouraging the isolation of the shared and private latent factors. This enables our model to learn useful shared structure across similar tasks and to disentangle cross-population representations in a weakly supervised way. We demonstrate the utility of our method on several applications including image denoising, sub-group discovery, and continual learning.",
        "Introduction": "  INTRODUCTION Unsupervised learning of latent representations is widely used for dimensionality reduction, density estimation, and structure or sub-group discovery among other applications. Methods for recovering such representations typically rely on the assumption that the observed data is a manifestation of only a limited number of factors of variation ( Locatello et al., 2019 ;  Bengio et al., 2013 ). The variational autoencoder (VAE) ( Kingma & Welling, 2013 ), a combination of a non-linear latent variable model and an amortized inference scheme ( Dayan et al., 1995 ), is a popular method for recovering such latent structure. VAEs and their extensions have received considerable attention in recent years and have been shown useful for modeling text ( Miao et al., 2016 ), images ( Gulrajani et al., 2016 ), and other data exhibiting complex correlations ( Gómez-Bombarelli et al., 2018 ). However, barring a few exceptions ( Bouchacourt et al., 2018 ;  Severson et al., 2019 ), this line of work has assumed the data to be independent and identically distributed. In this work, we consider the task of modeling independent but not identically distributed data. We are particularly interested in analyzing data comprising two or more distinct but related sub- populations. Such data arise frequently in practice. For instance, patients suffering from an ailment may exhibit symptomatic heterogeneity based on gender or environmental factors, documents in a corpus may exhibit semantic or syntactic similarities based on genre or authorship, images may cluster depending on the image subject. Our goal is to provide rich descriptions of such data by re- covering latent representations that disentangle factors of variation common to all populations from those that are unique to a particular population. Motivated by this challenge, we propose non-linear latent variable models that explicitly account for the heterogeneity in the data. Coupled with an inference procedure that encourages high (low) mutual information between the latent representa- tion and the data within (across) a population, we show that our models are indeed able to recover population-specific representations that are salient and disentangled across differing populations as well as shared representations that isolate commonalities between populations. Through careful experiments, we vet the effectiveness of our approach and demonstrate that the learned representations are useful for a diverse set of applications including image denoising, unsu- pervised sub-group discovery, and continual learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the representation power of the multi-head self attention layer in Transformer architectures, and proposes a new way to set the head size which allows for both an increase in the number of heads per layer and a decrease in the embedding size, without hurting the performance. Experiments on language modeling, natural language inference and question answering tasks show that the proposed fixed head size Transformer can match the performance of the BERT LARGE model, a Transformer with an embedding size of 1024.",
        "Abstract": "Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. This leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the existing architectures gives rise to this limitation, which we further validate with our  experiments. As a solution, we propose a new way to set the projection size in attention heads that allows us to train models with a relatively smaller embedding dimension, without sacrificing the performance.",
        "Introduction": "  INTRODUCTION Attention based architectures, such as Transformers, have been effective for sequence modelling tasks such as machine translation ( Gehring et al., 2017 ;  Vaswani et al., 2017 ), question answering, sentence classification ( Radford et al., 2018 ;  Devlin et al., 2018 ) and document generation ( Liu et al., 2018 ). These models have emerged as better alternatives to the recurrent models - RNNs ( Sutskever et al., 2014 ), LSTMs ( Hochreiter & Schmidhuber, 1997 ) and GRUs ( Cho et al., 2014 ). This is mainly due to their feed forward structure, which removes the sequential processing bottleneck for sequence data, making them easier to train compared to the recurrent models. Self attention models also have found applications in vision ( Wang et al., 2018 ), adversarial networks ( Zhang et al., 2018 ), reinforcement learning ( Zambaldi et al., 2018 ;  Li, 2017 ) and speech recognition ( Chiu et al., 2018 ). Recent advances in using the self attention models in natural language tasks have been made by first using a language modeling task to pre-train the models and then fine tuning the learned models on specific downstream tasks.  Radford et al. (2018)  and  Devlin et al. (2018)  used Transformers to pre-train a language model and showed that the fine tuned model outperforms LSTMs on many natural language understanding and question answering tasks. For example, BERT ( Devlin et al., 2018 ), a 24 layer transformer model, is shown to achieve the state of the art performance on several NLP tasks, including on the SQuAD dataset. These advances, in addition to novel pre-training tasks, relied on bigger models with a larger embedding size. BERT model uses an embedding size of 1024 ( Devlin et al., 2018 ); GPT-2 uses models with embedding size up to 1600 ( Radford et al., 2019 ). A single Transformer block consists of two key components: a multi-head self attention layer followed by a feed forward layer ( Vaswani et al., 2017 ). A single head in a multi-head attention layer, computes self attention between the tokens in the input sequence, which it then uses to compute a weighted average of embeddings for each token. To keep the number of parameters fixed in the attention layer, each head projects the data into a lower dimensional subspace, dimension of which scales as 1/(number of heads), and computes the self attention in this subspace. This projection size for each head is commonly known as the head size. Despite the advances in using Transformer models for various tasks, their functioning and design choices still remain mysterious and are not well understood. Can the attention layer learn arbitrary contextual representations? What is the role of the feed forward layer in the Transformer block? Do we need such a large embedding size to capture the context of all the tokens? Answering these questions requires an understanding of the representation power of the units in the Transformer. In this paper we take some of the first steps towards developing such an understanding of the Transformer. In particular, we focus on the representation power of the multi-head self attention layer. First, we analyze the representation power of a single self attention unit and show that it crucially depends on the projection sizes used to compute the dot product attention. We next study the advantage of having multiple heads in the attention layer. It is generally believed that increasing the number of heads helps by allowing the heads to compute context from different representation subspaces at different positions. However, increasing the number of heads decreases the head size, decreasing the expressive power of individual heads. We show that reducing the head size to a value below the input sequence length hurts the representation power of each head. This is because a smaller head size introduces a rank constraint on the projection matrices in each head, and limits their representation power. We indeed notice this effect in practice: while the performance improves with increasing the number of heads in the beginning ( Devlin et al., 2018 ), we notice a drop in the performance once the number of heads increases beyond a certain threshold, as seen in  Table 1  and  Fig. 1  (see also Table 4(A) in  Vaswani et al. (2017) ). This heuristic of scaling the head size inversely with the number of heads was proposed initially in  Vaswani et al. (2017)  and has become the standard way of using multi-head attention ( Radford et al., 2018 ;  Devlin et al., 2018 ). In order to avoid hurting the performance, the existing Transformer models allow for multiple heads by increasing the embedding size, which in turn increases the head size. However, larger embedding size, in addition to increasing the number of parameters, makes it expensive to use the model and the learned embeddings in downstream tasks, as the downstream model sizes scale with the embedding size of the tokens. For example, the inference time and memory required in retrieval tasks increases linearly with the embedding size. Based on these observations, we propose a new way to set the projection size in the attention heads, in which each head has a fixed head size that is independent of both the number of heads and the embedding size of the model. This allows us to train models with a relatively smaller embedding size without affecting the head size. It also allows us to increase the number of heads per layer to improve the performance. Another advantage of the fixed head size Transformer is, unlike the standard Transformer, which requires the number of heads to be a factor of the embedding size, we are free to set arbitrary number of heads as required for the task. We evaluate this fixed head size Transformer on language modeling (LM1B dataset), natural lan- guage inference (MNLI dataset) and question answering tasks (SQuAD dataset). We show that the modified Transformer trained with an embedding size of 512 can match the performance of the BERT LARGE ( Devlin et al., 2018 ), a Transformer with an embedding size of 1024 (see  Fig. 2 ). We further present experimental results evaluating the effect of different choices of the head size and the embedding size in the Section 4. The contributions of this paper are summarized below. • We analyze the representation power of the multi-head self attention layer and show the limitation the embedding size places on the number of heads. • We propose a new way to set the head size, and show the proposed fixed head size layers are strictly better than the standard multi-head attention layers in terms of their expressive power. This modification allows us to both increase the number of heads per layer and decrease the embedding size, without hurting the performance. • We experimentally show that the fixed head size Transformer can be trained with a smaller embedding size and more heads on three standard NLP tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the relationship between perturbation-based and invariance-based adversarial examples, and how increasing a model's robustness to perturbation-based adversarial examples can increase its vulnerability to invariance-based adversarial examples. It introduces analytical constructions and empirical evidence to demonstrate this relationship, and introduces the first algorithm that crafts invariance-based adversarial examples for the 0 and ∞ norms. The paper also shows that many common models disagree with human labelers on these examples, and breaks a provably-robust defense on MNIST.",
        "Abstract": "Adversarial examples are malicious inputs crafted to cause a model to misclassify them. In their most common instantiation, \"perturbation-based\" adversarial examples introduce  changes to the input that leave its true label unchanged, yet result in a different model prediction.  Conversely, \"invariance-based\" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed. So far, the relationship between these two notions of adversarial examples has not been studied, we close this gap.\n\nWe demonstrate that solely achieving perturbation-based robustness is insufficient for complete adversarial robustness. Worse, we find that classifiers trained to be Lp-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts. We construct theoretical arguments and analytical examples to justify why this is the case. We then illustrate empirically that the consequences of excessive perturbation-robustness can be exploited to craft new attacks. Finally, we show how to attack a provably robust defense --- certified on the MNIST test set to have at least 87% accuracy (with respect to the original test labels) under perturbations of Linfinity-norm below epsilon=0.4 --- and reduce its accuracy (under this threat model with respect to an ensemble of human labelers) to 60% with an automated attack, or just 12% with human-crafted adversarial examples.",
        "Introduction": "  INTRODUCTION Research on adversarial examples is motivated by a spectrum of questions. These range from the security of models deployed in the presence of real-world adversaries, to the need to capture limitations of representations and their (in)ability to generalize ( Gilmer et al., 2018a ). The broadest accepted definition of an adversarial example is \"an input to a ML model that is intentionally designed by an attacker to fool the model into producing an incorrect output\" ( Goodfellow & Papernot, 2017 ). Many formal definitions of adversarial examples were introduced since their initial discovery ( Szegedy et al., 2013 ;  Biggio et al., 2013 ). In a majority of work, adversarial examples are formalized as adding a perturbation δ to some test example x to obtain an input x * that produces an incorrect model outcome. 1 We refer to this class of malicious inputs as perturbation-based adversarial examples. To enable concrete progress, the adversary's capabilities may optionally be constrained by placing a bound on the maximum perturbation δ added to the original input. The goal of this constraint is to ensure that semantics of the input are left unaffected by the perturbation δ. In the computer vision domain, p norms have grown to be the default metric to measure semantic similarity. This led to a series of proposals for increasing the robustness of models to perturbation-based adversaries that operate within the constraints of an p ball. These include robust optimization ( Madry et al., 2017 ), explicit regularization of a model's Lipschitz constant ( Cisse et al., 2017 ), or a variety of techniques to build models that are provably robust to small p perturbations ( Wong & Kolter, 2018 ;  Raghunathan et al., 2018 ;  Zhang et al., 2019 ). In this paper, we show that optimizing a model's robustness to l p -bounded perturbations is not only insufficient to address the lack of generalization identified via adversarial examples, but also potentially harmful. Intuitively, as p distances are only crude approximations to the true visual Under review as a conference paper at ICLR 2020 similarity in a given task, over-optimizing a model's robustness to p -bounded perturbations also renders the model invariant to actual semantics of the underlying task. Excessive invariance of a model to class semantics can give rise to a new class of adversarial perturbations called invariance adversarial examples ( Jacobsen et al., 2019 ). These are perturbations that successfully change the human-assigned ground-truth label of an input, while keeping the model's classification of the perturbed input unchanged (for example, we take an image of a '1' digit and perturb it-without changing the model's classification-until a human would agree that the digit now represents a '7'). See  Figure 1  for an intuitive illustration of this phenomenon. Whereas previous efforts (e.g., ( Jacobsen et al., 2019 )) explored different notions of adversarial examples independently, our work is the first to expose a complex relationship between perturbation- based and invariance-based adversarial examples. Specifically, we introduce analytical constructions and empirical evidence that shows that increasing a model's robustness to perturbation-based adversarial examples can increase the model's vulnerability to invariance-based adversarial examples. We formally show how to construct a model that is robust to perturbation-based adversarial examples but not to invariance-based adversarial examples. We further demonstrate how an imperfect model for the adversarial spheres task proposed by  Gilmer et al. (2018b)  is either vulnerable to perturbation- based or invariance-based attacks-depending on whether the point attacked is on the inner or outer sphere. Hence, these two types of adversarial examples are needed to fully account for model failures. Finally, we empirically demonstrate with a new attack how an adversary can exploit the excessive invariance of certain models. We introduce the first algorithm that crafts invariance-based adversarial examples for the 0 and ∞ norms, and show that many common models disagree with human labelers on these examples. In particular, our algorithm breaks a provably-robust defense on MNIST ( Zhang et al., 2019 ). 2 This model is certified to have 87% test-accuracy under l ∞ -perturbations of radius ε = 0.4. Yet, on our automatically-generated invariance-based adversarial examples, the model only agrees with the human-assigned label in 60% of the cases; when we manually craft invariance adversarial examples we reduce accuracy to just 12% as determined by an ensemble of humans.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new learning principle for unsupervised models, the Capacity-Constrained InfoMax, which allows for learning a good representation while maintaining optimal generative performance. The paper describes the Variational AutoEncoder (VAE) from an information theoretic perspective and derives a variational lower bound for the maximal mutual information of a generative model belonging in a certain family. The paper also defines and estimates bounds for the network capacity for a variational autoencoder. The theoretical arguments are confirmed by experiments.",
        "Abstract": "We propose the Variational InfoMax AutoEncoder (VIMAE), an autoencoder based on a new learning principle for unsupervised models: the Capacity-Constrained InfoMax, which allows the learning of a disentangled representation while maintaining optimal generative performance. The variational capacity of an autoencoder is defined and we investigate its role. We associate the two main properties of a Variational AutoEncoder (VAE), generation quality and disentangled representation, to two different information concepts, respectively Mutual Information and network capacity. We deduce that a small capacity autoencoder tends to learn a more robust and disentangled representation than a high capacity one. This observation is confirmed by the computational experiments.",
        "Introduction": "  INTRODUCTION A common assumption in machine learning is that any visible data x ∈ X is completely described by some generative factor o, living in a smaller hidden space O, i.e. x = g(o) with g a (possibly stochastic) generative function. The aim of unsupervised representation learning research is to find a representation z of the generative factor o living in a known space Z describing, as well as o, the visible data x. This is particularly relevant because the learnt small representation z is task agnostic and, in principle, can be used as input for networks performing different tasks, leading to faster and more robust learning (generalisation property), ( Rifai et al., 2011 ). Many models f φ : X → Z trying to learn such representations have been proposed ( Dinh et al., 2016 ;  Hinton et al., 2006 ;  Maddison et al., 2017 ;  Radford et al., 2015 ), but recently in order to solve this problem it was proposed to consider a dual problem: define a priori z and find a generator map g θ , such that for any z, g θ (z) is an element of X . In particular, two families of probabilistic generative models have become dominant: Variational AutoEncoder (VAE) ( Kingma & Welling, 2013 ;  Rezende et al., 2014 ) and Generative Adversarial Network (GAN) ( Goodfellow et al., 2014 ). The common idea of the two approaches is that a good generator p θ (x|z) is the one able to generate the data that is as close as possible to the visible one, i.e. that with respect a certain metric D, the distance between the marginal p θ (x) = E p(z) [p θ (x|z)] and the visible distribution p D (x) is minimal. In this manuscript we restrict our attention to the VAE model, since by its architecture, it is the only one where the learnt representation, possibly from different datasets, can be used as input for networks performing different tasks, ( Achille et al., 2018 ;  Ramapuram et al., 2017 ). Although VAE, by its training robustness and general good generative performance is the most popular model for representation learning, in particular cases it suffers from the uninformative representation issue: the representation is entangled and the generative model tends to be independent of z, i.e. p θ (x|z) ≈ p θ (x). As highlighted in the next section such behaviour is intrinsic in the variational loss, the Evidence Lower BOund (ELBO), encouraging a less informative representation. Following the direction suggested in ( Alemi et al., 2017 ;  Zhao et al., 2017 ) we describe the VAE from an information theoretic perspective. Such description lead us to two observations: the WAE ( Tolstikhin et al., 2017 ) and InfoVAE ( Zhao et al., 2017 ) models are actually maximising a lower bound of the mutual information associated to a generator p θ (x|z) belonging in a certain family Under review as a conference paper at ICLR 2020 P, the Variational InfoMax (VIM); and given that the capacity of the network is a function of the entropy of the prior p(z), the VIM is the variational expression of the Capacity-Constrained InfoMax (CCIM). Following the analogies between the CCIM and the Information Bottleneck (IB), the theoretical principle associated with the ELBO, we deduce that the representation quality is associated to the capacity term. The theoretical arguments are confirmed by the performed experiments where we observe that, dif- ferently from what was argued in previous works ( Higgins et al., 2017 ;  Burgess et al., 2018 ), it is possible to train a model that is able to learn good (able to generalise) representations while main- taining optimal generative performance. The main contributions of the paper are summarised in the following points: • derivation of a variational lower bound for the maximal mutual information of a generaive model belonging in a certain family, see equation 7; • definition and bounds estimation for the network capacity for a variational autoencoder, see equation 9; • association of the two main properties of VAE, generation quality and good representa- tion, to two different information concepts, respectively Mutual Information and network capacity; • proposal of a new learning principle for unsupervised models: the Capacity-Constrained InfoMax, see equation 10, that allows both to learn a good representation while maintaining optimal generative performance. The work is divided as follows: in the second section we describe briefly the VAE and its variants; in the third and fourth sections we describe the variational infomax method and related work. We conclude the paper with the experimental results and the final observations.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the emergence of localist codes in neural networks (NNs) when there is an invariant in the data-set. It hypothesizes that localist codes should emerge when there is an invariant in the data-set and examines this hypothesis using simple networks with binary vectors as inputs. Results indicate that localist codes do emerge when there is an invariant in the data. This paper provides insight into the representation of data in the 'fc' layers of deep-NNs and contributes to the understanding of how NNs work and what data is used to make their decisions.",
        "Abstract": "Localist coding schemes are more easily interpretable than the distributed schemes but generally believed to be biologically implausible. Recent results have found highly selective units and object detectors in NNs that are indicative of local codes (LCs). Here we undertake a constructionist study on feed-forward NNs and find LCs emerging in response to invariant features, and this finding is robust until the invariant feature is perturbed by 40%. Decreasing the number of input data, increasing the relative weight of the invariant features and large values of dropout all increase the number of LCs. Longer training times increase the number of LCs and the turning point of the LC-epoch curve correlates well with the point at which NNs reach 90-100% on both test and training accuracy. Pseudo-deep networks (2 hidden layers) which have many LCs lose them when common aspects of deep-NN research are applied (large training data, ReLU activations, early stopping on training accuracy and softmax), suggesting that LCs may not be found in deep-NNs. Switching to more biologically feasible constraints (sigmoidal activation functions, longer training times, dropout, activation noise) increases the number of LCs. If LCs are not found in the feed-forward classification layers of modern deep-CNNs these data suggest this could either be caused by a lack of (moderately) invariant features being passed to the fully connected layers or due to the choice of training conditions and architecture. Should the interpretability and resilience to noise of LCs be required, this work suggests how to tune a NN so they emerge. ",
        "Introduction": "  INTRODUCTION With neural networks (NNs) being widely deployed in various tasks it is essential to understand how they work and what data is used to make their decisions. NNs used to be viewed as 'black boxes', but recent results ( Nguyen et al., 2016 ) have started to open that box. NNs came from the field of psychology as simple bio-inspired models, it has been debated whether information is represented in the brain in a distributed manner (from parallel distributed processing, PDP) or via a localist coding scheme. Although the distributed approach was the most popular, there are some results in neuroscience ( Quiroga et al., 2005 ) and psychology ( McClelland & Rumelhart, 1981 ) that are commensurate with a localist coding scheme, including a report of LCs in RNNs ( Bowers et al., 2014 ). Recently, there has been an explosion of interest in NNs, especially deep- NNs, as these algorithms are now commercially relevant, and this increase in their accuracy has been credited to sources of vastly more labelled data and novel training techniques like dropout ( Srivastava et al., 2014 ). Many newer researchers in NNs were perhaps unaware of the distributed- localist coding debate within psychology, and thus looked for localist-like codes in their NNs, and found indicative (of LC coding scheme) evidence of detectors for objects ( Zhou et al., 2018 ;  2015 ), concepts ( Karpathy et al., 2016 ;  Lakretz et al., 2019 ), features ( Nguyen et al., 2019 ; Erhan et al., 2009), textures ( Olah et al., 2017 ), single directions ( Morcos et al., 2018 ) etc., see ( Bowers, 2017 ) for a review. With faster and larger computers, it is possible, even with the increase in input data size, for deep- NNs to 'memorise' the data-set (an extreme form of overfitting): a process where the NN has simply learned a mapping between input and output vectors, as opposed to learning a rule which will allow it to generalise to unseen data that follows the underlying rule. Generalisation performance is often improved if NN training is stopped early, often when a validation set loss (val loss) stops improving, Under review as a conference paper at ICLR 2020 as the NN is prevented from further minimising its loss function by memorising the input (training) data. Single directions ( Morcos et al., 2018 ) have been implicated in memorization of the data-set. Localist codes (coding for a class A) are defined as units which are activated at a high (low) level for all members(that the NN gets correct) and low (high) level for all members of the other classes (class ¬A), i.e. the set of activations for class A is disjoint from the activations for class ¬A (see figure 8 in the appendix), and these codes are very strict measure of selectivity. As such, LCs are very easy to interpret, and the presence of them in NNs would make it easy to understand how the NN is working. This paper takes no position on whether or not localist codes exist in the brain or in deep-NNs. instead we take the constructionist science approach of asking when would we expect LCs to appear, and what aspects of the system, data-set and training conditions favour or disfavour their emergence. As NNs are considered (simplified) models for the brain, we can also take into account biological plausibility. We hypothesized that LCs should emerge when there was an invariant in the data-set. As deep-NNs take a long time to train, it is hard to get representative statistics, so we look at very simple networks (shallow: 3-layer and pseudo-deep: 4-layer) where it is possible to do hundreds of repeats and thus get resilient trends. The main insight of this work is that LCs do emerge when there is an invariant in the data. To set up a system with such a 'short-cut' we use a simple binary vectors as inputs, which are built from prototypes, such that there are 1/10 input bits that are always 1 for each class and these are the invariant bits, the 0s of each prototype are then filled in with a random mix of 1 and 0 of a known weight, see  figure 1 , thus, a given bit is always on for a given class, and maybe on or of for other classes. Note also, that in this set up, if the proportional weight of the prototype exceeds that of the random vector, then vectors belonging to the same class are 'closer' 1 to each other than those of separate classes, i.e. there is a larger between-class variance than within-class variance. The prototypes are also perturbed to increase the variance of the 'invariant' bits. If one views a deep conv- NN as a feature extraction machine (lower and convolutional layers) with a feature classification NN on top (the higher fully connected layers), then it is reasonable to suppose that a given class is likely to share features at the top convolutional layer, which would result in the activation vectors at that layer having a higher between group variance than within group, or possibly even invariant features for a class (perhaps object detectors), and so these experiments could give insight into the representation of data in the 'fc' layers of deep-NNs.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the impact of initialization hyper-parameters and activation functions on the training dynamics of deep neural networks. It is shown that an initialization known as the Edge of Chaos and a class of smooth activation functions improve the training dynamics compared to ReLU-like activation functions. The results are illustrated through simulations and the proofs are detailed in the Supplementary Material.",
        "Abstract": "Recent work by Jacot et al. (2018) has showed that training a neural network of any kind with gradient descent in parameter space is equivalent to kernel gradient descent in function space with respect to the Neural Tangent Kernel (NTK). Lee et al. (2019) built on this result to show that the output of a neural network trained using full batch gradient descent can be approximated by a linear model for wide networks. In parallel, a recent line of studies ( Schoenhols et al. (2017), Hayou et al. (2019)) suggested that a special initialization known as the Edge of Chaos leads to good performance. In this paper, we bridge the gap between this two concepts and show the impact of the initialization and the activation function on the NTK as the network depth becomes large. We provide experiments illustrating our theoretical results.",
        "Introduction": "  INTRODUCTION Deep neural networks have achieved state-of-the-art results on numerous tasks; see, e.g.,  Nguyen & Hein (2018) ,  Du et al. (2018b) ,  Zhang et al. (2017) . Although the loss function is not convex, Gradient Descent (GD) methods are often used successfully to learn these models. It has been actually recently shown that for certain overparameterized deep ReLU networks, GD converges to global minima (( Du et al., 2018a )). Similar results have been obtained for Stochastic Gradient Descent (SGD) (( Zou et al., 2018 )). The training dynamics of wide neural networks with GD is directly linked to kernel methods. Indeed,  Jacot et al. (2018)  showed that training a neural network with full batch GD in parameter space is equivalent to a functional GD i.e. a GD in a functional space with respect to a kernel called Neural Tangent Kernel (NTK).  Du et al. (2019)  used a similar approach to prove that full batch GD converges to global minima for shallow neural networks and  Karakida et al. (2018)  linked the Fisher Information Matrix to the NTK and studied its spectral distribution for infinite width networks. The infinite width limit for different architectures was studied by  Yang (2019)  who introduced a tensor formalism that can express most of the computations in neural networks.  Lee et al. (2019)  studied a linear approximation of the full batch GD dynamics based on the NTK and gave an method to approximate the NTK for different architectures. Finally,  Arora et al. (2019)  gives an efficient algorithm to compute exactly the NTK for convolutional architectures (Convolutional NTK or CNTK). In all of these papers, authors studied only the effect of infinite width on the NTK. The aim of this paper is to tackle the infinite depth limit. In parallel, the impact of the initialization and activation function on the performance of wide deep neural networks has been studied in  Hayou et al. (2019) ,  Lee et al. (2018) ,  Schoenholz et al. (2017) ,  Yang & Schoenholz (2017) . These works analyze the forward/backward propagation of some quantities through the network at the initial step as a function of the initial parameters and the activation function. They propose a set of parameters and activation functions so as to ensure a deep propagation of the information at initialization. While experimental results in these papers suggest that such selection also leads to overall better training procedures (i.e. beyond the initialization step), it remains unexplained why this is the case. In this paper, we link the initialization hyper-parameters and the activation function to the behaviour of the NTK which controls the training of DNNs, this could potentially explain the good performance. We provide a comprehensive study of the impact of the initialization and the activation function on the NTK and therefore on the resulting training dynamics for wide and deep networks. In particular, we show that an initialization known as the Edge Under review as a conference paper at ICLR 2020 of Chaos ( Yang & Schoenholz, 2017 ) leads to better training dynamics and that a class of smooth activation functions discussed in ( Hayou et al., 2019 ) also improves the training dynamics compared to ReLU-like activation functions (see also  Clevert et al. (2016) ). We illustrate these theoretical results through simulations. All the proofs are detailed in the Supplementary Material which also includes additional theoretical and experimental results.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces novel visual attention mechanisms for image captioning that are endowed with the capability of selecting only the relevant features of the image. We propose replacing softmax with sparsemax and introduce a new attention mechanism, Total-Variation Sparse Attention (TVMAX), which encourages sparse attention over contiguous 2D regions. We evaluate the various attention mechanisms through empirical and qualitative comparison, and a human evaluation experiment. Results show that our proposed attention mechanisms improve the quality of the generated captions and increase interpretability.",
        "Abstract": "Visual attention mechanisms have been widely used in image captioning models. In this paper, to better link the image structure with the generated text, we replace the traditional softmax attention mechanism by two alternative sparsity-promoting transformations: sparsemax and Total-Variation Sparse Attention (TVmax). With sparsemax, we obtain sparse attention weights, selecting relevant features.  In order to promote sparsity and encourage fusing of the related adjacent spatial locations, we propose TVmax.  By selecting relevant groups of features, the TVmax transformation improves interpretability. We present results in the Microsoft COCO and Flickr30k datasets, obtaining gains in comparison to softmax.  TVmax outperforms the other compared attention mechanisms in terms of human-rated caption quality and attention relevance.",
        "Introduction": "  INTRODUCTION The goal of image captioning is to generate a fluent textual caption that describes a given image ( Farhadi et al., 2010 ;  Kulkarni et al., 2011 ;  Vinyals et al., 2015 ;  Xu et al., 2015 ). Image captioning is a multimodal task: it combines text generation with the detection and identification of objects in the image, along with their relations. While neural encoder-decoder models have achieved impressive performance in many text generation tasks ( Bahdanau et al., 2015 ;  Vaswani et al., 2017 ;  Chorowski et al., 2015 ;  Chopra et al., 2016 ), it is appealing to design image captioning models where structural bias can be injected to improve their adequacy (preservation of the image's information), therefore strengthening the link between their language and vision components. State-of-the-art approaches for image captioning ( Liu et al., 2018a ; b ;  Anderson et al., 2018 ;  Lu et al., 2018 ) are based on encoder-decoders with visual attention. These models pay attention either to the features generated by convolutional neural networks (CNNs) pretrained on image recognition datasets, or to detected bounding boxes. In this paper, we focus on the former category: visual at- tention over features generated by a CNN. Without explicit object detection, it is up to the attention mechanism to identify relevant image regions, in an unsupervised manner. A key component of attention mechanisms is the transformation that maps scores into probabilities, with softmax being the standard choice ( Bahdanau et al., 2015 ). However, softmax is strictly dense, i.e., it devotes some attention probability mass to every region of the image. Not only is this wasteful, it also leads to \"lack of focus\": for complex images with many objects, this may lead to vague captions with substantial repetitions.  Figure 1  presents an example in which this is visible: in the caption generated using softmax (top), the model attends to the whole image at every time step, leading to a repetition of \"bowl of fruit.\" This undesirable behaviour is eliminated by using our alternative solutions: sparsemax (middle) and the newly proposed TVMAX (bottom). In this work, we introduce novel visual attention mechanisms by endowing them with a new capabil- ity: that of selecting only the relevant features of the image. To this end, we first propose replacing softmax with sparsemax ( Martins & Astudillo, 2016 ). While sparsemax has been previously used in NLP for attention mechanisms over words, it has never been applied to computer vision to attend over image regions. With sparsemax, the attention weights obtained are sparse, leading to the selec- tion (non-zero attention) of only a few relevant features. Second, to further encourage the weights of related adjacent spatial locations to be the same (e.g., parts of an object), we introduce a new at- tention mechanism: Total-Variation Sparse Attention (which we dub TVMAX), inspired by prior work in structured sparsity ( Tibshirani et al., 2005 ;  Bach et al., 2012 ). With TVMAX, sparsity is allied to the ability of selecting compact regions. According to our human evaluation experiments, Under review as a conference paper at ICLR 2020 this leads to better interpretability, since the model's behaviour is better understood by looking at the selected image regions when a particular word is generated. It also leads to a better selection of the relevant features, and consequently to the improvement of the generated captions. This paper introduces three main contributions: • We propose a novel visual attention mechanism using sparse attention, based on sparse- max ( Martins & Astudillo, 2016 ), that improves the quality of the generated captions and increases interpretability. • We introduce a new attention mechanism, TVMAX, that encourages sparse attention over contiguous 2D regions, giving the model the capability of selecting compact objects. We show that TVmax can be evaluated by composing a proximal operator with a sparsemax projection, and we provide a closed-form expression for its Jacobian. This leads to an efficient implementation of its forward and backward pass. • We perform an empirical and qualitative comparison of the various attention mechanisms considered. We also carry out a human evaluation experiment, taking into account the generated captions as well as the perceived relevance of the selected regions.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a framework for learning a generative model that can generate samples which approximate complex data while also giving rise to a disentangled and interpretable representation. The framework combines Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) to achieve the best of both worlds; high quality generated data and a disentangled representation. The framework is general and works in both the supervised and unsupervised settings. Additionally, the framework can be combined with common methods for discovering disentangled representations such as β-VAE to extract control variables and treat them as labels to generate images that do not compromise on generative quality.",
        "Abstract": "Learning disentangled representations of  data is one of the central themes in unsupervised learning in general and generative modelling in particular.  In this work,  we tackle a slightly more intricate scenario where the observations are generated from a conditional distribution of some known control variate and some latent noise variate.  To this end, we present a hierarchical model and a training method (CZ-GEM) that leverages some of the recent developments in likelihood-based and likelihood-free generative models.  We show that by formulation, CZ-GEM introduces the right inductive biases that ensure the disentanglement of the control from the noise variables, while also keeping the components of the control variate disentangled. This is achieved without compromising on the quality of the generated samples. Our approach is simple, general, and can be applied both in supervised and unsupervised settings.",
        "Introduction": "  INTRODUCTION Consider the following scenario: a hunter-gatherer walking in the African Savannah some 50,000 years ago notices a lioness sprinting out of the bush towards her. In a split second, billions of photons reaching her retinas carrying an enormous amount of information: the shade of the lioness' fur, the angle of its tail, the appearance of every bush in her field of view, the mountains in the background and the clouds in the sky. Yet at this point there is a very small number of attributes which are of importance: the type of the charging animal, its approximate velocity and its location. The rest are just details. The significance of the concept that the world, despite its complexity, can be described by a few explanatory factors of variation, while ignoring the small details, cannot be overestimated. In machine learning there is a large body of work aiming to extract low-dimensional, interpretable representations of complex, often visual, data. Interestingly, many of the works in this area are associated with developing generative models. The intuition is that if a model can generate a good approximation of the data then it must have learned something about its underlying representation. This representation can then be extracted either by directly inverting the generative process (Srivastava et al., 2019b) or by extracting intermediate representations of the model itself (Kingma & Welling, 2014;  Higgins et al., 2017 ). Clearly, just learning a representation, even if it is low-dimensional, is not enough. The reason is that while there could be many ways to compress the information captured in the data, allowing good enough approximations, there is no reason to a priori assume that such a representation is interpretable and disentangled in the sense that by manipulating certain dimensions of the representation one can control attributes of choice, say the pose of a face, while keeping other attributes unchanged. The large body of work on learning disentangled representations tackles this problem in several settings; fully supervised, weakly supervised and unsupervised, depending on the available data (Tran et al., 2018; Reed et al., 2014;  Jha et al., 2018 ;  Mathieu et al., 2016 ;  Higgins et al., 2017 ;  Chen et al., 2018 ;  Kim & Mnih, 2018 ;  Chen et al., 2016 ;  Nguyen-Phuoc et al., 2019 ;  Narayanaswamy et al., 2017 ). Ideally, we would like to come up with an unsupervised generative model that can generate samples which approximate the data to a high level of accuracy while also giving rise to a disentangled and interpretable representation. In the last decade two main approaches have captured most of the attention; Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs). In their original versions, both GANs ( Goodfellow et al., 2014 ) and VAEs (Kingma & Welling, 2014) were trained in an unsupervised manner and gave rise to entangled representations. Over the years, many methods to improve the quality of the generated data as well as the disentanglement of the representations have been suggested ( Brock et al., 2018 ;  Kingma & Dhariwal, 2018 ;  Nguyen-Phuoc et al., 2019 ;  Jeon et al., 2018 ). By and large, GANs are better than VAEs in the quality of the generated data while VAEs learn better disentangled representations, in particular in the unsupervised setting. In this paper, we present a framework for disentangling a small number of control variables from the rest of the latent space which accounts for all the additional details, while maintaining a high quality of the generated data. We do that by combining VAE and GAN approaches thus enjoying the best of both worlds. The framework is general and works in both the supervised and unsupervised settings. Let us start with the supervised case. We are provided with paired examples (x, c) where x is the observation and c is a control variate. Crucially, there exists a one-to-many map from c to the space of observations, and there are other unobserved attributes z (or noise) that together completely define x. For instance, if x were an image of a single object, c controls the orientation of the object relative to the camera and z could represent object identity, texture or background. Our goal is to learn a generative model p θ (x|c, z) that fulfills two criteria: 1. z p θ (x|c, z)p(c)p(z)dz matches the joint distribution z p(x|c, z)p(c)p(z)dz: If we were learning models of images, we would like the generated images to look realistic and match the true conditional distribution p(x|c). 2. The posterior is factorized p(c, z|x; θ) = p(c|x; θ)p(z|x; θ): We would like the control variate to be disentangled from the noise. For example, changing the orientation of the object should not change the identity under our model. This problem setup can occur under many situations such as learning approximate models of simula- tors, 3D reconstructions, speaker recognition (from speech), and even real-world data processing in the human brain as in the hunter-gatherer example above. We argue that a naive implementation of a graphical model as shown in  Figure 2  (left), e.g. by a conditional GAN ( Mirza & Osindero, 2014 ), does not satisfy Criterion 2. In this model, when we condition on x, due to d-separation, c and z could become dependent, unless additional constraints are posed on the model. This effect is demonstrated in Figure 1(a). To overcome this we split the generative process into two stages by replacing C with a subgraph (C → Y ) as shown in  Figure 2  (center). First, we generate a crude approximation y of the data which only takes c into account. The result is a blurry average of the data points conditioned on c, see  Figure 2  (right). We then feed this crude approximation into a GAN-based generative model which adds the rest of the details conditioned on z. We call this framework CZ-GEM. The conditioning on z in the second stage must be done carefully to make sure that it does not get entangled with y. To that end we rely on architectural choices and normalization techniques from the style transfer literature ( Huang & Belongie, 2017 ) 2 . The result is a model which generates images of high quality while disentangling c and z as can be clearly seen in Figure 1(b). Additionally, in the unsupervised setting, when the labels c are not available, (C → Y ) can be realized by β-VAE, a regularized version of VAE which has been shown to learn a disentangled representation of its latent variables ( Higgins et al., 2017 ;  Burgess et al., 2018 ). In Section 3 we provide implementation details for both the supervised and unsupervised versions. We summarize our two main contributions: 1. Architectural biases: We break down the architecture to model an intermediate representa- tion that lends itself to interpretability and disentanglement, and then (carefully) use a GAN based approach to add the rest of the details, thus enjoying a superior image generation quality compared to VAEs. 2. Unsupervised discovery: We show that our model can be combined easily with common methods for discovering disentangled representations such as β-VAE to extract c and treat them as labels to generate images that do not compromise on generative quality.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to combining classical approaches to artificial intelligence with modern machine learning techniques to build a \"scientist agent\" capable of making and testing hypotheses about its environment. We formulate hypothesis verification as joint learning of an action policy that generates observations relevant to verification of hypotheses and a prediction function which uses the observations to predict whether the hypothesis is true or false. We show that our approach allows combining the explicit hypothesis testing of classical AI with the use of scalable statistical ML.",
        "Abstract": "In this paper, we formulate hypothesis verification as a reinforcement learning problem. Specifically, we aim to build an agent that, given a hypothesis about the dynamics of the world can take actions to generate observations which can help predict whether the hypothesis is true or false. Our first observation is that agents trained end-to-end with the reward fail to learn to solve this problem. In order to train the agents, we exploit the underlying structure in the majority of hypotheses -- they can be formulated as triplets (pre-condition, action sequence,  post-condition). Once the agents have been pretrained to verify hypotheses with this structure, they can be fine-tuned to verify more general hypotheses.  Our work takes a step towards a ``scientist agent'' that develops an understanding of the world by generating and testing hypotheses about its environment.",
        "Introduction": "  INTRODUCTION In fields of natural sciences (physics, biology etc.), we follow scientific methods - building and testing hypotheses to develop an understanding of the world. Many classical approaches to artificial intelligence attempted to mirror this process ( Brachman & Levesque, 2004 ;  Davis & Marcus, 2015 ), building (symbolic) knowledge representations about the world that allow the making and testing of hypotheses. However, this process bears little resemblance to the way in which current machine learning (ML) systems learn. Both traditional IID and interactive learning settings use a single user- specified objective function that codifies a high-level task, but places no constraint on the underlying knowledge formed about the environment. In standard ML approaches, particularly those based on deep learning, any representation of the world is embedded in the weights of the model, and there is no explicit mechanism for formulating or testing hypotheses. In this paper we take a modest step towards combining the classical approaches with the successes of modern ML to build a \"scientist agent\". When fully realized, such agent would be able to both make and test hypotheses about its environment. In this work we focus on the latter. Unlike standard supervised problems, there is no standard formulation, and no benchmarks or environments for hypothesis verification in interactive environments. A key contribution of our paper is framing the problem of hypothesis verification and presenting a feasible formulation for it. Specifically, we build an agent that, given a hypothesis about the dynamics of the world, can take actions to verify if the hypothesis is true or not. We formulate hypothesis verification as joint learning of: (a) an action policy that generates observations which are relevant to verification of hypotheses and; (b) a prediction function which uses the observations to predict whether the hypothesis is true or false. We first show that even in simple environments, agents trained end-to-end using deep reinforcement learning methods cannot learn policies that can generate observations to verify the hypothesis. To remedy this, we exploit the underlying structure of hypotheses - they can often be formulated as a triplet of a pre-condition, an action sequence, and a post-condition that is causally related to the pre-condition and actions. Using this common structure, we are able to seed our action policy to learn behaviors which alter the truth of the pre-condition and post-condition. We show that this policy can be fine-tuned to learn how to verify more general hypotheses that do not necessarily fit into the triplet structure. Thus our approach allows combining the explicit hypothesis testing of classical AI with the use of scalable statistical ML.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a straightforward and elegant method for expressing the computation of an arbitrary neuron's activity to a single linear projection in the input space. This projection consists of a switched weight vector and a switched bias that easily lend themselves to sensitivity analysis and decomposition of the internal computation. Additionally, a new approach for interpretability analysis, called inactive state sensitivity (Insens), is introduced which uses switched linear projections to aggregate the contribution of patterns in the input that deactivate neurons in the network. The proposed methods are demonstrated on several networks and image-based datasets, and are only constrained by the requirement that the network must use ReLU activation functions for its hidden neurons.",
        "Abstract": "We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks",
        "Introduction": "  INTRODUCTION It is notoriously hard to interpret how deep networks accomplish the tasks for which they are trained. At the same time, due to the pervasiveness of deep learning in numerous aspects of computing, it is increasingly important to gain understanding of how they work. There are risks associated with the possibility that a neural network might not be \"looking\" at the \"right\" patterns ( Nguyen et al.; Geirhos et al., 2019 ), as well as opportunities to learn from the network's capable of better than human performance ( Sadler & Regan, 2019 ). Hence, there is ongoing effort to improve the interpretation and interpretability of the internal representation of neural networks. What makes this interpretation of the inside of a neural network hard is the high dimensionality and the distributed nature of its internal computation. Aside from the first hidden layer, neurons operate in an abstract high-dimensional space. If that was not hard enough, the analysis of individual components of the network (such as activity of individual neurons) is rarely instructive, since it is the intricate relationships and interplay of those components that contain the \"secret sauce\". The two broad approaches to dealing with this complexity is to either use simpler interpretable models to approximate what a neural network does, or to trace back the elements of the computation into the input space in order to make the internal dynamics relatable to the input. In the latter approach we are typically interested in neurons' sensitivity - how the changes in network input affect their output, and decomposition - how different components of the input contribute to the output. In this paper we propose a straightforward and elegant method for expressing the computation of an arbitrary neuron's activity to a single linear projection in the input space. This projection consists of a switched weight vector and a switched bias that easily lend themselves to sensitivity analysis (analogous to gradient-based sensitivity) and decomposition of the internal computation. We also in- troduce a new approach for interpretability analysis, called inactive state sensitivity (Insens), which uses switched linear projections to aggregate the contribution of patterns in the input that deactivate neurons in the network. We demonstrate on several networks and image-based datasets that Insens provides a comprehensive picture of a deep network's internal computation. The only constraint for the proposed methods is that the network must use ReLU activation functions for its hidden neurons.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the role of the hippocampus in encoding and learning multiple tasks, such as allocentric and egocentric spatial tasks, in a continual reinforcement learning setting. Neural data from the hippocampus CA1 population-level activity is analysed to extract meaningful latent features, which are found to be directly related to how the neural population encodes multiple tasks and reward signals. Standard reinforcement learning algorithms, such as temporal difference learning and Q-learning theory, are contrasted with the experimental observations. Results show that deep Q networks can achieve an average performance similar to animals, but fails to achieve the same relearning speed.",
        "Abstract": "The hippocampus has long been associated with spatial memory and goal-directed\nspatial navigation. However, the region’s independent role in continual learning of\nnavigational strategies has seldom been investigated. Here we analyse populationlevel\nactivity of hippocampal CA1 neurons in the context of continual learning of\ntwo different spatial navigation strategies. Demixed Principal Component Analysis\n(dPCA) is applied on neuronal recordings from 612 hippocampal CA1 neurons\nof rodents learning to perform allocentric and egocentric spatial tasks. The components\nuncovered using dPCA from the firing activity reveal that hippocampal\nneurons encode relevant task variables such decisions, navigational strategies and\nreward location. We compare this hippocampal features with standard reinforcement\nlearning algorithms, highlighting similarities and differences. Finally, we\ndemonstrate that a standard deep reinforcement learning model achieves similar\naverage performance when compared to animal learning, but fails to mimic animals\nduring task switching. Overall, our results gives insights into how the hippocampus\nsolves reinforced spatial continual learning, and puts forward a framework\nto explicitly compare biological and machine learning during spatial continual\nlearning.",
        "Introduction": "  INTRODUCTION The hippocampus has been long known to play a crucial role in spatial navigation ( O'keefe & Nadel (1978) ;  Tolman (1948) ). However, there is still an ongoing debate in the field on how exactly the hippocampus is involved in the encoding and learning of multiple tasks, such as allocentric and egocentric spatial tasks ( Ekstrom et al. (2014) ;  Feigenbaum & Rolls (1991) ;  Fidalgo & Martin (2016) ). In particular, it is not clear how the hippocampal neural population may encode such tasks and how these representations relate to the ability of animals for continual spatial learning. In reinforcement learning (RL), despite the growing interest in the topic, how to solve the continual learning problem is still unresolved ( Parisi et al. (2019) ). Previous theoretical work has explored how animals may rely on the hippocampus to learn to explore a given environment based by combining RL algorithms and spatial coding ( Foster et al. (2000) ). More recently, there has been growing interest in understanding how to solve continual learning in deep neural networks ( Zenke et al. (2017) ;  Kirkpatrick et al. (2017) ). But how both biological and artificial neural networks encode and solve spatial tasks in a continual reinforcement learning setting is not known. Here, we first focus on analysing neural data from hippocampus CA1 population-level activity to extract meaningful latent features. We find representations that are directly related to how the neural population encodes not only multiple tasks, but also reward signals, such as reward prediction errors. Next, we contrast these experimental observations with standard reinforcement learning algorithms, namely temporal difference learning and Q-learning theory. Finally, we show that deep Q networks can achieve an average performance similar to animals, but fails to achieve the same relearning speed.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Smart Ternary Quantization (STQ), a novel approach to reduce the memory footprint and computational cost of deep neural networks (DNNs) by allowing for a mix of 1-bit and 2-bit precision weights during training. STQ automatically quantizes weights into binary or ternary depending on a trainable control parameter, leading to mixed bit precision models that outperform ternary networks in terms of accuracy and memory consumption. This is the first attempt to design a single training algorithm for low-bit mixed precision training.",
        "Abstract": "Neural network models are resource hungry. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this resource requirements. Ternary quantization provides a more flexible model and often beats binary quantization in terms of accuracy, but doubles memory and increases computation cost. Mixed quantization depth models, on another hand, allows a trade-off between accuracy and memory footprint. In such models, quantization depth is often chosen manually (which is a tiring task), or is tuned using a separate optimization routine (which requires training a quantized network multiple times). Here, we propose Smart Ternary Quantization (STQ) in which we modify the quantization depth directly through an adaptive regularization function, so that we train a model only once. This method jumps between binary and ternary quantization while training. We show its application  on image classification.",
        "Introduction": "  INTRODUCTION Deep Neural Networks (DNN) models have achieved tremendous attraction because of their success on a wide variety of tasks including computer vision, automatic speech recognition, natural language processing, and reinforcement learning ( Goodfellow et al., 2016 ). More specifically, in computer vision DNN have led to a series of breakthrough for image classification ( Krizhevsky et al., 2017 ), ( Simonyan & Zisserman, 2014 ), ( Szegedy et al., 2015 ), and object detection ( Redmon et al., 2015 ), ( Liu et al., 2015 ), (Ren et al., 2015). DNN models are computationally intensive and require large memory to store the model parameters. Computation and storage resource requirement becomes an impediment to deploy such models in many edge devices due to lack of memory, computation power, battery, etc. This motivated the researchers to develop compression techniques to reduce the cost for such models. Recently, several techniques have been introduced in the literature to solve the storage and compu- tational limitations of edge devices. Among them, quantization methods focus on representing the weights of a neural network in lower precision than the usual 32-bits float representation, saving on the memory footprint of the model. Binary quantization ( Courbariaux et al., 2015 ), (Hubara et al., 2016), ( Rastegari et al., 2016 ), ( Zhou et al., 2016 ), ( Lin et al., 2017 ) represent weights with 1 bit precision and ternary quantization ( Lin et al., 2015 ), ( Li & Liu, 2016 ), ( Zhu et al., 2016 ) with 2 bits precision. While the latter frameworks lead to significant memory reduction compared to their full precision counterpart, they are constrained to quantize the model with 1 bit or 2 bits, on demand. We relax this constraint, and present Smart Ternary Quantization (STQ) that allows mixing 1 bit and 2 bits layers while training the network. Consequently, this approach automatically quantizes weights into binary or ternary depending upon a trainable control parameter. We show that this ap- proach leads to mixed bit precision models that beats ternary networks both in terms of accuracy and memory consumption. Here we only focus on quantizing layers because it is easier to implement layer-wise quantization at inference time after training. However, this method can be adapted for mixed precision training of sub-network, block, filter, or weight easily. To the best of our knowledge this is the first attempt to design a single training algorithm for low-bit mixed precision training.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Variational Autoencoders (VAEs) are a popular approach for modeling complex high-dimensional distributions, but tend to generate unrealistic objects due to the properties of Maximum Likelihood Estimation (MLE). Generative Adversarial Networks (GANs) are known for their ability to sample realistic objects, but suffer from mode-seeking behavior. This paper proposes a novel approach to train VAEs in an adversarial manner, using an adversarially trained discriminator to estimate the implicit likelihood in the loss function. The proposed Implicit λ-Jeffreys Autoencoder (λ-IJAE) minimizes the λ-Jeffreys divergence and is evaluated on CIFAR10 and TinyImagenet datasets. Results show a state-of-the-art trade-off between generation and reconstruction quality, and a default choice for λ is derived that establishes a reasonable compromise between mode-seeking and mass-covering behaviour.",
        "Abstract": "We propose a new form of an autoencoding model which incorporates the best properties of variational autoencoders (VAE) and generative adversarial networks (GAN). It is known that GAN can produce very realistic samples while VAE does not suffer from mode collapsing problem. Our model optimizes λ-Jeffreys divergence between the model distribution and the true data distribution. We show that it takes the best properties of VAE and GAN objectives. It consists of two parts. One of these parts can be optimized by using the standard adversarial training, and the second one is the very objective of the VAE model. However, the straightforward way of substituting the VAE loss does not work well if we use an explicit likelihood such as Gaussian or Laplace which have limited flexibility in high dimensions and are unnatural for modelling images in the space of pixels. To tackle this problem we propose a novel approach to train the VAE model with an implicit likelihood by an adversarially trained discriminator. In an extensive set of experiments on CIFAR-10 and TinyImagent datasets, we show that our model achieves the state-of-the-art generation and reconstruction quality and demonstrate how we can balance between mode-seeking and mode-covering behaviour of our model by adjusting the weight λ in our objective. ",
        "Introduction": "  INTRODUCTION Variational autoencoder (VAE) ( Kingma et al., 2014 ;  Rezende et al., 2014 ;  Titsias & Lázaro-Gredilla, 2014 ) is one of the most popular approaches for modeling complex high-dimensional distributions. It has been applied successfully to many practical problems. It has several nice properties such as learning low-dimensional representations for the objects and ability to conditional generation. Due to an explicit reconstruction term in its objective, one may ensure that VAE can generate all objects from the training set. These advantages, however, come at a price. It is a known fact that VAE tends to generate unrealistic objects, e.g., blurred images. Such behaviour can be explained by the properties of a maximum likelihood estimation (MLE) which is used to fit a restricted VAE model p θ (x) in data that comes from a complex distribution p * (x). We can equivalently reformulate this MLE objective as a minimization of the forward KL divergence D KL (p * (x) p θ (x)) which does not explicitly penalize the model p θ (x) for generating unrealistic objects ( Arjovsky & Bottou, 2017 ). As a result, many regions with the low value p * (x) may have a high value of p θ (x) in the case when the model p θ (x) has the limited capacity (see Figure 1a)). Another popular generative model is a generative adversarial network (GAN) ( Goodfellow et al., 2014 ) which is known for its ability to sample realistic objects. However, it suffers from an inability to cover the whole distribution p * (x) that leads to mode-seeking behavior, also known as mode-collapse ( Salimans et al., 2016 ;  Metz et al., 2016 ;  Goodfellow, 2016 ). The main reason of such behaviour is the properties of the reverse KL divergence D KL (p θ (x) p * (x)) (or the Jensen-Shanon divergence JSD(p θ (x) p * (x))) that is minimized during GAN training. These divergences D KL (p θ (x) p * (x)) and JSD(p θ (x) p * (x)) do not penalize the model p θ (x) for ignoring some high value regions of p * (x) ( Arjovsky & Bottou, 2017 ). As a result, the most probability mass of the restricted model p θ (x) can be concentrated in a small number of modes of p * (x) (see Figure 1b), 1c)). However, the straightforward way of substituting each KL term with GAN and VAE losses does not work well in practice if we use an explicit likelihood for object reconstruction in VAE objective. Such simple distributions as Gaussian or Laplace that are usually used in VAE have limited flexibility and are unnatural for modelling images in the space of pixels. To tackle this problem we propose a novel approach to train the VAE model in an adversarial manner. We show how we can estimate the implicit likelihood in our loss function by an adversarially trained discriminator. We theoretically analyze the introduced loss function and show that under assumptions of optimal discriminators, our model minimizes the λ-Jeffreys divergence J λ (p θ (x) p * (x)) and we call our method as Implicit λ-Jeffreys Autoencoder (λ-IJAE). In an extensive set of experiments, we evaluate the generation and reconstruction ability of our model on CIFAR10 ( Krizhevsky et al., 2009 ) and TinyImagenet datasets. It shows the state-of-the-art trade-off between generation and reconstruction quality. We demonstrate how we can balance between the ability of generating realistic images and the reconstruction ability by changing the weight λ in our objective. Based on our experimental study we derive a default choice for λ that establishes a reasonable compromise between mode-seeking and mass-covering behaviour of our model and this choice is consistent over these two datasets.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of the Transformer architecture in reinforcement learning (RL) settings. It is found that the canonical Transformer is difficult to optimize, often resulting in performance comparable to a random policy. To address this, a novel gated architecture, the Gated Transformer-XL (GTrXL), is proposed. It is shown to learn faster and more reliably, and to exhibit significantly better final performance than the canonical Transformer. The GTrXL is further demonstrated to achieve state-of-the-art results on the multitask DMLab-30 suite, to surpass LSTMs significantly on memory-based DMLab-30 levels, and to significantly outperform LSTMs on memory-based continuous control and navigation environments. Ablations are performed to test the final performance of the various components as well as the GTrXL's robustness to seed and hyperparameter sensitivity compared to LSTMs and the canonical Transformer. Results show a consistent superior performance while matching the stability of LSTMs, providing evidence that the GTrXL architecture can function as a drop-in replacement to the LSTM networks ubiquitously used in RL.",
        "Abstract": "Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially-observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially-observable environments.  ",
        "Introduction": "  INTRODUCTION It has been argued that self-attention architectures (Vaswani et al., 2017) deal better with longer tem- poral horizons than recurrent neural networks (RNNs): by construction, they avoid compressing the whole past into a fixed-size hidden state and they do not suffer from vanishing or exploding gradients in the same way as RNNs. Recent work has empirically validated these claims, demonstrating that self-attention architectures can provide significant gains in performance over the more traditional recurrent architectures such as the LSTM (Dai et al., 2019; Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019). In particular, the Transformer architecture (Vaswani et al., 2017) has had break- through success in a wide variety of domains: language modeling (Dai et al., 2019; Radford et al., 2019; Yang et al., 2019), machine translation (Vaswani et al., 2017; Edunov et al., 2018), summa- rization (Liu & Lapata), question answering (Dehghani et al., 2018; Yang et al., 2019), multi-task representation learning for NLP (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019), and algorithmic tasks (Dehghani et al., 2018). The repeated success of the transformer architecture in domains where sequential information pro- cessing is critical to performance makes it an ideal candidate for partially observable RL problems, where episodes can extend to thousands of steps and the critical observations for any decision often span the entire episode. Yet, the RL literature is dominated by the use of LSTMs as the main mecha- nism for providing memory to the agent (Espeholt et al., 2018; Kapturowski et al., 2019; Mnih et al., 2016). Despite progress at designing more expressive memory architectures (Graves et al., 2016; Wayne et al., 2018; Santoro et al., 2018) that perform better than LSTMs in memory-based tasks and partially-observable environments, they have not seen widespread adoption in RL agents perhaps due to their complex implementation, with the LSTM being seen as the go-to solution for environ- ments where memory is required. In contrast to these other memory architectures, the transformer Under review as a conference paper at ICLR 2020 is well-tested in many challenging domains and has seen several open-source implementations in a variety of deep learning frameworks 1 . Motivated by the transformer's superior performance over LSTMs and the widespread availability of implementations, in this work we investigate the transformer architecture in the RL setting. In particular, we find that the canonical transformer is significantly difficult to optimize, often resulting in performance comparable to a random policy. This difficulty in training transformers exists in the supervised case as well. Typically a complex learning rate schedule is required (e.g., linear warmup or cosine decay) in order to train (Vaswani et al., 2017; Dai et al., 2019), or specialized weight initialization schemes are used to improve performance (Radford et al., 2019). These measures do not seem to be sufficient for RL. In Mishra et al. (2018), for example, transformers could not solve even simple bandit tasks and tabular Markov Decision Processes (MDPs), leading the authors to hypothesize that the transformer architecture was not suitable for processing sequential information. However in this work we succeed in stabilizing training with a reordering of the layer normaliza- tion coupled with the addition of a new gating mechanism to key points in the submodules of the transformer. Our novel gated architecture, the Gated Transformer-XL (GTrXL) (shown in  Figure 1 , Right), is able to learn much faster and more reliably and exhibit significantly better final perfor- mance than the canonical transformer. We further demonstrate that the GTrXL achieves state-of- the-art results when compared to the external memory architecture MERLIN (Wayne et al., 2018) on the multitask DMLab-30 suite (Beattie et al., 2016). Additionally, we surpass LSTMs signifi- cantly on memory-based DMLab-30 levels while matching performance on the reactive set, as well as significantly outperforming LSTMs on memory-based continuous control and navigation envi- ronments. We perform extensive ablations on the GTrXL in challenging environments with both continuous actions and high-dimensional observations, testing the final performance of the various components as well as the GTrXL's robustness to seed and hyperparameter sensitivity compared to LSTMs and the canonical transformer. We demonstrate a consistent superior performance while matching the stability of LSTMs, providing evidence that the GTrXL architecture can function as a drop-in replacement to the LSTM networks ubiquitously used in RL.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a framework for Imitation Learning (IL) and Imitation Learning from Observations (ILfO) that incorporates recent advances in generative-adversarial training of deep neural networks. The framework utilizes Variational Divergence Minimization (VDM) with varying choices of f-divergence to enable sample-efficient imitation from expert demonstrations, both with and without the provision of expert action labels. The efficacy of the resulting algorithms is evaluated across a range of continuous-control tasks in the MuJoCo domain. Results show that the framework is a viable unification of adversarial imitation methods under the VDM principle, and that improvements in performance can be attained under an appropriate choice of f-divergence. However, there is still a significant performance gap between the recovered imitation policies and expert behavior for tasks with high dimensional observations.",
        "Abstract": "State-of-the-art results in imitation learning are currently held by adversarial methods that iteratively estimate the divergence between student and expert policies and then minimize this divergence to bring the imitation policy closer to expert behavior. Analogous techniques for imitation learning from observations alone (without expert action labels), however, have not enjoyed the same ubiquitous successes. \nRecent work in adversarial methods for generative models has shown that the measure used to judge the discrepancy between real and synthetic samples is an algorithmic design choice, and that different choices can result in significant differences in model performance. Choices including Wasserstein distance and various $f$-divergences have already been explored in the adversarial networks literature, while more recently the latter class has been investigated for imitation learning. Unfortunately, we find that in practice this existing imitation-learning framework for using $f$-divergences suffers from numerical instabilities stemming from the combination of function approximation and policy-gradient reinforcement learning. In this work, we alleviate these challenges and offer a reparameterization of adversarial imitation learning as $f$-divergence minimization before further extending the framework to handle the problem of imitation from observations only. Empirically, we demonstrate that our design choices for coupling imitation learning and $f$-divergences are critical to recovering successful imitation policies. Moreover, we find that with the appropriate choice of $f$-divergence, we can obtain imitation-from-observation algorithms that outperform baseline approaches and more closely match expert performance in continous-control tasks with low-dimensional observation spaces. With high-dimensional observations, we still observe a significant gap with and without action labels, offering an interesting avenue for future work.",
        "Introduction": "  INTRODUCTION Imitation Learning (IL) (Osa et al., 2018) refers to a paradigm of reinforcement learning in which the learning agent has access to an optimal, reward-maximizing expert for the underlying environment. In most work, this access is provided through a dataset of trajectories where each observed state is annotated with the action prescribed by the expert policy. This is often an extremely powerful learning paradigm in contrast to standard reinforcement learning, since not all tasks of interest admit easily-specified reward functions. Additionally, not all environments are amenable to the prolonged and potentially unsafe exploration needed for reward-maximizing agents to arrive at satisfactory policies (Achiam et al., 2017; Chow et al., 2019). While the traditional formulation of the IL problem assumes access to optimal expert action labels, the provision of such information can often be laborious (in the case of a real, human expert) or incur significant financial cost (such as using elaborate instrumentation to record expert actions). Addi- tionally, this restrictive assumption removes a vast number of rich, observation-only data sources from consideration (Zhou et al., 2018). To bypass these challenges, recent work (Liu et al., 2018; Torabi et al., 2018a;b; Edwards et al., 2019; Sun et al., 2019) has explored what is perhaps a more natural problem formulation in which an agent must recover an imitation policy from a dataset con- taining only expert observation sequences. While this Imitation Learning from Observations (ILfO) setting carries tremendous potential, such as enabling an agent to learn complex tasks from watching freely available videos on the Internet, it also is fraught with significant additional challenges. In Under review as a conference paper at ICLR 2020 this paper, we show how to incorporate recent advances in generative-adversarial training of deep neural networks to tackle imitation-learning problems and advance the state-of-the-art in ILfO. With these considerations in mind, the overarching goal of this work is to enable sample-efficient imitation from expert demonstrations, both with and without the provision of expert action labels. The rich literature on Generative Adversarial Networks (Goodfellow et al., 2014) has expanded in re- cent years to include alternative for- mulations of the underlying objec- tive that yield qualitatively differ- ent solutions to the saddle-point op- timization problem (Li et al., 2015; Dziugaite et al., 2015; Zhao et al., 2016; Nowozin et al., 2016; Ar- jovsky et al., 2017; Gulrajani et al., 2017). Of notable interest are the findings of Nowozin et al. (2016) who present Variational Divergence Minimization (VDM), a generaliza- tion of the generative-adversarial ap- proach to arbitrary choices of dis- tance measures between probability distributions drawn from the class of f -divergences (Ali & Silvey, 1966; Csiszár et al., 2004). Applying VDM with varying choices of f - divergence, Nowozin et al. (2016) encounter learned synthetic distribu- tions that can exhibit differences from one another while producing equally realistic samples. Trans- lating this idea for imitation is complicated by the fact that the optimization of the generator occurs via policy-gradient reinforcement learning (Sutton et al., 2000). Existing work in combining adver- sarial IL and f -divergences (Ke et al., 2019), despite being well-motivated, fails to account for this difference; the end results (shown partially in  Figure 1 , where TV-VIM is the method of Ke et al. (2019), and discussed further in later sections) are imitation-learning algorithms that scale poorly to environments with higher-dimensional observations. In this work, we assess the effect of the VDM principle and consideration of alternative f - divergences in the contexts of IL and ILfO. We begin by reparameterizing the framework of Ke et al. (2019) for the standard IL problem. Our version transparently exposes the choices practitioners must make when designing adversarial imitation algorithms for arbitrary choices of f -divergence. We then offer a single instantiation of our framework that, in practice, allows stable training of good policies across multiple choices of f -divergence. An example is illustrated in  Figure 1  where our methods (TV-VIM-sigmoid and TV-VIMO-sigmoid) result in significantly superior policies. We go on to extend our framework to encapsulate the ILfO setting and examine the efficacy of the resulting new algorithms across a range of continuous-control tasks in the MuJoCo (Todorov et al., 2012) do- main. Our empirical results validate our framework as a viable unification of adversarial imitation methods under the VDM principle. With the assistance of recent advances in stabilizing regulariza- tion for adversarial training (Mescheder et al., 2018), improvements in performance can be attained under an appropriate choice of f -divergence. However, there is still a significant performance gap between the recovered imitation policies and expert behavior for tasks with high dimensional obser- vations, leaving open directions for future work in developing improved ILfO algorithms.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a new architecture called conditional invertible neural network (cINN) for conditional image generation. cINN combines an INN with an unconstrained feed-forward network for conditioning, and is able to generate diverse images with high realism on par with existing approaches. We demonstrate a stable, maximum likelihood-based training procedure for jointly optimizing the parameters of the INN and the conditioning network. We also explore and manipulate emergent properties of the latent space, illustrated for MNIST digit generation and image colorization.",
        "Abstract": "In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). It combines the purely generative INN model with an unconstrained feed-forward network, which efficiently pre-processes the conditioning input into useful features.  All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INNs and other normalizing flow models have received very little attention in the literature in contrast to GANs, we find that cINNs can achieve comparable quality, with some remarkable properties absent in cGANs, e.g.  apparent immunity to mode collapse. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.",
        "Introduction": "  INTRODUCTION Generative adversarial networks (GANs) produce ever larger and more realistic samples (Kar- ras et al., 2017; Brock et al., 2019). Hence they have become the primary choice for a majority of image generation tasks. As such, their conditional variants (cGANs) would ap- pear to be the natural tool for conditional image generation as well, and they have suc- cessfully been applied in many scenarios (Ledig et al., 2017; Miyato & Koyama, 2018). However, a lack in diversity is especially common when the condition itself is an image, and special precautions have to be taken to avoid mode collapse and training stability continues to pose a challenge. Conditional variational autoencoders (cVAEs) do not suffer from the same problems. Training is generally stable, and since every data point is assigned a region in latent space, sampling yields the full variety of data seen during train- ing. However cVAEs come with drawbacks of their own: The assumption of a Gaussian pos- terior on the decoder side implies an L2 recon- struction loss, which is known to cause blur- riness. In addition, the partition of the latent space into diagonal Gaussians leads to either mode-mixing issues or regions of poor sample quality (Kingma et al., 2016). There has also been some success in combining aspects of both approaches for certain tasks, such as (Isola et al., 2017; Zhu et al., 2017b; Park et al., 2019). We propose a third approach, by extending In- vertible Neural Networks (INNs, Dinh et al. Under review as a conference paper at ICLR 2020 (2016); Kingma & Dhariwal (2018); Ardizzone et al. (2019)) for the task of conditional image generation, by adding conditioning inputs to their core building blocks. INNs are neural networks which are by construction bijective, efficiently invertible, and have a tractable Jacobian determinant. They represent transport maps between the input distribution p(x) and a prescribed, easy-to-sample- from latent distribution p(z). During training, the likelihood of training samples from p(x) is maximized in latent space, while at inference time, z-samples can trivially be transformed back to the data domain. Previously, INNs have been used successfully for unconditional image generation, e.g. by Dinh et al. (2016) and Kingma & Dhariwal (2018). Unconditional INN training is related to that of VAEs, but it compensates for some key disadvantages: Firstly, since reconstructions are perfect by design, no reconstruction loss is needed, and generated images do not become blurry. Secondly, each x maps to exactly one z in latent space, and there is no need for posteriors p(z | x). This avoids the VAE problem of disjoint or overlapping regions in latent space. In terms of training stability and sample diversity, INNs show the same strengths as autoencoder architectures, but with superior image quality. We find that these positive aspects apply to conditional INNs (cINNs) as well. One limitation of INNs is that their design restricts the use of some standard components of neural networks, such as pooling and batch normalization layers. Our conditional architecture alleviates this problem, as the conditional inputs can be preprocessed by a conditioning network with a standard feed-forward architecture, which can be learned jointly with the cINN to greatly improve its generative capabilities. We demonstrate the quality of cINNs for conditional image generation and uncover emergent properties of the latent space, for the tasks of conditional MNIST generation and diverse colorization of ImageNet. Given this, we believe that the cINN architecture brings the research field of INNs and other normalizing flow models a substantial step forward. Despite the fact that INNs have received very little attention in the literature in contrast to GANs, we find that cINNs can achieve comparable quality to cGANs, with some remarkable properties absent in cGANs. This includes diverse outputs by default, compared to sparse support or mode collapse in cGANs, easier explainability and interpretability of the learned latent representation, as well as simple and intuitive manipulation and interpolation of generated or existing images. Our work makes the following contributions: • We propose a new architecture called conditional invertible neural network (cINN), which combines an INN with an unconstrained feed-forward network for conditioning. It gener- ates diverse images with high realism on par with existing approaces, while adding some noteworthy and useful properties. • We demonstrate a stable, maximum likelihood-based training procedure for jointly optimiz- ing the parameters of the INN and the conditioning network. • We take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space. We illustrate this for MNIST digit generation and image colorization.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces two datasets, ImageNet-Vid-Robust and YTBB-Robust, to evaluate the robustness of convolutional neural networks (CNNs) to natural perturbations arising in videos. Through a thorough evaluation of over 45 different models, the authors find that natural perturbations induce a median accuracy drop of 16% and 10% respectively for classification tasks and a median 14 point drop in mAP for detection tasks. The results show that robustness to natural perturbations in videos is a significant challenge for current CNNs, and suggest that ensuring reliable predictions on every frame of a video is an important direction for future work.",
        "Abstract": "We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct ImageNet-Vid-Robust and YTBB-Robust, containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 percent on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions.",
        "Introduction": "  INTRODUCTION Convolutional neural networks (CNNs) still exhibit many troubling failure modes. At one extreme, p -adversarial examples cause large drops in accuracy for state-of-the-art models while relying only on visually imperceptible changes to the input image ( Goodfellow et al., 2014 ;  Biggio and Roli, 2018 ). However, this failure mode usually does not pose a problem outside a fully adversarial context because carefully crafted p -perturbations are unlikely to occur naturally in the real world. To study more realistic failure modes, researchers have investigated benign image perturbations such as rotations & translations, colorspace changes, and various image corruptions ( Fawzi and Frossard, 2015 ;  Engstrom et al., 2017 ;  Fawzi and Frossard, 2015 ;  Hendrycks and Dietterich, 2019 ). However, it is still unclear whether these perturbations reflect the robustness challenges arising in real data since the perturbations also rely on synthetic image modifications. Recent work has therefore turned to videos as a source of naturally occurring perturbations of images ( Zheng et al., 2016 ;  Azulay and Weiss, 2018 ;  Gu et al., 2019 ). In contrast to other failure modes, the perturbed images are taken from existing image data without further modifications that make the task more difficult. As a result, robustness to such perturbations directly corresponds to performance improvements on real data. However, it is currently unclear to what extent such video perturbations pose a significant robustness challenge.  Azulay and Weiss (2018)  and  Zheng et al. (2016)  only provide anecdotal evidence from a small number of videos.  Gu et al. (2019)  go beyond individual videos and utilize a large video dataset ( Real et al., 2017 ) in order to measure the effect of video perturbations more quantitatively. In their evaluation, the best image classifiers lose about 3% accuracy for video frames up to 0.3 seconds away. However, the authors did not employ humans to review the frames in their videos. Hence the accuracy drop could also be caused by significant changes in the video frames (e.g., due to fast camera or object motion). Since the 3% accuracy drop is small to begin with, it remains unclear whether video perturbations are a robustness challenge for current image classifiers. We address these issues by conducting a thorough evaluation of robustness to natural perturbations arising in videos. As a cornerstone of our investigation, we introduce two test sets for evaluating model robustness: ImageNet-Vid-Robust and YTBB-Robust, carefully curated from the ImageNet-Vid and Youtube-BB datasets, respectively ( Russakovsky et al., 2015 ;  Real et al., 2017 ). All images in the two datasets were screened by a set of expert labelers to ensure high annotation quality and minimize selection biases that arise when filtering a dataset with CNNs. To the best of Under review as a conference paper at ICLR 2020 our knowledge these are the first datasets of their kind, containing tens of thousands of images that are human reviewed and grouped into thousands of perceptually similar sets. In total, our datasets contain 3,139 sets of temporally adjacent and visually similar images (57,897 images total). We then utilize these datasets to measure the accuracy of current CNNs to small, naturally oc- curring perturbations. Our testbed contains over 45 different models, varying both architecture and training methodology (adversarial training, data augmentation, etc.). To better understand the drop in accuracy due to natural perturbations, we also introduce a robustness metric that is more stringent than those employed in prior work. Under this metric, we find that natural perturba- tions from ImageNet-Vid-Robust and YTBB-Robust induce a median accuracy drop of 16% and 10% respectively for classification tasks and a median 14 point drop in mAP for detection tasks. 1 Even for the best-performing classification models, we observe an accuracy drop of 14% for ImageNet-Vid-Robust and 8% for YTBB-Robust. Our results show that robustness to natural perturbations in videos is indeed a significant challenge for current CNNs. As these models are increasingly deployed in safety-critical environments that require both high accuracy and low latency (e.g., autonomous vehicles), ensuring reliable predictions on every frame of a video is an important direction for future work.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new method, MissDeepCausal, for causal inference with missing data. MissDeepCausal is inspired by existing work, but generalizes and extends it in different aspects. It relies on a non-linear latent variable model and the missing at random assumption for the missing data mechanisms, and takes into account the posterior distribution of the latent variables given observed data. The paper compares MissDeepCausal empirically with several state-of-the-art methods on simulated data.",
        "Abstract": "Inferring causal effects of a treatment, intervention or policy from observational data is central to many applications. However, state-of-the-art methods for causal inference seldom consider the possibility that covariates have missing values, which is ubiquitous in many real-world analyses.  Missing data greatly complicate causal inference procedures as they require an adapted unconfoundedness hypothesis which can be difficult to justify in practice. We circumvent this issue by considering latent confounders whose distribution is learned through variational autoencoders adapted to missing values. They can be used either as a pre-processing step prior to causal inference but we also suggest to embed them in a multiple imputation strategy to take into account the variability due to missing values.  Numerical experiments demonstrate the effectiveness of the proposed methodology especially for non-linear models compared to competitors.",
        "Introduction": "  INTRODUCTION Many methods have been developed to estimate the causal effect of an intervention, such as the administration of a treatment, on an outcome such as survival, from observational data, i.e., data that is potentially confounded by selection bias due to the absence of randomization. Classical ones include matching ( Iacus et al., 2012 ), inverse propensity weighting ( IPW, Horvitz & Thompson, 1952 ;  Rosenbaum & Rubin, 1983 ) and doubly robust methods ( Robins et al., 1994 ;  Chernozhukov et al., 2018 ;  Wager & Athey, 2018 ;  Athey et al., 2019 ). More recent proposals use deep learning methods that ensure balance of the population at the level of representation ( Johansson et al., 2016 ;  Shalit et al., 2017 ), infer the joint distribution of latent and observed confounders, the treatment and the outcome ( Louizos et al., 2017 ) or predict the counterfactuals with GANs ( Yoon et al., 2018 ). For a detailed review of existing literature on treatment effect estimation we refer to  Imbens (2004) ,  Lunceford & Davidian (2004)  and  Guo et al. (2019) . However, state-of-the-art methods still suffer from important shortcomings. In particular, they sel- dom consider the possibility that covariates have missing values, which is ubiquitous in many real- world situations ( Josse & Reiter, 2018 ) and has been widely discussed in different contexts ( Mayer et al., 2019a ;  van Buuren, 2018 ;  Little & Rubin, 2002 ). Although this question of missing attributes in the context of treatment effect estimation has been raised early in the development of causal infer- ence ( Rosenbaum & Rubin, 1984 ), there is still a lack of effective and consistent solutions address- ing this problem, with a few notable exceptions such as  Mattei & Mealli (2009) ;  Seaman & White (2014) ;  Yang et al. (2019) ; Kallus et al. (2018) which mainly focus on inverse propensity weighting (IPW) methods and  Kuroki & Pearl (2014)  who discuss identifiability of causal effects under mea- surement error or unobserved confounders. Recently,  Mayer et al. (2019b) , in addition to suggesting doubly robust estimators with missing data, classified the existing approaches into two families: the ones that adapt the causal inference assumptions to the missing values setting ( D'Agostino Jr & Ru- bin, 2000 ;  Blake et al., 2019 ) and the ones ( Mattei & Mealli, 2009 ;  Seaman & White, 2014 ; Kallus et al., 2018) that consider the classical machinery and missingness mechanisms assumptions ( Little & Rubin, 2002 ). While the former are based on the assumption of unconfoundedness with missing values, which can be difficult to assess in practice, the latter have been developed under strong para- Under review as a conference paper at ICLR 2020 metric assumptions about the outcome, treatment and covariates models, in addition to relying on missing values hypotheses that can also be difficult to meet in practice ( Yang et al., 2019 ). To avoid relying on the hypothesis of unconfoundedness with missing values or being in the very parametric (and linear) framework of multiple imputation ( Mattei & Mealli, 2009 ;  Seaman & White, 2014 ) and matrix factorization (Kallus et al., 2018), we propose a new method for causal inference with missing data, which we call MissDeepCausal. MissDeepCausal is inspired by the work of Kallus et al. (2018) in the sense that we consider a model with latent confounders, and assume that we only have access to covariates with missing values that are noisy proxies of the true latent confounders. However, our approach generalizes and extends the work of Kallus et al. (2018) in different aspects: (i) instead of linear factor analysis models with missing values, we consider non- linear versions using deep latent variable models ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ); (ii) we rely on the missing at random (MAR) ( Rubin, 1976 ) assumption for the missing data mecha- nisms, and not on the stronger missing completely at random (MCAR) one; (iii) we take into account the posterior distribution of the latent variables given observed data and not only their conditional expectation. This latter point allows us to define a multiple imputation strategy adapted to the latent confounders model, and to couple it with doubly robust treatment effect estimation ( Chernozhukov et al., 2018 ). In the remainder of this article we first introduce the problem framework and recall existing work for handling missing values in causal inference in Section 2. We then introduce two variants of our MissDeepCausal approach in Section 3. Finally we compare MissDeepCausal empirically with several state-of-the-art methods on simulated data in Section 4.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to address the problem of exposure bias in text summarization using an encoder-decoder network. The approach defines a loss function at the sequence level using an encoder-decoder network as a loss function. Experiments on the popular CNN/DailyMail dataset show that adding the recoder as a loss function improves a general abstractive summarizer, achieving significant improvements in the ROUGE metric and an especially large improvement in human evaluations.",
        "Abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.",
        "Introduction": "  INTRODUCTION Neural networks are a popular solution to the problem of text summarization, the task of taking as input a piece of natural language text, such as a paragraph or a news article, and generating a more succinct text that captures the most essential information from the original. One popular type of neural network that has achieved state of the art results is an attentional encoder- decoder neural network  See et al. (2017) ;  Paulus et al. (2018) ;  Celikyilmaz et al. (2018) . In an encoder-decoder network, the encoder scans over the input sequence by ingesting one word token at a time to create an internal representation. The decoder is trained to compute a probability distribu- tion over next words conditioned on a sequence prefix. A beam search decoder is typically used to find a high likelihood output sequence based on these conditional word probability distributions. Since the next word depends heavily on previous words, the decoder has little hope of a correct distribution over next words unless it has the correct previous words. Thus the decoder is typically trained using teacher forcing  Williams & Zipser (1989) , where the reference sequence prefix is always given to the decoder at each decoding step. In other words, regardless of what distributions are output by the decoder in training for timesteps (1, ..., t−1), at timestep t, it is given the reference sequence prefix (y * 1 , ..., y * t−1 ) and asked to output the distribution P (y t |y * 1 , ..., y * t−1 ). Teacher forcing suffers from exposure bias  Ranzato et al. (2016) . At test time, the input to the decoder is not the reference sequence prefix (y * 1 , ..., y * t−1 ) but a (y 1 , ..., y t−1 ) constructed based on the decoder's own outputs. Since the decoder is not perfect, as t increases so too does the disparity between these two sequence prefixes, causing an increasing mismatch between training and test conditions. Training at the sequence level can alleviate this discrepancy, but requires a differentiable loss func- tion. In the Related Work section we review previous efforts. We present a novel approach to address the problem by defining a loss function at the sequence level using an encoder-decoder network as a loss function. In training, the summarizer's beam search decoded output sequence is fed as input into another network called the recoder. The recoder is an independent attentional encoder-decoder trained to produce the reference summary. Our experiments show that adding the recoder as a loss function improves a general abstractive summarizer on the popular CNN/DailyMail dataset  Hermann et al. (2015) ;  Nallapati et al. (2016) , Under review as a conference paper at ICLR 2020 achieving significant improvements in the ROUGE metric and an especially large improvement in human evaluations.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the concept of adversarial attacks, which are small imperceptible perturbations that can lead to drastic performance degradation in deep neural networks. It discusses the research surge to develop simple routines to construct adversarial examples consistently, and how such adversaries can be both network and input agnostic. It also discusses the implications of DNNs being easily susceptible to simple attacks, and the active research directions of network defense and verification. Finally, it introduces robustness verification methods, which use verification methods to train robust networks.",
        "Abstract": "Training Deep Neural Networks (DNNs) that are robust to norm bounded adversarial attacks remains an elusive problem. While verification based methods are generally too expensive to robustly train large networks, it was demonstrated by Gowal et. al. that bounded input intervals can be inexpensively propagated from layer to layer through deep networks. This interval bound propagation (IBP) approach led to high robustness and was the first to be employed on large networks. However, due to the very loose nature of the IBP bounds, particularly for large/deep networks, the required training procedure is complex and involved. In this paper, we closely examine the bounds of a block of layers composed of an affine layer, followed by a ReLU, followed by another affine layer. To this end, we propose \\emph{expected} bounds (true bounds in expectation), which are provably tighter than IBP bounds in expectation. We then extend this result to deeper networks through blockwise propagation and show that we can achieve orders of magnitudes tighter bounds compared to IBP. Using these tight bounds, we demonstrate that a simple standard training procedure can achieve impressive robustness-accuracy trade-off across several architectures on both MNIST and CIFAR10.",
        "Introduction": "  INTRODUCTION Deep neural networks (DNNs) have demonstrated impressive performance in many fields of research with applications ranging from image classification ( Krizhevsky et al., 2012 ;  He et al., 2016 ) and semantic segmentation ( Long et al., 2015 ) to speech recognition ( Hinton et al., 2012 ), just to name a few. Despite this success, DNNs are still susceptible to small imperceptible perturbations, which can lead to drastic performance degradation, especially in visual classification tasks. Such perturbations are best known and commonly referred to as adversarial attacks. Early work showed that simple algorithms (e.g. maximizing the classification loss with respect to the input using a single optimization iteration ( Goodfellow et al., 2014 )) can easily construct such adversaries. Since then, a research surge has emerged to develop simple routines to construct adversarial examples consistently. For instance,  Moosavi-Dezfooli et al. (2016)  proposed a simple algorithm, called DeepFool, which finds the smallest perturbation that fools a linearized version of the network. Interestingly, the work of  Moosavi-Dezfooli et al. (2017)  demonstrated that such adversaries can be both network and input agnostic, i.e. universal deterministic samples that fool a wide range of DNNs across a large number of input samples. More recently, it was shown that such adversaries can also be as simple as Gaussian noise ( Bibi et al., 2018 ). Knowing that DNNs are easily susceptible to simple attacks can hinder the public confidence in them, especially for real-world deployment, e.g. in self-driving cars and devices for the visually impaired. Such a performance nuisance has prompted several active research directions, in particular, work towards network defense and verification. Network defense aims to train networks that are robust against adversarial attacks through means of robust training or procedures at inference time that dampen the effectiveness of the attack ( Madry et al., 2018 ;  Wong & Kolter, 2018 ;  Raghunathan et al., 2018 ;  Alfadly et al., 2019 ). On the other hand, verification aims to certify/verify for a given DNN that there exists no small perturbation of a given input that can change its output prediction ( Katz et al., 2017 ;  Sankaranarayanan et al., 2016 ;  Weng et al., 2018a ). However, there are also works at the intersection of both often referred to as robustness verification methods, which use verification methods to train robust networks. Such algorithms often try to minimize the exact (or upper bound) of the worst adversarial loss over all possible bounded energy (often measured in ∞ norm) perturbation around a given input.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the emergent communication protocols of deep network agents that interact to accomplish a goal. It is shown that the protocol shares an important property of human language, namely a tendency towards entropy minimization. This property is observed in two separate games designed to measure it, and is connected to the Information Bottleneck principle. It is also shown that when channel discreteness is relaxed, the entropy minimization property no longer holds, and the system becomes less robust against overfitting and adversarial noise.",
        "Abstract": "There is a growing interest in studying the languages emerging when neural agents are jointly trained to solve tasks requiring communication through a discrete channel.  We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.",
        "Introduction": "  INTRODUCTION There has recently been much interest in the analysis of the communication systems arising when deep network agents that interact to accomplish a goal are allowed to exchange language-like discrete messages ( Lazaridou et al., 2016 ;  Havrylov & Titov, 2017 ;  Choi et al., 2018 ;  Lazaridou et al., 2018 ). Understanding the emergent protocol is important if we want to eventually develop agents capable of interacting with each other and with us through language ( Mikolov et al., 2016 ;  Chevalier-Boisvert et al., 2019 ). The pursuit might also provide comparative evidence about how core properties of human language itself have evolved ( Kirby, 2002 ;  Hurford, 2014 ;  Graesser et al., 2019 ). While earlier studies reported ways in which deep agent protocols radically depart from human language ( Kottur et al., 2017 ;  Bouchacourt & Baroni, 2018 ;  Chaabouni et al., 2019 ;  Lowe et al., 2019 ), we show here that emergent communication shares an important property of the latter, namely a tendency towards entropy minimization. Converging evidence indicates that efficiency pressures are at work in language and other biological communication systems ( Ferrer i Cancho et al., 2013 ;  Gibson et al., 2019 ). One particular aspect of communicative efficiency, that has been robustly observed across many semantic domains, is a tendency to minimize lexicon entropy, to the extent allowed by the counteracting need for accuracy ( Zaslavsky et al., 2018 ;  2019 ). For example, while most languages distinguish grandmothers from grandfathers, very few have separate words for mother- and father-side grandmothers, as the latter distinction would make communication only slightly more accurate at the cost of an increase in lexicon complexity ( Kemp & Regier, 2012 ). We show here, in two separate games designed to precisely measure such property, that the protocol evolved by interacting deep agents is subject to the same complexity minimization pressure. Entropy minimization in natural language has been connected to the Information Bottleneck princi- ple ( Tishby et al., 1999 ). In turn, complexity reduction due to the Information Bottleneck provides a beneficial regularization effect on the learned representations ( Fischer, 2019 ; Alemi et al., 2016;  Achille & Soatto, 2018a ; b ). It is difficult to experimentally verify the presence of such effect in human languages, but we can look for it in our emergent language simulations. We confirm that, when relaxing channel discreteness, the entropy minimization property no longer holds, and the system becomes less robust against overfitting and adversarial noise.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a model for precise temporal localization of events in sequential data, despite only having access to noisy annotations for training. The model is designed to learn localization by mimicking the provided annotations, while being less dependent on the exact location of labels for training. This approach is necessary to learn the actual ground-truth and to infer event locations more accurately than the labels relied on for training.",
        "Abstract": "This work addresses the long-standing problem of robust event localization in the presence of temporally of misaligned labels in the training data. We propose a novel versatile loss function that generalizes a number of training regimes from standard fully-supervised cross-entropy to count-based weakly-supervised learning. Unlike classical models which are constrained to strictly fit the annotations during training, our soft localization learning approach relaxes the reliance on the exact position of labels instead. Training with this new loss function exhibits strong robustness to temporal misalignment of labels, thus alleviating the burden of precise annotation of temporal sequences. We demonstrate state-of-the-art performance against standard benchmarks in a number of challenging experiments and further show that robustness to label noise is not achieved at the expense of raw performance. ",
        "Introduction": "  INTRODUCTION The surge of deep neural networks ( LeCun et al., 2015 ;  Schmidhuber, 2015 ) has accentuated the evergrowing need for large corpora of data ( Banko & Brill, 2001 ;  Halevy et al., 2009 ). The main bottleneck for the efficient creation of datasets remains the annotation process. Over the years, while new labeling paradigms have emerged to alleviate this issue (e.g., crowdsourcing ( Deng et al., 2009 ) or external informa- tion sources ( Abu-El-Haija et al., 2016 )), these methods have also highlighted, and emphasized, the prevalence of label noise. Deep neural networks are unfortunately not immune to these perturbations as their intrinsic ability to memorize and learn label noise ( Zhang et al., 2017 ) can be the cause of training robustness issues and poor generalization perfor- mance. In this context, the development of models robust to label noise is essential. This work tackles the problem of precise temporal localiza- tion of events (i.e., determining when and which events occur) in sequential data (e.g. time series, video or audio sequences) despite only having access to poorly aligned annotations for training (see  Figure 1 ). This task is characterized by the dis- crepency between the precision required of the predictions during inference and the noisiness of the training labels. In- deed, while models are trained on inaccurate data, they are evaluated on their ability to predict event occurences as pre- cisely as possible with respect to the ground-truth. In such a setting, effective models have to infer event locations more accurately than the labels they relied on for training. This requirement is particularly challenging for most classical approaches that are designed to learn localization by strictly mimicking the provided annotations. Indeed, as the training labels themselves do not accurately reflect the event location, focusing on replicating these unreliable patterns is incompatible with the overall objective of learning the actual ground-truth. These challenges highlight the need for more relaxed learning approaches that are less dependent on the exact location of labels for training.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper considers methods to solve smooth unconstrained min-max optimization problems, which have a long history and a variety of applications. The convex-concave setting, where the objective function is convex in one input and concave in the other, is a classic min-max problem. Recently, interest in min-max optimization has surged due to the popularity of Generative Adversarial Networks (GANs), which involve solving a nonconvex min-max problem. This paper explores the use of no-regret algorithms to find local min-max solutions, as well as other methods for solving nonconvex min-max problems.",
        "Abstract": "While classic work in convex-concave min-max optimization relies on average-iterate convergence results, the emergence of nonconvex applications such as training Generative Adversarial Networks has led to renewed interest in last-iterate convergence guarantees. Proving last-iterate convergence is challenging because many natural algorithms, such as Simultaneous Gradient Descent/Ascent, provably diverge or cycle even in simple convex-concave min-max settings, and previous work on global last-iterate convergence rates has been limited to the bilinear and convex-strongly concave settings. In this work, we show that the Hamiltonian Gradient Descent (HGD) algorithm achieves linear convergence in a variety of more general settings, including convex-concave problems that satisfy a “sufficiently bilinear” condition. We also prove similar convergence rates for some parameter settings of the Consensus Optimization (CO) algorithm of Mescheder et al. 2017.",
        "Introduction": "  INTRODUCTION In this paper we consider methods to solve smooth unconstrained min-max optimization problems. In the most classical setting, a min-max objective has the form min x1 max x2 g(x 1 , x 2 ) where g : R d × R d → R is a smooth objective function with two inputs. The usual goal in such problems is to find a saddle point, also known as a min-max solution, which is a pair (x * 1 , x * 2 ) ∈ R d × R d that satisfies g(x * 1 , x 2 ) ≤ g(x * 1 , x * 2 ) ≤ g(x 1 , x * 2 ) (1) for every x 1 ∈ R d and x 2 ∈ R d . Min-max problems have a long history, going back at least as far as  Neumann (1928) , which formed the basis of much of modern game theory, and including a great deal of work in the 1950s when algorithms such as fictitious play were explored (Brown, 1951;  Robinson, 1951 ). The convex-concave setting, where we assume g is convex in x 1 and concave in x 2 , is a classic min-max problem that has a number of different applications, such as solving constrained convex optimization problems. While a variety of tools have been developed for this setting, a very popular approach within the machine learning community has been the use of so-called no-regret algorithms ( Cesa-Bianchi & Lugosi, 2006 ;  Hazan, 2016 ). This trick, which was originally developed by  Hannan (1957)  and later emerged in the development of boosting ( Freund & Schapire, 1999 ), provides a simple computational method via repeated play: each of the inputs x 1 and x 2 are updated iteratively according to no-regret learning protocols, and one can prove that the average-iterates (x 1 ,x 2 ) converge to a min-max solution. Recently, interest in min-max optimization has surged due to the enormous popularity of Generative Adversarial Networks (GANs), whose training involves solving a nonconvex min-max problem where x 1 and x 2 correspond to the parameters of two different neural nets ( Goodfellow et al., 2014 ). The fundamentally nonconvex nature of this problem changes two things. First, it is infeasible to find a \"global\" solution of the min-max objective. Instead, a typical goal in GAN training is to find a local min-max, namely a pair (x * 1 , x * 2 ) that satisfies (1) for all (x 1 , x 2 ) in some neighborhood of (x * 1 , x * 2 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents SGD with hardness weighted sampling, a novel, principled optimization method for training deep neural networks with Distributionally Robust Optimization (DRO). DRO is an alternative to Empirical Risk Minimization (ERM) that takes into account uncertainty in the empirical data distribution. The proposed method is inspired by Online Hard Example Mining (OHEM) and is computationally as efficient as SGD for ERM. We show that our method performs favorably to SGD in the case of class imbalance and formally link DRO in our method with OHEM. We also generalize recent results in the convergence theory of SGD with ERM to our SGD with hardness weighted sampling for DRO.",
        "Abstract": "Distributionally Robust Optimization (DRO) has been proposed as an alternative to Empirical Risk Minimization (ERM) in order to account for potential biases in the training data distribution. However, its use in deep learning has been severely restricted due to the relative inefficiency of the optimizers available for DRO compared to the wide-spread Stochastic Gradient Descent (SGD) based optimizers for deep learning with ERM. In this work, we demonstrate that SGD with hardness weighted sampling is a principled and efficient optimization method for DRO in machine learning and is particularly suited in the context of deep learning. Similar to a hard example mining strategy in essence and in practice, the proposed algorithm is straightforward to implement and computationally as efficient as SGD-based optimizers used for deep learning.  It only requires adding a softmax layer and maintaining an history of the loss values for each training example to compute adaptive sampling probabilities.  In contrast to typical ad hoc hard mining approaches, and exploiting recent theoretical results in deep learning optimization, we prove the convergence of our DRO algorithm for over-parameterized deep learning networks with ReLU activation and finite  number  of  layers  and parameters.  Preliminary results demonstrate the feasibility and usefulness of our approach.",
        "Introduction": "  INTRODUCTION In standard deep learning pipelines, a neural network h with parameters θ is trained by minimizing the mean of a per-example loss L over a training dataset {(x i , y i )} n i=1 , where x i are the inputs and y i are the labels. This corresponds to Empirical Risk Minimization (ERM), defined as the non-convex optimization problem Since the empirical risk is equal to the expectation of the per-example loss over the empirical training data distribution, an approximate solution of (1) can be obtained efficiently by Stochastic Gradient Descent (SGD) with a uniform sampling over the training data ( Bottou et al., 2018 ). This approach has led to remarkable results in terms of average performance, but may lead to outliers with high loss values compared to the average loss. Such cases can even be observed for elements belonging to the training data set. This is because solutions of ERM are prone to ignore a few hard examples in order to obtain a low mean per-example loss. In practice, outlier results have, for example, been consistently reported in the context of deep learn- ing for brain tumor segmentation, as illustrated in the recent annual BRATS challenges ( Bakas et al., 2018 ). For safety-critical systems, such as those used in healthcare, where outliers must be avoided, this is not satisfactory. Efficient biased sampling methods, including Online Hard Example Mining (OHEM) ( Shrivastava et al., 2016 ;  Loshchilov & Hutter, 2015 ;  Chang et al., 2017 ) and weighted sampling ( Bouchard et al., 2015 ;  Berger et al., 2018 ;  Gibson et al., 2018 ), have been proposed to mitigate this issue. However, even though these works typically start from an ERM formulation, it is not clear how those heuristics actually relate to ERM in theory. Under review as a conference paper at ICLR 2020 Distributionally Robust Optimization (DRO) is an alternative to ERM (1) that takes into account uncertainty in the empirical data distribution. Formally, training a deep neural network with DRO corresponds to the min-max non-convex-concave optimization problem arg min θ max q n i=1 q i L (h(x i ; θ), y i ) − 1 β n i=1 1 n φ (nq i ) (2) where φ is a convex function that defines a φ-divergence ( Csiszár et al., 2004 ), q = (q i ) n i=1 cor- responds to an arbitrary sampling distribution over the training data, and β > 0 is a robustness parameter. Instead of minimizing the mean per-example loss on the training dataset, DRO seeks the hardest weighted empirical training data distribution around the (uniform) empirical training data distribution. This suggests a link between DRO and OHEM. The parameter β allows DRO to interpolate between ERM (β → 0) and the minimization of the maximum per-example loss (β → +∞). Motivations for using the minimization of the maximum per-example loss for safety-critical applications have been discussed in ( Shalev-Shwartz & Wexler, 2016 ). DRO as a generalization of ERM for machine learning has been studied in ( Duchi et al., 2016 ;  Rafique et al., 2018 ;  Namkoong & Duchi, 2016 ;  Chouzenoux et al., 2019 ), but still lacks optimiza- tion methods that are computationally as efficient as SGD in the non-convex setting of deep learning. If one could efficiently solve the inner maximization problem in (2) for a given θ, DRO could be addressed by alternating between this maximization problem and a minimization scheme akin to the standard ERM (1), but over an adaptively weighted empirical distribution. However, even when a closed-form solution is available for the inner maximization problem, it requires performing a forward pass over the entire training dataset at each iteration. This cannot be done efficiently for large dataset. Previously proposed optimization methods for large-scale non-convex-concave problem of the form of (2) are based on the min-max structure of the problem, and consist in alternating between approx- imate maximization and minimization steps ( Rafique et al., 2018 ;  Lin et al., 2019 ; Jin et al., 2019). However, they differ from SGD methods for ERM by the introduction of additional hyperparame- ters for the optimizer such as a second learning rate and a ratio between the number of minimization and maximization steps. As a result, DRO is difficult to use as a replacement of ERM in practice. In addition, those min-max methods do not use the link between DRO and adaptive weighted sam- pling, therefore departing from efficient heuristics used in hard example mining. From a theoretical perspective, they further make the assumption that the model is either smooth or weakly-convex, but none of those properties are true for deep neural networks with ReLU activation functions that are largely used in practice. In this work, we propose SGD with hardness weighted sampling, a novel, principled optimization method for training deep neural networks with DRO (2) and inspired by OHEM. Compared to SGD, our method only requires introducing an additional softmax layer and maintaining a history of the stale per-example loss to compute sampling probabilities over the training data. Since the loss is already computed at each iteration for SGD, our SGD with hardness weighted sampling for DRO is computationally as efficient as SGD for ERM. In practice, we show that our method performs favorably to SGD in the case of class imbalance. We also formally link DRO in our method with OHEM ( Shrivastava et al., 2016 ). As a result, our method can be seen as a principled OHEM approach. In this context, the robustness parameter β controls the trade-off between exploitation and exploration in the OHEM process. Last but not least, we generalize recent results in the convergence theory of SGD with ERM and over-parameterized deep learning networks with ReLU activation functions ( Allen-Zhu et al., 2019 ; 2018;  Cao & Gu, 2019 ;  Zou & Gu, 2019 ) to our SGD with hardness weighted sampling for DRO. This is, to the best of our knowledge, the first convergence result for deep learning networks with ReLU trained with DRO. In this work, we focus on DRO with a φ-divergence ( Csiszár et al., 2004 ). In this case, the data distributions that are considered in the DRO problem (2) are restricted to sharing the support of the empirical training distribution. In other words, the weights assigned to the training data can change, but the training data itself remains unchanged. Another popular formulation is DRO with a Wasserstein distance ( Sinha et al., 2017 ;  Duchi et al., 2016 ;  Staib & Jegelka, 2017 ;  Chouzenoux et al., 2019 ). In contrast to φ-divergences, using a Wasser- stein distance in DRO seeks to apply small data augmentation to the training data to make the deep learning model robust to small deformation of the data, but the sampling weights of the training data distribution typically remains unchanged. In this sense, DRO with a φ-divergence and DRO with a Wasserstein distance can be considered as orthogonal endeavours. While we show that DRO with φ-divergence can be seen as a principled OHEM method, it has been shown that DRO with a Wasserstein distance can be seen as a principled adversarial training method ( Sinha et al., 2017 ;  Staib & Jegelka, 2017 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper considers the minimization of a differentiable and possible nonconvex objective function using first-order information. It provides theoretical work on the runtime of nonconvex optimization using early stopping based on a validation function. It derives upper bounds on the expected value of the stopping time and bounds on the Wasserstein distance between the training and validation sets. It applies the analysis to several settings, including stochastic gradient descent (SGD), stochastic variance reduced gradient (SVRG), decentralized SGD (DSGD) and stacked SGD (SSGD). The result is new bounds on the expected number of Incremental First-order Oracle (IFO) calls needed to generate approximate stationary points for some known algorithms.",
        "Abstract": "This work examines the convergence of stochastic gradient algorithms that use early stopping based on a validation function, wherein optimization ends when the magnitude of a validation function gradient drops below a threshold. We derive conditions that guarantee this stopping rule is well-defined and analyze the expected number of iterations and gradient evaluations needed to meet this criteria. The guarantee accounts for the distance between the training and validation sets, measured with the Wasserstein distance. We develop the approach for stochastic gradient descent (SGD), allowing for biased update directions subject to a Lyapunov condition. We apply the approach to obtain new bounds on the expected running time of several algorithms, including Decentralized SGD (DSGD), a variant of decentralized SGD, known as \\textit{Stacked SGD}, and the stochastic variance reduced gradient (SVRG) algorithm. Finally, we consider the generalization properties of the iterate returned by early stopping.",
        "Introduction": "  INTRODUCTION This work considers the minimization of a differentiable and possible nonconvex objective function: A generally accepted success criteria for algorithms that use only first-order information is that an approximate stationary point is generated. These are points x ∈ R d at which the function f has a small gradient. In a typical machine learning scenario, f is the average loss over a dataset of training examples, and the method of choice involves using some form of stochastic gradient method, for instance, stochastic gradient descent, or SGD (see Algorithm 1). The success of SGD in machine learning problems has led to many extensions of the algorithm, including variance-reduced and distributed variants (reviewed in Section 1.1). A common approach to stopping optimization in practice is to use early stopping based on a validation function. In this scenario, a stopping criterion is periodically evaluated on the validation function, and the algorithm stops once this criterion is met. The validation and training sets often are disjoint (although this is not required in the present work). Although this approach is used frequently, there is little theoretical work on the runtime of nonconvex optimization using early stopping based on a validation function. In general, the runtime and performance will depend on several factors, including the relation between the validation and training functions, and the desired level of solution accuracy. In this work, the stopping criterion is that the algorithm has generated a point, which is an approximate stationary point for the validation function. Our analysis focuses on bounding the number of iterations and gradient evaluations used until the algorithm meets the stopping criteria. Formally, we consider the stopping time defined as the first time an iterate has the property of being an approximate stationary point for the validation function, and we derive upper bounds on the expected value of this stopping time. Using bounds on the Wasserstein distance between the training and validation sets, we also may derive a bound on the stationary gap of the training function at the resulting iterate. As an extension, we also describe how Wasserstein concentration bounds can be used to bound the stationarity gap with respect to the testing distribution to which both the training and validation datasets are drawn. We apply our analysis to several settings, including stochastic gradient descent (SGD), stochastic variance reduced gradient (SVRG), decentralized SGD (DSGD) and stacked SGD (SSGD). The Under review as a conference paper at ICLR 2020 result is new bounds on the expected number of Incremental First-order Oracle (IFO) calls needed to generate approximate stationary points for some known algorithms (SGD; DSGD; SVRG), as well as a new algorithm (SSGD).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new benchmark for supervised few-shot classification, called CUB-FewShot, which does not rely on consistent class semantics across episodes. CUB-FewShot is based on the Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset (Wah et al., 2011) and is designed to evaluate the ability of supervised few-shot classification methods to adapt to changing class semantics. We compare the performance of several supervised few-shot classification methods on CUB-FewShot and on the Omniglot and miniImageNet benchmarks. Our results show that CUB-FewShot is more challenging than the Omniglot and miniImageNet benchmarks and that supervised few-shot classification methods are not able to adapt to changing class semantics as well as they adapt to consistent class semantics.\n\nAbstract: This paper presents a new benchmark for supervised few-shot classification, called CUB-FewShot, which does not rely on consistent class semantics across episodes. The benchmark is designed to evaluate the ability of supervised few-shot classification methods to adapt to changing class semantics. Results show that CUB-FewShot is more challenging than the Omniglot and miniImageNet benchmarks and that supervised few-shot classification methods are not able to adapt to changing class semantics as well as they adapt to consistent class semantics.",
        "Abstract": "We argue that the widely used Omniglot and miniImageNet benchmarks are too simple because their class semantics do not vary across episodes, which defeats their intended purpose of evaluating few-shot classification methods. The class semantics of Omniglot is invariably “characters” and the class semantics of miniImageNet, “object category”. Because the class semantics are so similar, we propose a new method called Centroid Networks which can achieve surprisingly high accuracies on Omniglot and miniImageNet without using any labels at metaevaluation time. Our results suggest that those benchmarks are not adapted for supervised few-shot classification since the supervision itself is not necessary during meta-evaluation. The Meta-Dataset, a collection of 10 datasets, was recently proposed as a harder few-shot classification benchmark. Using our method, we derive a new metric, the Class Semantics Consistency Criterion, and use it to quantify the difficulty of Meta-Dataset. Finally, under some restrictive assumptions, we show that Centroid Networks is faster and more accurate than a state-of-the-art learning-to-cluster method (Hsu et al., 2018). ",
        "Introduction": "  INTRODUCTION Supervised few-shot classification, sometimes simply called few-shot learning, consists in learning a classifier from a small number of examples. Being able to quickly learn new classes from a small number of labeled examples is desirable from a practical perspective because it removes the need to label large datasets. Typically, supervised few-shot classification is formulated as meta-learning on episodes, where each episode corresponds to two small sets of labeled examples called support and query sets. The goal is to train a classifier on the support set and to classify the query set with maximum accuracy. The Omniglot (Lake et al., 2011) and miniImageNet (Vinyals et al., 2016; Ravi & Larochelle, 2017) benchmarks have been heavily used to evaluate and compare supervised few-shot classification methods in the last few years (Vinyals et al., 2016; Ravi & Larochelle, 2017; Snell et al., 2017; Finn et al., 2017; Sung et al., 2018). Despite their popularity and their important role in pioneer- ing the few-shot learning field, we argue that the Omniglot and miniImageNet benchmarks should not be taken as gold standards for evaluating supervised few-shot classification because they rely on consistent class semantics across episodes. Specifically, Omniglot classes always correspond to alphabet characters, while miniImageNet classes always correspond to object categories as de- fined by the WordNet taxonomy (Miller, 1995; Russakovsky et al., 2015). One consequence is that benchmarks with consistent class semantics have similar class semantics between meta-training and meta-evaluation 1 . Therefore, they are too \"easy\" because they do not test the ability of supervised few-shot classification methods to adapt to new class semantics. From an applications perspective, being able to adapt to changing class semantics is a desirable feature. For instance, if the application is to organize users' personal photo gallery, different users might want to sort their personal photo gallery according to the different semantics, such as person identity, place or time.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an analysis of the latent structure of knowledge graph representation models, drawing a parallel between entity embeddings in knowledge graphs and unsupervised word embeddings. It uses recent understanding of PMI-based word embeddings to derive relation conditions, which describe relation-specific mappings between entity embeddings, and shows that properties of knowledge graph models fit predictions made from relation conditions. The paper also shows that the performance per relation of leading link prediction models corresponds to the ability of the model's architecture to meet the relation conditions of the relation's type. Finally, it provides novel insight into the prediction accuracy per relation of recent knowledge graph models.",
        "Abstract": "Many methods have been developed to represent knowledge graph data, which implicitly exploit low-rank latent structure in the data to encode known information and enable unknown facts to be inferred. To predict whether a relationship holds between entities, their embeddings are typically compared in the latent space following a relation-specific mapping. Whilst link prediction has steadily improved, the latent structure, and hence why such models capture semantic information, remains unexplained. We build on recent theoretical interpretation of word embeddings as a basis to consider an explicit structure for representations of relations between entities. For identifiable relation types, we are able to predict properties and justify the relative performance of leading knowledge graph representation methods, including their often overlooked ability to make independent predictions.",
        "Introduction": "  INTRODUCTION Knowledge graphs are large repositories of binary relations between words (or entities) in the form of fact triples (subject, relation, object). Many models have been developed for learning representations of entities and relations in knowledge graphs, such that known facts can be recalled and previously unknown facts can be inferred, a task known as link prediction. Recent link prediction models (e.g.  Bordes et al., 2013 ;  Trouillon et al., 2016 ;  Balažević et al., 2019b ) learn entity representations, or embeddings, of far lower dimensionality than the number of entities, by capturing latent structure in the data. Relations are typically represented as a mapping from the embedding of a subject entity to its related object entity embedding(s). Although the performance of knowledge graphlink prediction models has steadily improved for nearly a decade, relatively little is understood of the low-rank latent structure that underpins these models, which we address in this work. We start by drawing a parallel between entity embeddings in knowledge graphs and unsupervised word embeddings, as learned by algorithms such as Word2Vec (W2V) ( Mikolov et al., 2013 ) and GloVe ( Pennington et al., 2014 ). We assume that words have latent features, e.g. meaning(s), tense, grammatical type, that are innate and fixed, irrespective of what an embedding may capture (which may be only a part, subject to the embedding method and/or the data source); and that this same latent structure gives rise to patterns observed in the data, e.g. in word co-occurrence statistics and in which words are related to which. As such, an understanding of the latent structure from one embedding task (e.g. word embedding) might be useful to another (e.g. knowledge graph entity embedding). Recent work ( Allen & Hospedales, 2019 ;  Allen et al., 2019 ) theoretically explains how semantic properties are encoded in word embeddings that (approximately) factorise a matrix of word co- occurrence pointwise mutual information (PMI), e.g. as is known for W2V ( Levy & Goldberg, 2014 ). Semantic relationships between words (specifically similarity, relatedness, paraphrase and analogy) are proven to manifest as linear relationships between rows of the PMI matrix (subject to known error terms), of which word embeddings can be considered low-rank projections. This explains why similar words (e.g. synonyms) have similar embeddings; and embeddings of analogous word pairs share a common \"vector offset\". Importantly, this insight allows us to identify geometric relationships between such word embeddings necessary for other semantic relations to hold, such as those of knowledge graphs. These relation conditions describe relation-specific mappings between entity embeddings, i.e. relation representa- tions, providing a \"blue-print\" against which to consider knowledge graph representation models. We find that various properties of knowledge graph representation models, including the relative Under review as a conference paper at ICLR 2020 performance of leading link prediction models, accord with predictions based on these relation conditions, suggesting a commonality to the latent structure learned in word embedding models and knowledge graph representation models, despite the significant differences between their training data and methodology. In summary, the key contributions of this work are: • to use recent understanding of PMI-based word embeddings to derive what a relation represen- tation must achieve to map a subject word embedding to all related object word embeddings (relation conditions), based on which relations can be categorised into three types; • to show that properties of knowledge graph models fit predictions made from relation conditions, e.g. strength of a relation's relatedness aspect is reflected in the eigenvalues of its relation matrix; • to show that the performance per relation of leading link prediction models corresponds to the ability of the model's architecture to meet the relation conditions of the relation's type, i.e. the better the architecture of a knowledge graph representation model aligns with the form theoretically derived for PMI-based word embeddings, the better the model performs; and • noting how ranking metrics can be flawed, to provide novel insight into the prediction accuracy per relation of recent knowledge graph models, an evaluation metric we recommend in future.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents MIER, a meta-reinforcement learning algorithm that is both efficient and capable of extrapolating to out-of-distribution tasks. MIER formulates the meta-adaptation as an MDP identification problem, transforming the meta-RL problem into a supervised meta-learning problem. This allows for the stability and consistency of supervised learning methods to be leveraged, and for the policy to be improved without collecting extra data by relabeling data collected from other tasks. The result is an off-policy meta-RL algorithm that is sample efficient, stable, and capable of extrapolating to out-of-distribution tasks.",
        "Abstract": "Reinforcement learning algorithms can acquire policies for complex tasks automatically, however the number of samples required to learn a diverse set of skills can be prohibitively large. While meta-reinforcement learning has enabled agents to leverage prior experience to adapt quickly to new tasks, the performance of these methods depends crucially on how close the new task is to the previously experienced tasks. Current approaches are either not able to extrapolate well, or can do so at the expense of requiring extremely large amounts of data due to on-policy training. In this work, we present model identification and experience relabeling (MIER), a meta-reinforcement learning algorithm that is both efficient and extrapolates well when faced with out-of-distribution tasks at test time based on a simple insight: we recognize that dynamics models can be adapted efficiently and consistently with off-policy data, even if policies and value functions cannot. These dynamics models can then be used to continue training policies for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task. ",
        "Introduction": "  INTRODUCTION Meta-reinforcement learning algorithms can enable acquisition of new tasks from just a small number of samples by leveraging experience from previous related tasks ( Duan et al., 2016 ;  Wang et al., 2016 ;  Finn et al., 2017 ). However, the performance of these methods on new tasks depends crucially on how close the tasks are to the meta-training task distribution. Meta-learned models can adapt quickly to tasks that are similar to those seen during training, but can lose much of their benefit when adapting to tasks that are too far away from the meta-training distribution. This places a significant burden on the user to carefully construct meta-training task distributions that sufficiently cover the kinds of tasks that may be encountered at test time. Many meta-RL methods either utilize a variant of model-agnostic meta-learning (MAML) ( Finn et al., 2017 ;  Rothfuss et al., 2018 ;  Nagabandi et al., 2018 ), or an inference-based formulation with recur- rent ( Duan et al., 2016 ;  Wang et al., 2016 ), attentional ( Mishra et al., 2017 ), or variational ( Rakelly et al., 2019 ) methods. The latter class of methods generally fails to handle out-of-distribution tasks, be- cause the model cannot adequately deal with out-of-distribution inputs corresponding to the new task. Most of the former class of methods, where gradient-based adaptation corresponds to a well-defined and consistent learning process, require on-policy samples, resulting in high sample complexity for meta-training in order to produce efficient adaptation. In this paper, we make use of a simple insight to develop model identification and experience relabeling (MIER), a meta RL algorithm that is both efficient and which extrapolates effectively when faced with out-of-distribution tasks at meta-test time: we recognize that dynamics and reward models can be adapted consistently, using MAML-style update rules with off-policy data, even if policies and value functions cannot. These models can then be used to train new policies for out-of-distribution tasks without using meta-reinforcement learning at all, by generating synthetic experience for the new task. To be able to quickly learn dynamics models, we reformulate the meta-reinforcement learning problem as one of MDP identification. We use this approach since identifying a task involves determining its transition dynamics and reward function, which is exactly the information that a model of the Under review as a conference paper at ICLR 2020 task needs to represent. Specifically, we use a supervised meta-learning method that optimizes for a dynamics model initialization such that conditioned on a MDP context descriptor, prediction error on a validation batch of data sampled from the task is minimized, after updating only the context descriptor with a few gradient steps. Effective model training requires the validation batch to contain data corresponding to optimal behavior for the tasks, which we obtain by training a universal policy conditioned on the context descriptor. Note that since our formulation ensures that the context descriptor contains sufficient information about the task, the policy does not need to be meta-trained or adapted, and can hence be learned with simple and efficient off-policy RL algorithms, without needing to handle the complexity of meta-reinforcement learning. At test time, given out-of-distribution tasks, the adapted context descriptor would indeed be out of distribution and thus our context-conditioned model and policy might not perform well for the test task. However, since our method uses gradient descent which is a consistent learning method for adaptation, we can continue to improve our model using more gradient updates. We then leverage all of the experience collected from other tasks during meta-training, by using the learned model to relabel the next state and reward information, thus obtaining synthetic data to continue training the policy. Our main contribution is an off-policy meta-RL algorithm that is sample efficient, stable and which extrapolates well to out-of-distribution tasks. By formulating the meta-adaptation as MDP identifica- tion, we are able to transform the meta-RL problem into a supervised meta-learning problem and thus benefit from the stability and consistency of supervised learning methods. The consistency of our model also enables us to continue improving our policy without collecting extra data by relabeling data collected from other tasks, thus allowing us to efficiently adapt to out-of-distribution tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents CoResets for Accelerating Incremental Gradient Descent (CRAIG), a method for selecting a subset of training data points to speed up training of large machine learning models. CRAIG approximates the gradient of the full training data set, and uses a fast greedy algorithm to efficiently find a subset of the data that minimizes an upper-bound on the error of estimating the full gradient. CRAIG induces a natural ordering over data in the subset, and provides theoretical analysis and mathematical convergence guarantees. Experiments demonstrate that CRAIG speeds up incremental gradient methods, including SGD, SAGA, SVRG, Adam, Adagrad, and NAG, by up to 10x for convex and 3x for non-convex loss functions, while achieving practically the same loss and accuracy.",
        "Abstract": "Many machine learning problems reduce to the problem of minimizing an expected risk. Incremental gradient (IG) methods, such as stochastic gradient descent and its variants, have been successfully used to train the largest of machine learning models.  IG methods, however, are in general slow to converge and sensitive to stepsize choices. Therefore, much work has focused on speeding them up by reducing the variance of the estimated gradient or choosing better stepsizes. An alternative strategy would be to select a carefully chosen subset of training data, train only on that subset, and hence speed up optimization. However, it remains an open question how to achieve this, both theoretically as well as practically, while not compromising on the quality of the final model.  Here we develop CRAIG, a method for selecting a weighted subset (or coreset) of training data in order to speed up IG methods. We prove that by greedily selecting a subset S of training data that minimizes the upper-bound on the estimation error of the full gradient, running IG on this subset will converge to the (near)optimal solution in the same number of epochs as running IG on the full data. But because at each epoch the gradients are computed only on the subset S, we obtain a speedup that is inversely proportional to the size of S. Our subset selection algorithm is fully general and can be applied to most IG methods. We further demonstrate practical effectiveness of our algorithm, CRAIG, through an extensive set of experiments on several applications, including logistic regression and deep neural networks. Experiments show that CRAIG, while achieving practically the same loss, speeds up IG methods by up to 10x for convex and 3x for non-convex (deep learning) problems.",
        "Introduction": "  INTRODUCTION Mathematical optimization lies at the core of training large-scale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system's resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to the problem of optimizing a regularized empirical risk function. Given a convex loss l, and a µ-strongly convex regularizer r, one aims to find model parameter vector x * over the parameter space X that minimizes the loss f over the training data V : where V = {1, . . . , n} is an index set of the training data, and functions f i : R d → R are associated with training examples (a i , y i ), where a i ∈ R d is the feature vector, and y i is the point i's label. The standard Gradient Descent can find the minimizer of this problem, but requires repeated com- putations of the full gradient ∇f (x)-sum of the gradients over all training data points/functions i-and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), Under review as a conference paper at ICLR 2020 SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the full training data V , so that the model can only be trained on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle to select S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on an ordering over S and a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop CoResets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to directly approximate the gradient. That is, we aim to find a weighted and ordered subset S of training data V that is representative of the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient of V maximizes a submodular facility location function. As a result, the subset S can be efficiently found using a fast greedy algorithm. A further benefit of our approach is that set S is created incrementally which induces a natural ordering over data in S. Thus, rather than processing data points in a random or arbitrary order, CRAIG processes them using in the order provided by the procedure, which we show further speeds up the convergence of the method. We also provide theoretical analysis of CRAIG and prove the convergence of our approach. In particular, for a µ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most , we prove that IG method with diminishing stepsize α k = α/k τ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2R /µ neighborhood of the optimal solution at rate O(1/ √ k). Here, R = min{d 0 , (rγ max C + )/µ} where d 0 is the initial distance to the optimum, C is an upper-bound on the norm of the gradients, r = |S|, and γ max is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing stepsize on subset S converges to a 2 /µ neighborhood of the optimum solution at rate O(1/k τ ). The above convergence rates are the same as convergence rate of IG on V for a strongly convex risk (and smooth component functions), and therefore IG on S converges in the same number epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. Furthermore, CRAIG only requires the knowledge of estimated gradient differences and does not involve any (exact) gradient calculations. Therefore, CRAIG can be used as a simple preprocessing step before IG starts and no additional storage or gradient calculations are required during IG, which makes CRAIG extremely practical. As such CRAIG can be used to speed up any existing IG methods, including IG, Adam, SAGA, SVRG as we show in the experiments section. We demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regres- sion (a convex optimization problem) as well as training neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, SVRG, Adam, Adagrad, and NAG. In particular, CRAIG while achieving practically the same loss Under review as a conference paper at ICLR 2020 and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 10x for convex and 3x for non-convex loss functions. We also demonstrate that the deliberate ordering scheme of the CRAIG algorithm significantly improves convergence time.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the ability of artificial evolution to produce predictive world models, using the recently introduced world model architecture. We propose a Deep Innovation Protection (DIP) approach to optimize co-evolving heterogeneous neural systems, which is able to find a solution to the VizDoom: Take Cover task. Our results show that DIP allows evolution to carefully orchestrate the training of the components in these heterogeneous architectures, and that the emergent world models learned to predict events important for the survival of the agent, even though they were not explicitly trained to predict the future.",
        "Abstract": "Evolutionary-based optimization approaches have recently shown promising results in domains such as Atari and robot locomotion but less so in solving 3D tasks directly from pixels. This paper presents a method called Deep Innovation Protection (DIP) that allows training  complex world models end-to-end for such 3D environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in a world model, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn a model of the world without the need for a specific forward-prediction loss. ",
        "Introduction": "  INTRODUCTION The ability of the brain to model the world arose from the process of evolution. It evolved because it helped organisms to survive and strive in their particular environments and not because such forward prediction was explicitly optimized for. In contrast to the emergent neural representations in nature, current world model approaches are often directly rewarded for their ability to predict future states of the environment ( Schmidhuber, 1990 ;  Ha & Schmidhuber, 2018 ;  Hafner et al., 2018 ;  Wayne et al., 2018 ). While it is undoubtedly useful to be able to explicitly encourage a model to predict what will happen next, in this paper we are interested in what type of representations can emerge from the less directed process of artificial evolution and what ingredients might be necessary to encourage the emergence of such predictive abilities. In particular, we are building on the recently introduced world model architecture introduced by  Ha & Schmidhuber (2018) . This agent model contains three different components: (1) a visual module, mapping high-dimensional inputs to a lower-dimensional representative code, (2) an LSTM-based memory component, and (3) a controller component that takes input from the visual and memory module to determine the agent's next action. In the original approach, each component of the world model was trained separately and to perform a different and specialised function, such as predict- ing the future. While  Risi & Stanley (2019)  demonstrated that these models can also be trained end-to-end through a population-based genetic algorithm (GA) that exclusively optimizes for final performance, the approach was only applied to the simpler 2D car racing domain and it is an open question how such an approach will scale to the more complex 3D VizDoom task that first validated the effectiveness of the world model approach. Here we show that a simple genetic algorithm fails to find a solution to solving the VizDoom task and ask the question what are the missing ingredients necessary to encourage the evolution of more powerful world models? The main insight in this paper is that we can view the optimization of a heterogeneous neural network (such as world models) as a co-evolving system of multiple different sub-systems. The other important insight is that representational innovations discovered in one sub- system (e.g. the visual system learns to track moving objects) require the other sub-systems to adapt. In fact, if the other systems are not given time to adapt, such innovation will likely initially have an adversarial effect on overall performance! In order to optimize such co-evolving heterogeneous neural systems, we propose to reduce the se- lection pressure on individuals whose visual or memory system was recently changed, given the controller component time to readapt. This Deep Innovation Protection (DIP) approach is inspired by the recently introduced morphological innovation protection method of  Cheney et al. (2018) , which allows for the scalable co-optimization of controllers and robot body plans. Our approach is able to find a solution to the VizDoom: Take Cover task, which was first solved by the original Under review as a conference paper at ICLR 2020 world model approach ( Ha & Schmidhuber, 2018 ). More interestingly, the emergent world models learned to predict events important for the survival of the agent, even though they were not explicitly trained to predict the future. Additionally, our investigates into the training process show that DIP allows evolution to carefully orchestrate the training of the components in these heterogeneous ar- chitectures. We hope this work inspires more research that focuses on investigating representations emerging from approaches that do not necessarily only rely on gradient-based optimization.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper aims to investigate whether limiting the complexity of the extracted representation using the Conditional Entropy Bottleneck (CEB) can make models more robust in terms of test set generalization, worst-case robustness, and typical-case robustness. The paper will experimentally evaluate the effectiveness of this approach in making models more robust.",
        "Abstract": "We  demonstrate  that  the  Conditional  Entropy  Bottleneck  (CEB)  can  improve model robustness.  CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the IMAGENET-C Common Corruptions Benchmark.",
        "Introduction": "  INTRODUCTION We aim to make models that make meaningful predictions beyond the data they were trained on. Generally we want our models to be robust. Broadly, robustness is the ability of a model to continue making valid predictions as the distribution the model is tested on moves away from the empirical training set distribution. The most commonly reported robustness metric is simply test set per- formance, where we verify that our model continues to make valid predictions on what we hope represents valid draws from the exact same data generating procedure. Adversarial robustness tests robustness in a worst case setting, where an attacker (Szegedy et al., 2013) makes limited targeted modifications to the input that are as fooling as possible. Many ad- versarial attacks have been proposed and studied (Szegedy et al., 2013; Carlini & Wagner, 2017b;a; Kurakin et al., 2016a; Madry et al., 2017). Most machine-learned systems are currently believed to be vulnerable to adversarial examples. Many defenses have been proposed, but very few have demonstrated robustness against a powerful, general-purpose adversary (Carlini & Wagner, 2017a; Athalye et al., 2018). While robustness to adversarial attacks continues to attract interest, recent dis- cussions have emphasized the need to consider other forms of robustness as well (Engstrom et al., 2019). The Common Corruptions Benchmark (Hendrycks & Dietterich, 2019) measures image mod- els robustness to more mild but real world sorts of perturbations. Even these modest perturbations can be very fooling for traditional architectures. One of the few general purpose strategies that demonstrably improves model robustness is Data Augmentation (Cubuk et al., 2018; Lopes et al., 2019; Yin et al., 2019). However, it would be nice to identify loss-based solutions that can work in tandem with the data augmentation approaches. Intuitively, by performing modifications of the inputs at training time, the model is prevented from being too sensitive to particular features of the inputs that don't survive the augmentation procedure. Alternatively, we can try to make our models more robust by making them less sensitive to the inputs in the first place. The goal of this work is to experimentally investigate whether, by systematically limiting the complexity of the extracted representation using the Conditional Entropy Bottleneck (CEB), we can make our models more robust in all three of these senses: test set generalization (e.g., classification accuracy on \"clean\" test inputs), worst-case robustness, and typical-case robustness.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to few-shot regression (FSR) in the domain of pharmaceutical R&D, specifically for modelling biological assays relevant in the early stages of drug discovery. We argue that robust FSR algorithms are needed to tackle this challenge, and propose a deep kernel learning (DKL) approach that combines the strengths of metric learning and initialization- and optimization-based methods, while also allowing for the incorporation of domain-specific knowledge when making predictions. We apply our algorithms to modelling specific assays and readouts, and demonstrate improved accuracy and uncertainty estimation compared to existing methods.",
        "Abstract": "Due to the significant costs of data generation, many prediction tasks within drug discovery are by nature few-shot regression (FSR) problems, including accurate modelling of biological assays.  Although a number of few-shot classification and reinforcement learning methods exist for similar applications, we find relatively few FSR methods meeting the performance standards required for such tasks under real-world constraints. Inspired by deep kernel learning, we develop a novel FSR algorithm that is better suited to these settings. Our algorithm consists of learning a deep network in combination with a kernel function and a differentiable kernel algorithm. As the choice of the kernel is critical, our algorithm learns to find the appropriate one for each task during inference. It thus performs more effectively with complex task distributions, outperforming current state-of-the-art algorithms on both toy and novel, real-world benchmarks that we introduce herein. By introducing novel benchmarks derived from biological assays, we hope that the community will progress towards the development of FSR algorithms suitable for use in noisy and uncertain environments such as drug discovery.",
        "Introduction": "  INTRODUCTION Following breakthroughs in domains including computer vision, autonomous driving, and natural language processing, deep learning methods are now entering the domain of pharmaceutical R&D. Recent successes include the deconvolution of biological targets from -omics data (Min et al., 2017), generation of drug-like compounds via de novo molecular design (Xu et al., 2019), chemical synthesis planning (Segler and Waller, 2017; Segler et al., 2017), and multi-modal image analysis for quantification of cellular response (Min et al., 2017). A common characteristic of these applications, however, is the availability of high quality, high quantity training data. Unfortunately, many critical prediction tasks in the drug discovery pipeline fail to satisfy these requirements, in part due to resource and cost constraints (Cherkasov et al., 2014). We therefore focus this work on modelling biological assays (bio-assays) relevant in the early stages of drug discovery, primarily binding and cellular readouts. Under the constraints of an active drug discovery program, the data from these assays, consisting of libraries of molecules and their associated real-valued activity scores, is often relatively small and noisy (refer to statistics in Section 5). In many contexts, it can be challenging to build a training set of even a few dozen samples per individual assay. Modelling an assay is thus best viewed as a few-shot regression (FSR) problem, with many variables (including experimental conditions, readouts, concentrations, and instrument configurations) accounting for the data distribution being generated. Practically, these variables make it infeasible to compare data collected across different assays, thereby making it difficult to learn predictive models from molecular structures. Furthermore, as bio-assay modelling is intended to be used for prioritizing molecules for subsequent evaluation (e.g. Bayesian optimization) and efficiently exploring the overall chemical space (e.g. active learning), accurate prediction and uncertainty estimation using few datapoints is critical to successful application in drug discovery. It is our view that robust FSR algorithms are needed to tackle this challenge. Specifically, we argue that these algorithms should remain accurate in noisy environments, and also provide well-calibrated uncertainty estimates to inform efficient exploration of chemical space during molecular optimization. Fortunately, recent advances in few-shot learning have led to new algorithms that learn efficiently and generalize adequately from small training data (Wang and Yao, 2019; Chen et al., 2019). Most Under review as a conference paper at ICLR 2020 have adopted the meta-learning paradigm (Thrun and Pratt, 1998; Vilalta and Drissi, 2002), where some prior knowledge is learned across a large collection of tasks and then transferred to new tasks in which there are limited amounts of data. Such algorithms tend to differ in two aspects: the nature of the meta-knowledge captured and the amount of adaptation performed at test-time for new tasks or datasets. The meta-knowledge refers to the domain specific prior needed to solve each task most effectively. Due to the size of the total chemical space accessible when modelling bio-assays (Bohacek et al., 1996), there is a particular need for the meta-knowledge to be sufficiently rich so as to allow for extrapolation and uncertainty estimation in unseen regions of chemical space at test-time (i.e. for new tasks). Given that the same molecule can behave differently across different assays, greater test-time adaptation is also required and must be accounted for during modelling. In previous work, metric learning methods (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Garcia and Bruna, 2017; Bertinetto et al., 2018) accumulate meta-knowledge in high capacity covariance/distance functions and use simple base-learners such as k-nearest neighbor (Snell et al., 2017; Vinyals et al., 2016) or low capacity neural networks (Garcia and Bruna, 2017) to produce adequate models for new tasks. However, they do not adapt the covariance functions nor the base- learners at test-time. Initialization- and optimization-based methods (Finn et al., 2017; Kim et al., 2018; Ravi and Larochelle, 2016) that learn the initialization points and update rules for gradient descent-based algorithms, respectively, allow for improved adaptation on new tasks but remain time consuming and memory inefficient. We therefore argue that to ensure optimal performance when modelling bio-assays, it is crucial to combine the strengths of both types of methods while also allowing for the incorporation of domain-specific knowledge when making predictions. We achieve this by framing FSR as a deep kernel learning (DKL) task, deriving novel algorithms that we apply to modelling specific assays and readouts.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new Deep Reinforcement Learning (DRL) algorithm, Deep Randomized Least Squares Value Iteration (DRLSVI), which combines the exploration mechanism of Randomized Least Squares Value Iteration (RLSVI) with the representation learning mechanism of Deep Q-Networks (DQN). DRLSVI uses standard DQN to learn state representation and explores by using the last layer's activations of DQN as state representation for RLSVI. The algorithm is evaluated on a toy-problem and several Atari benchmarks, and is shown to outperform DQN in both learning speed and performance.",
        "Abstract": "Exploration while learning representations is one of the main challenges Deep\nReinforcement Learning (DRL) faces today. As the learned representation is dependant in the observed data, the exploration strategy has a crucial role. The popular DQN algorithm has improved significantly the capabilities of Reinforcement\nLearning (RL) algorithms to learn state representations from raw data, yet, it uses\na naive exploration strategy which is statistically inefficient. The Randomized\nLeast Squares Value Iteration (RLSVI) algorithm (Osband et al., 2016), on the\nother hand, explores and generalizes efficiently via linearly parameterized value\nfunctions. However, it is based on hand-designed state representation that requires\nprior engineering work for every environment. In this paper, we propose a Deep\nLearning adaptation for RLSVI. Rather than using hand-design state representation, we use a state representation that is being learned directly from the data by a\nDQN agent. As the representation is being optimized during the learning process,\na key component for the suggested method is a likelihood matching mechanism,\nwhich adapts to the changing representations. We demonstrate the importance of\nthe various properties of our algorithm on a toy problem and show that our method\noutperforms DQN in five Atari benchmarks, reaching competitive results with the\nRainbow algorithm.",
        "Introduction": "  INTRODUCTION In Reinforcement Learning (RL), an agent seeks to maximize the cumulative rewards obtained from interactions with an unknown environment ( Sutton et al., 1998 ). Since the agent can learn only by its interactions with the environment, it faces the exploration-exploitation dilemma: Should it take actions that will maximize the rewards based on its current knowledge or instead take actions to potentially improve its knowledge in the hope of achieving better future performance. Thus, to find the optimal policy the agent needs to use an appropriate exploration strategy. Classic RL algorithms were designed to face problems in the tabular settings where a table con- taining a value for each state-action pair can be stored in the computer's memory. For more general settings, where generalization is required, a common practice is to use hand-designed state represen- tation (or state-action), upon which a function approximation can be learned to represent the value for each state and action. RL algorithms based on linear function approximation have demonstrated stability, data efficiency and enjoys convergence guarantees under mild assumptions ( Tsitsiklis & Van Roy, 1997 ;  Lagoudakis & Parr, 2003 ). They require that the desired learned function, e.g. Q- function, will be a linear combination of the state representation. This is, of course, a hard constraint as the representation is hand-designed, where the designer often does not know how the optimal value-function will look like. Furthermore, hand-designed representation is environment-specific and requires re-designing for every new environment. The DQN algorithm ( Mnih et al., 2015 ) has changed RL. Using Deep Neural Networks (DNN) as function approximators, the DQN algorithm enabled the learning of policies directly from raw high- dimensional data and led to unprecedented achievements over a wide variety of domains ( Mnih et al., 2015 ). Over the years, many improvements to DQN were presented, suggesting more fitting network architectures ( Wang et al., 2015 ), reducing overestimation (Van Hasselt et al., 2016;  Anschel et al., 2017 ) or improving its data efficiency ( Schaul et al., 2015 ). Despite its great success, DQN uses the overly simple -greedy strategy for exploration. This strategy is one of the simplest exploration Under review as a conference paper at ICLR 2020 strategies that currently exist. The agent takes random action with probability and takes the optimal action according to its current belief with probability 1 − . This strategy is commonly used despite its simplicity and proven inefficiency ( Osband et al., 2016 ). The main shortcoming of -greedy and similar strategies derives from the fact that they do not use observed data to improve exploration. To explore, it takes a completely random action, regardless of the experience obtained by the agent. Thompson Sampling (TS) ( Thompson, 1933 ), is one of the oldest heuristics to address the 'explo- ration/exploitation' trade-off in sequential decision-making problems. Its variations were proposed in RL ( Wyatt, 1998 ;  Strens, 2000 ) and various bandits settings ( Chapelle & Li, 2011 ;  Scott, 2010 ). For Multi-Armed Bandit (MAB) problems, TS is very effective both in theory ( Agrawal & Goyal, 2012 ; 2013) and practice ( Chapelle & Li, 2011 ). Intuitively, TS randomly takes actions according to the probability it believes to be optimal. In practice, a prior distribution is assumed over the model's parameters p(w), and a posterior distribution p(w|D) is computed using the Bayes theorem, where D is the observed data. TS acts by sampling models from the posterior distribution, and plays the best action according to these samples. Randomized Least Squares Value Iteration ( Osband et al., 2016 ) is an RL algorithm which uses lin- ear function approximation and is inspired by Thompson Sampling. It explores by sampling plau- sible Q-functions from uncertainty sets and selecting the action that optimizes the sampled models. This algorithm was proven to be efficient in tabular settings, with a bound on the expected regret that match the worst-case lower bound up to logarithmic factors. More importantly, it demonstrates efficiency even when generalization is required. Alas, as it assumes a linearly parametrized value function on a hand-designed state representation, the success of this algorithm crucially depends on the quality of the given state representation. In this paper, we present a new DRL algorithm that combines the exploration mechanism of RLSVI with the representation learning mechanism of DQN; we call it the Deep Randomized Least Squares Value Iteration (DRLSVI) algorithm. We use standard DQN to learn state representation and ex- plores by using the last layer's activations of DQN as state representation for RLSVI. To compen- sate for the constantly changing representation and the finite memory of DQN, we use a likelihood matching mechanism, which allows the transfer of information held by an old representation regard- ing past experience. We evaluate our method on a toy-problem - the Augmented Chain environment - for a qualitative evaluation of our method on a small MDP with a known optimal value function. Then, we compare our algorithm to the DQN and Rainbow algorithms on several Atari benchmarks. We show that it outperforms DQN both in learning speed and performance.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces Localised Generative Flows (LGFs), a new flow-based generative model for density estimation. LGFs are an extension of normalising flows, which model the target density as the marginal of a prior density obtained by a generative process. LGFs replace the single bijection with stacked continuous mixtures of bijections, allowing each bijection to focus on modelling only a local component of the target. We show empirically that LGFs outperform normalising flows across a variety of density estimation tasks.",
        "Abstract": "We argue that flow-based density models based on continuous bijections are limited in their ability to learn target distributions with complicated topologies, and propose localised generative flows (LGFs) to address this problem. LGFs are composed of stacked continuous mixtures of bijections, which enables each bijection to learn a local region of the target rather than its entirety. Our method is a generalisation of existing flow-based methods, which can be used without modification as the basis for an LGF model. Unlike normalising flows, LGFs do not permit exact computation of log likelihoods, but we propose a simple variational scheme that performs well in practice. We show empirically that LGFs yield improved performance across a variety of common density estimation tasks.",
        "Introduction": "  INTRODUCTION Flow-based generative models, often referred to as normalising flows, have become popular methods for density estimation because of their flexibility, expressiveness, and tractable likelihoods. Given the problem of learning an unknown target density p X on a data space X , normalising flows model p X as the marginal of X obtained by the generative process Z ∼ p Z , X := g −1 (Z), (1) where p Z is a prior density on a space Z, and g : X → Z is a bijection. 1 Assuming sufficient regularity, it follows that X has density p X (x) = p Z (g(x))| det Dg(x)| (see e.g.  Billingsley (2008) ). The parameters of g can be learned via maximum likelihood given i.i.d. samples from p X . To be effective, a normalising flow model must specify an expressive family of bijections with tractable Jacobians. Affine coupling layers ( Dinh et al., 2014 ; 2016), autoregressive transformations ( Germain et al., 2015 ;  Papamakarios et al., 2017 ), ODE-based transformations ( Grathwohl et al., 2018 ), and invertible ResNet blocks ( Behrmann et al., 2019 ) are all examples of such bijections that can be composed to produce complicated flows. These models have demonstrated significant promise in their ability to model complex datasets ( Papamakarios et al., 2017 ) and to synthesise novel data points ( Kingma & Dhariwal, 2018 ). However, in all these cases, g is continuous in x. We believe this is a significant limitation of these models since it imposes a global constraint on g −1 , which must learn to match the topology of Z, which is usually quite simple, to the topology of X , which we expect to be very complicated. We ar- gue that this constraint makes maximum likelihood estimation extremely difficult in general, leading to training instabilities and erroneous regions of high likelihood in the learned density landscape. To address this problem we introduce localised generative flows (LGFs), which generalise equation 1 by replacing the single bijection g with stacked continuous mixtures of bijections {G(·; u)} u∈U for an index set U. Intuitively, LGFs allow each G(·; u) to focus on modelling only a local component of the target that may have a much simpler topology than the full density. LGFs do not stipulate the form of G, and indeed any standard choice of g can be used as the basis of its definition. We pay a price for these benefits in that we can no longer compute the likelihood of our model exactly and must instead resort to a variational approximation, with our training objective replaced by the evidence lower bound (ELBO). However, in practice we find this is not a significant limitation, as the bijective structure of LGFs permits learning a high-quality variational distribution suitable for large-scale training. We show empirically that LGFs outperform their counterpart normalising flows across a variety of density estimation tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a theoretical analysis of the adaptive algorithm ADAM for nonconvex optimization, under a bounded step size assumption. This analysis follows the work of Reddi et al. (2018), which exhibited a simple convex stochastic optimization problem over a compact set where ADAM fails to converge due to its short-term gradient memory. The paper reviews existing variants of ADAM to overcome its convergence issues.",
        "Abstract": "Although Adam is a very popular algorithm for optimizing the weights of neural networks, it has been recently shown that it can diverge even in simple convex optimization examples. Therefore, several variants of Adam have been proposed to circumvent this convergence issue. In this work, we study the algorithm for smooth nonconvex optimization under a boundedness assumption on the adaptive learning rate. The bound on the adaptive step size depends on the Lipschitz constant of the gradient of the objective function and provides safe theoretical adaptive step sizes. Under this boundedness assumption, we show a novel first order convergence rate result in both deterministic and stochastic contexts. Furthermore, we establish convergence rates of the function value sequence using the Kurdyka-Lojasiewicz property.",
        "Introduction": "  INTRODUCTION Consider the unconstrained optimization problem min x∈R d f (x), (1) where f : R d → R is a differentiable map and d is an integer. Gradient descent is one of the most classical algorithms to solve this problem. Since the seminal work Robbins and Monro (1951), its stochastic counterpart became one of the most popular algorithms to solve machine learning problems (see  Bottou et al. (2018)  for a recent survey). Recently, a class of algorithms called adap- tive algorithms which are variants of stochastic gradient descent became very popular in machine learning applications. Using a coordinate-wise step size computed using past gradient information, the step size is adapted to the function to optimize and does not follow a predetermined step size schedule. Among these adaptive algorithms, ADAM ( Kingma and Ba, 2015 ) is very popular for op- timizing the weights of neural networks. However, recently,  Reddi et al. (2018)  exhibited a simple convex stochastic optimization problem over a compact set where ADAM fails to converge because of its short-term gradient memory. Moreover, they proposed an algorithm called AMSGRAD to fix the convergence issue of ADAM . This work opened the way to the emergence of other variants of ADAM to overcome its convergence issues (see Section 3 for a detailed review). In this work, under a bounded step size assumption, we propose a theoretical analysis of ADAM for nonconvex optimization.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper evaluates the existing and widely used batch normalization and layer normalization in the context of off-policy TD learning methods. Results improve only little over those without normalization and often are substantially worse. A new feature normalization scheme - cross-normalization - is introduced which normalizes features based on a combination of two datasets. Cross-normalization consistently improves performance over the target network baseline, and is demonstrated for two popular, state-of-the-art off-policy learning methods: DDPG and TD3. Additionally, the paper empirically evaluates stability for cross-normalization in the context of linear function approximators.",
        "Abstract": "Off-policy temporal difference (TD) methods are a powerful class of reinforcement learning (RL) algorithms. Intriguingly, deep off-policy TD algorithms are not commonly used in combination with feature normalization techniques, despite positive effects of normalization in other domains. We show that naive application of existing normalization techniques is indeed not effective, but that well-designed normalization improves optimization stability and removes the necessity of target networks. In particular, we introduce a normalization based on a mixture of on- and off-policy transitions, which we call cross-normalization. It can be regarded as an extension of batch normalization that re-centers data for two different distributions, as present in off-policy learning. Applied to DDPG and TD3, cross-normalization improves over the state of the art across a range of MuJoCo benchmark tasks.\n",
        "Introduction": "  INTRODUCTION Data and feature normalization are well established techniques in supervised learning that reduce training times and increase the performance of deep networks ( LeCun et al., 1998 ; Ioffe & Szegedy, 2015). Intriguingly, normalization is not very common in deep reinforcement learning. In this paper, we first evaluate the existing and widely used batch normalization and layer normaliza- tion in the context of off-policy TD learning methods. Results improve only little over those without normalization and often are substantially worse. This is surprising, since according to experience in supervised learning, normalization should improve stability. In deep off-policy TD learning, rather target networks have been the crucial part to stabilize optimization ( Mnih et al., 2015 ;  Lillicrap et al., 2016 ). Interestingly, we find that layer normalization allows us to remove target networks completely and still ensure stable training. Nevertheless, the performance with layer normalization is on average inferior to the variant with target networks. In contrast to supervised learning, in off-policy TD learning there is not a single data distribution, but two distributions: one due to actions in off-policy transitions, and one due to actions proposed by the current policy. Consequently, we introduce a new feature normalization scheme - cross-normalization. It is an adaptation of batch normalization that normalizes features based on a combination of these two datasets. Reliable statistics for the normalization are ensured by computing the running sufficient statistics. In contrast to previous normalization approaches, cross-normalization consistently improves performance over the target network baseline. Learning is faster and empirically stable without the use of target networks. We demonstrate these effects for two popular, state-of-the-art off-policy learning methods: DDPG ( Lil- licrap et al., 2016 ) and TD3 ( Fujimoto et al., 2018 ). Adding cross-normalization to both methods consistently improves their performance on multiple MuJoCo benchmarks. The paper also empirically evaluates stability for cross-normalization in the context of linear function approximators. This study on simpler problems gives some intuitive insights how normalization by mean subtraction can help stabilize divergent problems.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to Unsupervised Progressive Learning (UPL), a challenging problem in which an agent observes a sequence of unlabeled data vectors and is occasionally given a small number of labeled examples of one or more classes. The goal is to learn salient representations of the unlabeled input stream so that the agent can, at any point in time, classify a given set of test data based on the set of classes it knows about so far. We consider scenarios in which the distribution associated with each class may also change with time, and propose an online learner that can track changes in the distribution. We evaluate our approach on several datasets and show that it can successfully learn to classify data from a growing set of classes.",
        "Abstract": "We  first pose  the  Unsupervised  Progressive  Learning  (UPL)  problem:   learning salient representations from a non-stationary stream of unlabeled data in which the number of object classes increases with time.  If some limited labeled data is also available, those representations can be associated with specific classes, thus enabling classification tasks.  To solve the UPL problem, we propose an architecture that involves an online clustering module, called Self-Taught Associative Memory (STAM). Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical representations rather than specific examples. The goal of this paper is to introduce the UPL problem, describe the STAM architecture, and evaluate the latter in the UPL context. ",
        "Introduction": "  INTRODUCTION We start by posing a challenging problem, referred to as Unsupervised Progressive Learning (UPL) (see  Figure 1 ). In the UPL problem, the agent observes a sequence (or stream) of unlabeled data vectors {x t } t∈N with x t ∈ R n . Each vector x t is associated with a class k(x t ) and the vectors of class k follow a distribution F k . The class information, however, is hidden from the agent. Occasionally, the agent may be given a small number of labeled examples of one or more classes. These examples are meant to associate \"names\" (i.e., class labels) with the learned representations enabling classification tasks in which the set of output classes stays constant (\"persistent tasks\") or increases (\"expanding tasks\"). We denote as L t the set of class labels the agent has seen up to time t. This set is gradually increasing, meaning that the agent progressively learns about more classes. In the UPL context, the goal is to learn in an online manner salient representations of the unlabeled input stream so that the agent can, at any point in time t, classify a given set of test data based on the set of classes L t it knows about so far. We require an online learner for pragmatic reasons: it would not be possible or desirable in practice to store and/or process all previously seen data and learn a new model offline every time there is a change in L t . The online nature of the problem constraints the solution space: methods that require multiple passes over the training data and/or randomly sampled minibatches are not applicable in the UPL context. We assume that the distribution F k associated with class k may also change with time - but this is a slow and gradual process so that an online learner can track changes in F k . Abrupt changes would require that the agent forgets what was previously learned about class k - we do not consider that possibility in this paper. We do not add any further constraints on the structure of the data sequence. For instance, it is possi- ble that the learner first observes a labeled example of class k at time t (and so k ∈ L t ) even though it has not seen any unlabeled examples of that class prior to t - this would require a transfer-learning capability so that the learner can classify k based on representations it has previously learned from other classes. Another interesting scenario is when the unlabeled data arrive in separated class phases, which are unknown to the agent, so that each phase includes data from only few new classes - this is a challenging task from the perspective of catastrophic forgetting because the learner should not forget previously learned classes for which it does not see any new examples. We consider such UPL scenarios in Section 3. It is plausible that UPL represents how animals learn, at least in the case of perceptual learning ( Goldstone, 1998 ): they observe their environment, which is predominantly \"unlabeled\", and so they learn to gradually distinguish between a growing number of different object categories even when they do not have a way yet to name them. Later, some of those classes may be associated with words (in the case of humans) ( Ashby and Maddox, 2005 ), or more generally, with a specific taste, odor, reward, fear, etc. ( Watanabe et al., 2001 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a deep learning approach for improving the accuracy of scene image analysis by leveraging semantic coherence between neighboring tiles. A convolutional neural network (CNN) architecture is proposed for this purpose, and empirical results demonstrate the viability of this approach on a disease density estimation task. Additionally, the paper discusses the limitations of the synthetic minority oversampling technique (SMOTE) when used for oversampling an under-represented class.",
        "Abstract": "High intra-class diversity and inter-class similarity is a characteristic of remote sensing scene image data sets currently posing significant difficulty for deep learning algorithms on classification tasks. To improve accuracy, post-classification\nmethods have been proposed for smoothing results of model predictions. However, those approaches require an additional neural network to perform the smoothing operation, which adds overhead to the task. We propose an approach that involves learning deep features directly over neighboring scene images without requiring use of a cleanup model. Our approach utilizes a siamese network to improve the discriminative power of convolutional neural networks on a pair\nof neighboring scene images. It then exploits semantic coherence between this pair to enrich the feature vector of the image for which we want to predict a label.\nEmpirical results show that this approach provides a viable alternative to existing methods. For example, our model improved prediction accuracy by 1 percentage point and dropped the mean squared error value by 0.02 over the baseline, on a disease density estimation task. These performance gains are comparable with results from existing post-classification methods, moreover without implementation overheads.",
        "Introduction": "  INTRODUCTION Remote sensing scene image analysis is emerging as an important area of research for application of deep learning algorithms. Application areas include land-use land-cover analysis, urban planning, and natural disaster detection. A deep learning task for labeling a scene image is typically formulated as conditional probability of the form in Eq. 1  Liu et al. (2019) , Albert et al. (2017) ,  Nogueira et al. (2016) ,  Castelluccio et al. (2015) ,  Mnih (2013) ,  Mnih & Hinton (2010) , P (l i |s i ) (1) where l i is label for image patch s i . This formulation is sufficient for problems where spatial sit- uatedness of a scene, which embodies knowledge of semantic likeness between neighborhoods in the geophysical world, is not important. However, for problems which require knowledge of neigh- borhood the formulation in Eq. 1 becomes inadequate. An example of such a problem would be estimating disease density for a small geographical region of interest, in which case the probability of label l is likely to depend on the labels for neighboring regions due to semantic coherence among them. The problem of how to improve model prediction by leveraging semantic coherence among neigh- boring scene images has previously been considered in the literature. Previous studies consider the problem as a post-classification task. For example,  Bischof et al. (1992)  used a second classifier to do pixel smoothing to refine predictions made by another classifier. Based on a 5x5 window, a filter assigns pixels to the majority class if it had been assigned a different class. In  Mnih (2013) , a post-processing architecture is suggested for incorporating structure into image patch prediction. It involves stacking neural networks (NN) such that the output from a previous one becomes input for the next. Idea is for each network to clean up predictions of previous one in order to progressively Under review as a conference paper at ICLR 2020 improve overall accuracy. While improved model performance was achieved by these methods, they have overhead of performing same classification task in at least two stages. In other words, you need a minimum of two NN to perform the same classification task. Unlike post-classification methods, this work considers the problem of improving model accuracy on scene images by exploiting knowledge of neighboring scenes as part of the model training process. We make the assumption that l is conditionally co-dependent on information embedded in scene image i and in other similar, neighboring image j such that the problem is formulated as probability of the form in Eq. 2, P (l i |S i , S j ) = Π m i=1 P (l i |s i , s j ) (2) where s j is image for a neighboring tile that is most similar to index tile i and P (l i |S i , S j ) is observed probability distribution. We used Convolutional Neural Networks (CNN) for modeling the observed probability distribution in Eq. 2. A network architecture is proposed for training our model consisting of four components: a siamese sub-network, a similarity metric learning component, a convolutional network, and a decision layer. The siamese sub-network takes two neighboring scene images as input and extracts features from each. The similarity learning component evaluates how similar the input images are, using the extracted features. If the two input images are found to be similar the convolutional network learns additional features based on the merged feature vector, otherwise those from the index tile are used alone. We implemented the decision layer to perform classification or regression. A baseline model was implemented that takes a single image, the index tile i, as input. Empirical results show the proposed model consistently outperforms the baseline. In addition to improving predictive performance with a relatively small training set, our model is fast to train since it uses a pre-trained model for the siamese sub-network. Furthermore, it does not require another NN to smooth out its predictions as is the case with post-classification approaches, while achieving comparable performance gain. In summary,our contributions include the following. 1. We propose an approach for training a probabilistic deep learning model to improve predic- tion accuracy by exploiting semantic coherence between neighboring tiles in aerial scene images. A CNN architecture is suggested for this purpose. 2. We provide empirical evidence that demonstrates the viability of this approach on a disease density estimation task. 3. Lastly, we discovered an important limitation of the synthetic minority over-sampling tech- nique (SMOTE). This method fails when used for oversampling an under-represented class whereby knowledge of spatial proximity between scene image data points must be pre- served, an important requirement under the framework of learning deep features over neigh- boring scene images introduced in this work.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper studies the empirical success of Maximum Entropy Reinforcement Learning (MaxEnt RL) algorithms, which modify the Reinforcement Learning (RL) objective to add an entropy term. We show that MaxEnt RL provides the optimal control solution in settings with uncertainty and variability in the reward function, and is equivalent to two more challenging problems: (1) regret minimization in a meta-POMDP, and (2) robust-reward control. Our analysis suggests that the empirical benefits of MaxEnt RL arise by implicitly solving control problems with variability in the reward.",
        "Abstract": "Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods.",
        "Introduction": "  INTRODUCTION Reinforcement learning (RL) searches for a policy that maximizes the expected, cumulative reward. In fully observed Markov decision processes (MDPs), this maximization always has a deterministic policy as a solution. Maximum entropy reinforcement learning (MaxEnt RL) is a modification of the RL objective that further adds an entropy term to the objective. This additional entropy term causes MaxEnt RL to seek policies that (1) are stochastic, and (2) have non-zero probability of sampling every action. MaxEnt RL can equivalently be viewed as probability matching between trajectories visited by the policy and a distribution defined by exponentiating the reward (See Section 2). MaxEnt RL has appealing connections to probabilistic inference ( Dayan & Hinton, 1997 ;  Neumann et al., 2011 ;  Todorov, 2007 ;  Kappen, 2005 ;  Toussaint, 2009 ;  Rawlik et al., 2013 ;  Theodorou et al., 2010 ;  Ziebart, 2010 ), prompting a renewed interest in recent years ( Haarnoja et al., 2018b ;  Abdolmaleki et al., 2018 ;  Levine, 2018 ). MaxEnt RL can also be viewed as using Thompson sampling ( Thompson, 1933 ) to collect trajectories, where the posterior belief is given by the exponentiated return. Empirically, MaxEnt RL algorithms achieve good performance on a number of simulated ( Haarnoja et al., 2018b ) and real-world ( Haarnoja et al., 2018a ;  Singh et al., 2019 ) control tasks, and can be more robust to perturbations ( Haarnoja et al., 2018c ). There is empirical evidence that behavior similar MaxEnt RL is used by animals in the natural world. While standard reinforcement learning is often used as a model for decision making ( Scott, 2004 ;  Liu & Todorov, 2007 ;  Todorov & Jordan, 2002 ), many animals, including humans, do not consistently make decisions that maximize expected utility. Rather, they engage in probability matching, choosing actions with probability proportional to how much utility that action will provide. Examples include ants ( Lamb & Ollason, 1993 ), bees ( Greggers & Menzel, 1993 ), fish ( Bitterman et al., 1958 ), ducks ( Harper, 1982 ), pigeons ( Bullock & Bitterman, 1962 ;  Graf et al., 1964 ), and humans, where it has been documented so extensively that  Vulkan (2000)  wrote a survey of surveys of the field. This effect has been observed not just in individuals, but also in the collective behavior of groups of animals (see  Stephens & Krebs (1986) ), where it is often described as obtaining the ideal free distribution. Probability matching is not merely a reflection of youth or ignorance. Empirically, more intelligent creatures are more likely to engage in probability matching. For example, in a comparison of Yale students and rats,  Gallistel (1990)  found that the students nearly always performed probability Under review as a conference paper at ICLR 2020 matching, while rats almost always chose the maximizing strategy. Similarly, older children and adults engage in probability matching more frequently than young children ( Stevenson & Odom, 1964 ;  Weir, 1964 ). While prior work has offered a number of explanations of probability matching ( Vulkan, 2000 ;  Gaissmaier & Schooler, 2008 ;  Wozny et al., 2010 ;  Sakai & Fukai, 2008 ), its root cause remains an open problem. The empirical success of MaxEnt RL algorithms on RL problems is surprising, as MaxEnt RL optimizes a different objective than standard RL. The solution to every MaxEnt RL problem is stochastic, while deterministic policies can always be used to solve standard RL problems ( Puterman, 2014 ). While RL can be motivated from the axioms of utility theory ( Russell & Norvig, 2016 ), MaxEnt RL has no such fundamental motivation. It remains an open question as to whether the standard MaxEnt RL objective actually optimizes some well-defined notion of risk or regret that would account for its observed empirical benefits. This paper studies this problem, and aims to answer the following question: if MaxEnt RL is the solution, then what is the problem? Answering this question is a first step towards understanding the empirical success of MaxEnt RL algorithms, and our analysis will suggest that MaxEnt RL might be applicable to problems typically considered to be much more complex than standard RL. In this paper, we show that MaxEnt RL provides the optimal control solution in settings with uncertainty and variability in the reward function. More precisely, we show that MaxEnt RL is equivalent to two more challenging problems: (1) regret minimization in a meta-POMDP, and (2) robust-reward control. The first setting, the meta-POMDP, is a partially observed MDP where the reward depends on an unobserved portion of the state, and where multiple episodes in the original MDP correspond to a single extended trial in the meta-POMDP. While seemingly Byzantine, this type of problem setting arises in a number of real-world settings discussed in Section 3. Optimal policies for the meta-POMDP must explore at test-time, behavior that cannot result from maximizing expected utility. In the second setting, robust-reward control, we consider an adversary that chooses some aspects of the reward function. Intuitively, we expect stochastic policies to be most robust because they are harder to exploit, as we formalize in Section 5. Even if the agent will eventually be deployed in a setting without adversaries, the adversarial objective bounds the worst-case performance of that agent. Our result in this setting can be viewed as an extension of prior work connecting the principle of maximum entropy to two-player games ( Ziebart et al., 2011 ;  Grünwald et al., 2004 ). While both robust-reward control and regret minimization in a meta-POMDP are natural problems that arise in many real-world scenarios, neither is an expected utility maximization problem, so we cannot expected optimal control to solve these problems. In contrast, we show that MaxEnt RL provides solutions to both. In summary, our analysis suggests that the empirical benefits of MaxEnt RL arise by implicitly solving control problems with variability in the reward.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Generative Adversarial Networks (GANs) are a class of generative models based on a competitive game between a generator and a discriminator. The generator attempts to generate realistic new data, while the discriminator attempts to distinguish generated from real data. Both players are parameterized by neural networks that are trained simultaneously by a variant of stochastic gradient descent (SGD). The loss function of the generator is given by the binary cross entropy.",
        "Abstract": "Generative adversarial networks (GANs) are capable of producing high quality samples, but they suffer from numerous issues such as instability and mode collapse during training. To combat this, we propose to model the generator and discriminator as agents acting under local information, uncertainty, and awareness of their opponent. By doing so we achieve stable convergence, even when the underlying game has no  Nash equilibria. We call this mechanism \\emph{implicit competitive regularization} (ICR) and show that it is present in the recently proposed \\emph{competitive gradient descent} (CGD).\nWhen comparing CGD to Adam using a variety of loss functions and regularizers on CIFAR10, CGD shows a much more consistent performance, which we attribute to ICR.\nIn our experiments, we achieve the highest inception score when using the WGAN loss (without gradient penalty or weight clipping) together with CGD. This can be interpreted as minimizing a form of integral probability metric based on ICR.",
        "Introduction": "  INTRODUCTION Generative adversarial networks (GANs): ( Goodfellow et al., 2014 ) are a class of generative models based on a competitive game between a generator that tries to generate realistic new data, and a discriminator, that tries to distinguish generated from real data. In the original formulation, the two players are playing a zero-sum game with the loss function of the generator given by the binary cross entropy, Here, G is the probability distribution generated by the generator, D is the classifier provided by the discriminator, and P data is the target measure, for example the empirical distribution of the training data. In practice, both players are parameterized by neural networks that are trained simultaneously by a variant of stochastic gradient descent (SGD).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the use of convolutional neural networks (CNNs) in image processing. It discusses the Neocognitron model, which is based on the principle of local connections in a hierarchical architecture, and the use of kernels to process the input. It also examines the use of spatial subsampling to improve shift invariance. The paper further examines the number of filters required at each layer, and the by-pyramidal architecture used in models such as LeNet, VGG, ResNet, MobileNet, and NASNet. It is suggested that the reason for the progressive increase in the number of kernels is to compensate for a possible loss of representation caused by the spatial resolution reduction, or to improve performance by keeping a constant number of operations in each layer.",
        "Abstract": "Automatic neural network discovery methods face an enormous challenge caused for the size of the search space. A common practice is to split this space at different levels and to explore only a part of it. Neural architecture search  methods look for how to combine a subset of layers, which are the most promising, to create an architecture while keeping a predefined number of filters in each layer. On the other hand, pruning techniques take a well known architecture and look for the appropriate number of filters per layer. In both cases the exploration is made iteratively, training models several times during the search. Inspired by the advantages of the two previous approaches, we proposed a fast option to find models with improved characteristics. We apply a small set of templates, which are considered promising, for make a redistribution of the number of filters in an already existing neural network. When compared to the initial base models, we found that the resulting architectures, trained from scratch, surpass the original accuracy even after been reduced to fit the same amount of resources.",
        "Introduction": "  INTRODUCTION Convolutional neural networks are built by stacking layers of neurons following the principle ex- plained by Fukushima's Neocognitron model ( Fukushima, 1980 ). The Neocognitron design made neural networks invariant to shift in feature locations by arranging cells locally connected in a hier- archical architecture. Instead of connecting every neuron of the previous layer to all the neurons in the next layer, convo- lutional networks connections are only made to a small region. Given that different regions share the same weights, the layer can be implemented as a convolution operation using the set of shared weights known as kernel. The complete input is processed by shifting the kernel at uniform steps, normally overlapping parts of the input. To improve shift invariance, convolutional networks needs to rely less in the exact position of a feature and a simple solution is to have a lower resolution per- formed by averaging the values of neighbouring points in the image in a operation known as spatial subsampling. An important consideration to create a convolutional network model is the number of filters, required at every layer. The Neocognitron implementation for example, keeps a fixed number of filters for each layer in the model. A very common practice has been to use a by-pyramidal architecture. The number of filters across the different layers is usually increased as the size of the feature maps de- crease. This pattern was first proposed by  LeCun et al. (1998)  with the introduction of LeNet and can be observed in a diverse set of models such as VGG, ResNet and MobileNet (See Figure 5 in Ap- pendix). Even models obtained from automatic model discovery, like NASNet, follow this principle inasmuch as neural network methods are mainly formulated to search for layers and connections. It can be found in ( LeCun et al., 1998 ) that the reason behind this progressive increase in the number of kernels is to compensated a possible loss of the representation caused by the spatial resolution reduction. In recent models, what seems to be the real reason is a practical issue ( Chu & Krzyżak, 2014 ), to improve performance by keeping a constant number of operations in each layer.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel neural network based model to automatically discover topics in a text corpus in an unsupervised fashion. The model is based on a neural network that uses documents, words, and topics discrete lookup table embedding to represent probabilities of words given documents, probabilities of words given topics, and probabilities of topics given documents. To address the issue of large corpora, a second model is proposed that generates continuous document embedding using a neural auto-encoder. Experiments conducted on six datasets demonstrate that the proposed models are effective in discovering latent topics and achieve lower perplexity than latent Dirichlet allocation (LDA).",
        "Abstract": "In this paper we present a model for unsupervised topic discovery in texts corpora. The proposed model uses documents, words, and topics lookup table embedding as neural network model parameters to build probabilities of words given topics, and probabilities of topics given documents. These probabilities are used to recover by marginalization probabilities of words given documents. For very large corpora where the number of documents can be in the order of billions, using a neural auto-encoder based document embedding is more scalable then using a lookup table embedding as classically done. We thus extended the lookup based document embedding model to continuous auto-encoder based model. Our models are trained using probabilistic latent semantic analysis (PLSA) assumptions. We evaluated our models on six datasets with a rich variety of contents. Conducted experiments demonstrate that the proposed neural topic models are very effective in capturing relevant topics. Furthermore, considering perplexity metric, conducted evaluation benchmarks show that our topic models outperform latent Dirichlet allocation (LDA) model which is classically used to address topic discovery tasks.",
        "Introduction": "  INTRODUCTION Nowadays, with the digital era, electronic text corpora are ubiquitous. These corpora can be com- pany emails, news groups articles, online journal articles, Wikipedia articles, video metadata (titles, descriptions, tags). These corpora can be very large, thus requiring automatic analysis methods that are investigated by the researchers working on text content analysis ( Collobert et al., 2011 ;  Cambria & White, 2014 ). Investigated methods are about named entity recognition, text classification, etc ( Nadeau & Sekine, 2007 ;  S., 2002 ). An important problem in text analysis is about structuring texts corpora around topics (Daud et al., 2010;  Liu & Zhang, 2012 ). Developed tools would allow to summarize very large amount of text documents into a limited, human understandable, number of topics. In computer science many definitions of the concept of topic can be encountered. Two definitions are very popular. The first one defines a topic as an entity of a knowledge graph such as Freebase or Wikidata ( Bollacker et al., 2008 ;  Vrandečić & Krötzsch, 2014 ). The second one defines a topic as probability distribution over words of a given vocabulary ( Hofmann, 2001 ;  Blei et al., 2003 ). When topics are represented as knowledge graph entities, documents can be associated to identified concepts with very precise meaning. The main drawback is that knowledge graphs are in general composed of a very large number of entities. For example, in 2019, Wikidata counts about 40 million entities. Automatically identifying these entities requires building extreme classifiers trained with expensive labelled data (Puurula et al., 2014;  Liu et al., 2017 ). When topics are defined as probability distribution over words of vocabulary, they can be identified using unsupervised methods that automatically extract them from text corpora. A precursor of such methods is the latent semantic analysis (LSA) model which is based on the word-document co-occurrence counts matrix factorization ( Dumais, 1990 ). Since then, LSA has been extended to various probabilistic based models ( Hofmann, 2001 ;  Blei et al., 2003 ), and more recently to neural network based models ( Salakhutdinov & Hinton, 2009 ;  Larochelle & Lauly, 2012 ;  Wan et al., 2011 ;  Yao et al., 2017 ;  Dieng et al., 2017 ). In this paper, we propose a novel neural network based model to automatically, in an unsupervised fashion, discover topics in a text corpus. The first variation of the model is based on a neural Under review as a conference paper at ICLR 2020 networks that uses as input or parameters documents, words, and topics discrete lookup table em- bedding to represent probabilities of words given documents, probabilities of words given topics, and probabilities of topics given documents. However because in a given corpus, the number of documents can be very large, discrete lookup table embedding explicitly associating to each docu- ment a given embedded vector can be unpractical. For example, for the case of online stores such as Amazon, or video platforms such as Dailymotion or Youtube, the number of documents are in the order of billions. To overcome this limitation, we propose a model that generates continuous document embedding using a neural auto-encoder ( Kingma & Welling, 2013 ). Our neural topic models are trained using cross entropy loss exploiting probabilistic latent semantic analysis (PLSA) assumptions stating that given topics, words and documents can be considered independent. The proposed models are evaluated on six datasets: KOS, NIPS, NYtimes, TwentyNewsGroup, Wikipedia English 2012, and Dailymotion English. The four first datasets are classically used to benchmark topic models based on bag-of-word representation ( Dua & Graff, 2017 ). Wikipedia, and Dailymotion are large scale datasets counting about one million documents. These latter datasets are used to qualitatively assess how our models behave on large scale datasets. Conducted experiments demonstrate that the proposed models are effective in discovering latent topics. Furthermore, eval- uation results show that our models achieve lower perplexity than latent Dirichlet allocation (LDA) trained on the same datasets. The remainder of this paper is organized as follows. Section 2 discusses related work. Section 3 briefly presents principles of topics generation with PLSA. Section 4 presents the first version of the model we propose which is based on discrete topics, documents, and words embedding. Section 5 gives details about the second version of the model which is based on embedding documents using a continuous neural auto-encoder. Section 6 provides details about the experiments conducted to assess the effectiveness of the proposed models. Finally Section 7 derives conclusions and gives future research directions.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on pairwise similarity and dissimilarity constraints in metric learning problems. Specifically, given a universe of objects X, sets of pairs of objects labeled as similar and dissimilar, and some u, a mapping f is sought to find a mapping from X to a target metric space such that for all similar pairs, the distance is less than u, and for all dissimilar pairs, the distance is greater than u. In the case of Mahalanobis metric learning, the mapping f is linear and a matrix G is sought such that for all similar pairs, the distance is less than u, and for all dissimilar pairs, the distance is greater than u.",
        "Abstract": "Learning Mahalanobis metric spaces is an important problem that has found numerous applications. Several algorithms have been designed for this problem, including Information Theoretic Metric Learning (ITML) [Davis et al. 2007] and Large Margin Nearest Neighbor (LMNN) classification [Weinberger and Saul 2009].  We consider a formulation of Mahalanobis metric learning as an optimization problem,where the objective is to minimize the number of violated similarity/dissimilarity constraints.  We show that for any fixed ambient dimension, there exists a fully polynomial time approximation scheme (FPTAS) with nearly-linear running time.This result is obtained using tools from the theory of linear programming in low dimensions. We also discuss improvements of the algorithm in practice, and present experimental results on synthetic and real-world data sets. Our algorithm is fully parallelizable and performs favorably in the presence of adversarial noise.",
        "Introduction": "  INTRODUCTION Learning metric spaces is a fundamental computational primitive that has found numerous applications and has received significant attention in the literature. We refer the reader to  Kulis et al. (2013) ;  Li and Tian (2018)  for detailed exposition and discussion of previous work. At the high level, the input to a metric learning problem consists of some universe of objects X, together with some similarity information on subsets of these objects. Here, we focus on pairwise similarity and dissimilarity constraints. Specifically, we are given S, D Ă'X 2˘, which are sets of pairs of objects that are labeled as similar and dissimilar respectively. We are also given some u, ą 0, and we seek to find a mapping f : X Ñ Y , into some target metric space pY, ρq, such that for all x, y P S, ρpf pxq, f pyqq ď u, and for all x, y P D, ρpf pxq, f pyqq ě . In the case of Mahalanobis metric learning, we have X Ă R d , with |X| \" n, for some d P N, and the mapping f : R d Ñ R d is linear. Specifically, we seek to find a matrix G P R dˆd , such that for all tp, qu P S, we have }Gp´Gq} 2 ď u, (1) and for all tp, qu P D, we have",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper compares various measures of selectivity on the same set of units in AlexNet, VGG-16, and GoogLeNet trained on ImageNet and Places-365 datasets. We assess localist, precision, and CCMAS selectivity, as well as signal-detection measures such as recall with 100% and 95% precision, maximum informedness, specificity at maximum informedness, recall at maximum informedness, and false alarm rates at maximum informedness. We also assess selectivity with jitterplots and human interpretation of images generated by a state-of-the-art activation maximization (AM) method. Our findings show that the precision and CCMAS measures often provide misleadingly high estimates of object selectivity compared to other measures, and that the most selective units in CNNs are sensitive to some unknown feature that is weakly associated with the class in question.",
        "Abstract": "Various methods of measuring unit selectivity have been developed with the aim of better understanding how neural networks work.  But the different measures provide divergent estimates of selectivity, and this has led to different conclusions regarding the conditions in which selective object representations are learned and the functional relevance of these representations. In an attempt to better characterize object selectivity, we undertake a comparison of various selectivity measures on a large set of units in AlexNet, including localist selectivity, precision, class-conditional mean activity selectivity (CCMAS), network dissection, the human interpretation of activation maximization (AM) images, and standard signal-detection measures.  We find that the different measures provide different estimates of object selectivity, with precision and CCMAS measures providing misleadingly high estimates. Indeed, the most selective units had a poor hit-rate or a high false-alarm rate (or both) in object classification, making them poor object detectors.  We fail to find any units that are even remotely as selective as the 'grandmother cell' units reported in recurrent neural networks. In order to generalize these results, we compared selectivity measures on a few units in VGG-16 and GoogLeNet trained on the ImageNet or Places-365 datasets that have been described as 'object detectors'. Again, we find poor hit-rates and high false-alarm rates for object classification. ",
        "Introduction": "  INTRODUCTION There have been recent attempts to understand how neural networks (NNs) work by analyzing hidden units one-at-a-time using various measures such as localist selectivity (Bowers et al., 2014), class-conditional mean activity selectivity (CCMAS) ( Morcos et al., 2018 ), precision ( Zhou et al., 2015 ), network dissection ( Zhou et al., 2018a ), and activation maximization (AM) ( Erhan et al., 2009 ). These measures are all taken to provide evidence that some units respond highly selectively to categories of objects under some conditions. Not only are these findings surprising given the widespread assumption that NNs only learn highly distributed and entangled representations, they raise a host of questions, including the functional importance of these selective representations ( Zhou et al., 2018b ), the conditions in which they are learned (e.g.,  Morcos et al., 2018 ), and the relation between these representations and the selective neurons observed in cortex ( Bowers, 2009 ). To answer these question, it is necessary to have a better understanding of what these metrics actually measure, and how they relate to one another. Accordingly, we directly compare these measures of selectivity on the same set of units as well as adopt standard signal-detection measures in an attempt to provide better measures of single-unit selectivity to object category. In addition, to provide a more intuitive assessment of selectivity, we report jitterplots for a few of the most selective units that visually display how the unit responds to the different image categories. We focus on AlexNet ( Krizhevsky et al., 2012 ) trained on ImageNet (Deng et al., 2009) because many authors have studied the selectivity of single hidden units in this model using a range of quantitative ( Zhou et al., 2018a ; 2015) and qualitative ( Nguyen et al., 2017 ;  Yosinski et al., 2015 ;  Simonyan et al., 2013 ) methods. But we also compare different selectivity measures on specific units in VGG-16 ( Simonyan and Zisserman, 2014 ) and GoogLeNet ( Szegedy et al., 2015 ) trained on the the ImageNet and Places-365 Under review as a conference paper at ICLR 2020 datasets that were characterized by  Zhou et al. (2018a)  as \"object detectors\" based on their Network Dissection method ( Zhou et al., 2018a ). Our main findings are: 1. The precision and CCMAS measures are misleading with near-maximum selectivity scores associated with units that strongly respond to many different image categories. By contrast, the signal-detection measures more closely capture the level of selectivity displayed in the jitterplots (Sec. 3.1). 2. Units with interpretable AM images do not correspond to highly selective representations (Sec. 3.2). 3. The Network Dissection method also provides a misleading measure for \"object detectors\" (Sec. 3.3). In one line of research, Bowers et al. (2014; 2016) assessed the selectivity of single hidden units in recurrent neural networks (RNNs) designed to model human short-term memory. They reported many 'localist' or 'grandmother cell' units that were 100% selective for specific letters or words, where all members of the selective category were more active than and disjoint from all non-members, as can be shown in jitterplots ( Berkeley et al., 1995 ) (see  Fig. 1  for a unit selective to the letter 'j'). The authors argued that the network learned these representations in order to co-activate multiple letters or words at the same time in short-term memory without producing ambiguous blends of overlapping distributed patterns (the so-called 'superposition catastrophe'). Consistent with this hypothesis, localist units did not emerge when the model was trained on letters or words one-at-a-time (Bowers et al., 2014) (see  Fig. 1  for an example of a non-selective unit). In parallel, researchers have reported selective units in the hidden layers of various CNNs trained to classify images into one of multiple categories ( Zhou et al., 2015 ;  Morcos et al., 2018 ;  Zeiler and Fergus, 2014 ;  Erhan et al., 2009 ), for a review see  Bowers (2017) . For example,  Zhou et al. (2015)  assessed the selectivity of units in the pool5 layer of two CNNs trained to classify images into 1000 objects and 205 scene categories, respectively. They reported many highly selective units that they characterized as 'object detectors' in both networks. Similarly,  Morcos et al. (2018)  reported that CNNs trained on CIFAR-10 and ImageNet learned many highly selective hidden units, with CCMAS scores approaching the maximum of 1.0. These later findings appear to be inconsistent with Bowers et al. (2016) who failed to observe selective representations in fully connected NNs trained on stimuli one-at-a-time (see  Fig. 1 ), but the measures of selectivity that have been applied across studies are different, and accordingly, it is difficult to directly compare results. A better understanding of the relation between selectivity measures is vital given that different measures are frequently used to address similar issues. For example, both the human interpretability of generated images ( Le, 2013 ) and localist selectivity (Bowers et al., 2014) have been used to make claims about 'grandmother cells', but it is not clear whether they provide similar insights into unit selectivity. Similarly, based on their precision metric,  Zhou et al. (2015)  claim that the object detectors learned in CNNs play an important role in identifying specific objects, whereas  Morcos et al. (2018)  challenge this conclusion based on their finding that units with high CCMAS measures were not especially important in the performance of their CNNs and concluded: \"...it implies that methods for understanding neural networks based on analyzing highly selective single units, or finding optimal inputs for single units, such as activation maximization ( Erhan et al., 2009 ) may be misleading\". This makes a direct comparison between selectivity measures all the more important. In order to directly compare and have a better understanding of the different selectivity measures we assessed (1) localist, (2) precision, and (3) CCMAS selectivity of the conv5, fc6, and fc7 of AlexNet trained on ImageNet, and in addition, we employed a range of signal detection methods on these units, namely, (4) recall with 100% and 95% precision, (5) maximum informedness, (6) specificity at maximum informedness , and (7) recall (also called sensitivity) at maximum informedness, and false alarm rates at maximum informedness (described in Sec. 2). We also assessed the selectivity of a few units in VGG-16 and GoogLeNet models trained on the ImageNet and Places-365 dataset that were highly selective according to the Network Dissection method ( Zhou et al., 2018a ). We show that the precision and CCMAS measures often provide misleadingly high estimates of object selectivity compared to other measures, and we do not find any units that can be reasonably described as 'object detectors' given that the most selective units show a low hit-rate or a high false-alarm rate (or both) when classifying images. At best, the most selective units in CNNs are sensitive to some unknown feature that is weakly associated with the class in question. In addition to these quantitative measures and jitterplots we assessed selectivity with a common qualitative measure, namely, human interpretation of images generated by a state-of-the-art activation maximization (AM) method ( Nguyen et al., 2017 ). AM images are generated to strongly activate individual units, and some of them are interpretable by humans (e.g., a generated image that looks like a lighthouse, see  Fig. 1 ). For the first time, we systematically evaluated the interpretability of the AM images and compare these ratings with the selectivity measures for corresponding units. We show that the few hidden units with interpretable AM images are not highly selective.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel optimization approach for data-driven optimization problems, which can readily operate on high-dimensional inputs, utilize offline static data, and learn with minimal active data collection if needed. The proposed approach, Model Inversion Networks (MINs), maps from values to corresponding inputs, and leverages advances in deep generative modeling. Experiments demonstrate that MINs outperform prior methods on high-dimensional input spaces, perform competitively to Bayesian optimization methods on tasks with active data collection and lower-dimensional inputs, and substantially outperform prior methods on contextual optimization from logged data.",
        "Abstract": "In this work, we aim to solve data-driven optimization problems, where the goal is to find an input that maximizes an unknown score function given access to a dataset of input, score pairs. Inputs may lie on extremely thin manifolds in high-dimensional spaces, making the optimization prone to falling-off the manifold. Further, evaluating the unknown function may be expensive, so the algorithm should be able to exploit static, offline data. We propose model inversion networks (MINs) as an approach to solve such problems. Unlike prior work, MINs scale to extremely high-dimensional input spaces and can efficiently leverage offline logged datasets for optimization in both contextual and non-contextual settings. We show that MINs can also be extended to the active setting, commonly studied in prior work, via a simple, novel and effective scheme for active data collection. Our experiments show that MINs act as powerful optimizers on a range of contextual/non-contextual, static/active problems including optimization over images and protein designs and learning from logged bandit feedback.",
        "Introduction": "  INTRODUCTION Data-driven optimization problems arise in a range of domains: from protein design ( Brookes et al., 2019 ) to automated aircraft design ( Hoburg & Abbeel, 2012 ), from the design of robots ( Liao et al., 2019 ) to the design of neural net architectures ( Zoph & Le, 2017 ) and learning from logged feedback, such as optimizing user preferences in recommender systems. Such problems require optimizing unknown reward or score functions using previously collected data consisting of pairs of inputs and corresponding score values, without direct access to the score function being optimized. This can be especially challenging when valid inputs lie on a low-dimensional manifold in the space of all inputs, e.g., the space of valid aircraft designs or valid images. Existing methods to solve such problems often use derivative-free optimization ( Snoek et al. ). Most of these techniques require active data collection where the unknown function is queried at new inputs. However, when function evaluation involves a complex real-world process, such as testing a new aircraft design or evaluating a new protein, such active methods can be very expensive. On the other hand, in many cases there is considerable prior data - existing aircraft and protein designs, and advertisements and user click rates, etc. - that could be leveraged to solve the optimization problem. In this work, our goal is to develop an optimization approach to solve such optimization problems that can (1) readily operate on high-dimensional inputs comprising a narrow, low-dimensional manifold, such as natural images, (2) readily utilize offline static data, and (3) learn with minimal active data collection if needed. We can define this problem setting formally as the optimization problem x = arg max x f (x), (1) where the function f (x) is unknown, and we have access to a dataset D = {(x 1 , y 1 ), . . . , (x N , y N )}, where y i denotes the value f (x i ). If no further data collection is possible, we call this the data-driven model-based optimization setting. This can also be extended to the contextual setting, where the aim is to optimize the expected score function value across a context distribution. That is, π = arg max π E c∼p0(·) [f (c, π(c))], (2) where π maps contexts c to inputs x, such that the expected score under the context distribution p 0 (c) is optimized. As before, f (c, x) is unknown and we have access to a dataset D = {(c i , x i , y i )} N i=1 , Under review as a conference paper at ICLR 2020 where y i is the value of f (c i , x i ). Such contextual problems with logged datasets have been studied in the context of contextual bandits ( Swaminathan & Joachims, a; Joachims et al., 2018 ). A simple way to approach these model-based optimization problems is to train a proxy function f θ (x) or f θ (c, x), with parameters θ, to approximate the true score, using the dataset D. However, directly using f θ (x) in place of the true function f (x) in Equation (1) generally works poorly, because the optimizer will quickly find an input x for which f θ (x) outputs an erroneously large value. This issue is especially severe when the inputs x lie on a narrow manifold in a high-dimensional space, such as the set of natural images ( Zhu et al., 2016 ). The function f θ (x) is only valid near the training distribution, and can output erroneously large values when queried at points chosen by the optimizer. Prior work has sought to addresses this issue by using uncertainty estimation and Bayesian models ( Snoek et al., 2015 ) for f θ (x), as well as active data collection ( Snoek et al. ). However, explicit uncertainty estimation is difficult when the function f θ (x) is very complex or when x is high-dimensional. Instead of learning f θ (x), we propose to learn the inverse function, mapping from values y to corresponding inputs x. This inverse mapping is one-to-many, and therefore requires a stochastic mapping, which we can express as f −1 θ (y, z) → x, where z is a random variable. We term such models model inversion networks (MINs). MINs provide us with a number of desirable properties: they can utilize static datasets, handle high-dimensional input spaces such as images, can handle contextual problems, and can accommodate both static datasets and active data collection. We discuss how to design simple active data collection methods for MINs, leverage advances in deep generative modeling ( Goodfellow et al.; Brock et al., 2019 ), and scale to very high-dimensional input spaces. We experimentally demonstrate MINs in a range of settings, showing that they outperform prior methods on high-dimensional input spaces, perform competitively to Bayesian optimization methods on tasks with active data collection and lower-dimensional inputs, and substantially outperform prior methods on contextual optimization from logged data (Swaminathan & Joachims, a).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a gold standard for evaluating the quality of identifier embeddings in source code. We define a set of semantic relationships between identifiers and collect human ratings for these relationships. We then use these ratings to evaluate the quality of existing identifier embeddings. Our gold standard is the first of its kind and provides a benchmark for measuring the quality of identifier embeddings.",
        "Abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n",
        "Introduction": "  INTRODUCTION Reasoning about source code based on learned representations has various applications, such as predicting method names ( Allamanis et al., 2015 ), detecting bugs ( Pradel & Sen, 2018 ) and vulnera- bilities ( Harer et al., 2018 ), predicting types ( Malik et al., 2019 ), detecting similar code ( White et al., 2016 ;  Xu et al., 2017 ), inferring specifications ( DeFreez et al., 2018 ), code de-obfuscation ( Raychev et al., 2015 ;  Alon et al., 2018a ), and program repair ( Devlin et al., 2017 ). Many of these techniques are based on embeddings of source code, which map a given piece of code into a continuous vector representation that encodes some aspect of the semantics of the code. A core component of most code embeddings are semantic representations of identifier names, i.e., the names of variables, functions, classes, fields, etc. in source code. Similar to words in natural languages, identifiers are the basic building block of source code. Identifiers not only account for the majority of the vocabulary of source code, but they also convey important information about the (intended) meaning of code. To reason about identifiers and their meaning, code analysis techniques build on learned embeddings of identifiers, either by adapting embeddings that were originally pro- posed for natural languages ( Mikolov et al., 2013a ; b ) or with embeddings specifically designed for source code ( Alon et al., 2018a ). Given the importance of identifier embeddings, a crucial challenge is measuring how effective an embedding represents the semantic relationships between identifiers. For word embeddings in natu- ral language, the community has addressed this question through a series of gold standards ( Finkel- stein et al., 2002 ;  Bruni et al., 2014a ;  Rubenstein & Goodenough, 1965 ;  Miller & Charles, 1991 ;  Hill et al., 2015 ;  Gerz et al., 2016 ). These gold standards define how similar two words are based on ratings by human judges, enabling an evaluation that measures how well an embedding reflects the human ratings.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method for proving non-vacuous generalization bounds for deep neural networks using the PAC-Bayes framework. The authors propose to use a second order Taylor expansion of the loss around the minimum, in conjunction with the PAC-Bayes framework, to estimate the flatness of the minimum along all parameter directions. Experiments are conducted to show that, depending on the hardness of the dataset, it is possible to find cases where the set of feasible solutions implied from the lower bound and the set of non-vacuous PAC-Bayes bound solutions do not intersect.",
        "Abstract": "We investigate whether it's possible to tighten PAC-Bayes bounds for deep neural networks by utilizing the Hessian of the training loss at the minimum. For the case of Gaussian priors and posteriors we introduce a Hessian-based method to obtain tighter PAC-Bayes bounds that relies on closed form solutions of layerwise subproblems. We thus avoid commonly used variational inference techniques which can be difficult to implement and time consuming for modern deep architectures. We conduct a theoretical analysis that links the random initialization, minimum, and curvature at the minimum of a deep neural network to limits on what is provable about generalization through PAC-Bayes. Through careful experiments we validate our theoretical predictions and analyze the influence of the prior mean, prior covariance, posterior mean and posterior covariance on obtaining tighter bounds. ",
        "Introduction": "  INTRODUCTION Recently two works  Dziugaite & Roy (2017)   Zhou et al. (2018)  have made significant progress in proving generalization for deep neural networks. They manage to prove non-vacuous generalization Under review as a conference paper at ICLR 2020 bounds for a simplified Mnist dataset  Dziugaite & Roy (2017)  and the Imagenet dataset respectively  Zhou et al. (2018) . This stands in stark contrast with previous works  Bartlett et al. (2017)  Neyshabur et al. (2017b)  Golowich et al. (2017)  which yield bounds that are vacuous by several orders of magnitude and are usually motivated by simple empirical correlation with the generalization error. Both works rely crucially on applying the PAC-Bayes framework McAllester (1999) which typi- cally addresses the generalization error of stochastic classifiers. Given the randomized empirical error E θ∼Q [L(θ)], the randomized population error E θ∼Q [L(θ)], a prior weight distribution P , a posterior weight distribution Q and N training samples the PAC-Bayes bound gives a guarantee of the form E θ∼Q [L(θ)] ≤ E θ∼Q [L(θ)] + A (KL(Q||P ) + B)/N , (1) with some probability 1 − δ where A and B are constants related to the derivation. The PAC- Bayes bound models the complexity of a classifier as the KL-Divergence between a prior P and a posterior Q weight distribution. Apart from yielding state of the art bounds for SVMs, a further motivation for this framework, is it's Bayesian nature i.e. is the existance of the prior P . Complexity is not measured with respect to an arbitrary reference point, but as in the luckiness framework  Shawe-Taylor et al. (1998) , with respect to a reference point that can potentially incorporate our prior knowledge about good solutions to the classification problem, and can lead to possibly tighter bounds. In  Dziugaite & Roy (2017)  the authors model the weights as originating from a Gaussian posterior distribution with diagonal covariance θ = µ + ξ σ where ξ ∼ N (0, I). Then they optimize directly the stochastic objective resulting from the PAC-Bayes bound, by approximating stochastic quantities with MC sampling. Similarly in  Zhou et al. (2018)  the authorts first compress a DNN with an off the shelf compession algorithm, removing redundant parameters and applying the PAC-Bayes approach to the remaining weights. This can be seen as explicitly minimizing the length of a code describing the DNN. While compression in  Zhou et al. (2018)  is done explicitly,  Dziugaite & Roy (2017)  can also be seen under this light, the random variables form a variational code whose length is explicitly minimized  Blier & Ollivier (2018) . While these two works result in non-vacuous bounds they have a number of important limitations. • Importantly they provide generalization error guarantees for a different classifier than the original. Compressing the neural network or modelling it as originating from a Gaussian distribution whose mean is modified, results in finding a completely different point in the parameter space than the original. The function that the new weights describe might or might not be close to the original. This might seem like an academic problem. However we argue that analyzing networks that typically result from vanilla SGD is equally important potentially leading to better initialization and regularisation of SGD, as well as discovering inherent limitions of our generalization proof toolbox. • The bounds provided by  Dziugaite & Roy (2017) Zhou et al. (2018)  are non-vacuous but loose. What is the source of this looseness? Both methods rely in non-convex optimisation that might have simply not converged properly. This is a particular problem in  Dziugaite & Roy (2017)  where it is well known that VI techniques require laborious hyperparameter tuning, something that has hindered significantly the wide applicability of Bayesian neural networks  Wu et al. (2018) . Even when care is taken the resuls are widely considered sub- optimal in terms of uncertainty estimation for prediction tasks and code length description of the DNN  Wu et al. (2018) Blier & Ollivier (2018) . As such we argue that one stands to benefit from circumventing these non-convex optimisation procedures when possible. The PAC-Bayes bound can also be seen under the light of flat minima. Flat minima have been em- pirically shown to correlate with better generalization Neyshabur et al. (2017a) Keskar et al. (2016) . The PAC-Bayes bound can be interpreted as balancing two terms, the randommized empirical loss and the KL complexity term Neyshabur et al. (2017a). As the posterior can be arbitrarily chosen increasing the variance of the noise added to the parameters will typically decrease the KL term while increasing the empirical loss. If the neural network solution corresponds to a flat minimum, Under review as a conference paper at ICLR 2020 more noise can be added to the parameters without affecting the empirical performance Neyshabur et al. (2017a). In this work we propose to tackle the above problems by using a second order Taylor expansion of the loss around the minimum, in conjunction with the PAC-Bayes framework. The second order term parameterized by the Hessian matrix corresponds to the curvature of the loss around the minimum. In line with the works linking flat minima to better generalization Neyshabur et al. (2017a) Keskar et al. (2016) , we are effectively estimating the flatness of the minimum along all parameter dierec- tions. Thus we are able to add more noise to the flat directions and less noise to the curved ones, something that will typically decrease significantly the KL term in the PAC-Bayes framework while ensuring that the loss of the stochastic classifier remains small. Crucially for the case of Gaussian posteriors the optimal posterior covariance can be found in closed form, allowing us to circumvent a non-convex optimization procedure by incuring an approximation penalty due to using a second order approximation of the loss. The resulting approximation is closely linked to the Laplace approximation in Bayesian statistics  Bishop (2006) . It and similar approximations to the posterior have a appeared a number of times in the literature of bayesian neural networks yielding good results in a number of tasks  Maddox et al. (2019) ;  Khan et al. (2019) ;  Ritter et al. (2018) ; Zhang et al. (2017);  Khan et al. (2018) . Similarly second order approximations of the loss around a minimum have a long history in the literature of DNN compression  Dong et al. (2017) ;  Wang et al. (2019) ;  Peng et al. (2019) ; LeCun et al. (1990);  Hassibi & Stork (1993)  often yielding state of the art results in parameter reduction. Even though the resulting posterior is optimal with respect to our approximation, being able to chose a data driven informative prior still leaves room for looseness. Importantly the prior in the PAC-Bayes framework can depend on the data generating distribution but not on the training set used to train the evaluated classifier. Workarounds include choosing data driven priors by training a separate classifier on a different training set, or using the same training set but enforcing that the training set is not too informative about each individual training signal. The later can be formalized through the framework of differential privacy. Both of the above will usually involve some non- convex optimisation procedure leaving again doubt as to the optimality of the classifier complexity estimate. We show that for the case of Gaussian priors and posteriors with diagonal covariance we can derive the optimal invalid prior covariance in closed form. The invalid prior cannot be used to prove generalization, however it can be used as a sanity check to see whether proving generalization is possible in principle. The optimal solution with respect to both prior and posterior covariance results in a lower bound on a function closely related to the PAC-Bayes bound (but not exactly equal given that we make a number of approximations). Through experiments we find that, depending on the hardness of the dataset, one can find cases where the set of feasible solutions implied from the lower bound and the set of non-vacuous PAC-Bayes bound solutions don't intersect. One is unable to prove generalization, even through choosing a prior in an invalid manner. A number of works  Achille & Soatto (2018) ;  Dziugaite & Roy (2017) ;  Germain et al. (2016)  have noted the similarity between the stochastic PAC-Bayes objective and the objective For β = 1 this is known as the Evidence Lower Bound (ELBO) objective in Variational Inference literature  Kingma et al. (2015)Bishop (2006) . In the Information Bottleneck framework  Achille & Soatto (2018) Tishby et al. (2000) it is know as the IB-Lagrangian. More recently the same objective has been interpreted as the \"task complexity\"  Achille et al. (2019) . Then β has the role of regulating the amount of information in the randomized neural network  Achille & Soatto (2018) , smaller values correspond to more information and potential to overfit. While the objectives 1 and 2 are not entirely equivalent due to a square root term over the KL divergence in the PAC-Bayes case, we will be using the IB formulation as removing the square root will ease our derivations. We note that  Dziugaite & Roy (2017)  have used the two objectives intercheangably with no significant difference in results. A number of preprints have appeared on Arxiv trying to link PAC-Bayes to flat minima  Wang et al. (2018) ;  Li et al. (2019) ;  Tsuzuku et al. (2019) ;  Yang et al. (2019) . These typically involve heuristic choices for the optimal posterior, do not analyze the role of the prior, and focus on quantities that simply correlate with the generalization error. Motivating vacuous generalization bounds on the Under review as a conference paper at ICLR 2020 basis of empirical correlations with generalization error has been criticised in a number of recent works  Kawaguchi et al. (2017) ; Nagarajan & Kolter (2019b);  Pitas et al. (2019) .",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to set regression, a problem of learning to predict utilities for sets of objects. We propose a neural non-additive utility aggregation approach that uses the discrete Choquet integral to learn intermediate latent utilities for objects and a non-additive aggregation function for the intermediate utilities. This approach is better suited to deal with phenomena such as redundancy and synergies than the standard additive utility aggregation approach.",
        "Abstract": "Neural architectures for set regression problems aim at learning representations such that good predictions can be made based on the learned representations. This strategy, however, ignores the fact that meaningful intermediate results might be helpful to perform well. We study two new architectures that explicitly model latent intermediate utilities and use non-additive utility aggregation to estimate the set utility based on the latent utilities. We evaluate the new architectures with visual and textual datasets, which have non-additive set utilities due to redundancy and synergy effects. We find that the new architectures perform substantially better in this setup.",
        "Introduction": "  INTRODUCTION In this paper, we study the problem of learning to predict utilities for sets of objects, which we denote set regression. Let O = {o 1 , . . . , o } be a set of objects. Let C = {C 1 , . . . , C } be a set of sets that consist of objects from set O, i.e., C k = {o i , . . . , o j } with o i , . . . , o j ∈ O. During training, models only observe sets from C and their corresponding utilities and have to learn a function v : C → R that maps from sets to set utilities. At test time, utilities of unseen sets with potentially unseen objects have to be estimated. The standard idea to approach this problem in representation learning is to learn representations for sets in C such that the utilities can be predicted based on the generated representations. More formally, for sets C k ∈ C an aggregation function o∈C k ϕ(o) is learned that produces an aggre- gation of the individual object representations ϕ(o). A function v( o∈C ϕ(o)) is trained (usually jointly with the aggregation function) to predict the utility of the set. One often used aggregation strategy is to simply add the individual object representations ϕ(o)  Zaheer et al. (2017) . Another strategy is to use recurrent neural networks (RNNs) that learn to aggregate object representations. On a conceptual level, the idea of both approaches is to encode everything that is relevant to predict the utility of a set in its aggregated representation. This approach, however, ignores the fact that meaningful intermediate utilities for the objects contained in the sets can be predicted in many prob- lems. For example, intermediate utilities of sentences in automatic summarization can be predicted and aggregated to obtain the summary utility. A crucial limitation of this idea is the fact that objects in sets do not necessarily have an intrin- sic value that is independent from the other objects in the same set. For example, repeating the same information multiple times in automatic summarization does not improve the summary utility even though the individual sentences might contain crucial information. Furthermore, a sentence A might depend on another sentence B (e.g., to resolve pronouns) such that sentence A can only be understood if sentence B is also present in the summary. Even if sentence A contains valuable infor- mation, it does not have an intrinsic value. Its value becomes only effective when B is also included in the summary. Since such redundancy and synergy effects occur in many problems, additive utility aggregation is not appropriate. To resolve this problem, we propose neural non-additive utility aggregation. Contrary to previous approaches, we propose to learn intermediate latent utilities u for objects in O and a non-additive aggregation function for the intermediate utilities. To this end, we make use of the discrete Choquet integral, which is a non-additive generalization of the well-known Lebesgue integral. Instead of using additive measures, the Choquet integral makes use of non-additive capacities. Hence, it is well-suited to deal with phenomena such as redundancy and synergies.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Adversarial samples are carefully crafted inputs that can cause neural networks to misclassify. This poses a security risk for current deep neural network applications. In this paper, we propose a methodology to evaluate the representation of machine learning methods and reveal a link between deep representations' quality and attack susceptibility. We propose a test called Raw Zero-Shot and two metrics to evaluate DNN's representations.",
        "Abstract": "Neural networks have been shown vulnerable to adversarial samples.  Slightly perturbed input images are able to change the classification of accurate models, showing that the representation learned is not as good as previously thought. To aid the development of better neural networks, it would be important to evaluate to what extent are current neural networks' representations capturing the existing features. Here we propose a way to evaluate the representation quality of neural networks using a novel type of zero-shot test, entitled Raw Zero-Shot. The main idea lies in the fact that some features are present on unknown classes and that unknown classes can be defined as a combination of previous learned features without representation bias (a bias towards representation that maps only current set of input-outputs and their boundary). To evaluate the soft-labels of unknown classes, two metrics are proposed. One is based on clustering validation techniques (Davies-Bouldin Index) and the other is based on soft-label distance of a given correct soft-label.\nExperiments show that such metrics are in accordance with the robustness to adversarial attacks and might serve as a guidance to build better models as well as be used in loss functions to create new types of neural networks. Interestingly, the results suggests that dynamic routing networks such as CapsNet have better representation while current deeper DNNs are trading off representation quality for accuracy.",
        "Introduction": "  INTRODUCTION Adversarial samples are slightly perturbed inputs that can make neural networks misclassify. They are carefully crafted by searching for variations in the input that, for example, could decrease the soft-labels of the correct class. Since they were discovered some years ago ( Szegedy, 2014 ), the number of adversarial samples have grown in both number and types. Random noise was shown to be recognized with high confidence by neural networks ( Nguyen et al., 2015 ), universal perturbations, that can be added to almost any image to generate an adversarial sample, were shown to exist ( Moosavi-Dezfooli et al., 2017 ), and the addition of crafted patches was shown to cause networks to misclassify ( Brown et al., 2017 ). Only one pixel is enough to make networks misclassify ( Su et al., 2017 ). Such attacks can also be easily transferred to real-world scenarios ( Kurakin et al., 2016 ),( Athalye & Sutskever, 2018 ), which confers a big issue as well as a security risk for current deep neural networks' applications. Albeit the existence of many defences, there is not any known learning algorithm or procedure that can defend against adversarial attacks consistently. Many works have tried to defend by hiding or modifying the gradients to make neural networks harder to attack. However, a recent paper shows that most of these defences fall into the class of obfuscated gradients which have their shortcomings (e.g., they can be easily bypassed by transferable attacks) ( Athalye et al., 2018 ). Additionally, the use of an augmented dataset with adversarial samples (named adversarial training) is perhaps one of the most successful approaches to construct robust neural networks ( Goodfellow et al., 2014 ),( Huang et al., 2015 ), ( Madry et al., 2018 ). However, it is still vulnerable to attacks and has a strong bias in the type of adversarial samples used in training ( Tramèr et al., 2018 ). This shows that a deeper understanding of the issues is needed to enable more consistent defences to be created. Few works focused on understanding the reason behind such lack of robustness. In ( Goodfellow et al., 2014 ), it is argued that Deep Neural Networks's (DNN) linearity are one of the main reasons. Recent investigations reveal that attacks are changing where the algorithm is paying attention ( Vargas & Su, 2019 ), other experiments show that deep learning neural networks learn false Under review as a conference paper at ICLR 2020 structures that are easier to learn rather than the ones expected ( Thesing et al., 2019 ) and an accuracy and robustness trade-off for models were shown to exist ( Tsipras et al., 2019 ). In this paper, we propose a methodology of how to evaluate the representation of machine learning methods. Based on these metrics, we reveal a link between deep representations' quality and attack susceptibility. Specifically, we propose a test called Raw Zero-Shot and two metrics to evaluate DNN's representations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to quantifying, detecting, and characterizing anomalous patterns in the activation space of neural networks. The proposed method, Subset Scanning, efficiently maximizes nonparametric scan statistics in the activation space of neural networks to detect anomalous patterns that span multiple inputs (images). The method is applied to detect targeted adversarial noise added to inputs in order to change the labels to a target class. Empirical results show that detection power drastically increases when targeted images compose 8%-10% of the data under evaluation, and is near 1 when the proportion reaches 14%. This is the first work to apply subset scanning techniques to data generated from neural networks in order to detect anomalous patterns of activations that span multiple inputs (images).",
        "Abstract": "This work views neural networks as data generating systems and applies anomalous pattern detection techniques on that data in order to detect when a network is processing a group of anomalous inputs.  Detecting anomalies is a critical component for multiple machine learning problems including detecting the presence of adversarial noise added to inputs. More broadly, this work is a step towards giving neural networks the ability to detect groups of out-of-distribution samples.  This work introduces ``Subset Scanning methods from the anomalous pattern detection domain to the task of detecting anomalous inputs to neural networks.  Subset Scanning allows us to answer the question: \"``Which subset of inputs have larger-than-expected activations at which subset of nodes?\"  Framing the adversarial detection problem this way allows us to identify systematic patterns in the activation space that span multiple adversarially noised images.  Such images are ``\"weird together\".  Leveraging this common anomalous pattern, we show increased detection power as the proportion of noised images increases in a test set.   Detection power and accuracy results are provided for targeted adversarial noise added to CIFAR-10 images on a 20-layer ResNet using the Basic Iterative Method attack. ",
        "Introduction": "  INTRODUCTION The vast majority of data in the world can be thought of as created by unknown, and possibly complex, normal behavior of data generating systems. But what happens when data is generated by an alternative system instead? Fraudulent records, disease outbreaks, cancerous cells on pathology slides, or adversarial noised images are all examples of data that does not come from the original, normal system. These are the interesting data points worth studying. The goal of anomalous pattern detection is to quantify, detect, and characterize the data that are generated under these alternative systems. Furthermore, subset scanning extends these ideas to consider groups of data records that may only appear anomalous when viewed together (as a subset) due to the assumption that they were generated by the same alternative system. Neural networks may be viewed as one of these data generating systems. The activations are a source of high-dimensional data that can be mined to discover anomalous patterns. Mining activation data has implications for interpretable machine learning as well as more objective tasks such as detecting groups of out-of-distribution samples. This paper addresses the question: \"Which of the exponentially many subset of inputs (images) have higher-than-expected activations at which of the exponentially many subset of nodes in a hidden layer of a neural network?\" We treat this scenario as a search problem with the goal of finding a \"high-scoring\" subset of images × nodes by efficiently maximizing nonparametric scan statistics in the activation space of neural networks. The primary contribution of this work is to demonstrate that nonparametric scan statistics, efficiently optimized over node-activations × multiple inputs (images), are able to quantify the anomalousness of a subset of those inputs (images) into a real-valued \"score\". This definition of anomalousness is with respect to a set of clean \"background\" inputs (images) that are assumed to generate normal or expected patterns in the activation space of the network. Our method measures the deviance between the activations of a subset of inputs (images) under evaluation and the activations generated by the background inputs. The challenging aspect of measuring deviances in the activation space Under review as a conference paper at ICLR 2020 of neural networks is dealing with high-dimensional data, on the order of the number of nodes in a hidden layer × the number of inputs (images) under consideration. Therefore, the measure of anomalousness must be effective in capturing systematic (yet potentially subtle) deviances in a high-dimensional subspace and be computationally tractable. Subset scanning meets both of these requirements (see Section 2). The reward for addressing this difficult problem is an unsupervised, anomalous-input detector that can be applied to any input and to any type of neural network architecture. Neural networks univer- sally rely on their activation space to encode the features of their inputs and therefore quantifying deviations from expected behavior in the activation space has broad appeal and potential beyond detecting anomalous patterns in groups of images. Furthermore, an additional output of subset scan- ning not fully explored in this paper is the subset of nodes at which the subset of inputs (images) had the higher-than-expected activations. These may be used to characterize the anomalous pattern that is affecting the inputs. The second contribution of this work focuses on detection of targeted adversarial noise added to inputs in order to change the labels to a target class  Szegedy et al. (2013) ;  Goodfellow et al. (2014) ;  Papernot & McDaniel (2016) . Our critical insight to this problem is the ability to detect the presence of noise (i.e. an anomalous pattern) across multiple images simultaneously. This view is grounded by the idea that targeted attacks will create a subtle, but systematic, anomalous pattern of activations across multiple noised images. Therefore, during a realistic attack on a machine learning system, we expect a subset of the inputs to be anomalous together by sharing higher-than-expected acti- vations at similar nodes. Empirical results show that detection power drastically increases when targeted images compose 8%-10% of the data under evaluation. Detection power is near 1 when the proportion reaches 14%. In summary, this is the first work to apply subset scanning techniques to data generated from neural networks in order to detect anomalous patterns of activations that span multiple inputs (images). To the best of our knowledge, this is the first topic to address adversarial noise detection by considering images as a group rather than individually.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a general interpolation scheme for generative models that works well for arbitrary priors. A notion of a realism index of an element of the latent space is introduced, which can be either defined internally with the use of the prior latent density, or by some external feature space. An iterative algorithm is proposed to optimise an interpolation with respect to the introduced index, resulting in an interpolation that simultaneously tries to optimise the realism of generated points and the length of the curve transported to the input data space. This approach is especially valuable when the prior density is not convex-shaped or when a soap-bubble effect appears.",
        "Abstract": "In order to perform plausible interpolations in the latent space of a generative model, we need a measure that credibly reflects if a point in an interpolation is close to the data manifold being modelled, i.e. if it is convincing. In this paper, we introduce a realism index of a point, which can be constructed from an arbitrary prior density, or based on FID score approach in case a prior is not available. We propose a numerically efficient algorithm that directly maximises the realism index of an interpolation which, as we theoretically prove, leads to a search of a geodesic with respect to the corresponding Riemann structure. We show that we obtain better interpolations then the classical linear ones, in particular when either the prior density is not convex shaped, or when the soap bubble effect appears.",
        "Introduction": "  INTRODUCTION Since the advent of the Variational Auto-Encoder (VAE) (Kingma & Welling, 2013) and the Gen- erative Adversarial Network (GAN) ( Goodfellow et al., 2014 ), generative models became an area of intensive research, with new models being developed (e.g.  Kingma & Dhariwal (2018) ;  Larsen et al. (2015) ;  Tolstikhin et al. (2017) ). In these models, the data distribution is mapped into the latent space. An important profit from introducing the latent space is the ability to construct interpolations, i.e. traversals between latent representations of two different objects. Meaningfulness of decoded in- terpolations is often used as a supporting argument for networks generalisation capability ( Bowman et al., 2015 ;  Dumoulin et al., 2016 ). Interpolation is used commonly to show that models do not overfit, but generalise well ( Dumoulin et al., 2016 ;  Goodfellow et al., 2014 ;  Higgins et al., 2016 ; Kingma & Welling, 2013). Intuitively, a good interpolation should decode to meaningful objects, give a gradual transformation, and reflect the internal structure of the dataset. More precisely, we require that the interpolation curve (after transporting to the input space) is smooth and relatively short, while at the same time it goes through regions of high probability in the latent space (see leftmost projection in  Fig. 1 ). However, in some cases, even for a Gaussian prior, a linear interpolation could be of poor quality, e.g. due to the so called \"soap bubble effect\" ( Husar, 2017 ;  Lesniak et al., 2019 ). This may result in low quality samples in the middle of the path (White, 2016). The above argument puts the usability of simple linear interpolation in question and motivates further research in this area ( Agustsson et al., 2017 ;  Brock et al., 2016 ;  Kilcher et al., 2017 ;  Larsen et al., 2015 ;  Lesniak et al., 2019 ; White, 2016). In this paper, we construct a general interpolation scheme, which works well for arbitrary priors. We introduce a notion of a realism index of an element of the latent space, which naturally gener- alises to arbitrary curves. We show that in general the proposed method can be regarded as a search for geodesics in a respectively modified local Riemann structure. The realism index can be either defined internally, with the use of the prior latent density, or by some external feature space, e.g. similarly to Fréchet Inception Distance (FID) score ( Heusel et al., 2017 ). In addition, we propose a simple to implement iterative algorithm, that optimises an interpolation with respect to the intro- duced index. As a consequence of our approach, we obtain an interpolation which simultaneously tries to optimise the two following features: • the interpolating curve goes through regions where the generated points are realistic, • the length of the curve transported to the input data space is small. The above approach is especially valuable if either the prior density is not convex-shaped, or when a kind of a soap-bubble effect appears (i.e. when in the generative model, the points generated near the origin are of much worse quality than the ones chosen randomly).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents NoiseLearn, a novel, simple, and effective point cloud decoding approach that builds on the principles of PointNet to decode point clouds from a provided shape representation. The simplicity of the decoding architectures and the increase in performance are strong indicators that sample-based decoders should be considered as a default in future studies and systems. In addition, the paper investigates the operation of the decoders to gain insight into how the output point clouds are generated from a latent shape representation.",
        "Abstract": "Point clouds are a flexible and ubiquitous way to represent 3D objects with arbitrary resolution and precision. Previous work has shown that adapting encoder networks to match the semantics of their input point clouds can significantly improve their effectiveness over naive feedforward alternatives. However, the vast majority of work on point-cloud decoders are still based on fully-connected networks that map shape representations to a fixed number of output points. In this work, we investigate decoder architectures that more closely match the semantics of variable sized point clouds. Specifically, we study sample-based point-cloud decoders that map a shape representation to a point feature distribution, allowing an arbitrary number of sampled features to be transformed into individual output points. We develop three sample-based decoder architectures and compare their performance to each other and show their improved effectiveness over feedforward architectures. In addition, we investigate the learned distributions to gain insight into the output transformation. Our work is available as an extensible software platform to reproduce these results and serve as a baseline for future work.",
        "Introduction": "  INTRODUCTION Point clouds are an important data type for deep learning algorithms to support. They are commonly used to represent point samples of some underlying object. More generally, the points may be extended beyond 3D space to capture additional information about multi-sets of individual objects from some class. The key distinction between point clouds and the more typical tensor data types is that the information content is invariant to the ordering of points. This implies that the spatial relationships among points is not explicitly captured via the indexing structure of inputs and outputs. Thus, standard convolutional architectures, which leverage such indexing structure to support spatial generalization, are not directly applicable. A common approach to processing point clouds with deep networks is voxelization, where point clouds are represented by one or more occupancy-grid tensors (Zhou & Tuzel (2018),  Wu et al. (2018) ). The grids encode the spatial dimensions of the points in the tensor indexing structure, which allows for the direct application of convolutional architectures. This voxelization approach, however, is not appropriate in many use cases. In particular, the size of the voxelized representation depends on the spatial extent of the point cloud relative to the spatial resolution needed to make the necessary spatial distinctions (such as distinguishing between different objects in LIDAR data). In many cases, the required resolution will be unknown or result in enormous tensors, which can go beyond the practical space and time constraints of an application. This motivates the goal of developing architectures that support processing point cloud data directly, so that processing scales with the number of points rather than the required size of an occupancy grid. One naive approach, which scales linearly in the size of the point cloud, is to 'flatten' the point cloud into an arbitrarily ordered list. The list can then be directly processed by standard convolutional or fully-connected (MLP) architectures directly. This approach, however, has at least two problems. First, the indexing order in the list carries no meaningful information, while the networks do not encode this as a prior. Thus, the networks must learn to generalize in a way that is invariant to ordering, which can be data inefficient. Second, in some applications, it is useful for point clouds to consist of varying numbers of points, while still representing the same underlying objects. However, the number of points that can be consumed by the naive feedforward architecture is fixed. Under review as a conference paper at ICLR 2020 PointNet ( Qi et al., 2017 ) and Deepsets  Zaheer et al. (2017)  exhibit better performance over the MLP baseline with a smaller network by independently transforming each point into a high-dimensional representation with a single shared MLP that is identically applied to each individual point. This set of derived point features is then mapped to a single, fixed-sized dense shape representation using a symmetric reduction function. As such the architectures naturally scale to any number of input points and order invariance is built in as an architectural bias. As a result, these architectures have been shown to yield significant advantages in applications in which point clouds are used as input, such as shape classification. The success of PointNet and DeepSet style architectures in this domain shows that designing a network architecture to match the semantics of a point cloud results in a more efficient, and better performing network. Since point clouds are such a useful object representation, it's natural to ask how we should design networks to decode point clouds from some provided shape representation. This would allow for the construction of point cloud auto-encoders, which could serve a number of applications, such as anomaly detection and noise smoothing. Surprisingly, the dominant approach to designing such a differentiable point cloud decoder is to feed the dense representation of the desired object through a single feedforward MLP whose result is then reshaped into the appropriate size for the desired point cloud. This approach has similar issues as the flat MLP approach to encoding point clouds; the decoder can only produce a fixed-sized point cloud while point clouds are capable of representing objects at low or high levels of detail; the decoder only learns a single deterministic mapping from a shape representation to a point cloud while we know that point clouds are inherently random samples of the underlying object. The primary goal and contribution of this paper is to study how to apply the same lessons learned from the PointNet encoder's semantic congruence with point clouds to a point cloud decoder design. As such, we build on PointNet's principles to present the 'NoiseLearn' algorithm- a novel, simple, and effective point cloud decoding approach. The simplicity of the decoding architectures and the increase in performance are strong indicators that sample-based decoders should be considered as a default in future studies and systems. In addition, we investigate the operation of the decoders to gain insight into how the output point clouds are generated from a latent shape representation.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel sequence decoder that is able to copy entire spans of the input to the output in a single step. A training objective is derived that encourages the decoder to copy long spans when possible, as well as an adapted beam search method approximating the exact objective. Experiments on natural language and program source code editing tasks show that the span-copying decoder improves performance.",
        "Abstract": "Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing source code. In this paper, we argue that existing seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. We present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which we handle by deriving a new objective for training and a variation of beam search for inference that explicitly handle this problem.\n\nIn our experiments on a range of editing tasks of natural language and source code, we show that our new model consistently outperforms simpler baselines.",
        "Introduction": "  INTRODUCTION Intelligent systems that assist users in achieving their goals have become a focus of recent research. One class of such systems are intelligent editors that identify and correct errors in documents while they are written. Such systems are usually built on the seq2seq ( Sutskever et al., 2014 ) framework, in which an input sequence (the current state of the document) is first encoded into a vector repre- sentation and a decoder then constructs a new sequence from this information. Many applications of the seq2seq framework require the decoder to copy some words in the input. An example is ma- chine translation, in which most words are generated in the target language, but rare elements such as names are copied from the input. This can be implemented in an elegant manner by equipping the decoder with a facility that can \"point\" to words from the input, which are then copied into the output ( Vinyals et al., 2015 ;  Grave et al., 2017 ;  Gulcehre et al., 2016 ;  Merity et al., 2017 ). Editing sequences poses a different problem from other seq2seq tasks, as in many cases, most of the input remains unchanged and needs to be reproduced. When using existing decoders, this requires painstaking word-by-word copying of the input. In this paper, we propose to extend a decoder with a facility to copy entire spans of the input to the output in a single step, thus greatly reducing the number of decoder steps required to generate an output. This is illustrated in  Figure 1 , where we show how our model inserts two new words into a sentence by copying two spans of (more than) twenty tokens each. However, this decoder extension exacerbates a well-known problem in training decoders with a copying facility: a target sequence can be generated in many different ways when an output token can be generated by different means. In our setting, a sequence of tokens can be copied token-by- token, in pairs of tokens, . . . , or in just a single step. In practice, we are interested in encouraging our decoder to use as few steps as possible, both to speed up decoding at inference time as well as to reduce the potential for making mistakes. To this end, we derive a training objective that marginalises over all different generation sequences yielding the correct output, which implicitly encourages copying longer spans. At inference time, we solve this problem by a variation of beam search that \"merges\" rays in the beam that generate the same output by different means. In summary, this paper (i) introduces a new sequence decoder able to copy entire spans (Sect. 2); (ii) derives a training objective that encourages our new decoder to copy long spans when possible, as well as an adapted beam search method approximating the exact objective; (iii) includes extensive experiments showing that the span-copying decoder improves on editing tasks on natural language and program source code (Sect. 4).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a review of cheminformatics approaches to drug discovery, focusing on the use of quantitative structure-activity-relationship (QSAR) models to predict the binding affinity of small molecules to proteins. It discusses the two main types of QSAR models, single-task and multi-task, and the methodological concerns associated with their use. It also outlines potential solutions to these issues, such as transfer learning and active learning.",
        "Abstract": "In-silico protein-ligand binding prediction is an ongoing area of research in computational chemistry and machine learning based drug discovery, as an accurate predictive model could greatly reduce the time and resources necessary for the detection and prioritization of possible drug candidates. Proteochemometric modeling (PCM) attempts to make an accurate model of the protein-ligand interaction space by combining explicit protein and ligand descriptors. This requires the creation of information-rich, uniform and computer interpretable representations of proteins and ligands. Previous work in PCM modeling relies on pre-defined, handcrafted feature extraction methods, and many methods use protein descriptors that require alignment or are otherwise specific to a particular group of related proteins. However, recent advances in representation learning have shown that unsupervised machine learning can be used to generate embeddings which outperform complex, human-engineered representations. We apply this reasoning to propose a novel proteochemometric modeling methodology which, for the first time, uses embeddings generated via unsupervised representation learning for both the protein and ligand descriptors. We evaluate performance on various splits of a benchmark dataset, including a challenging split that tests the model’s ability to generalize to proteins for which bioactivity data is greatly limited, and we find that our method consistently outperforms state-of-the-art methods.",
        "Introduction": "  INTRODUCTION A main goal of cheminformatics in the area of drug discovery is to model the interaction of small molecules with proteins in-silico. The ability to accurately predict the binding affinity of a ligand towards a biological target without the need to conduct expensive in-vitro experiments has the po- tential to accelerate the drug development process by enabling early prioritization of promising drug candidates ( Cortés-Ciriano et al., 2015 ). A common approach is to train a machine learning algo- rithm to predict the binding affinity of ligands towards a certain biological target using a training set of compounds that have been experimentally measured on this target. This modality is commonly referred to as a quantitative structure-activity-relationship (QSAR) model ( van Westen et al., 2011 ). QSAR models can be broadly classified into two types: single-task QSAR models and multi-task QSAR models ( Figure 1 ). In single-task QSAR modeling, a model is trained separately for each pro- tein to predict a binary or continuous outcome (binding vs not-binding or the binding affinity) given a compound input. The machine learning model used could be anything from logistic regression to deep neural networks ( Lenselink et al., 2017 ;  Cherkasov et al., 2014 ).In multi-task modeling, a single model is trained to predict binding across multiple proteins simultaneously, allowing the model to take advantage of the correlations in binding activity between compounds on different tar- gets (Caruana, 1997;  Yuan et al., 2016 ). This is done, for example, by using a neural network with multiple output nodes where each output node corresponds to a different protein. Thus, multiple outputs are predicted given a compound input ( Simões et al., 2018 ;  Dahl et al., 2014 ). While these methods have been employed on various protein targets, there are methodological con- cerns to their use ( Lima et al., 2016 ;  Mitchell, 2014 ). Both single-task and multi-task models must be retrained from scratch if one wishes to incorporate binding data for a new protein, and both cannot be used at all to make predictions on new protein targets for which experimental data is absent ( van Westen et al., 2011 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an approach to curriculum learning that uses an internal curriculum within the agent, rather than a progression of tasks or environments. The action space of reinforcement learning agents is grown progressively, starting with a severely restricted action space and eventually using the most unrestricted action space. This approach requires some domain knowledge to identify a suitable hierarchy of action spaces, but can be applied to discretised continuous action spaces and large combinatorial action spaces. Empirical results demonstrate the efficacy of the approach in two simple control tasks and in a challenging set of problems with combinatorial action spaces in the context of StarCraft micromanagement. The proposed method substantially improves sample efficiency compared to learning any particular action space from scratch, a number of ablations, and an actor-critic baseline.",
        "Abstract": "In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data,  value estimates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.",
        "Introduction": "  INTRODUCTION The value of curricula has been well established in machine learning, reinforcement learning, and in biological systems. When a desired behaviour is sufficiently complex, or the environment too unforgiving, it can be intractable to learn the behaviour from scratch through random exploration. Instead, by \"starting small\" ( Elman, 1993 ), an agent can build skills, representations, and a dataset of meaningful experiences that allow it to accelerate its learning. Such curricula can drastically improve sample efficiency ( Bengio et al., 2009 ). Typically, curriculum learning uses a progression of tasks or environments. Simple tasks that provide meaningful feedback to random agents are used first, and some schedule is used to introduce more challenging tasks later during training ( Graves et al., 2017 ). However, in many contexts neither the agent nor experimenter has such unimpeded control over the environment. In this work, we instead make use of curricula that are internal to the agent, simplifying the exploration problem without changing the environment. In particular, we grow the size of the action space of reinforcement learning agents over the course of training. At the beginning of training, our agents use a severely restricted action space. This helps explo- ration by guiding the agent towards rewards and meaningful experiences, and provides low variance updates during learning. The action space is then grown progressively. Eventually, using the most unrestricted action space, the agents are able to find superior policies. Each action space is a strict superset of the more restricted ones. This paradigm requires some domain knowledge to identify a suitable hierarchy of action spaces. However, such a hierarchy is often easy to find. Continuous action spaces can be discretised with increasing resolution. Similarly, curricula for coping with the large combinatorial action spaces induced by many agents can be obtained from the prior that nearby agents are more likely to need to coordinate. For example, in routing or traffic flow problems nearby agents or nodes may wish to adopt similar local policies to alleviate global congestion. Our method will be valuable when it is possible to identify a restricted action space in which random exploration leads to significantly more meaningful experiences than random exploration in the full action space. We propose an approach that uses off-policy reinforcement learning to improve sample efficiency in this type of curriculum learning. Since data from exploration using a restricted action space is still valid in the Markov Decision Processes (MDPs) corresponding to the less restricted action spaces, we can learn value functions in the less restricted action space with 'off-action-space' data collected by exploring in the restricted action space. In our approach, we learn value functions corresponding to each level of restriction simultaneously. We can use the relationships of these value functions to Under review as a conference paper at ICLR 2020 each other to accelerate learning further, by using value estimates themselves as initialisations or as bootstrap targets for the less restricted action spaces, as well as sharing learned state representations. Empirically, we first demonstrate the efficacy of our approach in two simple control tasks, in which the resolution of discretised actions is progressively increased. We then tackle a more challenging set of problems with combinatorial action spaces, in the context of StarCraft micromanagement with large numbers of agents (50-100). Given the heuristic prior that nearby agents in a multiagent setting are likely to need to coordinate, we use hierarchical clustering to impose a restricted action space on the agents. Agents in a cluster are restricted to take the same action, but we progressively increase the number of groups that can act independently of one another over the course of training. Our method substantially improves sample efficiency on a number of tasks, outperforming learning any particular action space from scratch, a number of ablations, and an actor-critic baseline that learns a single value function for the behaviour policy, as in the work of  Czarnecki et al. (2018) . Code is available, but redacted here for anonymity.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the simultaneous generation of chemical compounds and geometries. We propose a novel approach based on a variational autoencoder (VAE) that is able to generate molecular graphs and their corresponding geometries. We demonstrate that our model is able to generate molecular graphs and geometries of arbitrary size and complexity, and that it is able to recover compounds from the QM9 database.",
        "Abstract": "Generating point clouds, e.g., molecular structures, in arbitrary rotations, translations, and enumerations remains a challenging task. Meanwhile, neural networks\nutilizing symmetry invariant layers have been shown to be able to optimize their\ntraining objective in a data-efficient way. In this spirit, we present an architecture\nwhich allows to produce valid Euclidean distance matrices, which by construction are already invariant under rotation and translation of the described object.\nMotivated by the goal to generate molecular structures in Cartesian space, we use\nthis architecture to construct a Wasserstein GAN utilizing a permutation invariant critic network. This makes it possible to generate molecular structures in a\none-shot fashion by producing Euclidean distance matrices which have a three-\ndimensional embedding.",
        "Introduction": "  INTRODUCTION Recently there has been great interest in deep learning based on graph structures ( Defferrard et al., 2016 ;  Kipf & Welling, 2016 ;  Gilmer et al., 2017 ) and point clouds ( Qi et al., 2017 ;  Li et al., 2018b ;  Yang et al., 2019 ). A prominent application example is that of molecules, for which both inference based on the chemical compound, i.e., the molecular graph structure ( Kearnes et al., 2016 ;  Janet & Kulik, 2017 ;  Winter et al., 2019a ), and based on the geometry, i.e. the positions of atoms in 3D space ( Behler & Parrinello, 2007 ;  Rupp et al., 2012 ;  Schütt et al., 2017a ;  Smith et al., 2017 ) are active areas of research. A particularly interesting branch of machine learning for molecules is the reverse problem of gen- erating molecular structures, as it opens the door for designing molecules, e.g., obtain new materi- als ( Sanchez-Lengeling & Aspuru-Guzik, 2018 ; Barnes et al., 2018; Elton et al., 2018;  Li et al., 2018a ), design or discover pharmacological molecules such as inhibitors or antibodies ( Popova et al., 2018 ;  Griffen et al., 2018 ), optimize biotechnological processes ( Guimaraes et al., 2017 ). While this area of research has exploded in the past few years, the vast body of work has been done on the generation of new molecular compounds, i.e. the search for new molecular graphs, based on string encodings of that graph structure or other representations ( Gómez-Bombarelli et al., 2018 ;  Winter et al., 2019b ). On the other hand, exploring the geometry space of the individual chemical compound is equally important, as the molecular geometries and their probabilities determine all equilibrium properties, such as binding affinity, solubility etc. Sampling different geometric struc- tures is, however, still largely left to molecular dynamics (MD) simulation that suffers from the rare event sampling problem, although recently machine learning has been used to speed up MD simu- lation ( Ribeiro et al., 2018 ;  Bonati et al., 2019 ;  Zhang et al., 2019 ;  Plattner et al., 2017 ;  Doerr & Fabritiis, 2014 ) or to perform sampling of the equilibrium distribution directly, without MD ( Noé et al., 2019 ). All of these techniques only sample one single chemical compound in geometry space. Here we explore-to our best knowledge for the first time in depth-the simultaneous generation of chemical compounds and geometries. The only related work we are aware of ( Gebauer et al., 2018 ; 2019) demonstrates the generation of chemical compounds, placing atom by atom with an autoregressive model. It was shown that the model can recover compounds contained in the QM9 database of small molecules ( Ruddigkeit et al., 2012 ;  Ramakrishnan et al., 2014 ) when trained on a subset, but different configurations of the same molecule were not analyzed. While autoregressive models seem to work well in the case of small (< 9 heavy atoms) molecules like the ones in the QM9 database, they can be tricky for larger structures as the probability to completely form complex structures, such as rings, decays with the number of involved steps.",
        "label": 0
    },
    {
        "Summary": "\nAbstract: This paper introduces a novel type of generative model based on denoising density estimators (DDEs), which supports efficient sampling and density estimation. The approach to construct a sampler is to minimize the Kullback-Leibler (KL) divergence between the generated density and the input data density. A core component of this approach is the density estimator, which is derived from the theory of denoising autoencoders. Compared to normalizing flows, the theory does not require any specific network architecture, and does not need to solve ODEs. In contrast to GANs, adversarial training is not required. The contributions of this paper are a novel approach to obtain a generative model, a density estimator based on denoising autoencoders, and its parameterization using neural networks.",
        "Abstract": "Learning generative probabilistic models that can estimate the continuous density given a set of samples, and that can sample from that density is one of the fundamental challenges in unsupervised machine learning. In this paper we introduce a new approach to obtain such models based on what we call denoising density estimators (DDEs). A DDE is a scalar function, parameterized by a neural network, that is efficiently trained to represent a kernel density estimator of the data. In addition, we show how to leverage DDEs to develop a novel approach to obtain generative models that sample from given densities. We prove that our algorithms to obtain both DDEs and generative models are guaranteed to converge to the correct solutions. Advantages of our approach include that we do not require specific network architectures like in normalizing flows, ODE solvers as in continuous normalizing flows, nor do we require adversarial training as in generative adversarial networks (GANs). Finally, we provide experimental results that demonstrate practical applications of our technique.\n",
        "Introduction": "  INTRODUCTION Learning generative probabilistic models from raw data is one of the fundamental problems in unsu- pervised machine learning. The defining property of such models is that they provide functionality to sample from the probability density represented by the input data. In other words, such mod- els can generate new content, which has applications in image or video synthesis for example. In addition, generative probabilistic models may include capabilities to perform density estimation or inference of latent variables. Recently, the use of deep neural networks has led to significant ad- vances in this area. For example, generative adversarial networks ( Goodfellow et al., 2014 ) can be trained to sample very high dimensional densities, but they do not provide density estimation or inference. Inference in Boltzman machines ( Salakhutdinov & Hinton, 2009 ) is tractable only under approximations ( Welling & Teh, 2003 ). Variational autoencoders ( Kingma & Welling, 2014 ) pro- vide functionality for both (approximate) inference and sampling. Finally, normalizing flows ( Dinh et al., 2014 ) perform all three operations (sampling, density estimation, inference) efficiently. In this paper we introduce a novel type of generative model based on what we call denoising density estimators (DDEs), which supports efficient sampling and density estimation. Our approach to construct a sampler is straightforward: assuming we have a density estimator that can be efficiently trained and evaluated, we learn a sampler by forcing its generated density to be the same as the input data density via minimizing their Kullback-Leibler (KL) divergence. A core component of this approach is the density estimator, which we derive from the theory of denoising autoencoders, hence our term denoising density estimator. Compared to normalizing flows, a key advantage of our theory is that it does not require any specific network architecture, except differentiability, and we do not need to solve ODEs like in continuous normalizing flows. In contrast to GANs, we do not require adversarial training. In summary, our contributions are as follows: • A novel approach to obtain a generative model by explicitly estimating the energy (un- normalized density) of the generated and true data distributions and minimizing the statis- tical divergence of these densities. • A density estimator based on denoising autoencoders called denoising density estimator (DDE), and its parameterization using neural networks, which we leverage to train our novel generative model.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a data-driven machine learning approach to learn heuristics used inside combinatorial optimization algorithms, such as the simplex or integer programming algorithms. This approach can be interpreted as instantiating the converse of the No-Free Lunch (NFL) theorem of search and optimization, which states that any advances made in quality or speed must be due to the algorithm's specialization to the distribution/family of instances it encounters. This approach can be used to design new pivot rules based on properties of the instance family, and can replace approaches which used to rely heavily on hand-designed features.",
        "Abstract": "Linear Programs (LPs) are a fundamental class of optimization problems with a wide variety of applications. Fast algorithms for solving LPs are the workhorse of many combinatorial optimization algorithms, especially those involving integer programming. One popular method to solve LPs is the simplex method which, at each iteration, traverses the surface of the polyhedron of feasible solutions. At each vertex of the polyhedron, one of several heuristics chooses the next neighboring vertex, and these vary in accuracy and computational cost. We use deep value-based reinforcement learning to learn a pivoting strategy that at each iteration chooses between two of the most popular pivot rules -- Dantzig and steepest edge. \nBecause the latter is typically more accurate and computationally costly than the former, we assign a higher wall time-based cost to steepest edge iterations than Dantzig iterations. We optimize this weighted cost on a neural net architecture designed for the simplex algorithm. We obtain between 20% to 50% reduction in the gap between weighted iterations of the individual pivoting rules, and the best possible omniscient policies for LP relaxations of randomly generated instances of five-city Traveling Salesman Problem. ",
        "Introduction": "  INTRODUCTION Machine learning has revolutionized many fields by leveraging large amounts of data to learn functions, replacing approaches which used to rely heavily on hand-designed features. One area where machine learning has not made much of an impact is heuristics used inside combinatorial optimization algorithms such as the simplex or integer programming algorithms. The No-Free Lunch (NFL) theorem ( Wolpert et al., 1997 ) of search and optimization essentially says there are no general-purpose optimization algorithms that work well on all problems. In practice, this implies that many optimization algorithms rely on several different kinds of hand-designed heuristics. But these lack theoretical guarantees, and they embody a one-size-fits-all approach that cannot specialize to the instance distribution. A clear historical example is that of linear programs (LPs). LPs require specification of a pivoting rule, and decades of research has generated many different options. However, no theory exists for how to choose pivoting rules based on the LP instance distributions encountered in practice, let alone how to design new pivot rules based on properties of the instance family. We propose to address this issue by using data-driven machine learning approaches to learn such heuristics, based on the data encountered by the combinatorial optimization algorithms in practice. Our approach can be interpreted as instantiating the converse of the NFL theorem: any advances made in quality or speed must be due to the algorithm's specialization to the distribution/family of instances it encounters.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents AutoLR, a novel automatic learning rate tuning scheme that incorporates an Explore-Exploit approach to achieve state of the art test accuracy on standard benchmarks. AutoLR utilizes an initial Explore phase with a high learning rate to land in a wide minima, followed by an Exploit phase that uses a local approximation of the optimization landscape to tune the learning rate. Extensive evaluation of AutoLR across a wide range of models and datasets, ranging from NLP to CNNs, and across multiple optimizers, demonstrates that AutoLR matches or beats test accuracy of state-of-the-art hand-tuned learning rate schedules.",
        "Abstract": "One very important hyperparameter for training deep neural networks is the\nlearning rate of the optimizer. The choice of learning rate schedule determines\nthe computational cost of getting close to a minima, how close you actually get\nto the minima, and most importantly the kind of local minima (wide/narrow)\nattained. The kind of minima attained has a significant impact on the\ngeneralization accuracy of the network. Current systems employ hand tuned\nlearning rate schedules, which are painstakingly tuned for each network and\ndataset. Given that the state space of schedules is huge, finding a\nsatisfactory learning rate schedule can be very time consuming. In this paper,\nwe present AutoLR, a method for auto-tuning the learning rate as training\nproceeds. Our method works with any optimizer, and we demonstrate results on\nSGD, Momentum, and Adam optimizers.\n\nWe extensively evaluate AutoLR on multiple datasets, models, and across\nmultiple optimizers. We compare favorably against state of the art learning\nrate schedules for the given dataset and models, including for ImageNet on\nResnet-50, Cifar-10 on Resnet-18, and SQuAD fine-tuning on BERT. For example,\nAutoLR achieves an EM score of 81.2 on SQuAD v1.1 with BERT_BASE compared to\n80.8 reported in (Devlin et al. (2018)) by just auto-tuning the learning rate\nschedule. To the best of our knowledge, this is the first automatic learning\nrate tuning scheme to achieve state of the art generalization accuracy on these\ndatasets with the given models.\n",
        "Introduction": "  INTRODUCTION Learning rate is one of the most important hyperparameters that impact deep neural network (DNN) training performance. It determines how fast we get close to a minima, which in turn determines the computational cost of optimization. It also determines how close we get to the minima, e.g. higher learning rates may get in the neighborhood of a minima much faster, but then just bounce at a height above the minima ( Xing et al. (2018) ;  Nar & Sastry (2018) ). The learning rate also determines the kind of minima (e.g. wide vs narrow) attained (Keskar et al. (2016)), which has a significant impact on the generalization accuracy. Therefore, it is not surprising that a lot of effort has gone into automatically tuning the learning rate (Schraudolph (1999); Schaul et al. (2013b);  Rolinek & Martius (2018) ). However, till date, none of these techniques have been able to deliver state of the art test accuracy on standard benchmarks. Instead, deep learning researchers today rely on a mixture of brute force search, augmented with simple heuristics such as using a staircase, polynomial, or exponential decay-based learning rate schedules. This problem is further exacerbated by the fact that different optimizers such as SGD or Adam work best on different datasets and require very different learning rate schedules. For example, while training image datasets such as Cifar-10 and ImageNet with SGD or Momentum, a staircase schedule with high starting learning rate typically performs best. On the other hand, with the Adam optimizer, a smaller starting learning rate is generally used. Further, in NLP tasks such as machine translation with Adam, typically a linear warmup is followed by polynomial decay, e.g. the Transformer network (Vaswani et al. (2017)) uses a linear warmup followed by an inverse square root decay. Similar schedules are used for BERT ( Devlin et al. (2018) ) pre-training, while BERT fine-tuning uses a linear decay. Our key idea to tackle this problem is driven by the following simple observation. Consider training Cifar-10/ImageNet using the typical staircase schedule. One can observe that the initial high learning Under review as a conference paper at ICLR 2020 rate results in training loss stagnating after a few epochs. However, if one were to decay the learning rate when loss stagnates, the achieved test accuracy is considerably lower as compared to letting the high learning rate continue for longer, despite no apparent improvement in training loss. Our insight is that this initial phase of training using a high learning rate, even with no improvement in loss, is crucial for generalization and is one of the key missing pieces from prior attempts at automatic learning rate tuning. We hypothesize that for DNNs, the number of narrow minimas far outnumber the wide minimas. To generalize well, we want the optimizer to land in wide minimas. An interesting intuitive observation is that a large learning rate can escape narrow minimas easily (as the optimizer can jump out of them with large steps), however once it reaches a wide minima, it is likely to get stuck in it (if the \"width\" of the wide minima is large compared to the step size). The above hypothesis motivates our Explore-Exploit scheme where we force the optimizer to first explore the landscape with a high learning rate for sometime in order to land in a wide minima. We should give the explore phase enough time so that the probability of landing in a wide minima is high. Once we are confident that the optimizer is stuck in the vicinity of a wide minima, we activate the Exploit phase of AutoLR. The basic idea of the exploit phase is as follows. We first look at how local perturbations in the current learning rate impact the training loss. Since we only look at local perturbations, we can model the loss as a function of these perturbations via a Taylor series expansion. We then make a quadratic approximation, and solve for the optimal perturbation in the learning rate which will minimize the loss. This is similar to Newton's method, but applied only in the descent direction (section 2). We do extensive validation that the quadratic is a good approximation here. Although the basic idea is simple, it is complicated by the fact that deep learning relies on stochastic optimization methods, such as SGD, which causes the loss values to be noisy across mini-batches, potentially throwing off our estimates. We discuss how we handle stochasticity in section 2.2. We demonstrate the efficacy of the explore-exploit approach by evaluating AutoLR across a wide range of models and datasets, ranging from NLP (SQuAD on BERT-base, Transformer on IWSLT) to CNNs (e.g. ImageNet on ResNet-50, Cifar-10 on ResNet18), and across multiple optimizers: SGD, Momentum and Adam. In all cases, AutoLR matches or beats test accuracy of state-of-the- art hand-tuned learning rate schedules. For example, on SQuAD v1.1 fine-tuning with BERT BASE , AutoLR is able achieve an EM score of 81.2, compared to 80.8 reported in  Devlin et al. (2018)  by just auto-tuning the learning rate schedule (all other parameters were unchanged). To the best of our knowledge, AutoLR is the first automatic learning rate scheduler to achieve state of the art results on these datasets for the given models. We show extensive evaluation of our method in section 3. The main contributions of our work are: 1. The observation that an initial Explore phase with high learning rate is crucial for good generalization in automatic learning rate schemes. 2. Incorporating this observation via an Explore-Exploit approach with a novel exploitation scheme for tuning learning rate using a local approximation of the optimization landscape. 3. The first automatic learning rate tuning scheme that beats or achieves generalization of state of the art learning rate schedules in multiple models/datasets including ImageNet.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to address the vulnerability of modern neural networks to adversarial perturbations. We derive a global bound on the Lipschitz constant of the gradient of deep neural networks with differentiable activation functions, providing an upper bound on the magnitude of the eigenvalues of the Hessian or the curvature values of the classification network. Using this global curvature bound, we develop computationally efficient methods for both the robustness certification and the adversarial attack problems. We also show that using our proposed curvature bounds as a regularizer during training leads to improved certified accuracy on 2,3 and 4 layer networks compared to standard adversarial training.",
        "Abstract": "A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\\it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for deep classifiers is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for deep classifiers with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded, we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness against adversarial examples. Putting these results together leads to our proposed {\\bf C}urvature-based {\\bf R}obustness {\\bf C}ertificate (CRC) and {\\bf C}urvature-based {\\bf R}obust {\\bf T}raining (CRT). Our numerical results show that CRC outperforms CROWN's certificate by an order of magnitude while CRT leads to higher certified accuracy compared to standard adversarial training and TRADES.\n",
        "Introduction": "  INTRODUCTION Modern neural networks achieve high accuracy on tasks such as image classification and speech recognition, but are known to be brittle to small, adversarially chosen perturbations of their inputs ( Szegedy et al., 2014 ). A classifier which correctly classifies an image x, can be fooled by an adversary to misclassify an adversarial example x + δ, such that x + δ is indistinguishable from x to a human. Adversarial examples can also fool systems when they are printed out on a paper and photographed with a smart phone ( Kurakin et al., 2016a ). Even in a black box threat model, where the adversary has no access to the model parameters, attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign that the vehicle would interpret as a yield or another sign ( Papernot et al., 2016 ). This trend is worrisome and suggests that these vulnerabilities need to be appropriately addressed before neural networks can be deployed in security critical applications. In the last couple of years, several empirical defenses have been proposed for training classifiers to be robust against adversarial perturbations ( Madry et al., 2018 ;  Samangouei et al., 2018 ;  Zhang et al., 2019 ;  Papernot et al., 2016 ;  Kurakin et al., 2016b ;  Miyato et al., 2017 ;  Zheng et al., 2016 ) Although these defenses robustify classifiers to particular types of attacks, they can be still vulnerable against stronger attacks ( Athalye et al., 2018 ;  Carlini & Wagner, 2017 ;  Uesato et al., 2018 ;  Athalye & Carlini, 2018 ). For example, ( Athalye et al., 2018 ) showed most of the empirical defenses proposed in ICLR 2018 can be broken by developing tailored attacks for each of them. To end the cycle between defenses and attacks, a line of work on certified defenses has gained attention where the goal is to train classifiers whose predictions are provably robust within some given region ( Huang et al., 2016 ;  Katz et al., 2017 ;  Ehlers, 2017 ;  Carlini et al., 2017 ;  Cheng et al., 2017 ;  Lomuscio & Maganti, 2017 ;  Dutta et al., 2018 ;  Fischetti & Jo, 2018 ;  Bunel et al., 2017 ;  Wang et al., 2018a ;  Wong & Kolter, 2017 ;  Wang et al., 2018b ;  Wong et al., 2018 ;  Raghunathan et al., 2018b ;a;  Dvijotham et al., 2018a ; b ;  Croce et al., 2018 ;  Singh et al., 2018 ;  Gowal et al., 2018 ;  Gehr et al., 2018 ;  Mirman et al., 2018 ;  Zhang et al., 2018b ;  Weng et al., 2018 ). These methods, however, do not scale to large and practical networks used in solving modern machine learning problems. Another line of Under review as a conference paper at ICLR 2020 defense work focuses on randomized smoothing where the prediction is robust within some region around the input with a user-chosen probability ( Liu et al., 2017 ;  Cao & Gong, 2017 ;  Lécuyer et al., 2018 ;  Li et al., 2018 ;  Cohen et al., 2019 ;  Salman et al., 2019 ). Although these methods can scale to large networks, certifying robustness with probability close to 1 often requires generating a large number of noisy samples around the input which leads to high test-time computational complexity. If the classifier f (.) was linear, the distance of an input point x to its decision boundary (i.e. the robustness certificate) can be computed efficiently using a convex optimization. For example, the l 2 robustness certificate in that case would be equal to f (x) ∇ x f (x) . However, modern classifiers based on neural networks are not linear and and can have non-zero curvatures in different parts of the input domain. The deviation of the classifier from the linear model makes the robustness certification problem to be a non-convex optimization which is often difficult to solve exactly. However, if we could compute global bounds on the maximum curvature values of the classification network, one may be able to compute computationally-efficient lower bounds on the robustness certificate even for non-linear deep classifiers. This is the key intuition of our results in this paper. In this work, we derive a global bound on the Lipschitz constant of the gradient of deep neural networks with differentiable activation functions (such as sigmoid, tanh, softplus, etc.). This provides an upper bound on the magnitude of the eigenvalues of the Hessian or the curvature values of the classification network. Using this global curvature bound and for the l 2 metric, we tackle both the certification and attack problems. In the certification problem and for a given pre-trained classifier, we provide a computationally-efficient lower bound on the distance of a point to the classification decision boundary. In the related attack problem, for a given input and a region around it, our goal is to find a perturbed input (an adversarial example) that maximizes the loss inside the given region. The outcome of the attack problem is then used in the adversarial training procedure ( Madry et al., 2018 ) to further robustify the network. Furthermore, our global curvature bound is differentiable and we show that adding it to the loss function as a regularizer boosts certified robustness measures. We note that other recent works (e.g.  Moosavi Dezfooli et al. (2019) ;  Qin et al. (2019) ) empirically show that using an estimate of curvature at inputs as a regularizer leads to empirical robustness on par with the adversarial training. In this work, however, we use a provable global upper bound on the curvature (and not an estimate) as a regularizer and show that it results in high certified robustness. Moreover, previous works have tried to certify robustness by bounding the Lipschitz constant of the neural network ( Szegedy et al., 2014 ;  Peck et al., 2017 ;  Zhang et al., 2018c ;  Anil et al., 2018 ;  Hein & Andriushchenko, 2017 ). Our approach, however, is based on bounding the Lipschitz constant of the gradient of deep neural networks. We discuss existing works in more details in Appendix A. Below, we state the key theoretical results of this paper informally while detailed statements of these results are presented in Section 4. Theorem (informal) 1. Let z (L) i denotes the i th logit of an L layer fully-connected neural network with differentiable activation functions. Then, the curvature of the neural network function is globally bounded as follows: mI ≼ ∇ 2 x z (L) i ≼ M I, ∀x ∈ R D where m and M can be computed efficiently using parameters of the network. This result along with the min-max theorem leads to the following curvature robustness certificate: Theorem (informal) 2. Consider a network whose curvature values are bounded. For a given input x (0) with the true label y and the attack target t (t ≠ y), let p * cert denote the exact robustness certificate, i.e. the distance of x (0) to the decision boundary. We can efficiently compute d * cert such that d * cert ≥ p * cert . Moreover, if the solution x (cert) for d * cert satisfies z We have similar results for the attack problem. For simplicity, we summarize definitions of p * cert , d * cert , p * attack , d * attack in  Table 1 . In summary, in this paper, we make the following contributions: • We provide global bounds on the eigenvalues of the Hessian of a deep neural network with differentiable activation functions (Theorem 3 and Theorem 4). In addition to the adversarial robustness problem, these bounds may be of an independent interest for readers. When is dual solvable? • Using the global curvature bounds, we develop computationally efficient methods for both the robustness certification as well as the adversarial attack problems (Theorems 1 and 2). • We show that using our proposed curvature bounds as a regularizer during training leads to improved certified accuracy on 2,3 and 4 layer networks (on the MNIST dataset) compared to standard adversarial training with PGD ( Madry et al., 2018 ) as well as TRADES ( Zhang et al., 2019 ). Moreover, our robustness certificate (CRC) outperforms CROWN's certificate ( Zhang et al., 2018b ) significantly while taking less time to compute.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method for solving challenging reinforcement learning environments by combining recent progress in AI, such as ensembles for uncertainty modeling, risk measures for exploration, AlphaZero Monte-Carlo Tree Search for planning, and self-imitation learning and hindsight for value function training. Experiments are conducted on the Sokoban, ChainEnv, and Toy Montezuma's Revenge environments, and results show that the proposed method is able to solve these environments with sparse rewards.",
        "Abstract": "We propose a reinforcement learning framework for discrete environments in which an agent optimizes its behavior on two timescales. For the short one, it uses tree search methods to perform tactical decisions. The long strategic level is handled with an ensemble of value functions learned using $TD$-like backups. Combining these two techniques brings synergies. The planning module performs \\textit{what-if} analysis allowing to avoid short-term pitfalls and boost backups of the value function. Notably, our method performs well in environments with sparse rewards where standard $TD(1)$ backups fail. On the other hand, the value functions compensate for inherent short-sightedness of planning. Importantly, we use ensembles to measure the epistemic uncertainty of value functions. This serves two purposes: a) it stabilizes planning, b) it guides exploration. \n\nWe evaluate our methods on discrete environments with sparse rewards: the Deep sea chain environment, toy Montezuma's Revenge, and Sokoban. In all the cases, we obtain speed-up of learning and boost to the final performance.",
        "Introduction": "  Introduction The model-free and model-based approaches to reinforcement learning (RL) have a comple- mentary set of strengths and weaknesses. While the former offers good asymptotic perfor- mance, it suffers from poor sample complexity. In contrast, the latter usually needs signif- icantly less training samples, but often fails to achieve state-of-the-art results on complex tasks (which is primarily attributed to models' imperfections). The interplay of model-based and model-free approaches in RL has received a lot of research attention. This led, for ex- ample, to strong AI systems like  Silver et al. (2017 ; 2018) or more recently to  Lowrey et al. (2018) , which is closely related to our work. When dealing with challenging new RL domains it is helpful to develop tools for address- ing strategic and tactical decision-making. These two perspectives complement each other: strategic perspective is global, static, and often imprecise, while tactical perspective is local, dynamic and exact. We argue that value function can be considered as an example of the former, while a planner as an example of the latter. Indeed, value function approximators provide noisy estimates (imprecise) of values (static) to every state (global). Conversely, planning provides optimal control, which starting from a given state (local) generates actions (dynamic) that are temporally coherent and result in a better-executed trajectories (exact). It is only reasonable to combine the two into one system. This is achieved by plugging the value function into the planner, to provide guided heuristics in the local search, shorten the search horizon, and make the search computationally efficient. For complex problems, the above setup has to be supplemented by an exploration strategy. Such a strategy might be based on modeling uncertainty of the value function approximation (e.g., using dropout sam- pling, variational inference, distributional RL, bootstrap ensemble, etc.). The uncertainty is quantified by a risk measure, which is then utilized by a planner to guide exploration. Consider a situation of an agent with limited memory and computational resources, being dropped into a complex and diverse environment. This setup excludes any hope for brute force strategy and implies that a perfect planner is out of reach. In this paper, we con- sider Sokoban, a classic logical puzzle known for its combinatorial complexity and a recent benchmark for RL, ChainEnv, a seemingly impossible task also known as a 'hay in a needle- Under review as a conference paper at ICLR 2020 stack' problem (see  Osband et al. (2018) ), and Toy Montezuma's Revenge, environment notoriously known for its exploration difficulty (see  Guo et al. (2019) ). Importantly, we focus on sparse reward variants of the aforementioned environments (see Section 3). Since of our method involves planning, an agent has to have an access to the world model. In this paper we assume that it is the perfect model. The reason is that challenges presented by the selected environments are the dominating factor driving difficulty of the task (in fact, answering the question of whether a Sokoban level is solvable is NP-hard, see e.g.  Dor & Zwick (1999) ). It is also in part due to this fact that a significant body of work dealing with difficult environments makes a similar assumption (see e.g.  Silver et al. (2017 ; 2018),  Orseau et al. (2018b) ). The main contribution of this work is showing how recent progress in AI can be brought to- gether to improve planning, value function learning, and exploration, in a way that together they form robust algorithms for solving challenging reinforcement learning environments. In particular: 1. For uncertainty modeling, we assume the point of view  Osband et al. (2018) , which uses ensembles to approximate posterior distribution. 2. In the spirit of  Lowrey et al. (2018) , we incorporate risk measures to guide explo- ration. 3. For the planner, we base on AlphaZero Monte-Carlo Tree Search (see  Silver et al. (2017) ), which we enhance with the possibility of exploiting the graph structure of environments and the properties of the optimal solutions. 4. In the value function training protocol, we introduce several improvements, includ- ing a version of self imitation learning mechanism ( Oh et al. (2018a) ) and hindsight ( Andrychowicz et al. (2017) ). The rest of the paper is organized as follows. In the next subsection we provide an overview of related work. In Section 2 we present and discuss our method in details. This is followed by a section with experiments and passing to conclusions. We provide code to our work https://github.com/learningandplanningICLR/learningandplanning and a dedicated website https://sites.google.com/view/learn-and-plan-with-ensembles with more details and movies.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents SoftAdam, a new optimizer that combines stochastic gradient descent (SGD) with an adaptive gradient and momentum. SoftAdam is expected to improve optimization results across a variety of problems, while also providing a convergence guarantee. It is an improvement over Adam, which has been successful at speeding up training and solving particular problems, but has been shown to have worse generalization at other problems.",
        "Abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.",
        "Introduction": "  INTRODUCTION One of the most common methods of training neural networks is stochastic gradient descent (SGD) ( Bottou et al. (2016) ). SGD has strong theoretical guarantees, including convergence in locally non-convex optimization problems ( Lee et al. (2016) ). It also shows improved generalization and stability when compared to other optimization algorithms ( Smith & Le (2018) ). There have been various efforts in improving the speed and generalization of SGD. One popular modification is to use an adaptive gradient ( Duchi et al. (2011) ), which scales the gradient step size to be larger in directions with consistently small gradients. Adam, an implementation that combines SGD with momentum and an adaptive step size inversely proportional to the RMS gradient, has been particularly successful at speeding up training and solving particular problems (Kingma & Ba (2014)). However, at other problems it pays a penalty in worse generalization ( Wilson et al. (2017) ;  Keskar & Socher (2017) ), and it requires additional modifications to achieve a convergence guarantee ( Reddi et al. (2018) ;  Li & Orabona (2018) ). Here we develop an intuition for adaptive gradient methods that allows us to unify Adam with SGD in a natural way. The new optimizer, SoftAdam, descends in a direction that mixes the SGD with Adam update steps. As such, it should be able to achieve equal or better optimization results across a variety of problems.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper studies the intrinsic bias of the parameter-function map for deep neural networks (DNNs). It is shown that DNNs have a strong intrinsic bias that allows for good generalisation, in spite of being in the overparameterised regime. The a-priori probability P (f ) of a DNN is defined as the probability that a particular function f is produced upon random sampling of the weight and threshold bias parameters. It is argued that studying neural networks with random parameters is not just relevant to find good initializations for optimization, but also to understand their generalization. It is further shown that for a wide range of maps M that obey a number of conditions such as being simple and having redundancy, they will be exponentially biased towards outputs of low Kolmogorov complexity. The parameter-function map of neural networks satisfies these conditions, and it is found empirically that the probability P (f ) of obtaining a function f upon random sampling of parameter weights satisfies the simplicity-bias bound. Additionally, some work is done to obtain a finer quantitative understanding of the simplicity bias of neural networks, taking some notion of \"smoothness\" as a tractable proxy for the complexity of a function.",
        "Abstract": "Understanding the inductive bias of neural networks is critical to explaining their ability to generalise.  Here,  \nfor one of the simplest neural networks -- a single-layer perceptron with $n$ input neurons,  one output neuron, and no threshold bias term -- we prove that upon random initialisation of weights, the a priori probability  $P(t)$ that it represents a Boolean function that classifies $t$ points in $\\{0,1\\}^n$ as $1$ has a remarkably simple form: $\nP(t) = 2^{-n} \\,\\, {\\rm for} \\,\\, 0\\leq t < 2^n$.\nSince a perceptron can express far fewer Boolean functions with small or large values of $t$ (low \"entropy\") than with intermediate values of $t$ (high \"entropy\") there is, on average, a strong intrinsic a-priori bias towards individual functions with low entropy. Furthermore, within a class of functions with fixed $t$, we often observe a further intrinsic bias towards functions of lower complexity.\nFinally, we prove that, regardless of the distribution of inputs, the bias towards low entropy becomes monotonically stronger upon adding ReLU layers, and empirically show that increasing the variance of the bias term has a similar effect.",
        "Introduction": "  INTRODUCTION In order to generalise beyond training data, learning algorithms need some sort of inductive bias. The particular form of the inductive bias dictates the performance of the algorithm. For one of the most important machine learning techniques, deep neural networks (DNNs) (LeCun et al., 2015), sources of inductive bias can include the architecture of the networks, e.g. the number of layers, how they are connected, say as a fully connected network (FCN) or as a convolutional neural net (CNN), and the type of optimisation algorithm used, e.g. stochastic gradient descent (SGD) versus full gradient descent (GD). Many further methods such as dropout (Srivastava et al., 2014), weight decay (Krogh & Hertz, 1992) and early stopping (Morgan & Bourlard, 1990) have been proposed as techniques to improve the inductive bias towards desired solutions that generalise well. What is particularly surprising about DNNs is that they are highly expressive and work well in the heavily overparameterised regime where traditional learning theory would predict poor generalisation due to overfitting (Zhang et al., 2016). DNNs must therefore have a strong intrinsic bias that allows for good generalisation, in spite of being in the overparameterised regime. Here we study the intrinsic bias of the parameter-function map for neural networks, defined in (Valle- Pérez et al., 2018) as the map between a set of parameters and the function that the neural net- work represents. In particular, we define the a-priori probability P (f ) of a DNN as the probability that a particular function f is produced upon random sampling (or initialisation) of the weight and threshold bias parameters. The prior at initialization, P (f ), should inform the inductive bias of SGD-trained neural networks, as long as SGD approximates Bayesian inference with P (f ) as prior sufficiently well Valle-Pérez et al. (2018). We explain this connection further, and give some ev- idence supporting this behavior of SGD, in Appendix L. This supports the idea studying neural networks with random parameters Poole et al. (2016); Lee et al. (2018); Schoenholz et al. (2017); Garriga-Alonso et al. (2018); Novak et al. (2018) is not just relevant to find good initializations for optimization, but also to understand their generalization. A naive null-model for P (f ) might suggest that without further information, one should expect that all functions are equally likely. However, recent very general arguments (Dingle et al., 2018) based on the coding theorem from Algorithmic Information Theory (AIT) (Li et al., 2008) have instead suggested that for a wide range of maps M that obey a number of conditions such as being simple Under review as a conference paper at ICLR 2020 (they have a low Kolmogorov complexity K(M )) and redundancy (multiple inputs map to the same output) then if they are sufficiently biased, they will be exponentially biased towards outputs of low Kolmogorov complexity. The parameter-function map of neural networks satisfies these conditions, and it was found empirically (Valle-Pérez et al., 2018) that, as predicted in (Dingle et al., 2018), the probability P (f ) of obtaining a function f upon random sampling of parameter weights satisfies the following simplicity-bias bound P (f ) 2 −(b K(f )+a) , (1) where K(f ) is a computable approximation of the true Kolmogorov complexity K(f ), and a and b are constants that depend on the network, but not on the functions. It is widely expected that real world data is highly structured, and so has a relatively low Kolmogorov complexity (Hinton & Van Camp, 1993; Schmidhuber, 1997). The simplicity bias described above may therefore be an important source of the inductive bias that allows DNNs to generalise so well (and not overfit) in the highly over-parameterised regime (Valle-Pérez et al., 2018). Nevertheless, this bound has limitations. Firstly, the only rigorously proven result is for the true Kol- mogorov complexity version of the bound in the case of large enough K(f ). Although it has been found to work remarkably well for small systems and computable approximations to Kolmogorov complexity (Valle-Pérez et al., 2018; Dingle et al., 2018), this success is not yet fully understood theoretically. Secondly, it does not explain why models like DNNs are biased; it only explains that, if they are biased, they should be biased towards simplicity. Also, the AIT bound is very general - it predicts a probability P (f ) that depends mainly on the function, and only weakly on the net- work. It may therefore not capture some variations in the bias that are due to details of the network architecture, and which may be important for practical applications. For these reasons it is of interest to obtain a finer quantitative understanding of the simplicity bias of neural networks. Some work has been done in this direction, showing that infinitely wide neural networks are biased towards functions which are robust to changes in the input (De Palma et al., 2018), showing that \"flatness\" is connected to function smoothness (Wu et al., 2016), or arguing that low Fourier frequencies are learned first by a ReLU neural network (Rahaman et al., 2018; Yang & Salman, 2019). All of these papers take some notion of \"smoothness\" as tractable proxy for the complexity of a function. One generally expects smoother functions to be simpler, although this is clearly a very rough measure of the Kolmogorov complexity.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the use of heuristic search wrappers for feature selection in classification problems. It formalizes the problem as a minimization of a loss function subject to a constraint on the number of features, and divides wrapper methods into three types: exhaustive search wrappers, random search wrappers, and heuristic search wrappers. It focuses on heuristic search wrappers, which are more computationally efficient than exhaustive search wrappers and have deterministic guarantees on the set of selected salient features.",
        "Abstract": "We propose a computationally efficient wrapper feature selection method - called Autoencoder and Model Based Elimination of features using Relevance and Redundancy scores (AMBER) - that uses a single ranker model along with autoencoders to perform greedy backward elimination of features. The ranker model is used to prioritize the removal of features that are not critical to the classification task, while the autoencoders are used to prioritize the elimination of correlated features. We demonstrate the superior feature selection ability of AMBER on 4 well known datasets corresponding to different domain applications via comparing the accuracies with other computationally efficient state-of-the-art feature selection techniques. Interestingly, we find that the ranker model that is used for feature selection does not necessarily have to be the same as the final classifier that is trained on the selected features. Finally, we hypothesize that overfitting the ranker model on the training set facilitates the selection of more salient features.",
        "Introduction": "  INTRODUCTION Feature selection is a preprocessing technique that ranks the significance of features to eliminate features that are insignificant to the task at hand. As examined by  Yu and Liu (2003) , it is a powerful tool to alleviate the curse of dimensionality, reduce training time and increase the accuracy of learn- ing algorithms, as well as to improve data comprehensibility. For classification problems,  Weston et al. (2001)  divide feature selection problems into two types: (a) given a fixed k d, where d is the total number of features, find the k features that lead to the least classification error and (b) given a maximum expected classification error, find the smallest possible k. In this paper, we will be focusing on problems of type (a).  Weston et al. (2001)  formalize this type of feature selection problems as follows. Given a set of functions y = f (x, α), find a mapping of data x → (x * σ), σ ∈ {0, 1} d , along with the parameters α for the function f that lead to the minimization of τ (σ, α) = V (y, f ((x * σ), α))dP (x, y), (1) subject to σ 0 = k, where the distribution P (x, y) - that determines how samples are generated - is unknown, and can be inferred only from the training set, x*σ = (x 1 σ 1 , . . . , x d σ d ) is an elementwise product, V (·, ·) is a loss function and · 0 is the L 0 -norm. Feature selection algorithms are of 3 types: Filter, Wrapper, and Embedded methods. Filters rely on intrinsic characteristics of data to measure feature importance while wrappers iteratively mea- sure the learning performance of a classifier to rank feature importance.  Li et al. (2017)  assert that although filters are more computationally efficient than wrappers, due to the absence of a learning algorithm that supervises the selection of features, the features selected by filters are not as good as those selected by wrappers. Embedded methods use the structure of learning algorithms to embed feature selection into the underlying model to reconcile the efficiency advantage of filters with the learning algorithm interaction advantage of wrappers. As examined by  Saeys et al. (2007) , embed- ded methods are model dependent because they perform feature selection during the training of the learning algorithm. This serves as a motivation for the use of wrapper methods that are not model de- pendent.  Weston et al. (2001)  define wrapper methods as an exploration of the feature space, where the saliency of subsets of features are ranked using the estimated accuracy of a learning algorithm. Hence, τ (σ, α) in (1) can be approximated by minimizing Under review as a conference paper at ICLR 2020 subject to σ ∈ {0, 1} d , where τ alg is a classifier having estimates of α. Wrappers methods can further be divided into three types: Exhaustive Search Wrappers, Random Search Wrappers, and Heuristic Search Wrappers. We will focus on Heuristic Search Wrappers that iteratively select or eliminate one feature at each iteration because unlike Exhaustive Search Wrappers, they are more computationally efficient and unlike Random Search Wrappers, they have deterministic guarantees on the set of selected salient features, as illustrated in  Hira and Gillies (2015) .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a novel approach to improve the state of geodesics in deep latent-variable models with respect to runtime and use the geodesic as a distance metric. The proposed approach, called flat manifold variational autoencoder, uses a powerful hierarchical prior representation in the context of VAEs and the simple penalisation of the curvature of the decoder. The results show that the model features geodesics which are approximated very well by the Euclidean distance, effectively removing the need for numerical optimisation and resulting in a speedup of several orders of magnitude.",
        "Abstract": "Latent-variable models represent observed data by mapping a prior distribution over some latent space to an observed space.  Often, the prior distribution is specified by the user to be very simple, effectively shifting the burden of a learning algorithm to the estimation of a highly non-linear likelihood function. This poses a problem for the calculation of a popular distance function, the geodesic between data points in the latent space, as this is often solved iteratively via numerical methods. These are less effective if the problem at hand is not well captured by first or second-order approximations. In this work, we propose less complex likelihood functions by allowing complex distributions and explicitly penalising the curvature of the decoder. This results in geodesics which are approximated well by the Euclidean distance in latent space, decreasing the runtime by a factor of 1,000 with little loss in accuracy. \n",
        "Introduction": "  INTRODUCTION Latent-variable models (LVMs) are a viable tool in data analysis: a set of observations is explained by a simpler set of latent variables in conjunction with a map from the latent space to the space of observation. Methods from this family (e.g., principal component analysis (Wold et al., 1987), non-negative matrix factorisation (Lee & Seung, 2001), generalised discriminant analysis (Baudat & Anouar, 2000), etc.) are standard tools, serving either as feature extractors for subsequent data processing pipelines, density estimators or dimensionality reducers for visualisation. Despite the maturity of the field, research has far from halted. While kernel methods (KernelPCA (Schölkopf et al., 1997), KernelNMF (Li & Ding, 2006), etc.) have been used to improve the applicability of LVMs to data inhibiting non-linear phenomena, neural formulations such as the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) or the generative adversarial network (GAN) (Goodfellow et al., 2014) have become popular recently, especially due to their enormous success on modelling natural images. Here, a simple prior distribution (such as a multivariate standard normal or a uniform distribution) is mapped to the space of observations by means of a powerful deep neural network. A learning algorithm then finds weights for that neural network such that the data distribution is approximated well. In case of GANs, this is a minimax game, while the evidence lower bound is maximised in the case of VAEs. If the data distribution is relatively complicated and the prior is relatively simple, the map from the latent to the observable space, the decoder, has to be sufficiently complex. In fact, it has to mimic the inverse CDF in parts. This results in highly non-linear neural networks. As an example, the separation of two modes in the probability landscape has to be implemented by a flat CDF, which in turn requires an infinitely steep inverse CDF. Not only does this often pose difficulties for gradient-based learning. It also hinders the calculation of geodesics, the shortest paths from one point to another, as measured by the rate of change in the decoder along the path in latent space. Geodesics are often solved numerically, i.e. through a gradient-based optimisation of first or second order. In a cartography scenario, it is crucial to project the three-dimensional earth onto a two-dimensional map. When applying VAEs, the problem is summarised to map from a two-dimensional Euclidean latent space (the map) to the three-dimensional observation space (the earth surface). If, by construction, the decoder exhibits regions of high curvature, the stage is especially bad for such methods. We aim to improve the state of geodesics in deep latent-variable models with respect to runtime and use the geodesic as a distance metric. We expect that short (approximate) geodesics under the learned Under review as a conference paper at ICLR 2020 model indicate similarity of data points in question. If we assume that a simple prior and a simple decoder are insufficient to represent complex data distributions and a complex decoder is detrimental to the calculation of geodesics, we need the prior to be sufficiently complex to allow the decoder to be sufficiently simple. Our solution is the use of a powerful hierarchical prior representation in the context of VAEs and the simple penalisation of the curvature of the decoder. We name this approach flat manifold variational autoencoder since the Riemannian manifold of the decoder is isometric to Euclidean space. We show empirically that the resulting model features geodesics which are approximated very well by the Euclidean distance. This effectively removes the need for numerical optimisation, reducing the calculation of an approximate geodesic to that of a simple Euclidean distance calculation in latent space. This is accompanied by a speedup of several orders of magnitude, rendering the method practical for applications in real-time scenarios.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a deep generative model for learning semantic sentence embeddings that encourages source separation of parallel sentences, isolating what they have in common in a latent semantic embedding and explaining what is left over with language-specific latent vectors. Experiments demonstrate that the proposed approach is effective, allowing the learning of high-capacity deep transformer architectures while still generalizing to new domains, significantly outperforming a variety of state-of-the-art baselines. Further, the model better handles cross-lingual semantic similarity than multilingual translation baseline approaches, indicating that stripping away language-specific information allows for better comparisons between sentences from different languages. Lastly, the paper analyzes the encoded information and its relationship to language distance to English.",
        "Abstract": "Semantic sentence embedding models take natural language sentences and turn them into vectors, such that similar vectors indicate similarity in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time. Second, we use high- capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of se- mantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of test where simple word overlap is not a good indicator of similarity.",
        "Introduction": "  INTRODUCTION Learning useful representations of language has been a source of recent success in natural language processing (NLP). Much work has been done on learning representations for words (Mikolov et al., 2013; Pennington et al., 2014) and sentences (Kiros et al., 2015; Conneau et al., 2017). More recently, deep neural architectures have been used to learn contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018) which have enabled state-of-the-art results on many tasks. We focus on learning semantic sentence embeddings in this paper, which play an important role in many downstream applications. Since they do not require any labelled data for fine-tuning, sentence embeddings are useful for a variety of problems right out of the box. These include Semantic Textual Similarity (STS; Agirre et al. (2012)), mining bitext (Zweigenbaum et al., 2018), and paraphrase identification (Dolan et al., 2004). Semantic similarity measures also have downstream uses such as fine-tuning machine translation systems (Wieting et al., 2019a). There are three main ingredients when designing a sentence embedding model: the architecture, the training data, and the objective function. Many architectures including LSTMs (Hill et al., 2016; Conneau et al., 2017; Schwenk & Douze, 2017; Subramanian et al., 2018), Transformers (Cer et al., 2018; Reimers & Gurevych, 2019), and averaging models (Wieting et al., 2016a; Arora et al., 2017) have found success for learning sentence embeddings. The choice of training data and objective are intimately intertwined, and there are a wide variety of options including next-sentence predic- tion (Kiros et al., 2015), machine translation (Espana-Bonet et al., 2017; Schwenk & Douze, 2017; Schwenk, 2018; Artetxe & Schwenk, 2018), natural language inference (NLI) (Conneau et al., 2017), and multi-task objectives which include some of the previously mentioned objectives (Cer et al., 2018) as well as additional tasks like constituency parsing (Subramanian et al., 2018). Surprisingly, despite ample testing of more powerful architectures, the best performing models for many sentence embedding tasks related to semantic similarity often use simple architectures that are mostly agnostic to the interactions between words. For instance, some of the top performing Under review as a conference paper at ICLR 2020 techniques use word embedding averaging (Wieting et al., 2016a), character n-grams (Wieting et al., 2016b), and subword embedding averaging (Wieting et al., 2019b) to create representations. These simple approaches are competitive with much more complicated architectures on in-domain data and generalize well to unseen domains, but are fundamentally limited by their inability to capture word order. Training these approaches generally relies on discriminative objectives defined on paraphrase data (Ganitkevitch et al., 2013; Wieting & Gimpel, 2018) or bilingual data (Wieting et al., 2019b). The inclusion of latent variables in these models has also been explored (Chen et al., 2019). Intuitively, bilingual data in particular is promising because it potentially offers a useful signal for learning the underlying semantics of sentences. Within a translation pair, properties shared by both sentences are more likely semantic, while those that are divergent are more likely stylistic or language-specific. While previous work learning from bilingual data perhaps takes advantage of this fact implicitly, the focus of this paper is modelling this intuition explicitly, and to the best of our knowledge, this has not not been explored in prior work. Specifically, we propose a deep generative model that is encouraged to perform source separation on parallel sentences, isolating what they have in common in a latent semantic embedding and explaining what is left over with language-specific latent vectors. At test time, we use inference networks (Kingma & Welling, 2013) for approximating the model's posterior on the semantic and source-separated latent variables to encode monolingual sentences. Finally, since our model and training objective are generative, our approach does not require knowledge of the distance metrics to be used during evaluation, 1 and it has the additional property of being able to generate text. In experiments, we evaluate our probabilistic source-separation approach on a standard suite of STS evaluations. We demonstrate that the proposed approach is effective, most notably allowing the learning of high-capacity deep transformer architectures (Vaswani et al., 2017) while still generaliz- ing to new domains, significantly outperforming a variety of state-of-the-art baselines . Further, we conduct a thorough analysis by identifying subsets of the STS evaluation where simple word overlap is not able to accurately assess semantic similarity. On these most difficult instances, we find that our approach yields the largest gains, indicating that our system is modeling interactions between words to good effect. We also find that our model better handles cross-lingual semantic similarity than multilingual translation baseline approaches, indicating that stripping away language-specific information allows for better comparisons between sentences from different languages. Finally, we analyze our model to uncover what information was captured by the source separation into the semantic and language-specific variables and the relationship between this encoded infor- mation and language distance to English. We find that the language-specific variables tend to explain more superficial or language-specific properties such as overall sentence length, amount and loca- tion of punctuation, and the gender of articles (if gender is present in the language), but semantic and syntactic information is more concentrated in the shared semantic variables, matching our intu- ition. Language distance has an effect as well, where languages that share common structures with English put more information into the semantic variables, while more distant languages put more information into the language-specific variables. Lastly, we show outputs generated from our model that exhibit its ability to do a type of style transfer.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents Task-Optimal CCA (TOCCA), a new deep learning technique to project data from two views to a shared space that is also discriminative. TOCCA is derived from Canonical Correlation Analysis (CCA) and manipulates orthogonality constraints to obtain deep CCA approaches that compute a shared latent space that is also discriminative. Experiments on three different tasks demonstrate the effectiveness and versatility of TOCCA, showing a significant improvement in accuracy over previous state-of-the-art. In addition, TOCCA is more robust in the small sample size regime than alternative methods.",
        "Abstract": "Canonical Correlation Analysis (CCA) is widely used for multimodal data analysis and, more recently, for discriminative tasks such as multi-view learning; however, it makes no use of class labels.  Recent CCA methods have started to address this weakness but are limited in that they do not simultaneously optimize the CCA projection for discrimination and the CCA projection itself, or they are linear only. We address these deficiencies by simultaneously optimizing a CCA-based and a task objective in an end-to-end manner. Together, these two objectives learn a non-linear CCA projection to a shared latent space that is highly correlated and discriminative. Our method shows a significant improvement over previous state-of-the-art (including deep supervised approaches) for cross-view classification (8.5% increase), regularization with a second view during training when only one view is available at test time (2.2-3.2%), and semi-supervised learning (15%) on real data.",
        "Introduction": "  INTRODUCTION Parallel modalities of data are increasingly common in a variety of applications, including images and text, audio and video, parallel texts of different languages, and a variety of medical imaging and omics modalities for each patient. Each view provides essential information for classification and, when used together, can form a more accurate model. This is especially important for difficult discriminative tasks such as those with a small training set size. Canonical Correlation Analysis (CCA) is the most common method for computing a shared representation from two views of data by computing a space in which they are maximally correlated (Hotelling, 1936; Bie et al., 2005). In this paper we will demonstrate that, through optimizing for both discriminative features and correlation between views, we can improve classification accuracy for three real world scenarios. CCA is an unsupervised method but has been applied to many discriminative tasks (Kan et al., 2015; Sargin et al., 2007; Arora & Livescu, 2012). While some of the correlated CCA features are useful for discriminative tasks, many represent properties that are of no use for classification and obscure correlated information that is beneficial. This problem is magnified with recent non-linear extensions of CCA that use deep learning to make significant strides in improving correlation (Andrew et al., 2013; Wang et al., 2015a; 2016; Chang et al., 2018) but often at the expense of discriminative capability (cf. §5.1). Therefore, we present Task-Optimal CCA (TOCCA), a new deep learning technique to project the data from two views to a shared space that is also discriminative ( Fig. 1 ). Implementing a task-optimal variant of CCA required a fundamental change in formulation. We show that the CCA objective can equivalently be expressed as an 2 distance minimization in the shared space plus an orthogonality constraint. Orthogonality constraints help regularize neural networks (NNs) (Huang et al., 2018); we present three techniques to accomplish this. While our method is derived from CCA, by manipulating the orthogonality constraints, we obtain deep CCA approaches that compute a shared latent space that is also discriminative. Our family of solutions for supervised CCA required a crucial and non-trivial change in formulation. We demonstrate the effectiveness and versatility of our model for three different tasks: 1) cross- view classification on a variation of MNIST (LeCun, 1998), 2) regularization when two views are Under review as a conference paper at ICLR 2020 available for training but only one at test time on a cancer imaging and genomic data set with only 1,000 samples, and 3) semi-supervised representation learning to improve speech recognition. All experiments showed a significant improvement in accuracy over previous state-of-the-art. In addition, our approach is more robust in the small sample size regime than alternative methods. Overall, our experiments on real data show the effectiveness of our method in learning a shared space that is more discriminative than previous methods for a variety of practical problems.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel convolutional neural network (CNN) architecture, spatially shuffled convolution (ss convolution), which incorporates information outside of the regular convolution's receptive field (RF) by spatial shuffling. Experiments on CIFAR-10 and ImageNet 2012 datasets show that ss convolution improves the classification performance across various CNNs, indicating that the information outside of the RF is useful when processing local information. Analyses are conducted to examine why ss convolution improves the classification performance in CNNs and show that spatial shuffling allows the regular convolution to use the information outside of its RF.",
        "Abstract": "Convolutional Neural Networks (CNNs) are composed of multiple convolution layers and show elegant performance in vision tasks.\nThe design of the regular convolution is based on the Receptive Field (RF) where the information within a specific region is processed.\nIn the view of the regular convolution's RF, the outputs of neurons in lower layers with smaller RF are bundled to create neurons in higher layers with larger RF. \nAs a result, the neurons in high layers are able to capture the global context even though the neurons in low layers only see the local information.\nHowever, in lower layers of the biological brain, the information outside of the RF changes the properties of neurons.\nIn this work, we extend the regular convolution and propose spatially shuffled convolution (ss convolution).\nIn ss convolution, the regular convolution is able to use the information outside of its RF by spatial shuffling which is a simple and lightweight operation.\nWe perform experiments on CIFAR-10 and ImageNet-1k dataset, and show that ss convolution improves the classification performance across various CNNs.",
        "Introduction": "  INTRODUCTION Convolutional Neural Networks (CNNs) and their convolution layers ( Fukushima, 1980 ;  Lecun et al., 1998 ) are inspired by the finding in cat visual cortex ( Hubel & Wiesel, 1959 ) and they show the strong performance in various domains such as image recognition ( Krizhevsky et al., 2012 ;  Si- monyan & Zisserman, 2015 ;  He et al., 2016 ), natural language processing ( Gehring et al., 2017 ), and speech recognition ( Abdel-Hamid et al., 2014 ;  Zhang et al., 2016 ). A notable characteristic of the convolution layer is the Receptive Field (RF), which is the particular input region where a con- volutional output is affected by. The units (or neurons) in higher layers have larger RF by bundling the outputs of the units in lower layers with smaller RF. Thanks to the hierarchical architectures of CNNs, the units in high layers are able to capture the global context even though the units in low layers only see the local information. It is known that neurons in the primary visual cortex (i.e., V1 which is low layers) change the self- properties (e.g., the RF size ( Pettet & Gilbert, 1992 ) and the facilitation effect ( Nelson & Frost, 1985 )) based on the information outside of the RF ( D.Gilbert, 1992 ). The mechanism is believed to originate from (1) feedbacks from the higher-order area ( Iacaruso et al., 2017 ) and (2) intra- cortical horizontal connections ( D.Gilbert, 1992 ). The feedbacks from the higher-order area convey broader-contextual information than the neurons in V1, which allows the neurons in V1 to use the global context. For instance,  Gilbert & Li (2013)  argued that the feedback connections work as attention. Horizontal connections allow the distanced neurons in the layer to communicate with each other and are believed to play an important role in visual contour integration ( Li & Gilbert, 2002 ) and object grouping ( Schmidt et al., 2006 ). Though both horizontal and feedback connections are believed to be important for visual processing in the visual cortex, the regular convolution ignores the properties of these connections. In this work, we particularly focus on algorithms to introduce the function of horizontal connections for the regular convolution in CNNs. We propose spatially shuffled convolution (ss convolution), where the information outside of the regular convolution's RF is incorporated by spatial shuffling, which is a simple and lightweight operation. Our ss convolution is the same operation as the regular convolution except for spatial shuffling and requires no extra learnable parameters. The design of ss convolution is highly inspired by the function of horizontal connections. To test the effectiveness of the information outside of the regular convolution's RF in CNNs, we perform experiments on CIFAR-10 ( Krizhevsky, 2009 ) and ImageNet 2012 dataset ( Russakovsky et al., 2015 ) and show that ss convolution improves the classification performance across various CNNs. These results indicate that the information outside of the RF is useful when processing local information. In addition, we Under review as a conference paper at ICLR 2020 conduct several analyses to examine why ss convolution improves the classification performance in CNNs and show that spatial shuffling allows the regular convolution to use the information outside of its RF.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel problem setting in model-based reinforcement learning (RL) that resembles the human cognitive bias of imperfect information. A learning algorithm of a collective policy is proposed that leverages multiple agents' biased representations encoded from their internal models. The collective policy operates on agents' preestablished internal models and is trained solely inside agents' simulations. Experiments are conducted in the VIZDOOM environment, which reveals an unexpected behavior of agents becoming delusional. The source code, visual demonstrations, additional results, and in-depth discussions are released in a separate link.",
        "Abstract": "We consider a setting where biases are involved when agents internalise an environment.  Agents have different biases, all of which resulting in imperfect evidence collected for taking optimal actions.  Throughout the interactions, each agent asynchronously internalises their own predictive model of the environment and forms a virtual simulation within which the agent plays trials of the episodes in entirety. In this research, we focus on developing a collective policy trained solely inside agents' simulations, which can then be transferred to the real-world environment. The key idea is to let agents imagine together; make them take turns to host virtual episodes within which all agents participate and interact with their own biased representations. Since agents' biases vary, the collective policy developed while sequentially visiting the internal simulations complement one another's shortcomings. In our experiment, the collective policies consistently achieve significantly higher returns than the best individually trained policies.",
        "Introduction": "  INTRODUCTION We look at the world through our own lenses. Two people with the same exact observations create different internalizations of the world through their own mental models, collect different amounts of evidence, take different actions, and end up with different outcomes. For example, students attending a lecture look at different parts of the presentation slides, and doctors looking at the clinical log of a patient focus on different data and statistics and make different diagnoses. In model-based reinforcement learning (RL) ( Kaelbling et al., 1996 ;  Sutton & Barto, 1998 ), agents interact with an environment through models that are analogous to the human mental models ( Hamrick, 2019 ). However, just as our decisions governed by our mental models are often far from perfect, the agents' policies that are designed upon the models are suboptimal ( Box, 1976 ;  1979 ), as the inductive biases used to learn such models are not guaranteed to be correct ( Griffiths et al., 2010 ). In this research, we propose a learning algorithm of a collective policy that leverages multiple agents' biased representations encoded from their internal models. Like humans who often compensate for their cognitive biases through collaboration ( Woolley et al., 2010 ;  Muchnik et al., 2013 ), the goal of the collective policy is to make use of multiple internal models so that the biased representation formulated by each internal model complements one another to achieve higher returns than the best- performing individual policy. The collective policy in this research has the following characteristics: - Asynchronous shaping of agents' internal models: The collective policy operates on agents' preestablished internal models. Prior to learning the collective policy, agents construct their internal models over the course of their own interactions with the environment, which can be done asyn- chronously and independent of other agents. Note that this learning process of the collective policy does not fall into the conventional multi-agent RL settings, where agents are concurrently situated in the same environment ( Littman, 1994 ;  Panait & Luke, 2005 ;  Bu et al., 2008 ;  Buşoniu et al., 2010 ). - Collective policy learned solely inside agents' internal simulations: In model-free and model-based RL, policy learning typically requires a large amount of interaction with the environment, which hampers feasibility, time-efficiency ( Guez et al., 2018 ), and safety ( Garcıa & Fernández, 2015 ). This is particularly prohibitive in our setting, as during the collective policy learning, we need to force the agents to go through the same rollouts together, coordinate their actions, and interact with the environment in algorithmically intended ways (see e.g.,  Yahya et al. (2017) ;  Rubenstein et al. (2014) ;  Under review as a conference paper at ICLR 2020 Nagpal (2016) ). To overcome this, we propose a learning algorithm of the collective policy that involves zero interaction with the real environment; instead, the collective policy is trained solely inside agents' simulations, analogous to human's mental simulation or imagination ( Hamrick, 2019 ). In our learning algorithm, agents take turns to be the host that generates a virtual rollout, within which all agents participate in and learn the collective policy together. - Minimal number of parameters to learn the collective policy: In the proposed algorithm, the number of parameters and the training iterations of the collective policy function is same as those of the individual policy functions. This not only allows the efficient training, but also assures that the performance edge of the collective policy over the individual policies is not attributable to the model complexity but to the complementing evidences provided by the multiple internal models. Moreover, learning of the collective policy can precede learning of the individual policies. We make the following contributions in this paper: I. We present a novel problem setting in model-based RL that resembles the human cognitive bias of imperfect information. Note that our setting is clearly different from the partial observation settings in RL ( Astrom, 1965 ;  Kaelbling et al., 1998 ); in our setting, two agents at the same exact time and location in the environment encode different representations from the same observation. II. We compare the real environment and the internal simulation generated by agent models for training a policy. With appropriate early stopping epoch and τ , we can achieve much better scores at a fraction of the training time. III. We present a learning algorithm of the collective policy that operates on the simulations generated from internal models of agents and complements their biased representations, thereby achieving higher scores than that of the best-performing individual policy. IV. We experiment with VIZDOOM, a different environment in which we observe an unexpected behavior of agents becoming delusional, i.e., having the false belief about the reward system, preventing us from learning the collective policy in simulation. V. We release our source code, visual demonstrations, additional results, and in-depth discussions in a separate link 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the role of optimization algorithms in deep learning, specifically focusing on stochastic gradient descent (SGD) and its variants. It is argued that while the special deep architecture of these models is important to the success of deep learning, the optimization algorithms used to train these models play a key role in learning parameters that generalize well.",
        "Abstract": "Most modern learning problems are highly overparameterized, meaning that the model has many more parameters than the number of training data points, and as a result, the training loss may have infinitely many global minima (in fact, a manifold of parameter vectors that perfectly interpolates the training data). Therefore, it is important to understand which interpolating solutions we converge to, how they depend on the initialization point and the learning algorithm, and whether they lead to different generalization performances. In this paper, we study these questions for the family of stochastic mirror descent (SMD) algorithms, of which the popular stochastic gradient descent (SGD) is a special case. Recently it has been shown that, for overparameterized linear models, SMD converges to the global minimum that is closest (in terms of the Bregman divergence of the mirror used) to the initialization point, a phenomenon referred to as implicit regularization. Our contributions in this paper are both theoretical and experimental. On the theory side, we show that in the overparameterized nonlinear setting, if the initialization is close enough to the manifold of global optima, SMD with sufficiently small step size converges to a global minimum that is approximately the closest global minimum in Bregman divergence, thus attaining approximate implicit regularization. For highly overparametrized models, this closeness comes for free: the manifold of global optima is so high dimensional that with high probability an arbitrarily chosen initialization will be close to the manifold. On the experimental side, our extensive experiments on the MNIST and CIFAR-10 datasets, using various initializations, various mirror descents, and various Bregman divergences, consistently confirms that this phenomenon indeed happens in deep learning: SMD converges to the closest global optimum to the initialization point in the Bregman divergence of the mirror used. Our experiments further indicate that there is a clear difference in the generalization performance of the solutions obtained from different SMD algorithms. Experimenting on the CIFAR-10 dataset with different regularizers, l1 to encourage sparsity, l2 (yielding SGD) to encourage small Euclidean norm, and l10 to discourage large components in the parameter vector, consistently and definitively shows that, for small initialization vectors, l10-SMD has better generalization performance than SGD, which in turn has better generalization performance than l1-SMD. This surprising, and perhaps counter-intuitive, result strongly suggests the importance of a comprehensive study of the role of regularization, and the choice of the best regularizer, to improve the generalization performance of deep networks.",
        "Introduction": "  INTRODUCTION Deep learning has demonstrably enjoyed a great deal of success in a wide variety of tasks (Amodei et al., 2016; Graves et al., 2013; Krizhevsky et al., 2012; Mnih et al., 2015; Silver et al., 2016; Wu et al., 2016; LeCun et al., 2015). Despite its tremendous success, the reasons behind the good perfor- mance of these methods on unseen data is not fully understood (and, arguably, remains somewhat of a mystery). While the special deep architecture of these models seems to be important to the success of deep learning, the architecture is only part of the story, and it has been now widely recognized that the optimization algorithms used to train these models, typically stochastic gradient descent (SGD) and its variants, play a key role in learning parameters that generalize well.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the role of symmetry in the initialization stage of neural networks. We focus on the first \"non trivial\" case of neural networks, networks with one hidden layer, and consider the class of symmetric functions S = S n = n ∑ i=0 a i · 1 |x|=i : a 1 , . . . , a n ∈ {±1}. We show that symmetry-based initialization can be the difference between failure and success, and provide explicit representations for all functions in S.",
        "Abstract": "This work provides an additional step in the theoretical understanding of neural networks. We consider neural networks with one hidden layer and show that when learning symmetric functions, one can choose initial conditions so that standard SGD training efficiently produces generalization guarantees. We empirically verify this and show that this does not hold when the initial conditions are chosen at random. The proof of convergence investigates the interaction between the two layers of the network. Our results highlight the importance of using symmetry in the design of neural networks.",
        "Introduction": "  INTRODUCTION Building a theory that can help to understand neural networks and guide their construction is one of the current challenges of machine learning. Here we wish to shed some light on the role sym- metry plays in the construction of neural networks. It is well-known that symmetry can be used to enhance the performance of neural networks. For example, convolutional neural networks (CNNs) (see  Lecun et al. (1998) ) use the translational symmetry of images to classify images better than fully connected neural networks. Our focus is on the role of symmetry in the initialization stage. We show that symmetry-based initialization can be the difference between failure and success. On a high-level, the study of neural networks can be partitioned to three different aspects. We study these aspects for the first \"non trivial\" case of neural networks, networks with one hidden layer. We are mostly interested in the initialization phase. If we take a network with the appropriate architecture, we can always initialize it to the desired function. A standard method (that induces a non trivial learning problem) is using random weights to initialize the network. A different reason- able choice is to require the initialization to be useful for an entire class of functions. We follow the latter option. Our focus is on the role of symmetry. We consider the following class of symmetric functions S = S n = n ∑ i=0 a i · 1 |x|=i : a 1 , . . . , a n ∈ {±1} , where x ∈ {0, 1} n and |x| = ∑ i x i . The functions in this class are invariant under arbitrary permu- tations of the input's coordinates. The parity function π(x) = (−1) |x| and the majority function are well-known examples of symmetric functions. Expressiveness for this class was explored by  Minsky and Papert (1988) . They showed that the parity function cannot be represented using a network with limited \"connectivity\". Contrastingly, if we use a fully connected network with one hidden layer and a common activation function (like sign, sigmoid, or ReLU) only O(n) neurons are needed. We provide such explicit representations for all functions in S; see Lemmas 1 and 2.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the fundamental problem of objective mismatch in Model-based Reinforcement Learning (MBRL). Objective mismatch occurs when the learning of the forward dynamics model is decoupled from the subsequent controller and policy that it induces through the optimization of two different objective functions. This paper identifies and formalizes the problem of objective mismatch in MBRL, examines the signs of and the effects of objective mismatch on simulated control tasks, proposes a mechanism to mitigate objective mismatch, and discusses the impact of objective mismatch on existing MBRL and outlines future directions to address this problem.",
        "Abstract": "Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, leaving the general framework virtually unchanged since its conception. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t. the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of the one-step ahead prediction is not always correlated with downstream control performance. This observation highlights a critical flaw in the current MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of future research for addressing this issue.",
        "Introduction": "  INTRODUCTION Model-based reinforcement learning (MBRL) is a popular approach for learning to control nonlinear systems that cannot be expressed analytically ( Bertsekas, 1995 ;  Sutton and Barto, 2018 ;  Deisen- roth and Rasmussen, 2011 ;  Williams et al., 2017 ). MBRL techniques achieve the state of the art performance for continuous-control problems with access to a limited number of trials (e.g.  Chua et al. (2018) ;  Wang and Ba (2019) ) and in controlling systems given only visual observations with no observations of the original system's state (e.g.  Hafner et al. (2018) ;  Zhang et al. (2018) ). MBRL approaches typically learn a forward dynamics model that predicts how the dynamical system will evolve when a set of control signals are applied. This model is classically fit with respect to the maximum likelihood of a set of trajectories collected on the real system, and then used as part of a control algorithm to be executed on the system (e.g., model-predictive control). In this paper, we highlight a fundamental problem in the standard MBRL learning scheme: the objective mismatch issue. The learning of the forward dynamics model is decoupled from the subsequent controller and policy that it induces through the optimization of two different objective functions - negative log-likelihood (or its equivalent for deterministic models, e.g., the RMSE) of the single- or multi-step look-ahead prediction for the dynamics model, and task performance (i.e., reward) for the policy optimization. While the use of negative log-likelihood (NLL) for system identification is an historically accepted objective, it results in optimizing an objective that does not necessarily correlate to the actual performance. The contributions of this paper are to: 1) identify and formalize the problem of objective mismatch in MBRL; 2) examine the signs of and the effects of objective mismatch on simulated control tasks; 3) propose a initial mechanism to mitigate objective mismatch; 4) discuss the impact of objective mismatch on existing MBRL and outline future directions to address this problem. The Origin of Objective Mismatch: The Sub- tle Differences between MBRL and System Identification Many ideas and concepts in model-based RL are rooted in the field of opti- mal control and system identification ( Bertsekas, 1995 ;  Zhou et al., 1996 ;  Kirk, 2012 ;  Bryson, 2018 ;  Sutton and Barto, 2018 ). In system identifi- cation, the main idea is to use a two-step process where we first generate on the robot (optimal) elicitation trajectories τ to fit a dynamics model (typically analytical), and subsequently we ap- ply this model to a specific task. This particular scheme has multiple assumptions: 1) the data col- lected cover the entire state-action space (done by appropriate elicitation trajectories); 2) the pres- ence of large (virtually infinite) amount of data; 3) the global nature of the model resulting from the system identification process (which should be generalizable to a large range of tasks). With these assumptions, the general idea of system identification is effectively to collect large amount of data covering the whole state-space to create Under review as a conference paper at ICLR 2020 once a global model that is sufficiently accurate that we can at deployment time specify any desired task, and still obtain good performance. When adopting the idea of learning the dynamics model used in optimal control for MBRL, it is important to consider if these assumptions still hold. The assumption of virtually infinite data is visibly in tension with the explicit goal of MBRL which is to reduce the number of interactions with the environment by being \"smart\" about the sampling of new trajectories. In fact, in MBRL the offline data collection performed via elicitation trajectories is largely replaced by on-policy sampling (with the exception of some initial motor babbling ( Chua et al., 2018 )) in order to explicitly reduce the need to collect large amount of data. Moreover - in practice - in the MBRL setting the data will not usually cover the entire state-action space, since they are generated by a policy trying to optimize one specific task. In conjunction with the use of non-parametric models (i.e., not analytical model), this results in learned models that are strongly biased towards capturing the distribution of the data, and that are only locally accurate. Nonetheless, this is at first sight not an issue since the common MBRL setting consider the learning aimed at solving only one specific task, and rarely test for generalization capabilities of the learned dynamics. In practice, we can now see how the assumptions and goals of system identification are in contrast with the ones of MBRL. Understanding these differences and the downstream effects on algorithmic approach is crucial to design new families of MBRL algorithms.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel end-to-end learning framework for lifetime clustering without assuming proportional hazards and smoothly handling unobservable termination signals. The proposed framework is based on deep learning models and is capable of accurately predicting time-to-terminal-event for an individual while also clustering subjects based on their underlying lifetime distributions. The proposed framework is evaluated on both simulated and real-world datasets, and results show that it outperforms existing methods.",
        "Abstract": "The goal of lifetime clustering is to develop an inductive model that maps subjects into $K$ clusters according to their underlying (unobserved) lifetime distribution. We introduce a neural-network based lifetime clustering model that can find cluster assignments by directly maximizing the divergence between the empirical lifetime distributions of the clusters. Accordingly, we define a novel clustering loss function over the lifetime distributions (of entire clusters) based on a tight upper bound of the two-sample Kuiper test p-value. The resultant model is robust to the modeling issues associated with the unobservability of termination signals, and does not assume proportional hazards. Our results in real and synthetic datasets show significantly better lifetime clusters (as evaluated by C-index, Brier Score, Logrank score and adjusted Rand index) as compared to competing approaches.",
        "Introduction": "  INTRODUCTION Survival analysis is widely used to model the relationship between subject covariates and the time until a particular terminal event of interest (e.g., death, or quitting of social media) that marks the end of all activities (or measurements) of that subject (known as the lifetime of the subject). For instance, a subject's logins to a social network are her activities and the time until she quits the social network permanently is her lifetime. The lifetime of a subject can be unobserved for two possible reasons: (a) the terminal event was right-censored, i.e., the subject did not have a terminal event within the finite data-collection period, or (b) the terminal events are inherently unobservable. Right-censoring happens for instance when a patient is still alive at the time of data-collection. Unobservability happens for instance, in social networks, when a subject simply stops using the service but does not provide a clear termination signal by deleting her account. In such a scenario, the terminal events remain unobserved for most if not all subjects, even if the subjects quit the service within the data-collection period. Numerous survival methods have been proposed ( Witten and Tibshirani, 2010a ;  Hothorn et al., 2006 ;  Ishwaran et al., 2008 ) to predict the lifetime of a subject given her covariates and her activi- ties/measurements for a brief initial period of time, while also accounting for right-censoring. More recent deep learning models for lifetime prediction ( Lee et al., 2018 ;  Ren et al., 2018 ;  Chapfuwa et al., 2018 ) have achieved much success due to their flexibility to model complex relationships, and by avoiding limiting assumptions like parametric lifetime distributions ( Ranganath et al., 2016 ) and proportional hazards ( Katzman et al., 2018 ). In scenarios where terminal events are never observed (unobservability), it is a standard practice to introduce artificial termination signals through a predefined \"timeout\" for the period of inactivity, i.e., a social network user inactive for m months has her last observed activity declared a terminal event. Such a specification is typically arbitrary and can adversely affect the analysis depending on the \"timeout\" value used. Notwithstanding the fact that lifetimes are hard to predict without termination signals in the training data, we are generally interested in clustering subjects based on their underlying lifetime distribution to improve decision-making. Applications include identifying disease subtypes ( Gan et al., 2018 ), understanding the implications of distinct manufacturing processes on machine parts, and qualitatively analyzing different survival groups in a social network. Although accurately predicting time-to- terminal-event for an individual is important in a variety of applications, lifetime clustering plays a complementary role and provides a more holistic picture. Lifetime clustering remains a relatively unexplored topic despite being an important tool. Although traditional unsupervised clustering methods such as k-means and hierarchical clustering are popular Under review as a conference paper at ICLR 2020 for this task ( Bhattacharjee et al., 2001 ;  Sørlie et al., 2001 ;  Bullinger et al., 2004 ), they may produce clusters that are entirely uncorrelated with lifetimes ( Gaynor and Bair, 2013 ). Semi-supervised clustering ( Bair and Tibshirani, 2004 ) and supervised sparse clustering ( Witten and Tibshirani, 2010b ) employ a two-stage lifetime clustering process: (i) identify covariates associated with lifetime using Cox scores ( Cox, 1992 ), and (ii) treat these covariates differently while performing k-means clustering. They assume proportional hazards (i.e., constant hazard ratios over time) and require the presence of termination signals. Furthermore, a decoupled two-stage process such as the above is not guaranteed to obtain clusters with maximally distinct lifetime distributions; rather, we require an end-to-end learning framework that prescribes a loss function specifically over the lifetime distributions of different clusters. In this work we tackle the important task of inductive lifetime clustering without assuming propor- tional hazards, while also smoothly handling the unobservability of termination signals.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an online training scheme amenable to non-volatile memories to enable next generation edge devices. The proposed algorithm, Streaming Kronecker Sum Approximation (SKS), addresses the two key challenges of low write density and low auxiliary memory. Additionally, two techniques \"gradient max-norm\" and \"streaming batch norm\" are proposed to help training specifically in the online setting. Experiments are conducted to demonstrate the advantages of the proposed approach.",
        "Abstract": "The recent success of neural networks for solving difficult decision tasks has incentivized incorporating smart decision making \"at the edge.\" However, this work has traditionally focused on neural network inference, rather than training, due to memory and compute limitations, especially in emerging non-volatile memory systems, where writes are energetically costly and reduce lifespan. Yet, the ability to train at the edge is becoming increasingly important as it enables applications such as real-time adaptability to device drift and environmental variation, user customization, and federated learning across devices. In this work, we address four key challenges for training on edge devices with non-volatile memory: low weight update density, weight quantization, low auxiliary memory, and online learning. We present a low-rank training scheme that addresses these four challenges while maintaining computational efficiency. We then demonstrate the technique on a representative convolutional neural network across several adaptation problems, where it out-performs standard SGD both in accuracy and in number of weight updates.",
        "Introduction": "  INTRODUCTION Deep neural networks have shown remarkable performance on a variety of challenging inference tasks. As the energy efficiency of deep-learning inference accelerators improves, some models are now being deployed directly to edge devices to take advantage of increased privacy, reduced network bandwidth, and lower inference latency. Despite edge deployment, training happens predominately in the cloud. This limits the privacy advantages of running models on-device and results in static models that do not adapt to evolving data distributions in the field. Efforts aimed at on-device training address some of these challenges. Federated learning aims to keep data on-device by training models in a distributed fashion ( Konecný et al., 2016 ). On-device model customization has been achieved by techniques such as weight-imprinting ( Qi et al., 2018 ), or by retraining limited sets of layers. On-chip training has also been demonstrated for handling hardware imperfections ( Zhang et al., 2017 ;  Gonugondla et al., 2018 ). Despite this progress with small models, on-chip training of larger models is bottlenecked by the limited memory size and compute horsepower of edge processors. Emerging non-volatile (NVM) memories such as resistive random access memory (RRAM) have shown great promise for energy and area-efficient inference ( Yu, 2018 ). However, on-chip training requires a large number of writes to the memory, and RRAM writes cost significantly more energy than reads (e.g., 10.9 pJ/bit versus 1.76 pJ/bit ( Wu et al., 2019 )). Additionally, RRAM endurance is on the order of 10 6 writes ( Grossi et al., 2019 ), shortening the lifetime of a device due to memory writes for on-chip training. In this paper, we present an online training scheme amenable to NVM memories to enable next generation edge devices. Our contributions are (1) an algorithm called Streaming Kronecker Sum Approximation (SKS), and its analysis, which addresses the two key challenges of low write density and low auxiliary memory; (2) two techniques \"gradient max-norm\" and \"streaming batch norm\" to help training specifically in the online setting; (3) a suite of adaptation experiments to demonstrate the advantages of our approach.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines the effects of noise injection on convolutional neural networks (CNNs) for image processing tasks. It explores the use of additive and multiplicative noise, which has long been used in signal processing for regression-based methods, to create more robust models. The paper also discusses the reasons for noise injection, such as making the model more robust against the occurrence of noise over the input data, and encouraging the model to learn the various aspects of each class by occluding random features. The effects of injecting different types of noises into images for varying CNN architectures are assessed based on their performance and noise robustness. The paper also discusses proper ways on adding or applying noise to a CNN for image classification tasks.",
        "Abstract": "Noise injection is a fundamental tool for data augmentation, and yet there is no widely accepted procedure to incorporate it with learning frameworks. This study analyzes the effects of adding or applying different noise models of varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise models that are distributed with different density functions are given common magnitude levels via Structural Similarity (SSIM) metric in order to create an appropriate ground for comparison. The basic results are conforming with the most of the common notions in machine learning, and also introduces some novel heuristics and recommendations on noise injection. The new approaches will provide better understanding on optimal learning procedures for image classification.",
        "Introduction": "  INTRODUCTION Convolutional Neural Networks (CNNs) find an ever-growing field of application throughout image and sound processing tasks, since the success of AlexNet ( Krizhevsky et al., 2012 ) in the 2012 ImageNet competition. Yet, training these networks still keeps the need of an \"artistic\" touch: even the most cited state-of-the-art studies employ wildly varying set of solvers, augmentation and regularization techniques ( Domhan et al., 2015 ). In this study, one of the crucial data augmentation techniques, noise injection, will be thoroughly analysed to determine the correct way of application on image processing tasks. Adding noise to the training data is not a procedure that is unique to the training of neural architec- tures: additive and multiplicative noise has long been used in signal processing for regression-based methods, in order to create more robust models ( Saiz et al., 2005 ). The technique is also one of the oldest data augmentation methods employed in the training of feed forward networks, as analysed by  Holmstrom & Koistinen (1992) , yet it is also pointed out in the same study that while using additive Gaussian noise is helpful, the magnitude of the noise cannot be selected blindly, as a badly-chosen variance may actually harm the performance of the resulting network (see  Gu & Rigazio (2014)  and  Hussein et al. (2017)  for more examples). The main reasons for noise injection to the training data can be listed as such in a non-excluding manner: first of all, injection of any noise type makes the model more robust against the occurrence of that particular noise over the input data (see  Braun et al. (2016)  and  Saiz et al. (2005)  for further reference), such as the cases of Gaussian additive noise in photographs, and Gaussian-Poisson noise on low-light charge coupled devices ( Bovik, 2005 ). Furthermore, it is shown that the neural networks optimize on the noise magnitude they are trained on ( Yin et al., 2015 ). Therefore, it is important to choose the correct type and level of the noise to augment the data during training. Another reason for noise addition is to encourage the model to learn the various aspects of each class by occluding random features. Generally, stochastic regularization techniques embedded inside the neural network architectures are used for this purpose, such as Dropout layers, yet it is also possible to augment the input data for such purposes as in the example of \"cutout\" regularization proposed by  Devries & Taylor (2017) . The improvement of the generalization capacity of a network is highly correlated with its performance, which can be scored by the accuracy over a predetermined test set. There has been similar studies conducted on the topic, with the example of  Koziarski & Cyganek (2017)  which focuses on the effects of noise injection on the training of deep networks and the possible denoising methods, yet they fail to provide a proper methodology to determine the level of Under review as a conference paper at ICLR 2020 noise to be injected into the training data, and use PSNR as the comparison metric between different noise types which is highly impractical (see Section 3). To resolve these issues, this study focuses on the ways to determine which noise types to combine the training data with, and which levels, in addition to the validity of active noise injection techniques while experimenting on a larger set of noise models. In the structure of this work, the effect of injecting different types of noises into images for varying CNN architectures is assessed based on their performance and noise robustness. Their interaction and relationship with each other are analyzed over (also noise-injected) validation sets. Finally as a follow-up study, proper ways on adding or applying noise to a CNN for image classification tasks are discussed.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes Neural Markov Logic Networks (NMLN) as a solution to the challenging task of learning a probability distribution over relational structures from one or few examples. NMLN models the statistics used to model the probability distribution as neural networks, eliminating the need for domain experts to design logical rules or structure learning based on combinatorial search. This opens the door to applications of the model to the generative setting.",
        "Abstract": "We introduce Neural Markov Logic Networks (NMLNs), a statistical relational learning system that borrows ideas from Markov logic. Like Markov Logic Networks (MLNs), NMLNs are an exponential-family model for modelling distributions over possible worlds, but unlike MLNs, they do not rely on explicitly specified first-order logic rules. Instead, NMLNs learn an implicit representation of such rules as a neural network that acts as a potential function on fragments of the relational structure. Interestingly, any MLN can be represented as an NMLN. Similarly to recently proposed Neural theorem provers (NTPs) (Rocktaschel at al. 2017), NMLNs can exploit embeddings of constants but, unlike NTPs, NMLNs work well also in their absence. This is extremely important for predicting in settings other than the transductive one. We showcase the potential of NMLNs on knowledge-base completion tasks and on generation of molecular (graph) data.",
        "Introduction": "  INTRODUCTION Parameters for a statistical relational model are typically estimated from one or more examples of relational structures that typically consist of a large number of ground facts. Examples of such structures are social networks (e.g. Facebook), protein-protein interaction networks, the Web, etc. A challenging task is to learn a probability distribution over such relational structures from one or few examples. One solution is based on the assumption that the relational structure has repeated regularities; this assumption is implicitly or explicitly used in most works on statistical relational learning. Then, statistics about these regularities can be computed for small substructures of the training examples and used to construct a distribution over the relational structures. Together with the maximum-entropy principle, this leads to distributions such as Markov logic networks ( Richard- son & Domingos, 2006 ;  Kuželka et al., 2018 ) In this paper, we propose Neural Markov Logic Networks (NMLN). Here, the statistics which are used to model the probability distribution are not known in advance, but are modelled as neural networks trained together with the probability distribution model. This is very powerful when com- pared to classical MLNs, where either domain experts are required to design some useful statistics about the domain of interest by hand (i.e. logical rules) or structure learning based on combinatorial search needs to be performed. These requirements normally limit a wide application of these models as out-of-the box tools. It is worth noticing that overtaking the need of such \"feature-engineering\" is one of the reasons behind the massive adoption of deep learning techniques. However, not much has been done in the same direction by the statistical relational learning community. Moreover, de- signing statistics as neural networks allows a more fine-grained description of the data, opening the doors to applications of our model to the generative setting.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a method for automatically computing a learning rate schedule for stochastic optimization methods, such as SGD, in an online fashion. The goal is to produce models with a small validation error. The method is based on gradient-based hyperparameter optimization and considers the optimization problem of finding an optimal schedule η* = (η*0, ..., η*T-1) ∈ RT+ for t = {0, ..., T-1} = [T], where E: Rd → R+ is an objective function, Φt: Rd × R+ → Rd is a (possibly stochastic) weight update dynamics, w ∈ Rd represents the initial model weights (parameters) and wt are the weights after t iterations. The horizon T should be large enough to effectively minimize the training error, while a too large value of T does not necessarily harm since ηk = 0 for k > T is still a feasible solution, implementing early stopping in this setting.",
        "Abstract": "We study the problem of fitting task-specific learning rate schedules from the perspective of hyperparameter optimization.  This allows us to explicitly search for schedules that achieve good generalization. We describe the structure of the gradient of a validation error w.r.t. the learning rates, the hypergradient, and based on this we introduce a novel online algorithm. Our method adaptively interpolates between two recently proposed techniques (Franceschi et al., 2017; Baydin et al.,2018), featuring increased stability and faster convergence. We show empirically that the proposed technique compares favorably with baselines and related methodsin terms of final test accuracy.",
        "Introduction": "  INTRODUCTION Learning rate (LR) adaptation for first-order optimization methods is one of the most widely studied aspects in optimization for learning methods - in particular neural networks - with early work dating back to the origins of connectionism ( Jacobs, 1988 ;  Vogl et al., 1988 ). More recent work focused on developing complex schedules that depend on a small number of hyperparameters ( Loshchilov & Hutter, 2017 ;  Orabona & Pál, 2016 ). Other papers in this area have focused on the optimization of the (regularized) training loss ( Schaul et al., 2013 ;  Baydin et al., 2018 ;  Wu et al., 2018 ) . While quick optimization is desirable, the true goal of supervised learning is to minimize the generalization error, which is commonly estimated by holding out part of the available data for validation. Hyperparameter optimization (HPO), a related but distinct branch of the literature, specifically focuses on this aspect, with less emphasis on the goal of rapid convergence on a single task. Research in this direction is vast (see  Hutter et al. (2019)  for an overview) and includes model-based ( Snoek et al., 2012 ;  Hutter et al., 2015 ), model-free ( Bergstra & Bengio, 2012 ;  Hansen, 2016 ), and gradient- based ( Domke, 2012 ;  Maclaurin et al., 2015 ) approaches. Additionally, works in the area of learning to optimize (Andrychowicz et al., 2016;  Wichrowska et al., 2017 ) have focused on the problem of tuning parameterized optimizers on whole classes of learning problems but require prior expensive optimization and are not designed to speed up training on a single specific task. The goal of this paper is to automatically compute in an online fashion a learning rate schedule for stochastic optimization methods (such as SGD) only on the basis of the given learning task, aiming at producing models with associated small validation error. We study the problem of finding a LR schedule under the framework of gradient-based hyperparameter optimization ( Franceschi et al., 2017 ): we consider as an optimal schedule η * = (η * 0 , . . . , η * T −1 ) ∈ R T + a solution to the following constrained optimization problem for t = {0, . . . , T − 1} = [T ], where E : R d → R + is an objective function, Φ t : R d × R + → R d is a (possibly stochastic) weight update dynamics,w ∈ R d represents the initial model weights (parameters) and finally w t are the weights after t iterations. We can think of E as either the training or the validation loss of the model, while the dynamics Φ describe the update rule (such as SGD, SGD-Momentum, Adam etc.). For example in the case of SGD, Φ t (w t , η t ) = w t − η t ∇L t (w t ), with L t (w t ) the (possibly regularized) training loss on the t-th minibatch. The horizon T should be large enough so that the training error can be effectively minimized, in order to avoid underfitting. Note that a too large value of T does not necessarily harm since η k = 0 for k >T is still a feasible solution, implementing early stopping in this setting.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new representation of multisets using weighted multiset automata, a variant of weighted finite-state (string) automata in which the order of the input symbols does not affect the output. This representation generalizes DeepSets and the Transformer's position encodings, and can be directly implemented inside a neural network. We show how to train these automata efficiently by approximating them with string automata whose weights form complex, diagonal matrices. We also discuss the application of our representation in both cases, and experiment with several variations on position encodings inspired by this justification. Our model is able to reach perfect performance on an extension of one of the tasks, whereas the original DeepSets model does no better than chance.",
        "Abstract": "Unordered, variable-sized inputs arise in many settings across multiple fields.  The ability for set- and multiset- oriented neural networks to handle this type of input has been the focus of much work in recent years.  We propose to represent multisets using complex-weighted multiset automata and show how the multiset representations of certain existing neural architectures can be viewed as special cases of ours.  Namely, (1) we provide a new theoretical and intuitive justification for the Transformer model's representation of positions using sinusoidal functions, and (2) we extend the DeepSets model to use complex numbers, enabling it to outperform the existing model on an extension of one of their tasks.  \n",
        "Introduction": "  INTRODUCTION Neural networks which operate on set-structured input have been gaining interest for their ability to handle unordered and variable-sized inputs ( Vinyals et al., 2015 ;  Wagstaff et al., 2019 ). They have been applied to various tasks, such as processing graph nodes ( Murphy et al., 2018 ), hypergraphs ( Maron et al., 2019 ), 3D image reconstruction ( Yang et al., 2019 ), and point cloud classification and image tagging ( Zaheer et al., 2017 ). Similar network structures have been applied to multiple instance learning ( Pevný and Somol, 2016 ). In particular, the DeepSets model ( Zaheer et al., 2017 ) computes a representation of each element of the set, then combines the representations using a commutative function (e.g., addition) to form a representation of the set that discards ordering information.  Zaheer et al. (2017)  provide a proof that any function on sets can be modeled this way, by encoding sets as base-4 fractions and using the universal function approximation theorem, but their actual proposed model is far simpler than the model constructed by the theorem. In this paper, we propose to compute representations of multisets using weighted multiset automata, a variant of weighted finite-state (string) automata in which the order of the input symbols does not affect the output. In some sense, this is the most general representation of a multiset that can be computed incrementally using only a finite amount of memory, and it can be directly implemented inside a neural network. We show how to train these automata efficiently by approximating them with string automata whose weights form complex, diagonal matrices. Our representation generalizes DeepSets slightly, and it also turns out to be a generalization of the Trans- former's position encodings ( Vaswani et al., 2017 ). In Sections 4 and 5, we discuss the application of our representation in both cases. • The Transformer ( Vaswani et al., 2017 ) models the absolute position of a word within a sentence. This position can be thought of as a multiset over a single element, and indeed the Transformer uses a position encoding involving sinusoidal functions that turns out to be a special case of our representation. So weighted multiset automata provide a new theoretical and intuitive justification for sinusoidal position encodings. We also experiment with several variations on position encodings Under review as a conference paper at ICLR 2020 inspired by this justification, and although they do not yield any improvement, we do find that learned position encodings in our representation do better than learning a different vector for each absolute position. • We extend the DeepSets model to use our representation, which amounts to upgrading it from real to complex numbers. On an extension of one of their tasks (adding a sequence of one-digit numbers and predicting the units digit), our model is able to reach perfect performance, whereas the original DeepSets model does no better than chance.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper investigates the properties of winning tickets, sparse subnetworks of over-parameterized neural networks that can achieve good predictions when trained in isolation. We empirically answer open questions on winning tickets and the data-labels distribution on which they are generated, such as the extent to which they are label-dependent and the impact of reducing the number of samples per class or the number of classes used for winning ticket generation. Our experiments show that winning tickets are surprisingly robust to many of these data/label distribution changes, and that finding winning tickets can be accelerated by a factor 5× on ImageNet by using only a subset of the data.",
        "Abstract": "The lottery ticket hypothesis argues that neural networks contain sparse subnetworks, which, if appropriately initialized (the winning tickets), are capable of matching the accuracy of the full network when trained in isolation. Empirically made in different contexts, such an observation opens interesting questions about the dynamics of neural network optimization and the importance of their initializations. However, the properties of winning tickets are not well understood, especially the importance of supervision in the generating process. In this paper, we aim to answer the following open questions: can we find winning tickets with few data samples or few labels? can we even obtain good tickets without supervision? Perhaps surprisingly, we provide a positive answer to both, by generating winning tickets with limited access to data, or with self-supervision---thus without using manual annotations---and then demonstrating the transferability of the tickets to challenging classification tasks such as ImageNet.\n",
        "Introduction": "  INTRODUCTION Recently,  Frankle & Carbin (2019)  have observed that sparse subnetworks of over-parameterized neural networks could achieve good predictions when trained in isolation, as long as they are appro- priately initialized; these \"lucky\" starting points have been termed winning tickets. Building these tickets is typically achieved by pruning the weights with lowest magnitude of an over-parametrized network that has been trained to convergence, before resetting the remaining weights to their initial values, or at some point early in training, and repeating the procedure. The properties of these ef- fective subnetworks and their initialization are however not well understood yet. For example, the original lottery ticket hypothesis argued that these winning tickets emerge from the network initial- ization. Yet, original experiments were performed on classification tasks with shallow architectures and small datasets, and further experiments on deeper models and more challenging problems sug- gest that they may rather appear later, though still early, during training ( Frankle et al., 2019 ). Recent works (Morcos et al., 2019;  Yu et al., 2019 ;  Zhou et al., 2019 ) have studied other properties of the winning tickets generation process: Morcos et al. (2019) have shown that winning tickets can be transferred across datasets with similar natural image statistics, and  Yu et al. (2019)  have exposed the existence of lottery tickets to other domains, such as text and reinforcement learning. In this paper, we expand on this line of work and empirically answer other important open questions on winning tickets and the data-labels distribution on which they are generated. In particular, we want to know if \"good\" tickets can be obtained when few data samples, or few labels, or even no labels, are available. Answering these questions is important since it may speed-up the winning ticket generation process, which is computationnally expensive, and also open new perspectives about generating sparse subnetworks that may be trained efficiently on new tasks. To this effect, we design several experiments that isolate the impact of the data and label distributions on the quality of the resulting winning tickets. First, we analyze the extent to which winning tickets are label-dependent by generating \"label-agnostic\" winning tickets with self-supervised tasks ( Gi- daris et al., 2018 ;  Doersch & Zisserman, 2017 ). Then, we evaluate the impact of reducing the num- bers of samples per class or the number of classes used for winning ticket generation. Finally, we evaluate the performance of semi-supervised approaches to winning ticket generation. Most of our experiments are conducted on ImageNet; we remark indeed that deep networks trained on smaller Under review as a conference paper at ICLR 2020 datasets such as CIFAR-10 are already sparse at convergence, making pruning less challenging, and conclusions drawn about lottery tickets potentially misleading if this effect is not accounted for. Overall, our experiments show that winning tickets are surprisingly robust to many of these data/label distribution changes. Indeed, using only 10% of the dataset or removing entirely the labels still leads to effective winning tickets. As a result, we manage to extract winning tickets 5× faster by training on a subset of data, with no modifications to the core algorithm. This is of partic- ular interest because finding winning tickets is highly data, labels and resource-demanding since it requires training a network to convergence at least once (and generally dozens of times in practice). Our paper makes the following contributions: (i) We combine self-supervised learning with winning tickets generation, showing that good winning tickets can be found without labels. (ii) We show that finding winning tickets can be accelerated by a factor 5× on ImageNet by using only a subset of the data. (iii) We also show that using large datasets is important to study lottery tickets, since deep networks trained on CIFAR-10 are natually sparse, making conclusions potentially misleading.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a technique for improving the calibration of deep neural networks (DNNs) by replacing the cross-entropy loss conventionally used when training multi-class classification networks with the focal loss proposed by Lin et al. (2017). We provide a theoretically justified way to choose the focal loss hyperparameter, γ, automatically for each sample and show it to outperform all the baseline models. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of different network architectures, to show that DNNs trained with focal loss are more calibrated than those trained with cross-entropy loss, MMCE, and Brier loss. Our experiments show that significant improvements in calibration over temperature scaling alone, and state-of-the-art results, can be achieved by training with focal loss and then performing temperature scaling.",
        "Abstract": "Miscalibration -- a mismatch between a model's confidence and its correctness -- of Deep Neural Networks (DNNs) makes their predictions hard for downstream components to trust. Ideally, we want networks to be accurate, calibrated and confident. Temperature scaling, the most popular calibration approach, will calibrate a DNN without affecting its accuracy, but it will also make its correct predictions under-confident. In this paper, we show that replacing the widely used cross-entropy loss with focal loss allows us to learn models that are already very well calibrated. When combined with temperature scaling, focal loss, whilst preserving accuracy and yielding state-of-the-art calibrated models, also preserves the confidence of the model's correct predictions, which is extremely desirable for downstream tasks. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to theoretically justify the empirically excellent performance of focal loss. We perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (SST, 20 Newsgroup) datasets, and with a wide variety of different network architectures, and show that our approach achieves state-of-the-art accuracy and calibration in almost all cases.",
        "Introduction": "  INTRODUCTION Deep neural networks have dominated computer vision and machine learning in recent years, and this has led to their widespread deployment in real-world systems ( Cao et al., 2018 ;  Chen et al., 2018 ;  Kamilaris and Prenafeta-Boldú, 2018 ;  Ker et al., 2018 ;  Wang et al., 2018 ). State-of-the-art networks achieve high levels of accuracy for many tasks. However, many current multi-class classification networks in particular are poorly calibrated, in the sense that the probability values that they associate with the class labels they predict for different test samples overestimate the likelihoods of those class labels being correct in the real world. This is a major problem, since if networks are routinely overconfident, then downstream components cannot trust their predictions. The underlying cause is hypothesised to be that these networks' high capacity leaves them vulnerable to overfitting on the negative log-likelihood (NLL) loss they conventionally use during training ( Guo* et al., 2017 ). Given the importance of this problem, numerous suggestions for how to address it have been proposed. Much work has been inspired by early approaches from the pre-deep learning era such as Platt scaling ( Platt, 1999 ), histogram binning ( Zadrozny and Elkan, 2001 ), isotonic regression ( Zadrozny and Elkan, 2002 ), and Bayesian binning and averaging ( Naeini et al., 2015 ;  Naeini and Cooper, 2016 ). As deep learning has become more dominant, works have begun to directly target the calibration of deep networks. For example, Guo et al. ( Guo* et al., 2017 ) have popularised a modern variant of Platt scaling known as temperature scaling, which works by dividing a network's logits by a scalar T > 0 (learnt on a validation subset) prior to performing softmax. Temperature scaling has the desirable property that it can improve the calibration of a network without in any way affecting its accuracy. Mozafari et al. ( Mozafari et al., 2018 ) noted the downsides of using cross-entropy loss with temperature scaling, and proposed an alternative loss called Attended-NLL that helps temperature scaling achieve better calibration. More recently, Shrikumar and Kundaje ( Shrikumar and Kundaje, 2019 ) have proposed an extension to temperature scaling that adds class-specific bias parameters to help eliminate systematic bias when performing domain adaptation. Separately, Hendrycks et al. ( Hendrycks et al., 2019 ) have studied the effects of pre-training (vs. training from scratch) on model robustness and uncertainty: they make the interesting observation that because long periods of training can cause a network to become miscalibrated, tuning a pre-trained network, which facilitates faster convergence, can seemingly lead to a more calibrated model. Notably, since their approach Under review as a conference paper at ICLR 2020 complements temperature scaling, the two techniques can also be used together to achieve even better calibration overall. Whilst temperature scaling's simplicity and effectiveness have made it a popular and state-of-the-art network calibration technique, it does have downsides. For example, whilst it scales the logits to reduce the network's confidence in incorrect predictions, this also slightly reduces the network's confidence in predictions that were actually correct. By contrast, Kumar et al. ( Kumar et al., 2018 ) eschew temperature scaling altogether in favour of minimising a differentiable proxy for calibration error at training time, called Maximum Mean Calibration Error (MMCE). However, they also use temperature scaling as a post-processing step to obtain better results than cross-entropy followed by temperature scaling ( Guo* et al., 2017 ). In this paper, we propose a technique for improving network calibration that works by replacing the cross-entropy loss conventionally used when training multi-class classification networks with the focal loss proposed by Lin et al. ( Lin et al., 2017 ) for dense object detection. Since focal loss, as shown in §4, is dependent on a hyperparameter, γ, which needs to be cross-validated, we also provide a theoretically justified way to choose γ automatically for each sample and show it to outperform all the baseline models. Informally, the intuition behind using focal loss is to direct the network's attention during training towards samples for which it is currently predicting a low probability value for the correct class, since trying to reduce the NLL on samples for which it is currently predicting a high probability value for the correct class is liable to lead to NLL overfitting, and thereby miscalibration ( Guo* et al., 2017 ). More formally, we show in §4 that focal loss can be seen as implicitly regularising the weights of the network by causing the gradient norm to be lower than it would have been with cross-entropy loss as training proceeds, which we would theoretically expect to reduce overfitting and improve the calibration of the network. In §5, we perform extensive experiments on a variety of computer vision (CIFAR-10/100) and NLP (20 Newsgroups/SST) datasets, and with a wide variety of different network architectures (e.g. ResNet-110/50, Wide-ResNet, DenseNet), to show that this is indeed the case. Our experiments show that in almost all cases, DNNs trained with focal loss are more calibrated than those trained with cross-entropy loss, MMCE, and  Brier loss Brier (1950) . Moreover, since our approach, like that of ( Hendrycks et al., 2019 ), is complementary to the temperature scaling, significant improvements in calibration over temperature scaling alone, and state-of-the-art results, can be achieved by training with focal loss and then performing temperature scaling.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a method to accelerate the training of deep neural networks by selecting only those instances that accelerate the training convergence of the network. The selection process is continuously performed throughout the training process at each step and in every training epoch. The selection criterion is based on computations that are calculated as part of the forward pass, taking advantage of the cheaper inference compute.",
        "Abstract": "We present a selective sampling method designed to accelerate the training of deep neural networks. To this end,  we introduce a novel measurement,  the {\\it minimal margin score} (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched.   For multi-class linear classification,  the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting.  In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks.  The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection.\nFinally, we demonstrate an additional speedup when we adopt a more aggressive learning-drop regime while using the MMS selective sampling method.",
        "Introduction": "  INTRODUCTION Over the last decade, deep neural networks have become the machine learning method of choice in a variety of application domains, demonstrating outstanding, often close to human-level, perfor- mances in a variety of tasks. Much of this tremendous success should be attributed to the availability of resources; a massive amount of data and compute power, which in turn fueled the impressive and innovative algorithmic and modeling development. However, resources, although available, come with a price. Data in the big data era is available, but reliable labeled data is always a challenge, and so are the ETL (Extract-Transform-Load) processes, data transfer, and storage. With the introduc- tion of GPUs, compute power is readily available, making the training of deep architectures feasible. However, the training phase, which to a large extent, relies on stochastic gradient descent methods, requires a large number of computational resources as well as a substantial amount of time. A closer look at the compute processes highlights the fact that there is a significant difference in the com- pute effort between the inference (forward pass) and the model update (back-propagation) where the latter being far more demanding. The implication is evidenced by the performance charts that hardware manufactures publish, where performance matrices such as throughput (e.g. image per second) are up to 10x better at inference vs. training for popular deep neural network architectures. In this paper, we address the computing challenge. Specifically, we suggest a method to select for the back-propagation pass only those instances that accelerate the training convergence of the deep neural network, thus speeding up the entire training process. The selection process is continuously performed throughout the training process at each step and in every training epoch. Our selection criterion is based on computations that are calculated anyhow as an integral part of the forward pass, thus taking advantage of the \"cheaper\" inference compute.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a batch active learning strategy to train machine learning models that is robust to noisy oracles. The proposed method is based on importance sampling and clustering to draw a batch of samples that are simultaneously diverse and important to the model. Model uncertainty is incorporated into the sampling probability to compensate for poor estimation of the importance scores when the training data is too small to build a meaningful model. Additionally, a denoising layer is introduced to deep networks to further robustify active learning to noisy oracles. Results demonstrate that the proposed method outperforms state-of-the-art methods in both noise-free and noisy scenarios.",
        "Abstract": "We study the problem of training machine learning models incrementally using active learning with access to imperfect or noisy oracles. We specifically consider the setting of batch active learning, in which multiple samples are selected as opposed to a single sample as in classical settings so as to reduce the training overhead. Our approach bridges between uniform randomness and score based importance sampling of clusters when selecting a batch of new samples. Experiments on\nbenchmark image classification datasets (MNIST, SVHN, and CIFAR10) shows improvement over existing active learning strategies. We introduce an extra denoising layer to deep networks to make active learning robust to label noises and show significant improvements.\n",
        "Introduction": "  INTRODUCTION Supervised learning is the most widely used machine learning method, but it requires labelled data for training. It is time-consuming and labor-intensive to annotate a large dataset for complex super- vised machine learning models. For example, ImageNet ( Russakovsky et al., 2015 ) reported the time taken to annotate one object to be roughly 55 seconds. Hence an active learning approach which selects the most relevant samples for annotation to incrementally train machine learning models is a very attractive avenue, especially for training deep networks for newer problems that have littel annotated data. Classical active learning appends the training dataset with a single sample-label pair at a time. Given the increasing complexity of machine learning models, it is natural to expand active learning pro- cedures to append a batch of samples at each iteration instead of just one. Keeping such training overhead in mind, a few batch active learning procedures have been developed in the literature ( Wei et al., 2015 ;  Sener & Savarese, 2018 ;  Sinha et al., 2019 ). When initializing the model with a very small seed dataset, active learning suffers from the cold- start problem: at the very beginning of active learning procedures, the model is far from being accurate and hence the inferred output of the model is incorrect/uncertain. Since active learning relies on output of the current model to select next samples, a poor initial model leads to uncertain estimation of selection criteria and selection of wrong samples. Prior art on batch active learning suffers performance degradation due to this cold-start problem. Most active learning procedures assume the oracle to be perfect, i.e., it can always annotate samples correctly. However, in real-world scenarios and given the increasing usage of crowd sourcing, for example Amazon Mechanical Turk (AMT), for labelling data, most oracles are noisy. The noise induced by the oracle in many scenarios is resolute. Having multiple annotations on the same sample cannot guarantee noise-free labels due to the presence of systematic bias in the setup and leads to consistent mistakes. To validate this point, we ran a crowd annotation experiment on ESC50 dataset ( Piczak, 2015 ): each sample is annotated by 5 crowdworkers on AMT and the majority vote of the 5 annotations is considered the label. It turned out for some classes, 10% of the samples are annotated wrong, even with 5 annotators. Details of the experiment can be found in Appendix A. Under such noisy oracle scenarios, classical active learning algorithms such as ( Chen et al., 2015a ) under-perform as shown in  Figure 1 . Motivating from these observations, we fashion a batch active learning strategy to be robust to noisy oracles. The main contributions of this work are as follows: (1) we propose a batch sample selection method based on importance sampling and clustering which caters to drawing a batch which is simultaneously diverse and important to the model; (2) we incorporate model uncertainty into the sampling probability to compensate poor estimation of the Under review as a conference paper at ICLR 2020 importance scores when the training data is too small to build a meaningful model; (3) we introduce a denoising layer to deep networks to robustify active learning to noisy oracles. Main results, as shown in  Fig. 3  demonstrate that in noise-free scenario, our method performs as the best over the whole active learning procedure, and in noisy scenario, our method outperforms significantly over state-of-the-art methods.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes utilizing gradients as a representation to characterize information that has not been learned from the training data but is currently presented in the input data. Through comprehensive analysis with activation-based representations, the effectiveness of gradient representation in characterizing the information that has not been learned for deep networks is demonstrated. Furthermore, a gradient-based anomaly detection algorithm is proposed that outperforms state-of-the-art algorithms based on activation representations.",
        "Abstract": "Deep networks face challenges of ensuring their robustness against inputs that cannot be effectively represented by information learned from training data. We attribute this vulnerability to the limitations inherent to activation-based representation. To complement the learned information from activation-based representation, we propose utilizing a gradient-based representation that explicitly focuses on missing information. In addition, we propose a directional constraint on the gradients as an objective during training to improve the characterization of missing information. To validate the effectiveness of the proposed approach, we compare the anomaly detection performance of gradient-based and activation-based representations. We show that the gradient-based representation outperforms the activation-based representation by 0.093 in CIFAR-10 and 0.361 in CURE-TSR datasets in terms of AUROC averaged over all classes. Also, we propose an anomaly detection algorithm that uses the gradient-based representation, denoted as GradCon, and validate its performance on three benchmarking datasets. The proposed method outperforms the majority of the state-of-the-art algorithms in CIFAR-10, MNIST, and fMNIST datasets with an average AUROC of 0.664, 0.973, and 0.934, respectively.",
        "Introduction": "  INTRODUCTION The generalizable representation of data from deep network has largely contributed to the success of deep learning in diverse applications ( Bengio et al., 2013 ). The representation from deep networks is often obtained in the form of activation. The activation is constructed by the weights which contain specific knowledge learned from training samples. Recent studies reveal that deep networks still face robustness issues when input that cannot be properly represented by learned knowledge is given to the networks ( Goodfellow et al., 2014 ;  Hendrycks & Dietterich, 2018 ;  Liang et al., 2017 ). One of the reasons for the vulnerability of deep networks is the limitation in the activation-based representation, which inherently focused on the learned knowledge. However, the part of the input that causes problems in deep networks is mainly from the information that deep networks were not able to learn from the training data. Therefore, it is more appropriate to complement the representation of input data from the perspective of information that has not been learned for enhancing the robustness of machine learning algorithms. The gradient is another fundamental element in deep networks that is utilized to learn new informa- tion from given inputs by updating model weights ( Goodfellow et al., 2016 ). It is generated through backpropagation to train deep networks by minimizing designed loss functions ( Rumelhart et al., 1986 ). During the training of network, the gradient with respect to the weights provides directional information to update the deep network and learn a better representation for the inputs. In other words, gradients guide the network to learn new information that was not learned from data that it has seen so far but is presented in the current input. Considering this role during training, gradi- ents can provide a complementary perspective with respect to activation and characterize missing information that the network has not learned for each unseen image. We demonstrate the role of gradients with an example in  Fig. 1 . Assume that a deep network has only learned curved edge features from training images of the digit '0'. During testing, the digit '6' is given to the network. The digit '6' consists of both learned information (curved edges) and missing information (straight edges on top). Since the activation-based representation is constructed Under review as a conference paper at ICLR 2020 based on the information that the network has already learned, the curved part of the digit '6' will be characterized effectively by the activation. However, the network still has to learn the straight edge features to perform successfully on the digit '6'. Therefore, the gradients which guide updates in the deep network can characterize straight edge information that has not been learned. We propose analyzing the representa- tion capability of gradients in charac- terizing missing information for deep networks. Gradients have been uti- lized in diverse applications such as adversarial attack generation and vi- sualization ( Zeiler & Fergus, 2014 ;  Goodfellow et al., 2014 ). How- ever, using gradients with respect to weight as the representation of data has not been actively explored yet. Through the comprehensive analy- sis with activation-based representa- tions, we show the effectiveness of gradient representation in characterizing the information that has not been learned for deep net- work. Furthermore, we show that gradient representation can achieve state-of-the-art performance in detecting potentially invalid data for the network. The main contributions of this paper are three folds: i We propose utilizing gradients as a representation to characterize information that has not been learned from the training data but is currently presented in the input data. ii We analyze the representation capability of gradient compared to activation for detecting sam- ples which possess features that have not been learned for the network. iii We propose a gradient-based anomaly detection algorithm that outperforms state-of-the-art al- gorithms based on activation representations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Action Grammar Reinforcement Learning (AG-RL), a novel approach to hierarchical reinforcement learning that uses grammar inference techniques to form a hierarchical representation of actions. AG-RL operates by using the observed actions of the agent within a time window to infer an action grammar, which is then appended to the agent's action set in the form of macro-actions. Results show that AG-RL is able to consistently and significantly improve sample efficiency across a wide range of Atari settings.",
        "Abstract": "From a young age humans learn to use grammatical principles to hierarchically combine words into sentences. Action grammars is the parallel idea; that there is an underlying set of rules (a \"grammar\") that govern how we hierarchically combine actions to form new, more complex actions. We introduce the Action Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of action grammars to consistently improve the sample efficiency of Reinforcement Learning agents. AG-RL works by using a grammar inference algorithm to infer the “action grammar\" of an agent midway through training, leading to a higher-level action representation. The agent's action space is then augmented with macro-actions identified by the grammar. We apply this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action version of Soft Actor-Critic (AG-SAC) and find that it improves performance in 8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested Atari games (median +96%, maximum +3,756%) respectively without substantive hyperparameter tuning. We also show that AG-SAC beats the model-free state-of-the-art for sample efficiency in 17 out of the 20 tested Atari games (median +62%, maximum +13,140%), again without substantive hyperparameter tuning.",
        "Introduction": "  INTRODUCTION Reinforcement Learning (RL) has made great progress in recent years, successfully being applied to settings such as board games ( Silver et al., 2017 ), video games ( Mnih et al., 2015 ) and robot tasks ( OpenAI et al., 2018 ). Some of this advance is due to the use of deep learning techniques to avoid induction biases in the mapping of sensory information to states. However, widespread adoption of RL in real-world domains has remained limited primarily because of its poor sample efficiency, a dominant concern in RL ( Wu et al., 2017 ), and complexity of the training process that need to be managed by various heuristics. Hierarchical Reinforcement Learning (HRL) attempts to improve the sample efficiency of RL agents by making their policies to be hierarchical rather than single level. Using hierarchical policies can lead not only to faster learning, but also ease human understanding of the agent's behaviour - this is because higher-level action representations are easier to understand than low-level ones ( Beyret et al., 2019 ). Identifying the right hierarchical policy structure is, however a non-trivial task ( Osa et al., 2019 ) and so far progress in hierarchical RL has been slow and incomplete, as no truly scalable and successful hierarchical architectures exist ( Vezhnevets et al., 2016 ). Not surprisingly most state- of-the-art RL agents at the moment are not hierarchical. In contrast, humans, use hierarchical grammatical principles to communicate using spoken/written language (Ding et al., 2012) but also for forming meaningful abstractions when interacting with var- ious entities. In the language case, for example, to construct a valid sentence we generally combine a noun phrase with a verb phrase ( Yule, 2015 ). Action grammars are an analogous idea proposing there is an underlying set of rules for how we hierarchically combine actions over time to produce new actions. There is growing neuroscientific evidence that the brain uses similar processing strate- gies for both language and action, meaning that grammatical principles are used in both neural repre- sentations of language and action ( Faisal et al., 2010 ;  Hecht et al., 2015 ;  Pastra & Aloimonos, 2012 ). We hypothesise that using action grammars would allow us to form a hierarchical action represen- tation that we could use to accelerate learning. Additionally, in light of the neuroscientific findings, hierarchical structures of action may also explain the interpretability of hierarchical RL agents, as their representations are structurally more similar to how humans structure tasks. In the following we will explore the use of grammar inference techniques to form a hierarchical representation of actions that agents can operate on. Just like much of Deep RL has focused on forming unbiased sensory representations from data-driven agent experience, here we explore how data-driven agent experience of actions can contribute to forming efficient representations for learning and controlling tasks. Action Grammar Reinforcement Learning (AG-RL) operates by using the observed actions of the agent within a time window to infer an action grammar. We use a grammar inference algorithm to substitute repeated patterns of primitive actions (i.e. words) into temporal abstractions (rules, anal- ogous to a sentence). Similarly, we then replace repeatedly occurring rules of temporal abstractions with higher-level rules of temporal abstractions (analogous to paragraphs), and so forth. These ex- tracted action grammars (the set of rules, rules of rules, rules of rules of rules, etc) is appended to the agent's action set in the form of macro-actions so that agents can choose (and evaluate) primitive actions as well as any of the action grammar rules. We show that AG-RL is able to consistently and significantly improve sample efficiency across a wide range of Atari settings.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method for constructing adversarial examples that explicitly preserve the semantics of the inputs. The method is composed of three steps: (i) manifold learning, (ii) perturbation invariance, and (iii) adversarial attack. The manifold learning step uses a variational inference technique to learn the low dimensional geometric summaries of the inputs. The perturbation invariance step perturbs the elements of the learned manifold while ensuring the perturbed elements remain within the manifold. The adversarial attack step leverages the rich semantics of the inputs and the perturbations to impose adversarial constraints to produce adversarial examples. The proposed method is efficient and end-to-end, and provides reliable adversarial training signals to robustify deep learning models.",
        "Abstract": "While progress has been made in crafting visually imperceptible adversarial examples, constructing semantically meaningful ones remains a challenge. In this paper, we propose a framework to generate semantics preserving adversarial examples. First, we present a manifold learning method to capture the semantics of the inputs. The motivating principle is to learn the low-dimensional geometric summaries of the inputs via statistical inference. Then, we perturb the elements of the learned manifold using the Gram-Schmidt process to induce the perturbed elements to remain in the manifold. To produce adversarial examples, we propose an efficient algorithm whereby we leverage the semantics of the inputs as a source of knowledge upon which we impose adversarial constraints. We apply our approach on toy data, images and text, and show its effectiveness in producing semantics preserving adversarial examples which evade existing defenses against adversarial attacks.",
        "Introduction": "  INTRODUCTION In response to the susceptibility of deep neural networks to small adversarial perturbations ( Szegedy et al., 2014 ), several defenses have been proposed ( Liu et al., 2019 ;  Sinha et al., 2018 ;  Raghunathan et al., 2018 ;  Madry et al., 2017 ;  Kolter & Wong, 2017 ). Recent attacks have, however, cast serious doubts on the robustness of these defenses ( Athalye et al., 2018 ;  Carlini & Wagner, 2016 ). A standard way to increase robustness is to inject adversarial examples into the training inputs ( Goodfellow et al., 2014a ). This method, known as adversarial training, is however sensitive to distributional shifts between the inputs and their adversarial examples ( Ilyas et al., 2019 ). Indeed, distortions, occlusions or changes of illumination in an image, to name a few, do not always preserve the nature of the image. In text, slight changes to a sentence often alter its readability or lead to substantial differences in meaning. Constructing semantics preserving adversarial examples would provide reliable adversarial training signals to robustify deep learning models, and make them generalize better. However, several approaches in adversarial attacks fail to enforce the semantic relatedness that ought to exist between the inputs and their adversarial counterparts. This is due to inadequate characterizations of the semantics of the inputs and the adversarial examples -  Song et al. (2018)  and  Zhao et al. (2018b)  confine the distribution of the latents of the adversarial examples to a Gaussian. Moreover, the search for adversarial examples is customarily restricted to uniformly-bounded regions or conducted along suboptimal gradient directions ( Szegedy et al., 2014 ;  Kurakin et al., 2016 ;  Goodfellow et al., 2014b ). In this study, we introduce a method to address the limitations of previous approaches by constructing adversarial examples that explicitly preserve the semantics of the inputs. We achieve this by char- acterizing and aligning the low dimensional geometric summaries of the inputs and the adversarial examples. The summaries capture the semantics of the inputs and the adversarial examples. The alignment ensures that the adversarial examples reflect the unbiased semantics of the inputs. We decompose our attack mechanism into: (i.) manifold learning, (ii.) perturbation invariance, and (iii.) adversarial attack. The motivating principle behind step (i.) is to learn the low dimensional geometric summaries of the inputs via statistical inference. Thus, we present a variational inference technique that relaxes the rigid Gaussian prior assumption typically placed on VAEs encoder networks ( Kingma & Welling, 2014 ) to capture faithfully such summaries. In step (ii.), we develop an approach around the manifold invariance concept of ( Roussel, 2019 ) to perturb the elements of the learned manifold while ensuring the perturbed elements remain within the manifold. Finally, in step (iii.), we propose a learning algorithm whereby we leverage the rich semantics of the inputs and the perturbations as a source of knowledge upon which we impose adversarial constraints to produce adversarial examples. Unlike ( Song et al., 2018 ;  Carlini & Wagner, 2016 ;  Zhao et al., 2018b ;  Goodfellow et al., 2014b ) that resort to a costly search of adversarial examples, our algorithm is efficient and end-to-end.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the utility of various methods to mitigate overfitting in machine learning models, particularly when using powerful function approximators such as deep neural networks. A novel \"activation difference\" (actdiff) regularizer is presented which directly mitigates this behaviour. Additionally, a method is presented where an autoencoder/UNet is trained to reconstruct a masked version of the input, indirectly controlling feature representations used for classification. The performance of these methods is compared on synthetic and real-life medical datasets, and the differences in their feature attributions are demonstrated using saliency maps.",
        "Abstract": "Overfitting is a common issue in machine learning, which can arise when the model learns to predict class membership using convenient but spuriously-correlated image features instead of the true image features that denote a class. These are typically visualized using saliency maps. In some object classification tasks such as for medical images, one may have some images with masks, indicating a region of interest, i.e., which part of the image contains the most relevant information for the classification. We describe a simple method for taking advantage of such auxiliary labels, by training networks to ignore the distracting features which may be extracted outside of the region of interest, on the training images for which such masks are available. This mask information is only used during training and has an impact on generalization accuracy in a dataset-dependent way. We observe an underwhelming relationship between controlling saliency maps and improving generalization performance.",
        "Introduction": "  INTRODUCTION Overfitting is a common problem in machine learning, particularly when one uses powerful function approximators such as deep neural networks. When training these models with backpropagation, the network will evolve from modelling simple to more complicated functions until it finds salient discriminative features in the data. Once the model has found these, the gradients of the loss do not encourage the model to find other discriminative features in the data, even if they exist ( Reed & Marks, 1999 ). In the classification case, this can be problematic if there exists some distractor feature x d in the data that is correlated with one of the output classes. This is a common issue in industry data (e.g., medical) where datasets are typically small and there are many confounding variables. Consider the extreme case in a binary classification problem where in the training distribution there exists a confounding distractor element x d of the input data such that for D train , p(y = 1|x d ) = 1, while in the validation distribution D valid , p(y = 0|x d ) = 1 ( Figure 1 ). In this scenario, predicting using x d is easier than predicting using the true features that denote class membership and a classifier trained on D train with traditional classification loss would predict the incorrect class with 100% probability on D valid . This is a textbook example of overfitting ( Goodfellow et al., 2016 ;  Reed & Marks, 1999 ). The existence of these overfit features is the motivation behind methods seeking to learn domain-invariant representations ( Ganin & Lempitsky, 2014 ;  Fernando et al., 2014 ), and is a common problem with real-world data ( Badgeley et al., 2019 ;  Zhao et al., 2019 ;  Young et al., 2019 ). In this paper, we explore the utility of various methods that allow one to use a mask on the input data to guide the network to avoid predicting from the defined region and penalize the network for attributing a prediction to a distractor. We present a synthetic dataset that encourages all models tested to overfit to an easy to represent distractor instead of a more complicated counting task. We present a novel \"activation difference\" (actdiff ) regularizer which mitigates this behaviour directly. We also present a method where we train an autoencoder/UNet to reconstruct a masked version of the input, indirectly controlling feature representations used for classification. We compare these methods with the recently-proposed gradmask ( Simpson et al., 2019b ), and present an expanded analysis of this algorithm's behaviour. All code for this paper, and this dataset, are available here: https://github.com/bigtrellis2222/activmask. We compare the real-life performance of these methods on open medical datasets with traditional classifiers, and demonstrate the differences in their feature attributions using saliency maps. Finally, we describe a medical dataset curated from two openly-available X-ray databases, and describe how samples can be drawn from each to generate a dataset biased by a site-diagnosis correlation inspired by previous work ( Zhao et al., 2019 ). We demonstrate that, similarly to our synthetic datasets, classifiers are likely to predict using features unrelated to the task, and demonstrate that the proposed methods do mitigate this and often successfully refine the saliency maps to focus on the correct anatomy. However they do not consistently prevent overfitting.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to pixel-level out-of-distribution (OOD) detection for machine learning applications. It adapts existing state-of-the-art image-level OOD detection methods to the new task of pixel-level OOD classification and compares their performance on a new dataset designed for this task. The paper also proposes pixel-level OOD detection performance metrics, two new datasets for pixel-level OOD detection, and a new metric for pixel-level OOD detection called MaxIoU. The results of the evaluation show that the best performing pixel-level OOD detection methods were derived from image-level OOD detection methods that were not necessarily the best performing on the image-level OOD detection task.",
        "Abstract": "The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.",
        "Introduction": "  INTRODUCTION Many applications using machine learning (ML) may benefit from out of distribution (OOD) de- tection to improve safety. When inputs are determined to be out of distribution, the output of an ML algorithm should not be trusted. A large body of research exists for detecting entire images as OOD for the task of image classification. Image-level OOD detection outputs a classification for the entire image; this coarse level of detection may be inadequate for many safety critical applications, including autonomous driving. Most of the pixels in an image taken from an onboard camera will be in distribution (ID), i.e. an image of a road scene with cars, people, and roadway-but an unusual object that was not part of the training set may cause only a small number of OOD pixels. Extend- ing the framework to semantic segmentation networks will allow each pixel to have an \"in\" or \"out of\" distribution classification. Applied to autonomous driving, groups of pixels classified as OOD would be considered as unknown objects. Depending on the location of the unknown objects, a planner would then proceed with caution or hand over control to a safety driver. Another application is automatic tagging of images with OOD objects, which would then be sent for human labelling.  Figure 1  shows a failure case where OOD detection is beneficial. The two crates are predicted as road. The right image of this figure shows the result of pixel-level OOD detection using one of the proposed methods, which clearly identifies the unusual objects. This paper adapts existing state-of-the-art image-level OOD detection methods to the new task of pixel-level OOD classification and compares their performance on a new dataset designed for this task. In addition to adapting the methods, we address the question of whether the best-performing image-level methods maintain their performance when adapted to the new task. In order to answer this question, we also propose pixel-level OOD detection performance metrics, drawing both on Under review as a conference paper at ICLR 2020 existing image-level OOD detection and semantic segmentation performance metrics. Further, we design two new datasets for pixel-level OOD detection with test images that contain both pixels that are in distribution and pixels that are out of distribution, evaluated with two different network architectures-PSPNet (Zhao et al., 2016) and DeeplabV3+ ( Chen et al., 2018 ). Somewhat sur- prisingly, our evaluation shows that the best performing pixel-level OOD detection methods were derived from image-level OOD detection methods that were not necessarily the best performing on the image-level OOD detection task. In summary, the contributions of this paper are the following: • adaptation of image-level OOD detection methods to pixel-level OOD detection and their evaluation; • training and evaluation datasets for pixel-level OOD detection evaluation derived from ex- isting segmentation datasets; and • a new metric for pixel-level OOD detection, called MaxIoU.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to unsupervised learning in neural networks, which allows for continual learning with delayed feedback. The proposed method maintains a queue of recent data samples and updates the model parameters with unlabeled data. This approach introduces the same delayed feedback of the human brain into artificial neural networks, and eliminates the need for a training and testing phase.",
        "Abstract": "Most of the artificial neural networks are using the benefit of labeled datasets whereas in human brain, the learning is often unsupervised. The feedback or a label for a given input or a sensory stimuli is not often available instantly. After some time when brain gets the feedback, it updates its knowledge. That's how brain learns. Moreover, there is no training or testing phase. Human learns continually. This work proposes a model-agnostic continual learning framework which can be used with neural networks as well as decision trees to incorporate continual learning. Specifically, this work investigates how delayed feedback can be handled. In addition, a way to update the Machine Learning models with unlabeled data is proposed. Promising results are received from the experiments done on neural networks and decision trees. ",
        "Introduction": "  INTRODUCTION The high representational capacity of neural networks as well as the discovery of good training methods, and the access to fast processing units resulted in super human accuracy in various artificial intelligent tasks. Supervised image classification results has improved to super human levels by various convolutional neural network(CNN) architectures such as AlexNet ( Krizhevsky et al., 2012 ), VGG( Simonyan & Zisserman, 2014 ), GoogleNet( Szegedy et al., 2015 ), and  ResNetHe et al. (2016) . Most of the CNNs are learned with supervision.The limitation of labeled dataset opens the path for unsupervised learning. Moreover, human brain often does continual learning in an unsupervised manner. As an example, take an infant. It does not know most of the things in the environment. But it sees everything. Whenever somebody gives some detail or a label about the object that the baby has seen, it updates its knowledge. This work names this detail or label as delayed feedback. Hence, the key difference in CNNs is that we feed everything with labels at the start which does not occur in human brain. In addition, humans learn continually. The distinction of training and testing is not realized in actual learning of human.  Figure 1  explains a delayed feedback scenario. Suppose an infant sees 3 different cars. But it does not know anything about the car. After sometime, somebody shows a new car and labels it as a car. Now the infant learns that the previous unknowns vehicles should be cars. In this case, the baby does not know any car before seeing these objects. This scenario can be extended to different types or shapes of cars as well where baby knows about one set of cars and how it can learn other different cars. Unsupervised learning is discussed predominantly in 2 categories. One is reconstructing the original data using auto-encoders and the latent representations are used for the downstream tasks( Hinton et al., 2011 ). Especially this resulted in good dimensionality reduction techniques such as t-SNE ( Maaten & Hinton, 2008 ). The second one is instead of reconstructing the given data, the model learns a contrastive task which can produce latent representations which are useful for other AI tasks ( Hjelm et al., 2018 ), ( Schneider et al., 2019 ). Both categories can find better representations to capture the variability of the data in the input space. In either ways, the task specific information extraction is not possible due to learning the same data or a contrastive task. The area of updating a supervised model parameters using an unlabeled data is not explored as per our knowledge. In this work, an idea is proposed to update the model to improve the accuracy over some new data. Mainly, two contributions are presented in this paper. 1. A way to handle delayed feedback is presented and implemented by maintaining a queue to store some recent data of samples. This concept introduces the same delayed feedback of actual brain in artificial neural networks. 2. Instead of having training and testing phase, a new continual learning model is proposed which can update the model even with unlabeled data.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a method, Wasserstein Adversarial Regularization (WAR), to tackle the problem of overfitting on noisy labels without access to a clean validation dataset. WAR regularizes predictions in areas of the feature space close to the decision boundary of conflicting classes, mitigating the influence of noisy labels. Experiments in five datasets under label noise conditions show that WAR outperforms the state-of-the-art in providing robust class predictions.",
        "Abstract": "Noisy labels often occur in vision datasets, especially when they are obtained from crowdsourcing or Web scraping. We propose a new regularization method, which enables learning robust classifiers in presence of noisy data. To achieve this goal, we propose a new adversarial regularization scheme based on the Wasserstein distance.  Using this distance allows taking into account specific relations between classes by leveraging the geometric properties of the labels space.  Our Wasserstein Adversarial Regularization (WAR) encodes a selective regularization, which promotes smoothness of the classifier between some classes, while preserving sufficient complexity of the decision boundary between others. We first discuss how and why adversarial regularization can be used in the context of label noise and then show the effectiveness of our method on five datasets corrupted with noisy labels: in both benchmarks and real datasets, WAR outperforms the state-of-the-art\ncompetitors.",
        "Introduction": "  INTRODUCTION Deep neural networks require large amount of accurately annotated training samples to achieve good generalization performances. Unfortunately, annotating large datasets is a challenging and costly task, which is practically impossible to do perfectly for every task at hand. It is then most likely that datasets will contain incorrectly labeled data, which induces noise in those datasets and can hamper learning. This problem is often referred to as learning with label noise or noisy labels. The probability of facing this problem increases when the dataset contains several fine grained classes that are difficult to distinguish ( Schroff et al., 2011 ;  Krause et al., 2016 ;  Dubey et al., 2018 ). As pointed out in ( Zhang et al., 2017 ), deep convolutional neural networks have huge memorization abilities and can learn very complex functions. That is why training with noisy data labels can lead to poor generalization ( Arpit et al., 2017 ;  Wang et al., 2018 ;  Choi et al., 2018 ). Hence in this paper we propose a method tackling the problem of overfitting on noisy labels, and this without access to a clean validation dataset. This problem has been considered in recent literature, mainly in three ways. First are data clean- ing methods: ( Vahdat, 2017 ;  Xiao et al., 2015 ;  Li et al., 2017 ) learn relations between noisy and clean labels before estimating new labels for training. In ( Lee et al., 2018 ), few human verified labels were necessary to detect noisy labels and adapt learning. In ( Jiang et al., 2018 ;  Ren et al., 2018 ), the methods rely either on a curriculum or on meta-gradient updates to re-weight training sets and downweight samples with noisy labels. Second are Transition probability-based methods: ( Sukhbaatar & Fergus, 2014 ;  Patrini et al., 2017 ;  Hendrycks et al., 2018 ) estimate a probability for each label to be flipped to another class and use these estimations to build a noise transition matrix. In ( Sukhbaatar & Fergus, 2014 ), the authors add an extra linear layer to the softmax in order to learn the noise transition matrix itself, while ( Hendrycks et al., 2018 ) uses a small set of clean validation data to estimate it. ( Patrini et al., 2017 ) proposes a forward/backward loss correction method, which exploits the noise transition matrix to correct the loss function itself. Third are regularization-based methods . In ( Reed et al., 2015 ;  Ma et al., 2018 ), the authors use a mixture between the noisy labels and network predictions. In ( Tanaka et al., 2018 ), the regularization is achieved by alternatively op- timizing the network parameters and estimating the true labels while the authors of ( Han et al., 2018 ;  Yu et al., 2019 ) propose peer networks feedbacking each other about predictions for the noisy labels.  Song et al. (2019)  proposes to replace noisy labels in the mini-batch by the consistent network pre- dictions during training, while  Chen et al. (2019)  proposes noisy cross-validation to identify samples Under review as a conference paper at ICLR 2020 that have correct labels. In ( Wang et al., 2019 ;  Zhang & Sabuncu, 2018 ;  Ghosh et al., 2017 ), robust loss functions are proposed to overcome limitations of cross entropy loss function. In contrast to those works, we propose to regularize predictions in areas of the feature space close to the decision boundary of conflicting classes, therefore mitigating the influence of noisy labels. To do so, we use the adversarial regularization (AR) framework ( Goodfellow et al., 2015 ;  Miyato et al., 2018 ) on the noisy label problem. We use AR to reduce the discrepancy between the prediction of a true input sample and the one obtained by a near-by adversarial sample. To reduce this discrepancy, we use a loss based on the Wasserstein distance computed with respect to a ground cost encoding class similarities. This ground cost provides the flexibility to regularize with different strengths pairs of classes. This strength can depend on semantic relations, classes similarities, or prior knowledge (e.g. on annotators' mistakes). This way, the classifier can discriminate non-similar objects robustly under the presence of noise and class overlap. We name our proposed method Wasserstein Adversarial Regularization (WAR). WAR allows incorporating specific knowledge about the potential degree of mixing of classes through a ground cost that can be designed w.r.t. the problem at hand. Nevertheless, this knowledge might be unknown or difficult to craft. In this paper, we use distances between word embeddings of the class names to derive a semantic ground cost. Experiments in five datasets (Fashon-MNIST, CIFAR10, CIFAR100 and real life examples on clothing classification and a remote sensing semantic segmentation dataset) under label noise conditions show that WAR outperforms the state-of-the-art in providing robust class predictions.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents Contrastive Sensor Fusion (CSF), a method of learning representations for overhead imagery using the intuition that similar layouts of objects on the ground should have similar representations, regardless of the combination of sensors. CSF is evaluated on two tasks: semantic segmentation and land cover classification, and outperforms the state of the art on both tasks.",
        "Abstract": "In the application of machine learning to remote sensing, labeled data is often scarce or expensive, which impedes the training of powerful models like deep convolutional neural networks. Although unlabeled data is abundant, recent self-supervised learning approaches are ill-suited to the remote sensing domain. In addition, most remote sensing applications currently use only a small subset of the multi-sensor, multi-channel information available, motivating the need for fused multi-sensor representations. We propose a new self-supervised training objective, Contrastive Sensor Fusion, which exploits coterminous data from multiple sources to learn useful representations of every possible combination of those sources. This method uses information common across multiple sensors and bands by training a single model to produce a representation that remains similar when any subset of its input channels is used. Using a dataset of 47 million unlabeled coterminous image triplets, we train an encoder to produce semantically meaningful representations from any possible combination of channels from the input sensors. These representations outperform fully supervised ImageNet weights on a remote sensing classification task and improve as more sensors are fused.",
        "Introduction": "  INTRODUCTION Remote sensing data has become broadly available at the petabyte scale, offering unprecedented visibility into natural and human activity across the Earth. Many techniques have been recently Under review as a conference paper at ICLR 2020 developed for applying this data with machine learning to solve geospatial tasks like semantic segmentation ( Audebert et al., 2016 ), ( Kampffmeyer et al., 2016 ), broad-area search ( Keisler et al., 2019 ), and classification ( Maggiori et al., 2016 ), ( Sherrah, 2016 ). Due to the complexity and visuospatial nature of solving problems with aerial imagery, it is natural to use deep convolutional neural networks, but CNNs typically require large amounts of labeled data to achieve good performance. In remote sensing, these labels are usually scarce and hard to obtain; semantic segmentation requires boundaries to be labeled at single-pixel precision. A modern approach to the problem of data scarcity is semi-supervised learning, which uses unlabeled data to ease the task of learning from small amounts of labeled data. This approach is particularly well-suited to remote sensing because of the amount of unlabeled data available. While most self-supervised and unsupervised image analysis techniques focus on natural imagery, remote sensing differs in several critical ways, requiring a different approach. Where pictures taken by a human photographer often have one or few subjects, remote sensing images like those in  Figure 2  contain numerous objects such as buildings, trees, or factories. Additionally, the important content can change unpredictably within just a few pixels or between images at the same location from different times. Multiple satellite and aerial imaging platforms capture images of the same locations on earth with a wide variety of resolutions, spectral bands (channels), and revisit rates, such that any specific problem can require a different combination of sensor inputs ( Reiche et al., 2018 ;  Rustowicz et al., 2019 ). Recent research in semi-supervised learning has led to a wealth of methods that achieve success on problems like classifying natural images ( van den Oord et al., 2018 ;  Tian et al., 2019 ;  Hénaff et al., 2019 ) and understanding language ( Mikolov et al., 2013 ;  Devlin et al., 2018 ). These approaches almost universally rely on the \"distributional hypothesis\": the property that parts of the data that are close in time or space are similar. Previous work ( Jean et al., 2019 ) has learned representations for overhead imagery using the distributional hypothesis. However, we argue that it is less applicable in remote sensing, due to the aforementioned differences between overhead imagery and these domains. The distributional hypothesis still applies, but the scale of correlated patches is smaller, making it difficult to harvest scenes for related patches. We therefore modify these techniques to build on the intuition that similar layouts of objects on the ground should have similar representations, regardless of the combination of sensors. In this paper, we use this idea to develop a method of learning representations for overhead imagery, which we call Contrastive Sensor Fusion (CSF). We train a single model, unsupervised, to produce a representation of a scene given any subset of the sensors used during training over that scene. During training, we form two \"views\" of each scene from a small random subset of the available sensor channels (channel/sensor dropout). Then both views are encoded with the same network and the resulting representations are compared, using a contrastive loss to encourage the two views to have similar representations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper discusses the vulnerability of copyright detection systems to adversarial attacks and presents a proof of concept attack against real-world copyright detection systems for music. A differentiable implementation of a simple version of the \"Shazam\" algorithm for music fingerprinting is built in TensorFlow and used to create adversarial music that is easily recognizable to a human, while evading detection by a machine. The attack successfully fools industrial systems, including the AudioTag music recognition service and YouTube's Content ID system.",
        "Abstract": "It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks.  These vulnerabilities are especially apparent for neural network based systems.  As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube's Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.",
        "Introduction": "  INTRODUCTION Machine learning systems are easily manipulated by adversarial attacks, in which small perturba- tions to input data cause large changes to the output of a model. Such attacks have been demonstrated on a number of potentially sensitive systems, largely in an idealized academic context, and occasion- ally in the real-world ( Tencent, 2019 ;  Kurakin et al., 2016 ;  Athalye et al., 2017 ;  Eykholt et al., 2017 ;  Yakura & Sakuma, 2018 ;  Qin et al., 2019 ). Copyright detection systems are among the most widely used machine learning systems in industry, and the security of these systems is of foundational importance to some of the largest companies in the world. Despite their importance, copyright systems have gone largely unstudied by the ML se- curity community. Common approaches to copyright detection extract features, called fingerprints, from sampled video or audio, and then match these features with a library of known fingerprints. Examples include YouTube's Content ID, which flags copyrighted material on YouTube and enables copyright owners to monetize and control their content. At the time of writing this paper, more than 100 million dollars have been spent on Content ID, which has resulted in more than 3 billion dollars in revenue for copyright holders ( Manara, 2018 ). Closely related tools such as Google Jigsaw detect and remove videos that promote terrorism or jeopardized national security. There is also a regula- tory push for the use of copyright detection systems; the recent EU Copyright Directive requires any service that allows users to post text, sound, or video to implement a copyright filter. A wide range of copyright detection systems exist, most of which are proprietary. It is not possible to demonstrate attacks against all systems, and this is not our goal. Rather, the purpose of this paper is to discuss why copyright detectors are especially vulnerable to adversarial attacks and establish how existing attacks in the literature can potentially exploit audio and video copyright systems. As a proof of concept, we demonstrate an attack against real-world copyright detection systems for music. To do this, we reinterpret a simple version of the well-known \"Shazam\" algorithm for music fingerprinting as a neural network and build a differentiable implementation of it in TensorFlow ( Abadi et al., 2016 ). By using a gradient-based attack and an objective that is designed to achieve good transferability to black-box models, we create adversarial music that is easily recognizable to a human, while evading detection by a machine. With sufficient perturbations, our adversarial music Under review as a conference paper at ICLR 2020 successfully fools industrial systems, 1 including the AudioTag music recognition service ( AudioTag, 2009 ), and YouTube's Content ID system( Google, 2019 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a new class of test-time attacks in the form of Semantic Adversarial Examples (SAEs). We demonstrate how to generate SAEs that support a rich set of transformations using an inverse graphics framework. We evaluate the generated SAEs on the popular object detector SqueezeDet and show that by correctly choosing the semantic parameters, SAEs degrade performance by 28 percentage points. We also show that by augmenting the dataset using SAEs, we can boost the robustness of SqueezeDet by up to 15 percentage points. Our results indicate a tight coupling between the model architecture and SAE generation.",
        "Abstract": "Machine learning (ML) algorithms, especially deep neural networks, have demonstrated success in several domains. However, several types of attacks have raised concerns about deploying ML in safety-critical domains, such as autonomous driving and security. An attacker perturbs a data point slightly in the pixel space and causes the ML algorithm to misclassify (e.g. a perturbed stop sign is classified as a yield sign). These perturbed data points are called adversarial examples, and there are numerous algorithms in the literature for constructing adversarial examples and defending against them. In this paper we explore semantic adversarial examples (SAEs) where an attacker creates perturbations in the semantic space. For example, an attacker can change the background of the image to be cloudier to cause misclassification. We present an algorithm for constructing SAEs that uses recent advances in differential rendering and inverse graphics. ",
        "Introduction": "  INTRODUCTION Machine learning (ML) techniques, especially Deep Neural Networks (DNNs), have been successful in several domains, such as finance and healthcare. However, several test-time ( Biggio et al., 2013 ;  Szegedy et al., 2014 ;  Goodfellow et al., 2015 ;  Kurakin et al., 2016 ) and training-time ( Jagielski et al., 2018 ;  Shafahi et al., 2018 ) attacks have made their adoption in high-assurance applications, such as autonomous driving and security, problematic. ML techniques, such as generative models, have also been used for nefarious purposes such as generating \"deepfakes\" ( Liu et al., 2017 ;  Zhu et al., 2017 ). Our focus in this paper is on test-time attacks in which an adversary generates a slightly perturbed sample to fool a classifier or an object-detector. Let X be the sample space and Y be the space of labels. A classifier F is a function from X to Y . Given a sample x ∈ X, most attacks for constructing adversarial examples find a perturbation δ with a small norm (typical norms that are used are l ∞ , l 0 , and l 2 ) such that x +δ has a different label than x, i.e. F (x) = F (x +δ). In this paper we consider the problem of generating semantic adversarial examples (SAEs) ( Hosseini & Poovendran, 2018 ;  Joshi et al., 2019 ;  Qiu et al., 2019 ;  Dreossi et al., 2018b ). In these examples, there is a richer set of transformations T that capture semantically-meaningful changes to inputs to the ML model. We assume a norm on T (this norm is induced by various parameters corresponding to the transformations, such as angle of rotation and size of the translation). In our universe, an adversary is given a sample x and wishes to find a transformation parameterized by θ ∈ Θ with small norm such that F (τ (x, θ)) = F (x) (we consider untargeted attacks, but our ideas extend to targeted attacks as well) 1 . SAEs can also be viewed as outcomes of perturbations in a \"rich\" semantic feature space (e.g.,, texture of the image) rather than just the concrete feature space (e.g.,, pixels). Consequently, SAEs are physically realizable, and it is easy to understand how the changes in semantics results in an adversarial example. SAEs have been considered in the literature ( Xiao et al., 2018 ;  Dreossi et al., 2018b ;  Huang et al., 2019 ), but prior works typically consider a small set of fixed transformations (e.g., rotation and translation, or modifying a single object's texture). Our goal is to flexibly support a 1 τ is defined in § 3.2. Under review as a conference paper at ICLR 2020 richer set of transformations implemented in a state of the art renderer (e.g., changing the background of the image, weather conditions, or the time of day), as we specify in detail in § 2. There is evidence that SAEs can help with domain adaptation ( Volpi et al., 2018 ) or making the control loop more robust ( Dreossi et al., 2018b ), further motivating our approach. We also plan to open source our code to advance research in this space. To summarize, the main contributions of this paper are the following: • We present a new class of test-time attacks in the form of SAEs. We demonstrate how to generate SAEs that support a rich set of transformations (refer § 3) using an inverse graphics framework (refer § 2). Specifically, we show how one can systematically take techniques to perform attacks in the pixel space such as FGSM ( Goodfellow et al., 2015 ) and PGD ( Madry et al., 2017 ) and transform them to their semantic counterparts. • We evaluate the generated SAEs on the popular object detector SqueezeDet ( Wu et al., 2016 ). By correctly choosing the semantic parameters, SAEs degrade performance (char- acterized by the mean average precision or mAP) by 28 percentage points (refer § 5.1). • We show that by augmenting the dataset using SAEs, we can boost the robustness of SqueezeDet (characterized by mAP) by up to 15 percentage points (refer § 5.2). • We show that SAEs generated using SqueezeDet do not transfer to YOLOv3 ( Redmon & Farhadi, 2018 ), indicating a tight coupling between the model architecture and SAE generation (refer § 5.3). • While augmentation with SAEs improves robustness against SAEs, augmentation using traditional pixel-based perturbations does not produce the same effect (refer § A.1).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Simplicial Complex Networks (SCNs), a novel class of restricted neural networks that preserve the universal approximation property and can be trained using a forward pass and the backpropagation algorithm. SCNs do not require an activation function and architecture search in the way that conventional neural networks do, and their hidden units are conceptually well defined. This paper discusses the theoretical background of SCNs, provides an explanation of their training procedure, and presents experiments to demonstrate their effectiveness. The results of this paper suggest that SCNs can be used in the future for developing deep models that are interpretable, and robust to perturbations.",
        "Abstract": "Universal approximation property of neural networks is one of the motivations to use these models in various real-world problems. However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Another characteristic which makes these models interesting is that they can be trained with the backpropagation algorithm which allows an efficient gradient computation and gives these universal approximators the ability to efficiently learn complex manifolds from a large amount of data in different domains. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, we leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework. We introduce the notion of automatic subdivisioning and devise a particular type of neural networks for regression tasks: Simplicial Complex Networks (SCNs). SCN's architecture is defined with a set of bias functions along with a particular policy during the forward pass which alternates the common architecture search framework in neural networks. We believe the view of SCNs can be used as a step towards building interpretable deep learning models. Finally, we verify its performance on a set of regression problems.",
        "Introduction": "  INTRODUCTION It is well-known that under mild assumptions on the activation function, a neural network with one hidden layer and a finite number of neurons can approximate continuous functions. This characteristic of neural networks is generally referred to as the universal approximation property. There are various theoretical universal approximators. For example, a result of the  Stone-Weierstrass theorem Stone (1948) ;  Cotter (1990)  is that multivariate polynomials are dense in the space of continuous real valued functions defined over a hypercube. Another example is that the reproducing kernel Hilbert space (RKHS) associated with kernel functions with particular properties can be dense in the same space of functions. Kernel functions with this property are called universal kernels  Micchelli et al. (2006) . A subsequent result of this theory is that the set of functions generated by a Gaussian process regression with an appropriate kernel can approximate any continuous function over a hypercube with arbitrary precision. Although multivariate polynomials and Gaussian processes also have this approximation property, each has practical limitations that cause neural networks to be used more often in practice compared to these approaches. For instance, polynomial interpolations may result a model that overfits to the data and suffers from a poor generalization, and Gaussian processes often become computationally intractable for a large number of training data Bernardo et al.. Neural networks, with an efficient structure for gradient computation using backpropagation, can be trained using gradient based optimization for large datasets in a tractable time. Moreover, in contrast to existing polynomial interpolations, neural networks generalize well in practice. Theoretical and empirical understanding of the generalization power of neural networks is an ongoing research  Novak et al. (2018) ;  Neyshabur et al. (2017) . Topological Data Analysis (TDA), a geometric approach for data analysis, is a growing field which provides statistical and algorithmic methods to analyze the topological structures of data often referred to as point clouds. TDA methods mainly relied on deterministic methods until recently where Under review as a conference paper at ICLR 2020 statistical approaches were proposed for this purpose  Carriere et al. (2017) ;  Chazal & Michel (2017) . In general, TDA methods assume a point cloud in a metric space with an inducing distance (e.g. Euclidean, Hausdorff, or Wasserstein distance) between samples and build a topological structure upon point clouds. The topological structure is then used to extract geometric information from data  Chazal & Michel (2017) . These models are not trained with gradient based approaches and they are generally limited to predetermined algorithms whose application to high dimensional spaces may be challenging  Chazal (2016) . In this work, by leveraging geometrical perspective of TDA, we provide a class of restricted neural networks that preserve the universal approximation property and can be trained using a forward pass and the backpropagation algorithm. Motivated by the approximation theorem used to develop our method, Simplicial Complex Network (SCN) is chosen to refer these models. SCNs do not require an activation function and architecture search in the way that conventional neural networks do. Their hidden units are conceptually well defined, in contrast to feed-forward neural networks for which the role of a hidden unit is yet an ongoing problem. SCNs are discussed in more details in later sections. Our contribution can be summarized in building a novel class of neural networks which we believe can be used in the future for developing deep models that are interpretable, and robust to perturbations. The rest of this paper is organized as follows: Section 2 is specified for the explanation of SCNs and their training procedure. In section 3, related works are explained. Sections 4, 5, and 6 are specified to experiments, limitations, and conclusion.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the effects of compressing BERT, a pre-trained feature extractor, on its ability to transfer to downstream tasks. We use magnitude weight pruning to compress English BERT and observe the results on the General Language Understanding Evaluation (GLUE) benchmark. We find that low levels of pruning do not affect transfer to downstream tasks, while medium levels of pruning increase pre-training loss and prevent useful pre-training information from being transferred. We also observe that fine-tuning BERT on a specific task does not improve its prunability. Our findings suggest that BERT can be pruned prior to distribution without affecting its universality, and that BERT may be over-pruned during pre-training for a reasonable accuracy trade-off for certain tasks.",
        "Abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.",
        "Introduction": "  INTRODUCTION Pre-trained feature extractors, such as BERT ( Devlin et al., 2018 ) for natural language processing and VGG ( Simonyan & Zisserman, 2014 ) for computer vision, have become effective methods for improving the performance of deep learning models. In the last year, models similar to BERT have become state-of-the-art in many NLP tasks, including natural language inference (NLI), named entity recognition (NER), sentiment analysis, etc. These models follow a pre-training paradigm: they are trained on a large amount of unlabeled text via a task that resembles language modeling ( Yang et al., 2019 ;  Chan et al., 2019 ) and are then fine-tuned on a smaller amount of downstream data, which is labeled for a specific task. Pre-trained models usually achieve higher accuracy than any model trained on downstream data alone. The pre-training paradigm, while effective, still has some problems. While some claim that lan- guage model pre-training is a \"universal language learning task\" ( Radford et al., 2019 ), there is no theoretical justification for this, only empirical evidence. Second, due to the size of the pre-training dataset, BERT models tend to be slow and require impractically large amounts of GPU memory. BERT-Large can only be used with access to a Google TPU, and BERT-Base requires some opti- mization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware ( Sohoni et al., 2019 ). Training BERT-Base from scratch costs ∼$7k and emits ∼1438 pounds of CO 2 ( Strubell et al., 2019 ). Model compression ( Bucila et al., 2006 ), which attempts to shrink a model without losing accuracy, is a viable approach to decreasing GPU usage. It might also be used to trade accuracy for mem- ory in some low-resource cases, such as deploying to smartphones for real-time prediction. The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible? To explore these questions, we compressed English BERT using magnitude weight pruning ( Han et al., 2015 ) and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark ( Wang et al., 2019 ), a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude Under review as a conference paper at ICLR 2020 weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. This infor- mation is not equally useful to each task; tasks degrade linearly with pre-train loss, but at different rates. High levels of pruning, depending on the size of the downstream dataset, may additionally degrade performance by preventing models from fitting downstream datasets. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount. To our knowledge, prior work had not shown whether BERT could be compressed in a task-generic way, keeping the benefits of pre-training while avoiding costly experimentation associated with compressing and re-training BERT multiple times. Nor had it shown whether BERT could be over- pruned for a memory / accuracy trade-off for deployment to low-resource devices. In this work, we conclude that BERT can be pruned prior to distribution without affecting it's universality, and that BERT may be over-pruned during pre-training for a reasonable accuracy trade-off for certain tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to unsupervised learning of intuitive physics from raw videos. We propose a model that can learn to predict events from sparse sensory inputs, explore an environment to determine which parts of it can be navigated safely, and infer parameters of a new scenario by observing a few physical experiments. We evaluate our model on simulated scenarios and show that it can generalize spatially, temporally, and to a variable number of objects. We also investigate the problem of learning the parameters of new scenarios on the fly via experiences and propose an effective solution.",
        "Abstract": "We consider the problem of learning models of intuitive physics from raw, unlabelled visual input. Differently from prior work, in addition to learning general physical principles, we are also interested in learning ``on the fly'' physical properties specific to new environments, based on a small number of environment-specific experiences. We do all this in an unsupervised manner, using a meta-learning formulation where the goal is to predict videos containing demonstrations of physical phenomena, such as objects moving and colliding with a complex background. We introduce the idea of summarizing past experiences in a very compact manner, in our case using dynamic images, and show that this can be used to solve the problem well and efficiently. Empirically, we show, via extensive experiments and ablation studies, that our model learns to perform physical predictions that generalize well in time and space, as well as to a variable number of interacting physical objects.",
        "Introduction": "  INTRODUCTION Many animals possess an intuitive understanding of the physical world. They use this understanding to accurately and rapidly predict events from sparse sensory inputs. In addition to general physical principles, many animals also learn specific models of new environments as they experience them over time. For example, they can explore an environment to determine which parts of it can be navigated safely and remember this knowledge for later reuse. Authors have looked at equipping artificial intelligences (AIs) with analogous capabilities, but focusing mostly on performing predictions from instantaneous observations of an environment, such as a few frames in a video. However, such predictions can be successful only if observations are combined with sufficient prior knowledge about the environment. For example, consider predicting the motion of a bouncing ball. Unless key parameters such as the ball's elasticity are known a priori, it is impossible to predict the ball's trajectory accurately. However, after observing at least one bounce, it is possible to infer some of the parameters and eventually perform much better predictions. In this paper, we are interested in learning intuitive physics in an entirely unsupervised manner, by passively watching videos. We consider situations in which objects interact with scenarios that can only be partially inferred from their appearance, but that also contain objects whose parameters cannot be confidently predicted from appearance alone ( fig. 1 ). Then, we consider learning a system that can observe a few physical experiments to infer such parameters, and use this knowledge to perform better predictions in the future. Our model has three goals. First, it must learn without the use of any external or ad-hoc supervision. We achieve this by training our model from raw videos, using a video prediction error as a loss. Second, our model must be able to extract information about a new scenario by observing a few experiments, which we formulate as meta-learning. We also propose a simple representation of the experiments based on the concept of \"dynamic image\" that allows to process long experiments more efficiently than using a conventional recurrent network. Third, our model must learn a good representation of physics without access to any explicit or external supervision. Instead, we propose three tests to support this hypothesis. (i) We show that the model can predict far in the future, which is a proxy to temporal invariance. (ii) We further show that the model can extend to scenarios that are geometrically much larger than the ones used for training, which is a proxy to spatial invariance. (iii) Finally, we show that the model can generalize to several Under review as a conference paper at ICLR 2020 (A) (U) (B) what happens next? moving objects, which is a proxy to locality. Locality and time-space invariance are of course three key properties of physical laws and thus we should expect any good intuitive model of physics to possess them. In order to support these claims, we conduct extensive experiments in simulated scenarios, including testing the ability of the model to cope with non-trivial visual variations of the inputs. While the data is simpler than a real-world application, we nevertheless make substantial progress compared to previous work, as discussed in section 2. We do so by learning from passive, raw video data a good model of dynamics and collisions that generalizes well spatially, temporally, and to a variable number of objects. The scalability of our approach, via the use of the dynamic image, is also unique. Finally, we investigate the problem of learning the parameters of new scenarios on the fly via experiences and we propose an effective solution to do so.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel and efficient representation learning model to address the issue of data sparsity in text classification tasks. The model focuses on relevant words in the input while being able to generalise to semantically similar concepts. Two techniques are introduced to reflect this intuition: soft matching probabilities between each word and multiple vectors which represent semantic concepts, and an error boosting technique that exploits rationales. Results across a consistent number of baselines and three datasets indicate a significant improvement in performance, and the model is also interpretable.",
        "Abstract": "We propose a model to tackle classification tasks in the presence of very little training data. To this aim, we introduce a novel matching mechanism to focus on elements of the input by using vectors that represent semantically meaningful concepts for the task at hand.\nBy leveraging highlighted portions of the training data, a simple, yet effective, error boosting technique guides the learning process. In practice, it increases the error associated to relevant parts of the input by a given factor. Results on text classification tasks confirm the benefits of the proposed approach in both balanced and unbalanced cases, thus being of practical use when labeling new examples is expensive. In addition, the model is interpretable, as it allows for human inspection of the learned weights.",
        "Introduction": "  INTRODUCTION Gathering and labeling data is a task that can be expensive in terms of time, human effort and resources. When we cannot rely on already available datasets, training a model with acceptable performance on few data points annotated by few annotators, becomes critical in many practical applications. This is, indeed, especially important when the data is naturally imbalanced and the demands of gathering samples of the minority class are high. One important domain in which these issues arise is text classification, for example hate-speech ( Waseem & Hovy, 2016 ), web spam ( Castillo et al., 2007 ) and abuse detection ( Mishra et al., 2018 ). One effective approach to overcome the lack of training data is that of  Zaidan et al. (2007) , which consists of augmenting the few data available with rationales, i.e., highlighted portions of the input. Rationales are usually coupled with feature-engineering to be effective in low resource scenarios. An alternative way to deal with data sparsity, especially in text classification tasks, is to use pre- trained language models (LMs) that are fine-tuned on a target domain. While this approach has been tested on hundreds of training points ( Devlin et al., 2018 ;  Howard & Ruder, 2018 ), it is not clear how it behaves in an even scarcer setting, as the vast parameter space of an LM might pose a problem. Moreover, fine-tuning a model may require a considerable amount of computing power, therefore restricting its applicability. On the other hand, some embedding-based models represent the input as a weighted average of words ( Kalchbrenner et al., 2014 ;  Sheikh et al., 2016 ), where the weight is given by a parame- ter called \"reference vector\". However, these models cannot easily incorporate multiple reference vectors, and they are not interpretable since classification works on unreadable embedding features. In this paper, we propose a novel and efficient representation learning model to address the above issues. The underlying idea is to focus on relevant words in the input while being able to generalise to semantically similar concepts; this is something akin to what a human would do in the presence of data scarcity. We therefore introduce two techniques that should coexist to reflect our intuition. First of all, the model to focuses on specific words by computing soft matching probabilities between each word and multiple vectors which represent semantic concepts. Secondly, we guide the learning process to learn important concepts thanks to an error boosting technique that exploits rationales. Basically, it encourages the model to reduce the overall error by improving the prediction associated with highlighted words. Additionally, by direct inspection of model weights it is possible to understand what words it focuses on; in short, the model is interpretable 1 . Results across a consistent number of baselines and three Under review as a conference paper at ICLR 2020 datasets also indicate a significant improvement in performance. Interestingly, we always outper- form fine-tuned models when little training data is available. Our model can also assist users to train a classifier for a very specific task. As an example, consider training an abstract filtering system with rationales provided by the user itself. The model will then learn to filter out papers that are not matching the user's preferences. The rest of the paper is structured as follows: Section 2 provides an overview of the existing litera- ture, highlighting similar and different ideas; Section 3 formally introduces the problem as well as our model, providing intuition behind our architectural choices; Section 4 details our experiments and shows our findings, with a thorough ablation study that disentangles the contribution of each part of the model and a use case on interpretability; finally, Section 5 summarizes our work.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: Bitcoin is a virtual currency system that enables users to make secure, unique transactions without the need for a central trusted authority. It has become the most widely used cryptocurrency, with a market capitalization of approximately $177.8 billion. Bitcoin transactions consist of two sets: a set of source addresses and a set of destination addresses. While bitcoin address data is publicly available, it is not straightforward to analyze address transaction data since it is not aggregated in one block/place.",
        "Abstract": "Bitcoin is a virtual coinage system that enables users to trade virtually free of a central trusted authority. All transactions on the Bitcoin blockchain are publicly available for viewing, yet as Bitcoin is built mainly for security it’s original structure does not allow for direct analysis of address transactions. \nExisting analysis methods of the Bitcoin blockchain can be complicated, computationally expensive or inaccurate. We propose a computationally efficient model to analyze bitcoin blockchain addresses and allow for their use with existing machine learning algorithms. We compare our approach against Multi Level Sequence Learners (MLSLs), one of the best performing models on bitcoin address data.",
        "Introduction": "  INTRODUCTION Bitcoin(Nakamoto) is a virtual coinage system that functions much like a standard currency, enabling users to provide virtual payment for goods and services free of a central trusted authority. Bitcoin relies on the transmission of digital information, utilizing cryptographic methods to ensure secure, unique transactions. Individuals and businesses transact with the coin electronically on a peer- to-peer network utilizing a shared transaction ledger (the Blockchain). It caught wide attention beginning in 2011, and various altcoins a general name for all other cryptocurrencies post-Bitcoin soon appeared It has placed itself as the most widespread and commonly used cryptocurrency with no signs of slow- ing down( Chan et al., 2017 ). Representing over 81% of the total market of cryptocurrencies(coi), Its market capitalization is estimated to be approximately $177.8 Billioncoi accounting for about 90% of the total market capitalization of Virtual Currencies(Houben & Snyers). Bitcoin uses public key cryptography to generate secure addresses for users where each address is a public key, and use of the bitcoins stored in it requires signing with a private key. These address identifiers are used by their owners to hold bitcoin pseudonymously. A typical Bitcoin transaction consists of two sets: a set of source addresses and a set of destination addresses. Coins in the source addresses are collected and then sent in differing amounts to the destination addresses. (Houben & Snyers) While bitcoin address data is publicly available, it is not straightforward to analyze address transac- tion data since it is not aggregated in one block/place.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a comprehensive analysis of cross-lingual embeddings, focusing on the comparison between mapping and joint learning methods. We demonstrate that joint learning is more effective in mitigating the issues of cross-lingual hubness and structural non-isometry, and yields better performance on cross-lingual benchmarks. We also propose a novel joint learning algorithm which is able to learn cross-lingual embeddings from parallel data.",
        "Abstract": "Recent advances in cross-lingual word embeddings have primarily relied on mapping-based methods, which project pretrained word embeddings from different languages into a shared space through a linear transformation. However, these approaches assume word embedding spaces are isomorphic between different languages, which has been shown not to  hold in practice (Søgaard et al., 2018), and fundamentally limits their performance. This motivates investigating joint learning methods which can overcome this impediment, by simultaneously learning embeddings across languages via a cross-lingual term in the training objective. Given the abundance of parallel data available (Tiedemann, 2012), we propose a bilingual extension of the CBOW method which leverages sentence-aligned corpora to obtain robust cross-lingual word and sentence representations. Our approach significantly improves cross-lingual sentence retrieval performance over all other approaches, as well as convincingly outscores mapping methods while maintaining parity with jointly trained methods on word-translation. It also achieves parity with a deep RNN method on a zero-shot cross-lingual document classification task, requiring far fewer computational resources for training and inference. As an additional advantage, our bilingual method also improves the quality of monolingual word vectors despite training on much smaller datasets.  We make our code and models publicly available.\n",
        "Introduction": "  INTRODUCTION Cross-lingual representations-such as embeddings of words and phrases into a single comparable feature space-have become a key technique in multilingual natural language processing. They of- fer strong promise towards the goal of a joint understanding of concepts across languages, as well as for enabling the transfer of knowledge and machine learning models between different languages. Therefore, cross-lingual embeddings can serve a variety of downstream tasks such as bilingual lex- icon induction, cross-lingual information retrieval, machine translation and many applications of zero-shot transfer learning, which is particularly impactful from resource-rich to low-resource lan- guages. Existing methods can be broadly classified into two groups ( Ruder et al., 2017 ): mapping meth- ods leverage existing monolingual embeddings which are treated as independent, and apply a post- process step to map the embeddings of each language into a shared space, through a linear transfor- mation ( Mikolov et al., 2013b ;  Conneau et al., 2017 ;  Joulin et al., 2018 ). On the other hand, joint methods learn representations concurrently for multiple languages, by combining monolingual and cross-lingual training tasks ( Luong et al., 2015 ;  Coulmance et al., 2015 ;  Gouws et al., 2015 ;  Vulic & Moens, 2015 ;  Chandar et al., 2014 ;  Hermann & Blunsom, 2013 ). While recent work on word embeddings has focused almost exclusively on mapping methods, which require little to no cross-lingual supervision, ( Søgaard et al., 2018 ) establish that their performance is hindered by linguistic and domain divergences in general, and for distant language pairs in particular. Principally, their analysis shows that cross-lingual hubness, where a few words (hubs) in the source language are nearest cross-lingual neighbours of many words in the target language, and structural non-isometry between embeddings do impose a fundamental barrier to the performance of linear mapping methods. ( Ormazabal et al., 2019 ) propose using joint learning as a means of mitigating these issues. Given parallel data, such as sentences, a joint model learns to predict either the word or context in both Under review as a conference paper at ICLR 2020 source and target languages. As we will demonstrate with results from our algorithm, joint methods yield compatible embeddings which are closer to isomorphic, less sensitive to hubness, and perform better on cross-lingual benchmarks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to global minimization of a black-box objective function, only accessible through a zeroth-order oracle. The proposed approach uses flexible distributions generated by bijective Generative Neural Networks (GNNs) to overcome the limitations of classical parametric search distributions. A tailored algorithm is designed to efficiently train GNNs for an Evolutionary Strategies (ES) objective. Experiments on a variety of objective functions show that this extension can significantly accelerate ES algorithms.",
        "Abstract": "Evolutionary Strategies (ES) are a popular family of black-box zeroth-order optimization algorithms which rely on search distributions to efficiently optimize a large variety of objective functions. This paper investigates the potential benefits of using highly flexible search distributions in ES algorithms, in contrast to standard ones (typically Gaussians). We model such distributions with Generative Neural Networks (GNNs) and introduce a new ES algorithm that leverages their expressiveness to accelerate the stochastic search. Because it acts as a plug-in, our approach allows to augment virtually any standard ES algorithm with flexible search distributions. We demonstrate the empirical advantages of this method on a diversity of objective functions.",
        "Introduction": "  INTRODUCTION We are interested in the global minimization of a black-box objective function, only accessible through a zeroth-order oracle. In many instances of this problem the objective is expensive to evaluate, which excludes brute force methods as a reasonable mean of optimization. Also, as the objective is potentially non-convex and multi-modal, its global optimization cannot be done greedily but requires a careful balance between exploitation and exploration of the optimization landscape (the surface defined by the objective). The family of algorithms used to tackle such a problem is usually dictated by the cost of one evalua- tion of the objective function (or equivalently, by the maximum number of function evaluations that are reasonable to make) and by a precision requirement. For instance, Bayesian Optimization (Jones et al., 1998; Shahriari et al., 2016) targets problems of very high evaluation cost, where the global minimum must be approximately discovered after a few hundreds of function evaluations. When aiming for a higher precision and hence having a larger budget (e.g. thousands of function eval- uations), a popular algorithm class is the one of Evolutionary Strategies (ES) (Rechenberg, 1978; Schwefel, 1977), a family of heuristic search procedures. ES algorithms rely on a search distribution, which role is to propose queries of potentially small value of the objective function. This search distribution is almost always chosen to be a multivariate Gaussian. It is namely the case of the Covariance Matrix Adaptation Evolution Strategies (CMA-ES) (Hansen & Ostermeier, 2001), a state-of-the-art ES algorithm made popular in the machine learn- ing community by its good results on hyper-parameter tuning (Friedrichs & Igel, 2005; Loshchilov & Hutter, 2016). It is also the case for Natural Evolution Strategies (NES) (Wierstra et al., 2008) algorithms, which were recently used for direct policy search in Reinforcement Learning (RL) and shown to compete with state-of-the-art MDP-based RL techniques (Salimans et al., 2017). Occa- sionally, other distributions have been used; e.g. fat-tails distributions like the Cauchy were shown to outperform the Gaussian for highly multi-modal objectives (Schaul et al., 2011). We argue in this paper that in ES algorithms, the choice of a standard parametric search distribu- tion (Gaussian, Cauchy, ..) constitutes a potentially harmful implicit constraint for the stochastic search of a global minimum. To overcome the limitations of classical parametric search distribu- tions, we propose using flexible distributions generated by bijective Generative Neural Networks (GNNs), with computable and differentiable log-probabilities. We discuss why common existing optimization methods in ES algorithms cannot be directly used to train such models and design a tailored algorithm that efficiently train GNNs for an ES objective. We show how this new algo- rithm can readily incorporate existing ES algorithms that operates on simple search distributions, Under review as a conference paper at ICLR 2020 (Update) Update π t to produce x of potentially smaller objective values. until convergence; like the Gaussian. On a variety of objective functions, we show that this extension can significantly accelerate ES algorithms. We formally introduce the problem and provide background on Evolutionary Strategies in Section 2. We discuss the role of GNNs in generating flexible search distributions in Section 3. We explain why usual algorithms fail to train GNNs for an ES objective and introduce a new algorithm in Section 4. Finally we report experimental results in Section 5.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper focuses on the problem of generalized zero-shot learning (GZSL) in the fine-grained scenario of paired images and their respective text descriptions. We explore ways of leveraging the co-occurrence of image and text to enable zero-shot flexibility in a wider array of high-level tasks such as segmentation or conditional image generation. We present two themes of contributions to this research direction.",
        "Abstract": "We address the problem of learning fine-grained cross-modal representations. We propose an instance-based deep metric learning approach in joint visual and textual space. The key novelty of this paper is that it shows that using per-image semantic supervision leads to substantial improvement in zero-shot performance over using class-only supervision. On top of that, we provide a probabilistic justification for a metric rescaling approach that solves a very common problem in the generalized zero-shot learning setting, i.e., classifying test images from unseen classes as one of the classes seen during training. We evaluate our approach on two fine-grained zero-shot learning datasets: CUB and FLOWERS. We find that on the generalized zero-shot classification task CLAREL consistently outperforms the existing approaches on both datasets.",
        "Introduction": "  INTRODUCTION Deep learning-based approaches have demonstrated superior flexibility and generalization capabilities in information processing on a wide variety of tasks, such as vision, speech and language ( LeCun et al., 2015 ). However, it has been widely realized that the transfer of deep representations to real-world applications is challenging due to the typical reliance on massive hand-labeled datasets. Learning in the low-labeled data regime, especially in the zero-shot ( Wang et al., 2019 ) and the few-shot ( Wang & Yao, 2019 ) setups, have recently received significant attention in the literature. In the problem of zero-shot learning (ZSL), the objective is to recognize categories that have not been seen during the training ( Larochelle et al., 2008 ). This is typically done by relying on anchor embeddings learned in one modality as prototypes and by associating a query embedding from the other modality with the closest prototype. In the generalized ZSL (GZSL) case ( Xian et al., 2018c ), the objective is more challenging as recognition is performed in the joint space of seen and unseen categories. ZSL, as well as its generalized counterpart, provide a viable framework to learn cross-modal representations that are flexible and adaptive. For example, in this paradigm, the adaptation to a new classification task based on text/image representation space alignment could be as easy as defining/appending/modifying a set of text sentences to define classes of new classifiers. This is an especially relevant problem as machine learning is challenged with the long tail of classes, and the idea of learning from pairs of images and sentences, abundant on the web, looks like a natural solution. Therefore, in this paper we specifically target the fine-grained scenario of paired images and their respective text descriptions. The uniqueness of this scenario is in the fact that the co-occurance of image and text provides a rich source of information. The ways of leveraging this source have not been sufficiently explored in the context of GZSL. Although we focus exclusively on the GZSL recognition setup in this paper, we believe that the research in this direction has potential to enable zero-shot flexibility in a wider array of high-level tasks such as segmentation or conditional image generation ( Zhang et al., 2018 ). The contributions of this work can be characterized under the following two themes.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a systematic study of the trade-off between performance and computational resources for Natural Language Processing (NLP) systems using hyperdimensional computing-based embedding. Experiments on a benchmark dataset for intent classification show that it is possible to reduce memory usage by ∼ 10x and speed-up training by ∼ 5x without compromising the F 1 score. The results of this study demonstrate the usefulness of hyperdimensional computing-based embedding for high-throughput, resource-constrained, and green computing systems.",
        "Abstract": "Recent advances in Deep Learning have led to a significant performance increase on several NLP tasks, however, the models become more and more computationally demanding. Therefore, this paper tackles the domain of computationally efficient algorithms for NLP tasks. In particular, it investigates distributed representations of n-gram statistics of texts. The representations are formed using hyperdimensional computing enabled embedding. These representations then serve as features, which are used as input to standard classifiers. We investigate the applicability of the embedding on one large and three small standard datasets for classification tasks using nine classifiers.  The embedding achieved on par F1 scores while decreasing the time and memory requirements by several times compared to the conventional n-gram statistics, e.g., for one of the classifiers on a small dataset, the memory reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84 times, respectively. For many classifiers on the large dataset, the memory reduction was about 100 times and train and test speed-ups were over 100 times. More importantly, the usage of distributed representations formed via hyperdimensional computing allows dissecting the strict dependency between the dimensionality of the representation and the parameters of n-gram statistics, thus, opening a room for tradeoffs.",
        "Introduction": "  INTRODUCTION Recent work ( Strubell et al., 2019 ) has brought significant attention by demonstrating potential cost and environmental impact of developing and training state-of-the-art models for Natural Language Processing (NLP) tasks. The work suggested several countermeasures for changing the situation. One of them recommends a concerted effort by industry and academia to promote research of more computationally efficient algorithms. The main focus of this paper falls precisely in this domain. In particular, we consider NLP systems using a well-known technique called n-gram statistics. The key idea is that hyperdimensional computing ( Kanerva, 2009 ) allows forming distributed repre- sentations of the conventional n-gram statistics ( Joshi et al., 2016 ). The use of these distributed representations, in turn, allows trading-off the performance of an NLP system (e.g., F 1 score) and its computational resources (i.e., time and memory). The main contribution of this paper is the systematic study of these tradeoffs on nine machine learning algorithms using several benchmark classification datasets. We demonstrate the usefulness of hyperdimensional computing-based em- bedding, which is highly time and memory efficient. Our experiments on a well-known dataset ( Braun et al., 2017 ) for intent classification show that it is possible to reduce memory usage by ∼ 10x and speed-up training by ∼ 5x without compromising the F 1 score. Several important use-cases are motivating the efforts towards trading-off the performance of a system against compu- tational resources required to achieve that performance: high-throughput systems with an extremely large number of requests/transactions (the power of one per cent); resource-constrained systems where computational resources and energy are scarce (edge computing); green computing systems taking into account the aspects of environmental sustainability when considering the efficiency of algorithms ( AI HLEG, 2019 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the concept of adversarial examples and their security threats, and discusses the state-of-the-art defense against them, adversarial training. It is noted that while adversarial training leads to robust models for the employed threat model used during training, the obtained robustness does not translate to other threat models. The paper also discusses the trade-off between robustness and accuracy in adversarial training.",
        "Abstract": "Adversarial training is the standard to train models robust against adversarial examples. However, especially for complex datasets, adversarial training incurs a significant loss in accuracy and is known to generalize poorly to stronger attacks, e.g., larger perturbations or other threat models. In this paper, we introduce  confidence-calibrated adversarial training (CCAT) where the key idea is to enforce that the confidence on adversarial examples decays with their distance to the attacked examples. We show that CCAT preserves better the accuracy of normal training while robustness against adversarial examples is achieved via confidence thresholding. Most importantly, in strong contrast to adversarial training, the robustness of CCAT generalizes to larger perturbations and other threat models, not encountered during training. We also discuss our extensive work to design strong adaptive attacks against CCAT and standard adversarial training which is of independent interest. We present experimental results on MNIST, SVHN and Cifar10.",
        "Introduction": "  INTRODUCTION Deep neural networks have shown tremendous improvements in various learning tasks including ap- plications in computer vision, natural language processing or text processing. However, the discov- ery of adversarial examples, i.e., nearly imperceptibly perturbed inputs that cause mis-classification, has revealed severe security threats, as demonstrated by attacking popular computer vision services such as Google Cloud Vision (Ilyas et al., 2018a) or Clarifai (Liu et al., 2016; Bhagoji et al., 2017b). As the number of safety- and privacy-critical applications is increasing, e.g., autonomous driving or medical imaging, this problem becomes even more important. In practice, adversarial training and its variants (Szegedy et al., 2013; Goodfellow et al., 2014; Madry et al., 2018), i.e., training on adversarial examples, can be regarded as the state-of-the-art to obtain models robust against adversarial examples. In contrast to many other defenses, and to the best of our knowledge, adversarial training has not been broken so far. However, adversarial training is known to increase test error significantly. Only on simple datasets such as MNIST (LeCun et al., 1998), adversarial training is able to preserve accuracy. This observation is typically described as a trade-off between robustness and accuracy (Tsipras et al., 2018; Stutz et al., 2019; Raghunathan et al., 2019; Zhang et al., 2019). Furthermore, while adversarial training leads to robust models for the employed threat model used during training, the obtained robustness does not translate to other threat models, e.g., other L p -balls (Sharma & Chen, 2017; Song et al., 2018; Madry et al., 2018; Tramèr & Boneh, 2019; Li et al., 2019a; Kang et al., 2019), larger perturbations than the ones used during training or distal adversarial examples (Hein et al., 2019).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Sparse Weight Activation Training (SWAT), a technique for reducing computations and memory consumption during training of convolutional neural networks (CNNs). SWAT uses a Top-K operation on the output activation gradients to sparsify the backward pass, and can train a broad range of deep CNNs with minimal accuracy loss on complex datasets like CIFAR10, CIFAR100, and ImageNet. SWAT reduces the total number of operations during training by 50%-80%, and achieves 23%-37% activation and 50%-80% weight footprint reduction during the backward pass. SWAT also learns sparse weights, allowing for pruning of the model during inference without sacrificing accuracy. Empirical studies are performed to provide insight into why SWAT performs well.",
        "Abstract": "Training convolutional neural networks (CNNs) is time consuming. Prior work has explored how to reduce the computational demands of training by eliminating gradients with relatively small magnitude. We show that eliminating small magnitude components has limited impact on the direction of high-dimensional vectors. However, in the context of training a CNN, we find that eliminating small magnitude components of weight and activation vectors allows us to train deeper networks on more complex datasets versus eliminating small magnitude components of gradients. We propose Sparse Weight Activation Training (SWAT), an algorithm that embodies these observations. SWAT reduces computations by 50% to 80% with better accuracy at a given level of sparsity versus the Dynamic Sparse Graph algorithm. SWAT also reduces memory footprint by 23% to 37% for activations and 50% to 80% for weights.",
        "Introduction": "  INTRODUCTION The usage of convolutional neural networks (CNNs) has dominated a wide variety of complex com- puter vision tasks, such as object recognition (Krizhevsky et al., 2012; Szegedy et al., 2015), object detection (Szegedy et al., 2013; Ren et al., 2015), and image restoration (Dong et al., 2014; Zhang et al., 2017). However, CNNs are compute and memory intensive; even a moderately sized CNN model, like ResNet-50 with tens of millions of parameters, requires billions of floating-point opera- tions and consumes tens of gigabytes to store weights and activations during training. Previous works propose techniques for reducing computations and memory consumption during CNN training. Such techniques include quantization where every operation is quantized in low- precision during training such a (Zhou et al., 2016; Choi et al., 2018; Wu et al., 2016; Wang et al., 2018), or, use fixed-point integers instead of floating-point numbers (Wu et al., 2018; Das et al., 2018). An orthogonal approach to reduce computations is sparsification, a process in which we eliminate computations involving small values. meProp (Sun et al., 2017; Wei et al., 2017) sparsifies back- propagating by selecting a subset of output gradients in each layer. Using only the top 5% of the gradients (ranked by magnitude), meProp can train a CNN and MLP on MNIST dataset without accuracy loss. The computational flow of meProp is shown in Figure 1a and 1b. meProp does not modify the forward pass. In the backward pass meProp performs a \"Top-K\" operation on the output activation gradients which sets components not ranked in the Top-K by magnitude to zero. It then uses the sparsified output activation gradients to (potentially more efficiently) compute the input activation and weight gradients. Our experiments suggest meProp fails to converge on larger networks and datasets. Recently, Liu et al. (2019) proposed a method of reducing computation during training and infer- ence by constructing a dynamic sparse graph (DSG) using random projection for dimensionality reduction. DSG loses accuracy on ImageNet dataset. In this work, we propose an alternative technique, Sparse Weight Activation Training (SWAT), that can train deep CNNs on complex data sets like ImageNet. Compared to DSG, SWAT is a straight- forward technique which uses less expensive Top-K operation, inspired by meProp, while achieving better accuracy than DSG on ImageNet. This paper provides the following contributions: Under review as a conference paper at ICLR 2020 • It shows that dropping gradients during back-propagation is harmful to network conver- gence especially when training a deeper model on a complex dataset. In this case the model suffers high accuracy loss. • It proposes SWAT, a sparse training algorithm that can train a broad range of deep CNNs with minimal accuracy loss on complex datasets like CIFAR10, CIFAR100, and ImageNet. SWAT reduces the total number of operations during training by 50%-80%. It also achieves 23%-37% activation and 50%-80% weight footprint reduction during the backward pass. • SWAT algorithm uses sparse weight both in the forward and backward passes, and therefore model learns sparse weights, i.e., a pruned architecture; If the model has been trained using SWAT with S% sparsity during training, then during inference, weight can be pruned to S% without sacrificing any loss in accuracy. • We perform empirical studies to provide insight into why 'SWAT performs well; we showed that Top-K sparsification in general preserves direction in high-dimensional space.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a new type of invertible model, Continuous Normalizing Flow (CNF), which employs ordinary differential equations (ODEs) to transform between the latent variables and the data. We propose a simple and efficient conditioning approach for CNF, InfoCNF, which partitions the latent code into two separate parts: class-specific supervised code and unsupervised code which is shared between different classes. We demonstrate the effectiveness of InfoCNF on a range of tasks, including image classification, image generation, and semi-supervised learning, and show that InfoCNF outperforms existing methods on image classification and semi-supervised learning tasks.",
        "Abstract": "Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. However, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the high-dimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, we propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs),  InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. We show empirically that InfoCNF improves the test accuracy over the baseline  while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance. ",
        "Introduction": "  INTRODUCTION Invertible models are attractive modelling choice in a range of downstream tasks that require accurate densities including anomaly detection ( Bishop, 1994 ; Chandola et al., 2009) and model-based reinforcement learning ( Polydoros & Nalpantidis, 2017 ). These models enable exact latent-variable inference and likelihood estimation. A popular class of invertible models is the flow-based generative models ( Dinh et al., 2017 ;  Rezende & Mohamed, 2015 ;  Kingma & Dhariwal, 2018 ;  Grathwohl et al., 2018 ) that employ a change of variables to transform a simple distribution into more complicated ones while preserving the invertibility and exact likelihood estimation. However, computing the likelihood in flow-based models is expensive and usually requires restrictive constraints on the architecture in order to reduce the cost of computation. Recently, ( Chen et al., 2018 ) introduced a new type of invertible model, named the Continuous Normalizing Flow (CNF), which employs ordinary differential equations (ODEs) to transform between the latent variables and the data. The use of continuous-time transformations in CNF, instead of the discrete ones, together with efficient numerical methods such as the Hutchinson's trace estimator ( Hutchinson, 1989 ), helps reduce the cost of determinant computation from O(d 3 ) to O(d), where d is the latent dimension. This improvement opens up opportunities to scale up invertible models to complex tasks on larger datasets where invertibility and exact inference have advantages. Until recently, CNF has mostly been trained using unlabeled data. In order to take full advantage of the available labeled data, a conditioning method for CNF - which models the conditional likelihood, as well as the posterior, of the data and the labels - is needed. Existing approaches for conditioning flow-based models can be utilized, but we find that these methods often do not work well on CNF. This drawback is because popular conditioning methods for flow-based models, such as in ( Kingma & Dhariwal, 2018 ), make use of the latent code for conditioning and introduce independent parameters for different class categories. However, in CNF, for invertibility, the dimension of the latent code needs to be the same as the dimension of the input data and therefore is substantial, which results in many unnecessary parameters. These additional but redundant parameters increase the complexity of the model and hinder learning efficiency. Such overparametrization also has a negative impact on other flow-based generative models, as was pointed out by ( Liu et al., 2019 ), but is especially bad in the case of CNF. This is because the ODE solvers in CNF are sensitive to the complexity of the Under review as a conference paper at ICLR 2020 model, and the number of function evaluations that the ODE solvers request in a single forward pass (NFEs) increases significantly as the complexity of the model increases, thereby slowing down the training. This growing NFEs issue has been observed in unconditional CNF but to a much lesser extent ( Grathwohl et al., 2018 ). It poses a unique challenge to scale up CNF and its conditioned variants for real-world tasks and data. Our contributions in this paper are as follows: Contribution 1: We propose a simple and efficient conditioning approach for CNF, namely InfoCNF. Our method shares the high-level intuition with the InfoGAN ( Chen et al., 2016 ), thus the eponym. In InfoCNF, we partition the latent code into two separate parts: class-specific supervised code and unsupervised code which is shared between different classes (see  Figure 1 ). We use the supervised code to condition the model on the given supervised signal while the unsupervised code captures other latent variations in the data since it is trained using all label categories. The supervised code is also used for classification, thereby reducing the size of the classifier and facilitating the learning. Splitting the latent code into unsupervised and supervised parts allows the model to separate the learning of the task-relevant features and the learning of other features that help fit the data. We later show that the cross-entropy loss used to train InfoCNF corresponds to the mutual information between the generated image and codes in InfoGAN, which encourages the model to learn disentangled representations.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a deep architecture that performs similarity-based representation learning, clustering of the data, and generation of data from each specific data mode. The model is evaluated on synthetic data, MNIST, and real-world biological data, and is shown to have superior clustering performance and efficiency for data generation purposes compared to the state-of-the-art.",
        "Abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standing problem and has been studied extensively. Recently, Deep Clustering gained popularity due to the non-linearity of neural networks, which allows for flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model. The model can learn multi-modal distributions of high-dimensional data and use these to generate realistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder (VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts. Additionally, we encourage the latent representation of our model to follow a Gaussian mixture distribution and to accurately represent the similarities between the data points. We assess the performance of our model on synthetic data, the MNIST benchmark data set, and a challenging real-world task of defining cell subpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets. MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to the baselines and we show that the MoE architecture in the decoder reduces the computational cost of sampling specific data modes with high fidelity.",
        "Introduction": "  INTRODUCTION Clustering has been studied extensively ( Aljalbout et al., 2018 ;  Min et al., 2018 ) in machine learning. Recently, many Deep Clustering approaches were proposed, which modified (Variational) Autoen- coder ((V)AE) architectures ( Min et al., 2018 ;  Zhang et al., 2017 ) or with varying regularization of the latent representation ( Dizaji et al., 2017 ;  Jiang et al., 2017 ;  Yang et al., 2017 ;  Fortuin et al., 2019 ). Reconstruction error usually drives the definition of the latent representation learned from an AE or VAE. The representation for AE models is unconstrained and typically places data objects close to each other according to an implicit similarity measure that also yields favorable reconstruction error. In contrast, VAE models regularize the latent representation such that the represented inputs follow a certain variational distribution. This construction enables sampling from the latent representation and data generation via the decoder of a VAE. Typically, the variational distribution is assumed standard Gaussian, but for example,  Jiang et al. (2017)  introduced a mixture of Gaussian variational distribution for clustering purposes. A key component of clustering approaches is the choice of similarity metric for the considered data objects which we try to group ( Irani et al., 2016 ). Such similarity metrics are either defined a priori or learned from the data to specifically solve classification tasks via a Siamese network architecture ( Chopra et al., 2005 ). Dimensionality reduction approaches, such as UMAP ( McInnes et al., 2018 ) or t-SNE ( van der Maaten & Hinton, 2008 ), allow to specify a similarity metric for projection and thereby define the data separation in the inferred latent representation. In this work, we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim- VAE), a new deep architecture that performs similarity-based representation learning, clustering of the data and generation of data from each specific data mode. Due to a combined loss function, Under review as a conference paper at ICLR 2020 it can be jointly optimized. We assess the scope of the model on synthetic data and we present superior clustering performance on MNIST. Moreover, in an ablation study, we show the efficiency and precision of MoE-Sim-VAE for data generation purposes in comparison to the most related state-of-the-art method ( Jiang et al., 2017 ). Finally, we show an application of MoE-Sim-VAE on a real-world clustering problem in biology on multiple datasets. • Embed the Mixture-of-Expert architecture into a Variational Autoencoder setup to train a separate generator for each data mode • Show superior clustering performance of the model on benchmark dataset and real-world biological data",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to improve the stability of actor-critic methods in deep reinforcement learning. Through a connection between deep RL and tabular RL, a hierarchical training procedure is proposed to ensure stability guarantees, i.e., the stationary distribution of the policies is shown to be stable. Empirical results across a range of continuous control tasks in MuJoCo demonstrate improved behavior in terms of stability and overall performance.",
        "Abstract": "In recent years, advances in deep learning have enabled the application of reinforcement learning algorithms in complex domains. However, they lack the theoretical guarantees which are present in the tabular setting and suffer from many stability and reproducibility problems \\citep{henderson2018deep}. In this work, we suggest a simple approach for improving stability and providing probabilistic performance guarantees in off-policy actor-critic deep reinforcement learning regimes. Experiments on continuous action spaces, in the MuJoCo control suite, show that our proposed method reduces the variance of the process and improves the overall performance.",
        "Introduction": "  INTRODUCTION Reinforcement Learning (RL) is a dynamical learning paradigm, in which the algorithm (also known as the 'agent') learns through sequential interaction with the environment. On each round, the agent performs an action, which transitions it into a new state, and is provided with a state-dependent reward. The goal of the agent is to maximize the cumulative reward it observes throughout these interactions with the environment. When considering tabular RL, in which there is a finite number of states and actions, there exist efficient algorithms with theoretical convergence and stability guar- antees [ 29 ;  13 ;  14 ;  34 ;  5 ;  4 ]. However, this is not the case when considering deep RL approaches - when a neural network is used to cope with large state spaces, such as images [ 19 ;  11 ], and/or large action spaces, for example in continuous control [ 17 ;  27 ]. The issue with deep RL is twofold: (i) the optimization process is finite, and as such does not follow many of the requirements for the process to converge, such as decaying learning rates [ 2 ], and (ii) the non-linearity of the function approximators (e.g., neural networks) results in a highly non-convex optimization landscape. Due to this, as can be seen in all empirical works [ 22 ;  26 ;  17 ;  6 ;  9 ], the learning process exhibits high variance; during a short time of training, a near-optimal policy may become arbitrarily bad. As a result, in recent years, the stability of RL algorithms has become a major concern of the research community [ 10 ]. Instead of tackling the stability problems in the optimization process, many previous works focused on different aspects of the loss function [ 6 ;  9 ;  30 ]. Then, they usually apply a standard variant of gradient descent algorithm on the modified loss function. While these approaches are capable of finding relatively good policies, they are strictly inferior when compared to their tabular counterparts - the tabular approaches ensure convergence and stability, whereas the practical variants exhibit instability and high variance. In this work, we suggest a novel approach that improves the stability of actor-critic methods. Specif- ically, we draw the connection between deep RL approaches and their tabular counterparts and high- light the issues inherent in deep RL algorithms. Through this connection, we propose a solution, in the form of a hierarchical training procedure. As opposed to the tabular case, our approach is not ensured to converge, but rather has stability guarantees, i.e., the stationary distribution of the policies is shown to be stable, meaning that with high probability the performance of the policy improves and does not suffer high degradation. We show, empirically across a range of continuous control tasks in MuJoCo, that our approach indeed results in improved behavior both in terms of stability and overall performance.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the use of linear embeddings for high-dimensional Bayesian optimization (HDBO). We provide new results that identify why linear embeddings have performed poorly in HDBO, and construct a representation with better properties for BO. We improve modelability by deriving a Mahalanobis kernel tailored for linear embeddings and adding polytope bounds to the embedding, and show how to maintain a high probability that the embedding contains an optimum. We demonstrate that using this representation for BO outperforms a wide range of previous approaches for HDBO, including on test functions up to D = 1000, and on real-world problems, such as gait optimization of a multi-legged robot.",
        "Abstract": "Bayesian optimization (BO) is a popular approach to optimize resource-intensive black-box functions.\nA significant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efficiency.\nA solution considered in previous literature is to embed the high-dimensional parameter space into a lower-dimensional manifold, often a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embeddings for BO. We thoroughly study and analyze the consequences of using linear embeddings and show that some of the design choices in current approaches adversely impact their performance. Based on this new theoretical understanding we propose ALEBO, a new algorithm for high-dimensional BO via linear embeddings that outperforms state-of-the-art methods on a range of problems.",
        "Introduction": "  INTRODUCTION Bayesian optimization (BO) is a robust, sample-efficient technique for optimizing expensive-to- evaluate black-box functions (Mockus, 1989; Jones, 2001). BO has been successfully applied to diverse applications, ranging from automated machine learning (Snoek et al., 2012; Hutter et al., 2011) to robotics (Lizotte et al., 2007; Calandra et al., 2015; Rai et al., 2018). One of the most active topics of research in BO is how to extend current methods to higher-dimensional spaces. A common framework to tackle this problem is to consider a high-dimensional BO (HDBO) task as a standard BO problem in a low-dimensional embedding, where the embedding can be either linear (typically a random projection) or nonlinear (e.g. via a multi-layer neural network); see Sec. 2 for a full review. An advantage of this framework is to explicitly decouple the problem of finding low-dimensional representations suitable for optimization from the actual optimization technique. In this paper we study the use of linear embeddings for HDBO, and in particular we re-examine prior efforts to use random linear projections. Random projections are attractive for BO because, by the Johnson-Lindenstrauss lemma, they can be approximately distance-preserving (Johnson & Lindenstrauss, 1984) without requiring any data to learn the embedding. Random embeddings come with several strong theoretical guarantees, but have shown mixed empirical performance for HDBO. The contributions of this paper are: 1) We provide new results that identify why linear embeddings have performed poorly in HDBO. We show that existing approaches produce representations that cannot be well-modeled by a Gaussian process (GP), or representations that likely do not contain an optimum (Sec. 4). 2) We construct a representation with better properties for BO (Sec. 5): we improve modelability by deriving a Mahalanobis kernel tailored for linear embeddings and adding polytope bounds to the embedding, and we show how to maintain a high probability that the em- bedding contains an optimum. 3) We show that using this representation for BO outperforms a wide range of previous approaches for HDBO, including on test functions up to D = 1000, and on real- world problems, such as gait optimization of a multi-legged robot (Sec. 6). These include the first results for HDBO with black-box constraints.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a meta-learning approach to deep neural network initializations for image semantic segmentation. We analyze the robustness of the representations that the model has meta-learned to perturbations in the hyperparameters of the update routine, and show that a fixed update routine does hinder generalization performance. We also provide empirical justification for our meta-learning approach for up to 400 densely labeled examples, and show that our meta-learned initializations are competitive with ImageNet trained initializations for up to 400 labeled examples. We will release our code and the FP-k dataset upon acceptance.",
        "Abstract": "While meta-learning approaches that utilize neural network representations have made progress in few-shot image classification, reinforcement learning, and, more recently, image semantic segmentation, the training algorithms and model architectures have become increasingly specialized to the few-shot domain. A natural question that arises is how to develop learning systems that scale from few-shot to many-shot settings while yielding human level performance in both. One scalable potential approach that does not require ensembling many models nor the computational costs of relation networks, is to meta-learn an initialization. In this work, we study first-order meta-learning of initializations for deep neural networks that must produce dense, structured predictions given an arbitrary amount of train- ing data for a new task. Our primary contributions include (1), an extension and experimental analysis of first-order model agnostic meta-learning algorithms (including FOMAML and Reptile) to image segmentation, (2) a formalization of the generalization error of episodic meta-learning algorithms, which we leverage to decrease error on unseen tasks, (3) a novel neural network architecture built for parameter efficiency which we call EfficientLab, and (4) an empirical study of how meta-learned initializations compare to ImageNet initializations as the training set size increases. We show that meta-learned initializations for image segmentation smoothly transition from canonical few-shot learning problems to larger datasets, outperforming random and ImageNet-trained initializations. Finally, we show both theoretically and empirically that a key limitation of MAML-type algorithms is that when adapting to new tasks, a single update procedure is used that is not conditioned on the data. We find that our network, with an empirically estimated optimal update procedure yields state of the art results on the FSS-1000 dataset, while only requiring one forward pass through a single model at evaluation time.",
        "Introduction": "  INTRODUCTION Humans have a remarkable capability to not only learn new concepts from a small number of labeled examples but also to gain expertise as more data becomes available. In recent years, there has been substantial progress in high accuracy image segmentation in the high data regime (see  Liu et al. (2019)  and their references). While meta-learning approaches that utilize neural network representations have made progress in few-shot image classification, reinforcement learning, and, more recently, image semantic segmentation, the training algorithms and model architectures have become increasingly specialized to the low data regime. A desirable property of a learning system is one that effectively applies knowledge gained from a few or many examples, while reducing the generalization gap when trained on little data and not being encumbered by its own learning routines when there are many examples. This property is desirable because training and maintaining multiple models is more cumbersome than training and maintaining one model. A natural question that arises is how to develop learning systems that scale from few-shot to many-shot settings while yielding competitive accuracy in both. One scalable potential approach that does not require ensembling many models nor the computational costs of relation networks, is to meta-learn an initialization. In this work, we specifically address the problem of meta-learning initializations for deep neural net- works that must produce dense, structured output, such as for the semantic segmentation of images. We ask the following questions: Under review as a conference paper at ICLR 2020 1. Do first-order MAML-type algorithms extend to the higher dimensional parameter spaces, dense prediction, and skewed distributions required of semantic segmentation? 2. How robust are the representations that the model has meta-learned to perturbations in the hyperparameters of the update routine? 3. Are MAML-type algorithms hindered by having a fixed update policy for training and testing tasks that is not conditioned on the available labeled examples for a new task? 4. What is the number of labeled examples beyond which a conventional approach to training deep neural networks outperforms the meta-learned initializations? To address the fourth question, we put together a small benchmark dataset, which we call FP-k, that contains 400 training examples for 5 tasks each. In recent works ( Li et al., 2017 ;  Shaban et al., 2017 ;  Rusu et al., 2018 ;  Zhang et al., 2019 ;  Lee et al., 2019 ), few-shot learning approaches have become increasingly complex and appear to be specialized to the few-shot domain. This raises many open questions, such as: What is the accuracy of a few-shot learning system when more labeled examples become available? After a certain number of labeled examples, will the few-shot learning system have the same accuracy as a simpler training approach such as conventional training via SGD? If so, what is the number of labeled examples beyond which a conventional approach to training deep neural networks outperforms a meta-learning system? We address these questions in 5.4, providing empirical justification for our meta-learning approach for up to 400 densely labeled examples. Through a series of theoretical and empirical analyses, we shed new light on the representations that model agnostic meta-learning algorithms learn and how they adapt to unseen tasks. In summary, we address the above research questions as follows: We show that MAML-type algorithms do extend to few shot image segmentation, yielding state of the art results when their update routine is optimized after meta-training and when the model is regularized. Addressing question 2, we find that the meta-learned initialization's performance when being evaluated on a task is particularly sensitive to changes in the update routine's hyperparameters (see  Figure 2 ). We show theoretically in 3.2 and empirically in our results (see  Table 2 ) that a fixed update routine does hinder generalization performance. Finally, we address question 4 by showing that our meta-learned initializations are competitive with ImageNet ( Deng et al., 2009 ) trained initializations for up to 400 labeled examples. We will release our code and the FP-k dataset upon acceptance.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel model, called AST-LSTM, for programming language processing. AST-LSTM is a tree-structured recurrent neural network that takes a sequence of small ASTs as input and captures both explicit parent-child-sibling relations and implicit code dependencies among code elements. Experiments on two programming language processing tasks show that AST-LSTM outperforms existing models.\n\nAbstract: This paper presents a novel model, AST-LSTM, for programming language processing. AST-LSTM is a tree-structured recurrent neural network that takes a sequence of small ASTs as input and captures both explicit parent-child-sibling relations and implicit code dependencies among code elements. Experiments on two programming language processing tasks show that AST-LSTM outperforms existing models.",
        "Abstract": "Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.",
        "Introduction": "  INTRODUCTION Understanding program code is a fundamental step for many software engineering tasks. Software developers often spend more than 50% of their time in navigating through existing code bases and understanding the code before they can implement new features or fix bugs ( Xia et al., 2018 ;  Evans Data Corporation, 2019 ;  Britton et al., 2012 ). If suitable models for programs are built, they can be useful for many tasks, such as classifying the functionality of programs ( Nix & Zhang, 2017 ;  Dahl et al., 2013 ;  Pascanu et al., 2015 ;  Rastogi et al., 2013 ), predicting bugs ( Yang et al., 2015 ;  Li et al., 2017 ;  2018 ), and providing bases for program translation ( Nguyen et al., 2017 ;  Gu et al., 2017 ). Different from natural language texts, programming languages have clearly defined grammars and compilable source code must follow rigid syntactical structures and can be unambiguously parsed into syntax trees. There can be complex control flows and data flows among various code elements all over a program that affect the semantic and functionality of the program. Some inter-dependent code elements can appear in an arbitrary order in the program (e.g., a function A calls another function B while A and B are spatially far away from each other); some code elements, such as local variable names, have no significant impact on code functionality. In the literature, tree-based convolutional neural networks (TBCNNs) have been proposed ( Peng et al., 2015 ;  Mou et al., 2016 ) to show promising results in programming language processing. TBC- NNs accept Abstract Syntax Trees (ASTs) of source code as the input, and capture explicit, struc- tural parent-child-sibling relations among code elements. Gated graph neural networks (GGNNs) ( Li et al., 2016 ) are also proposed as a way to learn graphs, and ASTs are extended to graphs with a variety of code dependencies added as edges among tree nodes to model code semantics ( Allamanis et al., 2018b ). While GGNNs capture more code semantics than TBCNNs, many additional edges among tree nodes have to be added through program analysis techniques, and many of the edges may be noise, contributing longer training time and lower performance. A recent model, known as ASTNN, based on a sequence of small ASTs for statements (instead of one big AST for the whole program) shows better performance than TBCNNs and GGNNs ( Zhang et al., 2019 ).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the trade-off between how well a student studies and how close the test questions are to the exercise questions. It proposes a distribution measurement to select features that do not vary from one domain to another, and links performance directly to the distance between train and test distribution.",
        "Abstract": "Learning theory tells us that more data is better when minimizing the generalization error of identically distributed training and test sets. However, when training and test distribution differ, this distribution shift can have a significant effect. With a novel perspective on function transfer learning, we are able to lower bound the change of performance when transferring from training to test set with the Wasserstein distance between the embedded training and test set distribution. We find that there is a trade-off affecting performance between how invariant a function is to changes in training and test distribution and how large this shift in distribution is. Empirically across several data domains, we substantiate this viewpoint by showing that test performance correlates strongly with the distance in data distributions between training and test set. Complementary to the popular belief that more data is always better, our results highlight the utility of also choosing a training data distribution that is close to the test data distribution when the learned function is not invariant to such changes.",
        "Introduction": "  INTRODUCTION Imagine there are two students who are studying for an exam. Student A studies by diligently learning the class material by heart. Student B studies by learning the underlying reasons for why things are the way they are. Come test day, student A is only able to answer test questions that are very similar to the class material while student B has no trouble answering different looking questions that follow the same reasoning. Distilled from this example, we note there is a trade-off between how \"well\" a student studied, i.e., how indifferent the student is to receiving exercise or test questions, and how close the test questions are to the exercise questions. While most machine learning work studies the generalization error, i.e., the error when testing on different samples from the same distribution, we do not take the match of train and test distribution as given. In fact, it appears that the distance between train and test distribution may be critical for successful \"generalization\". Following a similar line of thought,  Uguroglu & Carbonell (2011)  devised a distribution measurement to select only features that do not vary from one domain to another. In contrast, we are interested in linking performance directly to the distance between train and test distribution.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel learning paradigm called Variational Imitation Learning with Diverse-Quality Demonstrations (VILD) for sequential decision making. VILD models the level of demonstrators' expertise via a probabilistic graphical model and learns it along with a reward function that represents an intention of expert's decision making. Experiments on continuous-control benchmarks and real-world crowdsourced demonstrations demonstrate that VILD is robust against diverse-quality demonstrations and outperforms existing methods significantly. Additionally, VILD with importance sampling is data-efficient, since it learns the policy using a less number of transition samples.",
        "Abstract": "The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL paradigm called Variational Imitation Learning with Diverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive estimation approach is not suitable to large state and action spaces, and fix this issue by using a variational approach that can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",
        "Introduction": "  INTRODUCTION The goal of sequential decision making is to learn a policy that makes good decisions ( Puterman, 1994 ). As an important branch of sequential decision making, imitation learning (IL) ( Schaal, 1999 ) aims to learn such a policy from demonstrations (i.e., sequences of decisions) collected from experts. However, high-quality demonstrations can be difficult to obtain in reality, since such experts may not always be available and sometimes are too costly ( Osa et al., 2018 ). This is especially true when the quality of decisions depends on specific domain-knowledge not typically available to amateurs; e.g., in applications such as robot control ( Osa et al., 2018 ) and autonomous driving ( Silver et al., 2012 ). In practice, demonstrations are often diverse in quality, since it is cheaper to collect demonstrations from mixed demonstrators, containing both experts and amateurs (Audiffren et al., 2015). Unfortu- nately, IL in such settings tends to perform poorly, since low-quality demonstrations often negatively affect the performance of IL methods ( Shiarlis et al., 2016 ). For example, amateurs' demonstrations for robotics can be cheaply collected via a robot simulation ( Mandlekar et al., 2018 ), but such demonstrations may cause damages to the robot which is catastrophic in the real-world ( Shiarlis et al., 2016 ). Similarly, demonstrations for autonomous driving can be collected from drivers in public roads ( Fridman et al., 2017 ), which may contain traffic-accident demonstrations. Learning a self-driving car from these low-quality demonstrations may cause traffic accidents. When the level of demonstrators' expertise is known, multi-modal IL (MM-IL) can be used to learn a good policy with diverse-quality demonstrations ( Li et al., 2017 ;  Hausman et al., 2017 ;  Wang et al., 2017 ). Specifically, MM-IL aims to learn a multi-modal policy, where each mode of the policy represents the decision making of each demonstrator. When knowing the level of demonstrators' expertise, good policies can be obtained by selecting modes that correspond to the decision making of high-expertise demonstrators. However, in practice, it is difficult to truly determine the level of demonstrators' expertise beforehand. Without knowing the level of expertise, it is difficult to distinguish the decision making of experts and amateurs, and learning a good policy is challenging. To overcome the issue of MM-IL, pioneer works have proposed to estimate the quality of each demonstration using auxiliary information from experts (Audiffren et al., 2015;  Wu et al., 2019 ;  Brown et al., 2019 ). Specifically,  Audiffren et al. (2015)  inferred the demonstration quality using Under review as a conference paper at ICLR 2020 similarities between diverse-quality demonstrations and high-quality demonstrations, where the latter are collected in a small number from experts. In contrast,  Wu et al. (2019)  proposed to estimate the demonstration quality using a small number of demonstrations with confidence scores. Namely, the score value given by an expert is proportion to the demonstration quality. Similarly, the demonstration quality can be estimated by ranked demonstrations, where ranking from an expert is evaluated due to the relative quality ( Brown et al., 2019 ). To sum up, these methods rely on auxiliary information from experts, namely high-quality demonstrations, confidence scores, and ranking. In practice, these pieces of information can be scarce or noisy, which leads to a poor performance of these methods. In this paper, we consider a novel but realistic setting of IL where only diverse-quality demonstrations are available. Meanwhile, the level of demonstrators' expertise and auxiliary information from experts are fully absent. To tackle this challenging setting, we propose a new learning paradigm called variational imitation learning with diverse-quality demonstrations (VILD). The central idea of VILD is to model the level of demonstrators' expertise via a probabilistic graphical model, and learn it along with a reward function that represents an intention of expert's decision making. To scale up our model for large state and action spaces, we leverage the variational approach ( Jordan et al., 1999 ), which can be implemented using reinforcement learning (RL) for flexibility ( Sutton & Barto, 1998 ). To further improve data-efficiency of VILD when learning the reward function, we utilize importance sampling (IS) to re-weight a sampling distribution according to the estimated level of demonstrators' expertise. Experiments on continuous-control benchmarks and real-world crowdsourced demonstrations ( Mandlekar et al., 2018 ) denote that: 1) VILD is robust against diverse-quality demonstrations and outperforms existing methods significantly. 2) VILD with IS is data-efficient, since it learns the policy using a less number of transition samples.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: End-to-end neural network-based approaches have seen significant progress in recent years in the synthesis of realistic human speech. Text-to-speech (TTS) is an underdetermined problem, meaning the same text input has an infinite number of reasonable spoken realizations. Recent end-to-end TTS research has aimed to model and/or directly control the remaining variability in the output. Skerry-Ryan et al. (2018) and Lee & Kim (2019) have proposed methods to address the pitch range problem, but do not work for transfer between unrelated sentences. This paper presents a novel approach to address these issues.",
        "Abstract": "Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.",
        "Introduction": "  INTRODUCTION The synthesis of realistic human speech is a challenging problem that is important for natural human- computer interaction. End-to-end neural network-based approaches have seen significant progress in recent years (Wang et al., 2017;  Taigman et al., 2018 ;  Ping et al., 2018 ;  Sotelo et al., 2017 ), even matching human performance for short assistant-like utterances ( Shen et al., 2018 ). However, these neural models are sometimes viewed as less interpretable or controllable than more traditional models composed of multiple stages of processing that each operate on reified linguistic or phonetic representations. Text-to-speech (TTS) is an underdetermined problem, meaning the same text input has an infinite number of reasonable spoken realizations. In addition to speaker and channel characteristics, im- portant sources of variability in TTS include intonation, stress, and rhythm (collectively referred to as prosody). These attributes convey linguistic, semantic, and emotional meaning beyond what is present in the lexical representation (i.e., the text) ( Wagner & Watson, 2010 ). Recent end-to-end TTS research has aimed to model and/or directly control the remaining variability in the output.  Skerry-Ryan et al. (2018)  augment a Tacotron-like model (Wang et al., 2017) with a deterministic encoder that projects reference speech into a learned embedding space. The system can be used for prosody transfer between speakers (\"say it like this\"), but does not work for transfer between unrelated sentences, and does not preserve the pitch range of the target speaker.  Lee & Kim (2019)  partially address the pitch range problem by centering the learned embeddings using speaker-wise means.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes Group-Connected Multiplayer Perceptron (GMLP) networks, a method for end-to-end learning of expressive feature combinations. GMLP leverages feature groups limiting network connections to local group-wise connections and builds a feature hierarchy via merging groups as the network grows in depth. Experiments on five different real-world datasets demonstrate the effectiveness of GMLP compared to state-of-the-art methods in the literature. Ablation studies and visualizations on MNIST and synthesized data are also conducted to better understand the suggested architecture.",
        "Abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.",
        "Introduction": "  INTRODUCTION Deep neural networks have been quite successful across various machine learning tasks. However, this advancement has been mostly limited to certain domains. For example in image and voice data, one can leverage domain properties such as location invariance, scale invariance, coherence, etc. via using convolutional layers ( Goodfellow et al., 2016 ). Alternatively, for graph data, graph convolutional networks were suggested to leverage adjacency patterns present in datasets structured as a graph ( Kipf & Welling, 2016 ;  Xu et al., 2019 ). However, there has been little progress in learning deep representations for datasets that do not follow a particular known structure in the feature domain. Take for instance the case of a simple tabular dataset for disease diagnosis. Such a dataset may consist of features from different categories such as demographics (e.g., age, gender, income, etc.), examinations (e.g., blood pressure, lab results, etc.), and other clinical conditions. In this scenario, the lack of any known structure between features to be used as a prior would lead to the use of a fully-connected multilayer perceptron network (MLP). Nonetheless, it has been known in the literature that MLP architectures, due to their huge complexity, do not usually admit efficient training and generalization for networks of more than a few layers. In this paper, we propose Group-Connected Multiplayer Perceptron (GMLP) networks. The main idea behind GMLP is to learn and leverage expressive feature subsets, henceforth referred to as feature groups. A feature group is defined as a subset of features that provides a meaningful representation or high-level concept that would help the downstream task. For instance, in the disease diagnosis example, the combination of a certain blood factor and age might be the indicator of a higher level clinical condition which would help the final classification task. Furthermore, GMLP leverages feature groups limiting network connections to local group-wise connections and builds a feature hierarchy via merging groups as the network grows in depth. GMLP can be seen as an architecture that learns expressive feature combinations and leverages them via group-wise operations. The main contributions of this paper are as follows: (i) proposing a method for end-to-end learning of expressive feature combinations, (ii) suggesting a network architecture to utilize feature groups and Under review as a conference paper at ICLR 2020 local connections to build deep representations, (iii) conducting extensive experiments demonstrating the effectiveness of GMLP as well as visualizations and ablation studies for better understanding of the suggested architecture. We evaluated the proposed method on five different real-world datasets in various application domains and demonstrated the effectiveness of GMLP compared to state-of-the-art methods in the literature. Furthermore, we conducted ablation studies and comparisons to study different architectural and training factors as well as visualizations on MNIST and synthesized data. To help to reproduce the results and encouraging future studies on group-connected architectures, we made the source code related to this paper available online 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the potential of unsupervised pre-training of deep neural networks for learning rich representations from large unlabeled datasets. We discuss the successes of previous approaches such as word2vec, sequence to sequence autoencoders, language modeling, OpenAI GPT and BERT. We also discuss the challenges and open problems in the field of unsupervised representation learning.",
        "Abstract": "Learning rich representations from predictive learning without labels has been a longstanding challenge in the field of machine learning. Generative pre-training has so far not been as successful as contrastive methods in modeling representations of raw images. In this paper, we propose a neural architecture for self-supervised representation learning on raw images called the PatchFormer which learns to model spatial dependencies across patches in a raw image. Our method learns to model the conditional probability distribution of missing patches given the context of surrounding patches. We evaluate the utility of the learned representations by fine-tuning the pre-trained model on low data-regime classification tasks. Specifically, we benchmark our model on semi-supervised ImageNet classification which has become a popular benchmark recently for semi-supervised and self-supervised learning methods. Our model is able to achieve 30.3% and 65.5% top-1 accuracies when trained only using 1% and 10% of the labels on ImageNet showing the promise for generative pre-training methods.",
        "Introduction": "  INTRODUCTION Deep neural networks are capable of learning rich abstract representations from raw high dimen- sional data in an end-to-end fashion ( LeCun et al., 2015 ). A big weakness of these neural networks is the reliance on abundant labeled datasets. Self-supervised and unsupervised representation learn- ing approaches have been proposed to address this problem ( Bengio et al., 2007 ). It is still an open problem in the field to figure out how to take advantage of large unlabeled datasets, use them for learning rich representations and improving the data-efficiency of supervised learning systems. A classic example of successful unsupervised learning of rich representations is word2vec ( Mikolov et al., 2013 ) where the authors showed that distributed vector representations of words could be learned by contrastively predicting the neighboring words given surrounding words. The shift from word embeddings to sequence embeddings in recent times began when ( Dai & Le, 2015 ) showed that pre-trained sequence to sequence autoencoders on text corpora could be useful for a number of downstream tasks such as text classification and sentiment analysis. Followed by this, it was shown in ( Peters et al., 2018 ) that language modeling is useful in providing deep contextual sentence em- beddings that could be fine-tuned on a number of natural language understanding tasks. ( Howard & Ruder, 2018 ) is another example of such a success. In more recent times, the transformer ( Vaswani et al., 2017 ) has emerged as a powerful architecture to model complex dependencies across a long sequence using global self-attention. OpenAI Generative Pre-Training (GPT) ( Radford et al., 2018 ) showed that training large Transformer models on BooksCorpus could lead to rich and useful rep- resentations that could be fine-tuned on a variety of downstream tasks covering language under- standing, commonsense reasoning and question-answering. The biggest success in unsupervised pre-training was achieved by BERT ( Devlin et al., 2018 ) where the assumption for using causal lan- guage modeling was pointed out as unnecessary and it was shown that training deep transformers in a bi-directional fashion to perform the objective of masked language modeling and next sentence prediction could lead to rich and useful representations covering a wide span of natural language understanding downstream tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the problem of detecting out-of-distribution data in machine learning models. It provides error rate bounds on the goodness-of-fit test based on testing for typicality with respect to the base distribution, and proves theorems stating that the typical sets of any two distributions having sufficiently high KL divergence must have low intersection. The paper also proposes an ensemble method for conservative goodness-of-fit testing, and demonstrates on synthetic data sets that the typical set of standard learned distributions and the ground-truth distribution often have low intersection.",
        "Abstract": "Good methods of performing anomaly detection on high-dimensional data sets are\nneeded, since algorithms which are trained on data are only expected to perform\nwell on data that is similar to the training data. There are theoretical results on the\nability to detect if a population of data is likely to come from a known base distribution, \nwhich is known as the goodness-of-fit problem, but those results require\nknowing a model of the base distribution. The ability to correctly reject anomalous\ndata hinges on the accuracy of the model of the base distribution. For high dimensional \ndata, learning an accurate-enough model of the base distribution such that\nanomaly detection works reliably is very challenging, as many researchers have\nnoted in recent years. Existing methods for the goodness-of-fit problem do not ac-\ncount for the fact that a model of the base distribution is learned. To address that\ngap, we offer a theoretically motivated approach to account for the density learning \nprocedure. In particular, we propose training an ensemble of density models,\nconsidering data to be anomalous if the data is anomalous with respect to any\nmember of the ensemble. We provide a theoretical justification for this approach,\nproving first that a test on typicality is a valid approach to the goodness-of-fit\nproblem, and then proving that for a correctly constructed ensemble of models,\nthe intersection of typical sets of the models lies in the interior of the typical set\nof the base distribution. We present our method in the context of an example on\nsynthetic data in which the effects we consider can easily be seen.",
        "Introduction": "  INTRODUCTION Machine learning models are inherently non-robust to distributional shift, and at no fault of the model necessarily. There is no reason to expect that models should perform well on data that is dissimilar to the data on which they were trained. Interestingly, despite the fact that researchers and practitioners have been able to train models that perform exceptionally well on a variety of challenging tasks, we are still bad at reliably predicting when those models will fail. This implies that not only will the models have undefined behavior on out-of-distribution data, we are unable to detect when the models are presented with out-of-distribution data. This poses a conundrum, since we wish to deploy our high-performing models, yet often we can't since they can potentially have unpredictable behavior at unpredictable instances. Since training a model that is robust to all possible distributional shifts the model might encounter is potentially impossible, a more modest approach might be to come up with ways for detecting out-of-distribution data. These detections could then act as an indication that the model might be incorrect. This ability to predict incorrectness can go a long way in making systems more reliable. The problem of detecting out-of-distribution data has a long history, and is formally known as the goodness-of-fit problem. Statisticians have proved bounds on the probability of detecting popula- tions of out-of-distribution data, such as in  Barron (1989)  and  Balakrishnan et al. (2019) . These type of bounds show that certain tests can be performed which are capable of discerning (with non-trivial probability) that populations of data sampled from distributions at least some positive distance away from the base distribution are anomalous. However, in order to perform the proposed tests, an ex- plicit form of the probability density function (or probability mass function) describing the base distribution is needed. For most real-world data sets, this density is not known, and must be esti- Under review as a conference paper at ICLR 2020 mated. While there has been a lot of analysis on the ability to detect anomalous data, those analyses typically do not account for the fact that the base density for which the tests are designed is learned. Empirically, however, many researchers have noted that detecting anomalous data in high dimen- sions using learned densities is hard, even when using modern, powerful density estimators. For example, the researchers in  Nalisnick et al. (2018)  and  Choi & Jang (2018)  both claim that state-of- the-art learned densities are not suitable for anomaly detection since they assign higher probability to some out-of-distribution data than the data on which the models were trained. The authors in  Nalisnick et al. (2019)  realized that such a phenomenon is actually not cause for alarm, and is even expected with high-dimensional distributions. Therefore they propose performing an anomaly de- tection test based on the typicality of data under the learned density instead of the likelihood of data under the learned density. However, in that work, the authors noted such a test still performed poorly in some cases. In this article, we investigate why goodness-of-fit tests are challenging when using learned densities. In particular, we analyze how the typical set of learned distributions relates to the typical set of the ground truth distribution. In order to do this, we work with synthetic distributions in which the probability density function is known exactly. The contributions we make are summarized as the following: • We prove error rate bounds on the goodness-of-fit test based on testing for typicality with respect to the base distribution • We prove theorems stating that the typical sets of any two distributions having sufficiently high KL divergence must have low intersection, and that distributions having low KL di- vergence must have non-zero intersection. • We use these theorems to motivate our proposed method for conservative goodness-of-fit testing. Specifically, we propose training an ensemble of models, and show that by taking the intersection of their typical sets, we can approximately recover the typical set of the ground truth distribution. We show that such an ensemble can exist and give sufficient conditions for its existence. • We demonstrate on synthetic data sets that the typical set of standard learned distributions and the ground-truth distribution often have low intersection, even when the class of den- sities from which we approximate the ground truth contains the ground-truth. We validate that our proposed method addresses this issue.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a mechanism to train and prune a convolutional network during the earlier stages of training such that the resulting sparsity can be harvested for computational speedups. The proposed technique is tolerant to sparsity levels of up to 60-70% with under 1% accuracy degradation. The paper also explores the impact of various pruning granularities, sparsity levels, and learning-rate schedules on the network's convergence as well as adversarial robustness for CNNs on ImageNet and tinyImagenet. Results demonstrate that early stage dense training is crucial for maintaining high accuracy and that the proposed technique is as robust to adversarial FGSM attacks as fully dense models.",
        "Abstract": "This paper studies structured sparse training of CNNs with a gradual pruning technique that leads to fixed, sparse weight matrices after a set number of epochs. We simplify the structure of the enforced sparsity so that it reduces overhead caused by regularization. The proposed training methodology explores several options for structured sparsity.\n\nWe study various tradeoffs with respect to pruning duration, learning-rate configuration, and the total length of training.\nWe show that our method creates a sparse version of ResNet50 and ResNet50v1.5 on full ImageNet while remaining within a negligible <1% margin of accuracy loss. To make sure that this type of sparse training does not harm the robustness of the network, we also demonstrate how the network behaves in the presence of adversarial attacks.  Our results show that with 70% target sparsity, over 75% top-1 accuracy is achievable. ",
        "Introduction": "  INTRODUCTION Pruning weights can compress a network into a smaller model so that the model can fit into faster/smaller memory and therefore result in execution speedups (Han et al., 2016;  2015a ). To increase the accuracy  Han et al. (2015b)  and  Mao et al. (2017)  explore training the network dense after pruning. The resulting network can maintain accuracy based on the specified level of sparsity ( Mostafa & Wang, 2019 ;  Zhu & Gupta, 2017 ;  Han et al., 2015a ). Structured sparsity has been explored for RNNs and also CNNs where a certain number of non- zeros is allowed across various cross-sections of the weight tensors. These methods aim to speed up computation and reach some final level of sparsity for deployment.  Narang et al. (2017)  have shown promising results for structured training of RNNs while sparse CNNs could not achieve the same performance ( Mao et al., 2017 ). Recent work has demonstrated that structurally sparse training can speed up execution on GPUs ( He et al., 2017 ;  Lym et al., 2019 ;  Zhu & Gupta, 2017 ). However, these training mecha- nisms add regularization and computational overhead to eliminate unnecessary weights. The regularization term modifies the original training and can be expensive in hardware. While en- forcing coarse-grain sparsity  Lym et al. (2019)  provides significant speedups, the final network contains an insufficient degree of sparsity for deployment on edge devices.  Mostafa & Wang (2019)  show that with adaptive sparse training and dynamic reallocation of non- zeros sparsity levels up to 80% can be achieved. However, even though an additional 10 epochs of training are required, an accuracy loss of around 1-2% is still observed. The main drawback is the overhead incurred while implementing such technique on the target platform. Continuous reconfiguration of the sparsity pattern is expensive as it does not allow for compression of weights during training. To achieve speedups and a desired final degree of sparsity, we aim to apply the techniques in  Han et al. (2015b)  and  Mao et al. (2017)  at earlier stages in training at higher frequency within a period which we call the pruning era, usually a period of 20-30 epochs. During the pruning era, with fine granularity of at most a kernel size, we exploit one of the three proposed sparsity regimes. Subsequently, we fix the mask for the rest of the training to speed it up. Our motivation came from the insight that having a fixed sparse multiply-accumulate pattern allows weight compression during training and can save compute and energy in hardware (Han et al., 2016). We explore the impact of various pruning granularities, sparsity levels, and learning-rate schedules on the network's convergence as well as adversarial robustness for CNNs like Resnet-50 ( He et al., 2015 ) on ImageNet and tinyImagenet ( CS231N, 2015 ). Recent literature has shown that adversarial attacks are more successful on pruned neural net- works than they are on regular neural networks ( Wang et al., 2018 ). Given the danger of adversarial attacks in real world situations, we find that it is important to evaluate our sparsity techniques un- der adversarial robustness. We leverage the FGSM mechanism ( Goodfellow et al., 2014 ) to evaluate the adversarial robustness on our sparse models. This paper makes the following contributions: 1. We propose a mechanism to train and prune a convolutional network during the earlier stages of training such that this sparsity can be harvested for the computational speedups. To do this, we fix the sparse weight masks for the remainder of the training. 2. For fully connected sparsification, we eliminate blocks of fully connected weights based on their connection to the zeros in the previous convolutional layer. 3. We enforce structural, regularization free, magnitude-based pruning across two distinct dimensions and a combined version. These dimensions are inside convolution window R × S and across input/output feature matrix (C K ). 4. Our sparse models are as robust to adversarial FGSM attacks as fully dense models. 5. We demonstrate that early stage dense training is crucial for maintaining high accuracy. 6. The proposed technique is tolerant to sparsity levels of up to 60-70% with under 1% accuracy degradation. We can compensate by scheduling an extra learning rate drop and training for an extra 10 epochs. The rest of the paper is organized as follows. Section 2 explains our pruning methodology. Section 3 describes the experimental setup framework. Section 4 presents results and discusses their interpretation. Section 5 presents the related work. Section 6 concludes the paper.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes two new training methodologies for Deep Neural Networks (DNNs) that exclusively use half-precision (Brain Floating Point 16-bit (BF16)) for a large part of the training process. The first approach statically assigns either BF16 or Floating Point 32-bit (FP32) format to the model parameters involved in the training process, while the second dynamically switches between BF16 and mixed-precision (MP) during training depending on its progress. These approaches do not require mixed-precision arithmetic while computing linear algebra operations for a large portion of the training process, enabling them to deliver the same performance as if they were operating with half-precision arithmetic while providing the same model accuracy as if FP32 was used. This paper demonstrates that half-precision can be extensively used during DNNs training without the need for mixed-precision arithmetic.",
        "Abstract": "Mixed-precision arithmetic combining both single- and half-precision operands in the same operation have been successfully applied to train deep neural networks. Despite the advantages of mixed-precision arithmetic in terms of reducing the need for key resources like memory bandwidth or register file size, it has a limited capacity for diminishing computing costs and requires 32 bits to represent its output operands. This paper proposes two approaches to replace mixed-precision for half-precision arithmetic during a large portion of the training. The first approach achieves accuracy ratios slightly slower than the state-of-the-art by using half-precision arithmetic during more than 99% of training. The second approach reaches the same accuracy as the state-of-the-art by dynamically switching between half- and mixed-precision arithmetic during training. It uses half-precision during more than 94% of the training process. This paper is the first in demonstrating that half-precision can be used for a very large portion of DNNs training and still reach state-of-the-art accuracy.",
        "Introduction": "  INTRODUCTION The use of Deep Neural Networks (DNNs) is becoming ubiquitous in areas like computer vi- sion ( Krizhevsky et al., 2012 ;  Szegedy et al., 2015 ), speech recognition ( Hinton et al., 2012 ), or language translation ( Wu et al., 2016 ). DNNs display very remarkable pattern detection capacities and, more specifically, Convolutional Neural Networks (CNNs) are able to accurately detect and classify objects over very large image sets ( Krizhevsky et al., 2012 ). Despite this success, a large amount of samples must be exposed to the model for tens or even hundreds of times during train- ing until an acceptable accuracy threshold is reached, which drives up training costs in terms of resources like memory storage or computing time. To mitigate these very large training costs, approaches based on data representation formats simpler than the Floating Point 32-bit (FP32) standard have been proposed ( Courbariaux et al., 2014 ;  Gupta et al., 2015 ). These approaches successfully mitigate the enormous training costs of DNNs by using data representation formats that either reduce computing costs or diminish the requirements in terms of memory storage and bandwidth. In particular, some of these proposals have shown the bene- fits of combining half-precision and single-precision compute during training in terms of keeping model accuracy and reducing compute and memory costs ( Micikevicius et al., 2017 ;  Kalamkar et al., 2019 ). These approaches accelerate linear algebra operations by accumulating half-precision input operands to generate 32-bit outputs. While this mixed-precision (MP) arithmetic can successfully reduce the use of resources like memory bandwidth or hardware components like register file size, it has a very limited capacity for diminishing computing costs and it is unable to reduce output data size. In this paper we propose new training methodologies able to exclusively use half-precision for a large part of the training process, which constitutes a very significant improvement over mixed- precision approaches in terms of compute and memory bandwidth requirements. We propose two different approaches, the first one statically assigns either the Brain Floating Point 16-bit (BF16) or the FP32 format to the model parameters involved in the training process, while the second dynam- ically switches between BF16 and MP during training depending on its progress. Our approaches Under review as a conference paper at ICLR 2020 do not require mixed-precision arithmetic while computing linear algebra operations for a large por- tion of the training process, which enables them to deliver the same performance as if they were operating with half-precision arithmetic during the whole training while providing the same model accuracy as if FP32 was used. This paper is the first in demonstrating that half-precision can be extensively used during DNNs training without the need for mixed-precision arithmetic. We made our code available 1 .",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a two-part scheme to address the challenges posed by the Federated Learning (FL) setting, which is an attractive solution to address scalability and privacy concerns in machine learning. The first part of the scheme is an adaptive online tuning of hyper-parameters, formulated as an online reinforcement learning problem. The second part is a regularization term that punishes divergent representation learning across clients. We evaluate the performance of our scheme on image classification and keyword spotting tasks, and show that it consistently improves accuracy and robustness in the presence of heterogeneous client-side data distributions.",
        "Abstract": "  Federated learning is a distributed, privacy-aware learning scenario which trains a single model on data belonging to several clients. Each client trains a local model on its data and the local models are then aggregated by a central party. Current federated learning methods struggle in cases with heterogeneous client-side data distributions which can quickly lead to divergent local models and a collapse in performance. Careful hyper-parameter tuning is particularly important in these cases but traditional automated hyper-parameter tuning methods  would require several training trials which is often impractical in a federated learning setting. We describe a two-pronged solution to the issues of robustness and hyper-parameter tuning in federated learning settings. We propose a novel representation matching scheme that reduces the divergence of local models by ensuring the feature representations in the global (aggregate) model can be derived from the locally learned representations. We also propose an online hyper-parameter tuning scheme which uses an online version of the REINFORCE algorithm to find a hyper-parameter distribution that maximizes the expected improvements in training loss. We show on several benchmarks that our two-part scheme of local representation matching and global adaptive hyper-parameters significantly improves performance and training robustness.",
        "Introduction": "  INTRODUCTION The size of the data used to train machine learning models is steadily increasing, and the privacy concerns associated with storing and managing this data are becoming more pressing. Offloading model training to the data owners is an attractive solution to address both scalability and privacy concerns. This gives rise to the Federated Learning (FL) setting ( McMahan et al., 2016 ) where sev- eral clients collaboratively train a model without disclosing their data. In synchronous FL, training proceeds in rounds where at the beginning of each round, a central party sends the latest version of the model to the clients. The clients (or a subset of them) train the received model on their local datasets and then communicate the resulting models to the central party at the end of the round. The central party aggregates the client models (typically by averaging them) to obtain the new version of the model which it then communicates to the clients in the next round. The FL setting poses a unique set of challenges compared to standard stochastic gradient descent (SGD) learning on a monolithic dataset. In real-world settings, data from each client may be drawn from different distributions, and this heterogeneity can lead the local learning processes to diverge from each other, harming the convergence rate and final performance of the aggregate model. We address this challenge in two ways: (1) adaptive on-line tuning of hyper-parameters; and (2) adding a regularization term that punishes divergent representation learning across clients. Traditional hyper-parameter tuning schemes, such as random search ( Bergstra & Bengio, 2012 ) or Bayesian methods ( Bergstra et al., 2011 ;  Snoek et al., 2012 ), require several training runs to evaluate the fitness of different hyper-parameters. This is impractical in the FL setting, which strives to minimize unnecessary communication. To address this issue, we formulate the hyper-parameter Under review as a conference paper at ICLR 2020 selection problem as an online reinforcement learning (RL) problem: in each round, the learners perform an action by selecting particular hyper-parameter values, and at the end of the round get a reward which is the relative reduction in training loss. We then update the hyper-parameter selection policy online to maximize the rewards. Unlike centralized SGD, where there is a single trajectory of parameter updates, the FL setting has many local parameter trajectories, one per each active client. Even though they share the same initial point, these trajectories could significantly diverge. Averaging the endpoints of these divergent tra- jectories at the end of a training round would then result in a poor model ( Zhao et al., 2018 ). Clients with heterogeneous data distributions exacerbate this effect as each client could quickly learn repre- sentations specific to its local dataset. We introduce a scheme that mitigates the problem of divergent representations: each client uses the representations it learns to reconstruct the representations in the initial model it receives. A client is thus discouraged from learning representations that are too spe- cific and that discard information about the globally learned representations. We show that this representation matching scheme significantly improves robustness and accuracy in the presence of heterogeneous client-side data distributions. We evaluate the performance of our two-part scheme, representation matching and online hyper- parameter adjustments, on image classification tasks: MNIST, CIFAR10; and on a keyword spotting (KWS) task. We show that for homogeneous client-side data distributions, our scheme consistently improves accuracy. For heterogeneous clients, in addition to improving accuracy, our scheme im- proves training robustness and stops catastrophic training failures without having to manually tune hyper-parameters for each task.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an overview of the recent advancements in deep learning for image classification, focusing on techniques which help propagate structural information of the given image till the classification layer. It discusses the various architectures and techniques used to make deep learning models robust to variations like rotation, scaling, etc. It also provides an insight into the various applications of deep learning in image classification.",
        "Abstract": "We propose a novel framework, ICNN, which combines the input-conditioned filter generation module and a decoder based network to incorporate contextual information present in images into Convolutional Neural Networks (CNNs). In contrast to traditional CNNs, we do not employ the same set of learned convolution filters for all input image instances. And our proposed decoder network serves the purpose of reducing the transformation present in the input image by learning to construct a representative image of the input image class. Our proposed joint supervision of input-aware framework when combined with techniques inspired by Multi-instance learning and max-pooling, results in a transformation-invariant neural network. We investigated the performance of our proposed framework on three MNIST variations, which covers both rotation and scaling variance, and achieved 0.98% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and 0.68% error on Scaling MNIST, which is significantly better than the state-of-the-art results. Our proposed model also showcased consistent improvement on the CIFAR dataset. We make use of visualization to further prove the effectiveness of our input-aware convolution filters. Our proposed convolution filter generation framework can also serve as a plugin for any CNN based architecture and enhance its modeling capacity.",
        "Introduction": "  INTRODUCTION Deep learning has been used to obtain promising results for various machine learning applications in recent times. Computer Vision has been at the frontier, driving its success with the introduction of Convolutional Neural Networks (CNNs). Face Recognition  Li et al. (2015) ,  Image Classification Deng et al. (2009) ,  Action Recognition Chéron et al. (2015) , and Speech  AnalysisAbdel-Hamid et al. (2014) ;  Yenigalla et al. (2018)  are some of the areas which use CNNs extensively. LeNet, first introduced by  Lecun et al. (1998)  served as the prototype for the modern-day Convolu- tion Neural Network such as VGGNet  Simonyan & Zisserman (2014)  and  ResNet He et al. (2015) . These networks first consist of several layers of convolution, activation, and subsampling layers. The convolution filters are learned to extract local features from the image patches, which are sub- sampled using either max-pooling or average-pooling. Pooling reduces feature map resolution and sensitivity of the output to shifts and distortion. These local features are further combined hierarchi- cally to form high-level features. The high-level features are then fed to the fully connected layers to form a compressed 1-D feature vector, which is then fed to a soft-max layer for Image Classi- fication. These network uses techniques like data augmentation  Fawzi et al. (2016)  and auxiliary classification  Yin & Liu (2018)  to surpass many pre-established boundaries in Image Classification. Although there has been a significant improvement in image classification, local features extracted by learned filters keep information regarding neighboring pixels of the image patch, irrespective of its location in the image. This makes networks prone to errors when they encounter images having variations like rotation, scaling, etc., as this requires networks to withhold complete structural information of the image up until the soft-max layer. Designing neural networks robust to such variations have been under research recently, resulting in many techniques which help propagate structural information of the given image till classification.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to Imitation Learning (IL) that seeks to minimize the distance between the state-transition density functions induced by the agent and the expert. The proposed approach, eMDP, replaces the unknown transition kernel with a synthetic kernel that simulates the transition of state components for which the transition kernel is known and extracts from demonstrations the state components for which the kernel is unknown. A PAC result is derived to bound the error in the state-value function between the eMDP model and the genuine model that uses the original transition kernel. Empirical results are presented to demonstrate the benefits of using eMDP when the transition kernel is partially available and model-based approaches are not applicable.",
        "Abstract": "Model-based imitation learning methods require full knowledge of the transition kernel for policy evaluation. In this work, we introduce the Expert Induced Markov Decision Process (eMDP) model as a formulation of solving imitation problems using Reinforcement Learning (RL), when only partial knowledge about the transition kernel is available. The idea of eMDP is to replace the unknown transition kernel with a synthetic kernel that: a) simulate the transition of state components for which the transition kernel is known (s_r), and b) extract from demonstrations the state components for which the kernel is unknown (s_u). The next state is then stitched from the two components: s={s_r,s_u}. We describe in detail the recipe for building an eMDP and analyze the errors caused by its synthetic kernel. Our experiments include imitation tasks in multiplayer games, where the agent has to imitate one expert in the presence of other experts for whom we cannot provide a transition model. We show that combining a policy gradient algorithm with our model achieves superior performance compared to the simulation-free alternative.",
        "Introduction": "  INTRODUCTION Recent work on Imitation Learning (IL) offers a new approach to the problem.  Torabi et al. (2018)  and  Baram & Mannor (2018)  suggest defining success if the agent and the expert influence the environment in the same way and not if they necessarily take the same actions. They remove the need to label expert actions and allow to settle for examples that include nothing but state transitions. The objective function they define seeks to minimize the distance between the state-transition density functions induced by the agent and the expert. However, since the state-transition dynamics is usually complex and unknown, it can not be calculated explicitly and is instead estimated through sampling. For reasons such as cost, time and safety, the sampling occurs in simulation and not in the real plant. However, despite considerable advances in simulation technologies, it is still a limited tool when it comes to complex real-world problems. Therefore, the adoption of advanced IL methods is hindered. Consider the self-driving car example. Given their intent, physical simulation of pedestrians and vehicles can be done with high accuracy. However, simulating the intent itself, i.e., the decision-making process of those entities, is challenging. In this example, simulating the transition of other road users is hard (requires intent and physical transformation), however, simulating the ego car that is controlled externally is feasible (requires physical transformation only). The unwelcome solution, in this case, is to resort to behavior cloning (BC) that do not require simulation at all ( Pomerleau, 1991 ). However, as previously mentioned, BC follows a different success criterion that requires access to expert actions and is less in line with the true definition of success. But most importantly, BC discards two key ingredients of the problem: a) states order (i.e., it breaks trajectories), and b) partial knowledge about the transition kernel (ego car in the example above). In this paper we ask the following question: a Markov Decision Process (MDP), and we show that solving it amounts to solving an imitation problem that seeks to match the state densities at each step. The idea of eMDP is to replace the unknown transition kernel with a synthetic kernel that: 1) simulate the transition of state components for which the transition kernel is known, and 2) extract from demonstrations the state components for which the kernel is unknown (see illustration in  Figure 1 ). To understand the conditions when the use of eMDP is just, we derive a PAC result that bounds the error in the state-value function between the eMDP model and the genuine model that uses the original transition kernel. Finally, we show empirical results that stress the benefits of using eMDP when the transition kernel is partially available and model-based approaches are not applicable.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an algorithm, PROJECT AND FORGET, that uses Bregman projections with cutting planes to solve metric constrained problems with many (possibly exponentially) inequality constraints. This algorithm is a general purpose one that can solve large constrained convex optimization problems, not only those arising from metric constraints. The algorithm is demonstrated to solve the weighted correlation clustering problem on a graph with over 130,000 nodes, the metric nearness problem, and the information theoretic machine learning problem. Theoretical analysis is provided to show that the algorithm converges to the global optimal solution and the optimality error asymptotically decays at an exponential rate.",
        "Abstract": "Given a set of distances amongst points, determining what metric representation is most “consistent” with the input distances or the metric that captures the relevant geometric features of the data is a key step in many machine learning algorithms. In this paper, we focus on metric constrained problems, a class of optimization problems with metric constraints. In particular, we identify three types of metric constrained problems: metric nearness Brickell et al. (2008), weighted correlation clustering on general graphs Bansal et al. (2004), and metric learning Bellet et al. (2013); Davis et al. (2007). Because of the large number of constraints in these problems, however, researchers have been forced to restrict either the kinds of metrics learned or the size of the problem that can be solved.\nWe provide an algorithm, PROJECT AND FORGET, that uses Bregman projections with cutting planes, to solve metric constrained problems with many (possibly exponentially) inequality constraints. We also prove that our algorithm converges to the global optimal solution. Additionally, we show that the optimality error (L2 distance of the current iterate to the optimal) asymptotically decays at an exponential rate. We show that using our method we can solve large problem instances of three types of metric constrained problems, out-performing all state of the art methods with respect to CPU times and problem sizes.",
        "Introduction": "  INTRODUCTION Given a set of distances amongst data points, many machine learning algorithms are considerably \"easier\" once these distances adhere to a metric. Furthermore, learning what metric is most \"consistent\" with the input distances or the metric that best captures the relevant geometric features of the data (e.g., the correlation structure in the data) is a key step in efficient, approximation algorithms for classification, clustering, regression, feature selection, etc. Indyk (1999) provides a list of other computational problems such as nearest neighbor search, (approximate) proximity problems, facility location, and a variety of graph problems for which we have efficient approximation algorithms in a general metric space. Given the importance of metric representations of data sets, we focus on metric constrained problems, a class of optimization problems with metric constraints; i.e., optimization of a convex function subject to metric constraints, such as the triangle inequality, on all the output variables. In particular, we identify three types of metric constrained problems: metric nearness (Brickell et al. (2008)), weighted correlation clustering on general graphs (Bansal et al. (2004)), and metric learning (Bellet et al. (2013); Davis et al. (2007)). Briefly, the metric nearness problem seeks the closest metric to a given set of distances, the goal of correlation clustering is to partition nodes in a graph according to their similarity, and metric learning finds a metric on a dataset that is consistent with (dis)similarity information about the data points. All of these problems can be modeled as constrained convex optimization problems with a large number of constraints. Unfortunately, because of the large number of constraints, using standard optimization techniques, researchers have been forced to restrict either the kinds of metrics learned or the size of the problem that can be solved. Many of the existing methods for metric constrained problems suffer from some sort of significant drawback that hampers performance or restricts the instance size. Gradient based algorithms such as projected gradient descent (e.g., Beck & Teboulle (2009); Nesterov (1983)) or Riemannian gradient descent require a projection onto the space of all metrics, which in general, is an intractable problem. One modification of this approach is to subsample the constraints and then project onto the sampled set (see Nedić (2011); Polyak (2001); Wang & Bertsekas (2013); Wang et al. (2015)). For metric constrained problems, however, we have many more constraints than data points, so the condition numbers of the problems are quite high and these algorithms tend to require a large number of iterations. Another standard approach is to consider the Lagrangian and maintain a KKT type optimality condition. These methods run into two different kinds of problems. First, computing the gradient becomes an intractable problem for methods that maintain the KKT condition by using Newton's method. Examples of such methods include the interior point method and the barrier method. One fix could be to subsample the constraints and only compute those gradients, but this approach runs into the same drawbacks as before. The other option is to incrementally update the Lagrangian, looking at one constraint at a time. These methods, such as Bauschke & Lewis (2000); Iusem (1991); Iusem & De Pierro (1990), traditionally require us to cycle through all the constraints which is not feasible with metric constraints. One final approach is to use cutting planes. The performance of this method is very heavily dependent on the cut selection process (see Dey & Molinaro (2018); Poirrier & Yu (2019) for deep discussions about the cut selection process). The discovery of Gomory cuts and other subsequent work such as branch and bound, has led to the viability of the cutting plane method for solving mixed integer linear programs. This success has, however, not transferred to other problems. In general, if the cuts are not selected appropriately, the algorithm could potentially take an exponential number of iterations; i.e., add an exponential number of constraints. Thus, to use this method, we must show for each problem that the specific cutting plane selection method results in a feasible algorithm (see Chandrasekaran et al. (2012) for an example). In this paper, we provide an algorithm, PROJECT AND FORGET, that uses Bregman projections with cutting planes, to solve metric constrained problems with many (possibly exponentially) inequality constraints. In fact, the algorithm is a general purpose one that can solve large constrained convex optimization problems, not only those arising from metric constraints. We also develop a stochastic version of our algorithm. This version is a similar adaptation of the Bregman method, as Nedić (2011); Polyak (2001); Wang & Bertsekas (2013); Wang et al. (2015) are adaptations of the projected gradient method. This version of our algorithm can be used to solve problems where each data point (or pair, triple of data points) form a constraint. The major contributions of our paper is as follows: 1. Using a specific instantiation of the PROJECT AND FORGET algorithm, we solve the weighted correlation clustering problem on a graph with over 130, 000 nodes. To solve this problem with previous methods, we would need to solve a linear program with over 10 15 constraints. Furthermore, we demonstrate our algorithms superiority by outperforming the current state of the art in terms of CPU times. 2. We use our algorithm to develop a new algorithm that solves the metric nearness problem. We show that our algorithm outperforms the current state of the art with respect to CPU time and can be used to solve the problem for non-complete graphs. 3. We use the the stochastic version of our algorithm to develop a new algorithm to solve the information theoretic machine learning problem. We compare this against the standard method and show that in general we require fewer projections to solve the problem. Using our algorithm we can also solve the full version of the convex program presented in Davis et al. (2007) instead of a heuristic approximation. Thus, demonstrating that we can solve larger instances of the problem. 4. Finally, we prove that our algorithm converges to the global optimal solution. Additionally, we show that the optimality error (L 2 distance of the current iterate to the optimal) asymp- totically decays at an exponential rate. We also show that because of the FORGET step, when the algorithm terminates, the set of constraints that remain remembered are exactly the active constraints. Thus, our algorithm also finds the set of active constraints. We present the necessary background material and problem formulations in Section 2. In Section 3, we provide a general form of the PROJECT AND FORGET algorithm and detail its theoretical analysis. We instantiate our algorithm to solve three types of metric constrained problems in Section 4 and highlight the empirical performance. Complete proofs and discussion may be found in the sections in the Appendix.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a Convolutional Neural Network (CNN) for the detection and classification of malicious PDF files. The classifier is able to detect 94% of malicious samples with a False Positive Rate (FPR) of 0.2%, outperforming most antivirus vendors. It is also able to group more than 80% of the malware into different families with 76% agreement with the name given by the antivirus. This is the first paper to use a CNN for the classification of malicious PDF files and to investigate the ability to automatically classify PDF malware into different families.",
        "Abstract": "Malicious PDF files represent one of the biggest threats to computer security. To\ndetect them, significant research has been done using handwritten signatures or\nmachine learning based on manual feature extraction. Those approaches are both\ntime-consuming, requires significant prior knowledge and the list of features has\nto be updated with each newly discovered vulnerability. In this work, we propose\na novel algorithm that uses a Convolutional Neural Network (CNN) on the byte\nlevel of the file, without any handcrafted features. We show, using a data set\nof 130000 files, that our approach maintains a high detection rate (96%) of PDF\nmalware and even detects new malicious files, still undetected by most antiviruses.\nUsing automatically generated features from our CNN network, and applying a\nclustering algorithm, we also obtain high similarity between the antiviruses’ labels\nand the resulting clusters.",
        "Introduction": "  INTRODUCTION Malware programs are still making newspapers' headlines. They are used by criminal organizations, governments, and industry to steal money, spy or other unwanted activities. As millions of new malicious samples are discovered every day, spotting them before they harm a computer or a network remains one of the most important challenges in cybersecurity. During the last two decades, hackers kept finding new attack vectors, giving malware multiple forms. Some use the macros in Office documents while others exploit browser's vulnerabilities with javascript files. This diversity raised a need for new automated solutions. PDF is one of the most popular types of documents. Despite the lack of awareness of the population, it also became an important attack vector for computer systems. Dozens of vulnerabilities are dis- covered every year on Adobe Reader, the most popular software for reading PDF files (1), allowing hackers to take control of the victim's computer. PDF malware can be segmented into three main categories: (i) exploits, (ii) phishing and (iii) misuse of PDF capabilities. Exploits operate by taking advantage of a bug in the API of a PDF reader application, which allows the attacker to execute code in the victim's computer. This is usually done via JavaScript code, embedded in the file. In phishing attacks, the PDF itself does not have any malicious behavior but attempts to convince the user to click on a malicious link. Such campaigns have been discovered recently ( 2 ) and are, by nature, much harder to identify. The last category exploits some regular functionality of PDF files such as running a command or launching a file.All those attacks can lead to devastating consequences, such as downloading a malicious executable or stealing credentials from a website. Regardless of recent work in machine learning for malware detection, antivirus companies are still largely focusing on handwritten signatures to detect malicious PDF. This not only requires a lot of human resources but is also rarely efficient at detecting unknown variants or zero day attacks. Another popular solution is dynamic analysis by running the files in a controlled sandboxed en- vironment. Such approaches increase significantly the chance of detecting new malware, but take much longer and requires access to a sandbox virtual machine. They also still require a human to define the detection rules according to the file behavior. In this paper, we are using a Convolutional Neural Network (CNN) in order to detect any type of malicious PDF files. Without any preprocessing of the files, our classifier succeeds to detect 94% of the malicious samples of our test set while keeping a False Positive Rate (FPR) at 0.2%. Our classifier outperforms most of the antiviruses (AV) vendors available in the VirusTotal website.We also show that our network can successfully group more than 80% of the malware into different Under review as a conference paper at ICLR 2020 families agreeing at 76% with the name given by the AV. Finally, we will present some examples on which we were able to detect an attack before the AV (zero-day). To the best of our knowledge, this is the first paper using Convolutional Neural Network to classify malicious PDF files. It is also the first one that investigates the ability to automatically classify PDF malware into different families.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents the largest sound-action-vision dataset available, created with a Tilt-Bot robot. Through this dataset, the authors explore the synergy between sound and action to gain insight into what sound can be used for. They demonstrate that sound is indicative of fine-grained object information, action, and physical properties of objects. The dataset, along with audio embeddings, can be accessed online.",
        "Abstract": "Truly intelligent agents need to capture the interplay of all their senses to build a rich physical understanding of their world. In robotics, we have seen tremendous progress in using visual and tactile perception; however we have often ignored a key sense: sound. This is primarily due to lack of data that captures the interplay of action and sound. In this work, we perform the first large-scale study of the interactions between sound and robotic action. To do this, we create the largest available sound-action-vision dataset with 15,000 interactions on 60 objects using our robotic platform Tilt-Bot. By tilting objects and allowing them to crash into the walls of a robotic tray, we collect rich four-channel audio information. Using this data, we explore the synergies between sound and action, and present three key insights. First, sound is indicative of fine-grained object class information, e.g., sound can differentiate a metal screwdriver from a metal wrench. Second, sound also contains information about the causal effects of an action, i.e. given the sound produced, we can predict what action was applied on the object. Finally, object representations derived from audio embeddings are indicative of implicit physical properties. We demonstrate that on previously unseen objects, audio embeddings generated through interactions can predict forward models 24% better than passive visual embeddings. ",
        "Introduction": "  INTRODUCTION Imagine the opening of a champagne bottle! Most vivid imaginations not only capture the celebra- tory visuals but also the distinctive 'pop' sound created by the act. Our world is rich and feeds all of our five senses - vision, touch, smell, sound and taste. Of these, the sense of vision, touch and sound play a critical role in our rich physical understanding of objects and actions. A truly intelligent agent would need to capture the interplay of all the three senses to build a physical understanding of the world. In robotics, where the goal is to perform physical task, vision has always played a central role. Vision is used to infer the geometric shape ( Kar et al. (2015) ), track objects ( Xiang et al. (2015) ), infer object categories ( Krizhevsky et al. (2012) ) and even direct control ( Levine et al. (2016a) ). In recent years, the sense of touch has also received increasing attention for recognition ( Schneider et al. (2009) ) and feedback control ( Murali et al. (2018) ). But what about sound? From the squeak of a door, to the rustle of a dried leaf, sound captures rich object information that is often impercep- tible through visual or force data. Microphones (sound sensors) are also inexpensive and robust; yet we haven't seen sound data transform robot learning. There hardly exists any systems, algorithms or datasets that exploit sound as a vehicle to build physical understanding. Why is that? Why does sound appear to be second-class citizen among perceptual faculties? The key reason lies at the heart of sound generation. Sound generated through an interaction, say a robot striking an object, depends on the impact of the strike, the structure of the object, and even the location of the microphone. This intricate interplay that generates rich data, also makes it difficult to extract information that is useful for robotics. Although recent work has used sound to determine the amount of granular material in a container ( Clarke et al. (2018) ), we believe there lies much more information in the sound of interactions. But what sort of information can be extracted from this sound? In this paper, we explore the synergy between sound and action to gain insight into what sound can be used for. To begin this exploration we will first need a large and diverse dataset that contains both sound and action data. However, most existing sound datasets do not contain information about action, while most action datasets do not contain information about sound. To solve this, we create Under review as a conference paper at ICLR 2020 Visual data Tilt-Bot Audio data Action data the largest sound-action-vision dataset available with 15,000 interactions on over 60 objects with our Tilt-Bot robot  Figure 1 . Each object is placed in a tray mounted on a robot arm that is tilted with a random action until the object hits the walls of the tray and make a sound. This setup allows us to robustly collect sound and action data over a diverse set of objects. But how is this data useful? Through Tilt-Bot's data, we present three key insights about the role of sound in action. The first insight is that sound is indicative of fine-grained object information. This implies that just from the sound an object makes, a learned model can identify the object with 79.2% accuracy from set of diverse 60 objects, which includes 30 YCB objects ( Calli et al. (2015) ). Our second insight is that sound is indicative of action. This implies that just from hearing the sound of an object, a learned model can predict what action was applied to the object. On a set of 30 previously unseen objects, we achieve a 0.027 MSE error which is 42% better than learning from only visual inputs. Our final insight is that sound is indicative of physical properties of object. This implies that just from hearing the sound an object makes, a learned model can infer the implicit physical properties of the object. To test this implicit physics, we show that a learned audio-conditioned forward model achieves a L1 error of 0.193 on previously unseen objects, which is 24% lesser than forward models trained using visual information. This further indicates that audio embeddings, generated from a previous interaction, can capture information about the physics of an object significantly better than visual embeddings. One could envision using these features to learn policies that first interact to create sound and then use the inferred audio embeddings to perform actions ( Zhou et al. (2019) ). In summary, we present three key contributions in this paper: (a) we create the largest sound-action- vision robotics dataset; (b) we demonstrate that we can perform fine grained object recognition using only sound; and (c) we show that sound is indicative of action, both for post-interaction pre- diction, and pre-interaction forward modeling. Tilt-Bot's sound-action-vision data, along with audio embeddings can be accessed here: https://sites.google.com/view/iclr2020-sound-action.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to constructing a useful representation of the conditional probabilities p(x|y) and p(y|x) from samples (x1, y1),...,(xn, yn) from an unknown joint probability distribution p(x, y). This approach is based on the singular value decomposition of the channel (seen as an operator in Hilbert space) and keeping only the components with the largest singular values. This representation is equivalent to the decomposition in terms of canonical variables introduced in Lancaster (1958). The practical advantage of this representation for inference (or prediction) is that it reduces the evaluation of conditional expectations to that of empirical averages over the (unconditional) marginal p(y). Additionally, this paper proposes a strategy for extracting disentangled variables from the canonical variables. Experiments are conducted to demonstrate the effectiveness of this approach for performing inference on new data.",
        "Abstract": "We introduce an new technique to learn correlations between two types of data.\nThe learned representation can be used to directly compute the expectations of functions over one type of data conditioned on the other, such as Bayesian estimators and their standard deviations. \nSpecifically, our loss function teaches two neural nets to extract features representing the probability vectors of highest singular value for the stochastic map (set of conditional probabilities) implied by the joint dataset, relative to the inner product defined by the Fisher information metrics evaluated at the marginals.\nWe test the approach using a synthetic dataset, analytical calculations, and inference on occluded MNIST images. \nSurprisingly, when applied to supervised learning (one dataset consists of labels), this approach automatically provides regularization and faster convergence compared to the cross-entropy objective.\nWe also explore using this approach to discover salient independent features of a single dataset. ",
        "Introduction": "  INTRODUCTION Given samples (x 1 , y 1 ), . . . , (x n , y n ) from an unknown joint probability distribution p(x, y), we want to construct a useful representation of the conditional probabilities p(x|y) and p(y|x), so that we that we can infer one view from the other on new data. For instance, x and y could be past and future histories of dynamical data, visual and auditory inputs, actions and their effects, etc. Following the approach introduced in  Bény & Osborne (2013 ; 2015b) in the context of quantum information theory, we look at the problem as follows: the conditional distributions p(y|x) can be thought of as representing a noisy communication channel (stochastic map). This channel is a linear map between spaces of typically ludicrously large dimensions (the spaces of all probability distributions over x or y). We want a pair of small subspaces which best represent the channel. Specifically, we look for those vectors representing probability distributions over x which lose least distinguishability under the channel, where the distinguishability is measured by the χ 2 divergence. We show in Section 3 that this is equivalent to performing a certain singular value decomposition of the channel (seen as an operator in Hilbert space) and keep only the components with the largest singular values. Moreover, the full singular value decomposition is equivalent to the decomposition in terms of canonical variables introduced in  Lancaster (1958) , namely, p(y|x) = p(y) D i=1 η i u i (x)v i (y), (1) where u i and u j are real non-linear functions such that E(u i u j ) = δ ij , E(v i v j ) = δ ij , and 0 < η D ≤ · · · ≤ η 1 ≤ η 0 = 1 are the singular values. The practical advantage of this representation for inference (or prediction) is that it reduces the evaluation of conditional expectations to that of empirical averages over the (unconditional) marginal p(y). As observed in  Michaeli et al. (2016) , the span of the first k canonical variables u i , v j is what is learned by the deep canonical correlation analysis (DCCA) ( Andrew et al., 2013 ). Indeed, these Under review as a conference paper at ICLR 2020 variables are those which maximize the correlations E(u i v i ) subject to the same constraints as above. (This reduces to CCA ( Hotelling, 1936 ) when u i , v j are linear maps). In this work, besides establishing this new information-theoretical interpretation of canonical vari- ables and DCCA, we experiment with using this representation for performing inference on new data. Moreover, we propose a strategy for extracting disentangled variables from the canonical variables, inspired by analytical solutions.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a cycle-consistent Generative Adversarial Network (GAN) for anomaly detection in time series data. The proposed model is based on Long Short Term Memory (LSTM) cells and uses a reconstruction loss for the Encoder and Generator training, and a second Critic network to support the correct bidirectional mappings. The paper also introduces two similarity measures to evaluate the local similarity between the original and the reconstructed sequences, and combines this similarity measure and the Critic output into a function that gives robust anomaly scores for the time series. The evaluation of the proposed model is provided on well-known time series data sets and shows how it exceeds the performance of current state-of-the-art methods.",
        "Abstract": "Anomaly detection in time series data is an important topic in many domains. However, time series are known to be particular hard to analyze. Based on the recent developments in adversarially learned models, we propose a new approach for anomaly detection in time series data. We build upon the idea to use a combination of a reconstruction error and the output of a Critic network. To this end we propose a cycle-consistent GAN architecture for sequential data and a new way of measuring the reconstruction error. We then show in a detailed evaluation how the different parts of our model contribute to the final anomaly score and demonstrate how the method improves the results on several data sets. We also compare our model to other baseline anomaly detection methods to verify its performance.",
        "Introduction": "  INTRODUCTION With recent proliferation of devices collecting temporal observations, there has been an increasing demand for anomaly detection in time series. The main goal is to identify any behavior in the time series that is unusual, flag it and bring it for analysis. In many real world settings a continuous time series collected over long periods is provided, and the goal is to isolate anomalous sub sequences of varied lengths. One usually does not know where those sub sequences may exist, or how long or short each one would be and how many there are. In a classical setting, it is possible to segment the time series into many subsequences (overlapping or otherwise) of a certain length and apply methods focused on generating an anomaly score for each sequence in order to show how certain sequences are compared to others.  Chandola et al. (2008)  present a comparative study for several time series anomaly detection methods. They categorize the techniques into kernel-based, window-based and Markovian techniques. Practitioners who don't know how to segment may resort to less complex techniques, such as simple thresholding, to detect any data points that exceed the normal range ( Chandola et al., 2009 ). How- ever, many anomalies do not exceed any boundaries - for example, they may have values that are purportedly \"normal,\" but are actually unusual at the specific time that they occur. These contextual anomalies are naturally harder to identify, since the context of a signal is often not clear ( Chandola et al., 2009 ;  Ahmad et al., 2017 ). In recent years, Deep Learning-based methods have been developed to deal with such issues ( Kwon et al., 2017 ). These methods make use of the increased availability of data in order to learn the underlying structure of a time series, and to identify unusual changes in behavior using prediction or reconstruction errors ( Malhotra et al., 2015 ). Within this framework - learning a model, predicting and reconstructing sequences, and using reconstruction errors to detect anomalies - multiple variants have been developed ( Malhotra et al., 2015 ;  Hundman et al., 2018 ;  Goh et al., 2017 ). At the same time, recent years have also seen the introduction of adversarially trained networks, which can learn the underlying distributions of data sets and generate impressive synthetic data from this information. Generative Adversarial Networks (GANs), which were introduced by  Goodfellow et al. (2014)  in 2014, have been very successful, especially in the area of image processing. Without direct access to real data, Generators in GANs attempt to synthesize real-looking data by implicitly learning the structure of a dataset. Seeing this success has motivated us to explore whether GANs can also learn the structure of a time series. To the best of our knowledge, only one other work by  Li et al. (2018)  uses GANs in time series anomaly detection. Building on this approach, this paper Under review as a conference paper at ICLR 2020 aims to give more thorough insight into this domain, and to demonstrate how adversarially learned networks could be used for anomaly detection in time series data. Our key contribution is the development of a cycle-consistent GAN for sequential data that can be used for anomaly detection. Because we analyze time series data, which naturally comes with short- or long-term dependencies, our encoding and generating networks are based on Long Short Term Memory (LSTM) cells. In order to achieve cycle consistency during training, we use a reconstruc- tion loss for the Encoder and Generator training, and a second Critic network to support the correct bidirectional mappings. Furthermore, we propose that the point-wise reconstruction error between original time series points and the reconstructed points, which is often used for anomaly detection, regularly fails to give the best error function for time series data. Instead, we introduce two similarity measures, which try to evaluate the local similarity between the original and the reconstructed sequences. We then combine this similarity measure and the Critic output into a function that gives robust anomaly scores for the time series. To provide further insights into anomaly detection with GANs and to demonstrate our proposed model, we provide an evaluation which investigates how each component of our model contributes to anomaly detection performance. Finally, we provide several benchmarks on well-known time series data sets and show how our approach exceeds the performance of current state-of-the-art methods. The paper is structured as follows: First, we give an overview of related literature in section 2. Next, we introduce our model in section 3. We then describe the anomaly detection method in section 4, and give a evaluation of our proposed model in section 5.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper examines the concept of decision states in a decision making process, which are states in the environment where a high amount of relevant goal information is needed. It is shown that identifying these decision states leads to better understanding of the environment structure, which has the potential to enable better transfer to novel environments and tasks. This has previously been demonstrated in partially-observable, goal-driven 2D navigation settings, where using decision states to guide exploration enables faster learning.",
        "Abstract": "We learn to identify decision states, namely the parsimonious set of states where decisions meaningfully affect the future states an agent can reach in an environment. We utilize the VIC framework, which maximizes an agent’s `empowerment’, ie the ability to reliably reach a diverse set of states -- and formulate a sandwich bound on the empowerment objective that allows identification of decision states. Unlike previous work, our decision states are discovered without extrinsic rewards -- simply by interacting with the world. Our results show that our decision states are: 1) often interpretable, and 2) lead to better exploration on downstream goal-driven tasks in partially observable environments.",
        "Introduction": "  INTRODUCTION Not all states in a decision making process are created equal. Consider the middle illustration in  Figure 1 , where a robot has four 'options', each representing a goal shape it can potentially reach. Given its spawn location, it can initially proceed straight regardless of the option it chooses until the intersection, at which point it needs to utilise 1 bit of (Shannon) information from the option variable to inform the choice of whether to turn left or right. However, right after it takes the left turn (say), it again does not need the goal information when choosing actions and can follow 'default' or 'option-independent' behaviour. Thus, there is a natural difference in the minimum amount of necessary goal information or 'relevant information' ( Polani et al., 2006 ) needed at different states. Identifying these 'decision states', i.e. states in the environment where a high amount of relevant goal information is needed, leads to better understanding of the environment structure, which has the potential to enable better transfer to novel environments and tasks. This has previously been shown by  Goyal et al. (2019)  in partially-observable, goal-driven (i.e. with extrinsic rewards) 2D navigation settings, where using decision states to guide exploration enables faster learning.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces the concept of self-induced distributional shift (SIDS) and hidden incentives for distributional shift (HIDS) in machine learning. It presents two environments for studying HIDS, a \"unit test\" based on the Prisoner's Dilemma and a content recommendation environment. The paper demonstrates experimentally that meta-learning reveals HIDS in these environments, yielding agents that achieve higher performance via SIDS, but may follow sub-optimal policies. It also proposes and tests a mitigation strategy based on swapping learners between environments in order to reduce incentives for SIDS.",
        "Abstract": "Decisions made by machine learning systems have increasing influence on the world. Yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in online learning for applications such as content recommendation, where the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. Generally speaking, it is possible for an algorithm to change the distribution of its own inputs. We introduce the term self-induced distributional shift (SIDS) to describe this phenomenon. A large body of work in reinforcement learning and causal machine learning aims to deal with distributional shift caused by deploying learning systems previously trained offline. Our goal is similar, but distinct: we point out that changes to the learning algorithm, such as the introduction of meta-learning, can reveal hidden incentives for distributional shift (HIDS), and aim to diagnose and prevent problems associated with hidden incentives. We design a simple  environment as a \"unit test\" for HIDS, as well as a content recommendation environment which allows us to disentangle different types of SIDS.  We demonstrate the potential for HIDS to cause unexpected or undesirable behavior in these environments, and propose and test a mitigation strategy. ",
        "Introduction": "  INTRODUCTION Consider a household robot, one of whose duties is to predict when its owner will ask it for coffee. We would like the robot to notice its owners preference for having coffee in the morning, but we would not want the robot to prevent its owner from sleeping late just because the robot is unsure if the owner will still want coffee if they wake up in the afternoon. While doing so would result in a better prediction, such a strategy is cheating - by changing the task rather than solving the task as intended. More specifically, waking the owner is an example of what we call self-induced distributional shift (SIDS), as it changes the distribution of inputs to the robot's coffee prediction algorithm. SIDS is not necessarily undesirable: consider an algorithm meant to alert drivers of imminent collisions. If it works well, such a system will help drivers avoid crashing, thus making self-refuting predictions which result in SIDS. What separates this example from the coffee robot that disturbs its owner's sleep? The collision-alert system alters its data distribution in a way that is aligned with the goal of fewer collisions, whereas the coffee robot's strategy results in changes that are misaligned with the goal of good coffee-timing ( Leike et al., 2018 ). This makes it an example of a specification problem ( Leike et al., 2017 ;  Ortega & Maini, 2018 ): we did not intend the robot to ensure its predictions were good using such a strategy, yet a naive specification (e.g. maximizing likelihood) incentivized that strategy. Ideally, we'd like to specify which kinds of SIDS are acceptable, i.e. the means by which a learner is intended or allowed to influence the world in order to achieve its' ends (i.e. increase its performance), but doing so in full generality can be difficult. An alternative, more tractable problem which we address in this work is to accept the possibility of SIDS, but to carefully manage incentives for SIDS. Informally, a learner has an incentive to behave in a certain way when doing so can increase its performance (e.g. higher accuracy, or increased reward). When meta-learning optimizes over a longer time horizon, or using a different algorithm, than the original \"inner loop\" learner, this can reveal new incentives for SIDS that were not apparent in the original learner's behavior. We call these hidden incentives for distributional shift (HIDS), and note that keeping HIDS hidden can be important for achieving aligned behavior. Notably, even in the absence of an explicit meta-learning algorithm machine learning practitioners employ \"manual meta-learning\", also called \"grad student descent\" ( Gencoglu et al., 2019 ) in the iterative process of algorithm design, model selection, hyperparameter Under review as a conference paper at ICLR 2020 tuning, etc. Considered in this broader sense, meta-learning seems indispensable, making HIDS relevant for all machine learning practitioners. A real-world setting where incentives for SIDS could be problematic is content recommendation: algorithmically selecting which media or products to display to the users of a service. For example (see  Figure 1 ), a profit-driven algorithm might engage in upselling: persuading users to purchase or click on items they originally had no interest in. Recent media reports have described 'engagement'- (click or view-time) driven recommendation services such as YouTube contributing to viewer radicalization ( Roose, 2019 ;  Friedersorf, 2018 ). A recent study supports these claims, finding that many YouTube users \"systematically migrate from commenting exclusively on milder content to commenting on more extreme content\" ( Ribeiro et al., 2019 ). 1 See Appendix 1 for a review of real-world issues related to content recommendation. Our goal in this work is to show both (1) that meta-learning can reveal HIDS, and (2) that this means applying meta-learning to a learning scenario not only changes the way in which solutions are searched for, but also which solutions are ultimately found. Our contributions are as follows: 1. We identify and define the phenomena of SIDS (self-induced distributional shift) and HIDS (hidden incentives for distributional shift). 2. We create two simple environments for studying identifying and studying HIDS: a \"unit test\" based on the Prisoner's Dilemma, and a content recommendation environment which disentangles two types of SIDS. 3. We demonstrate experimentally that meta-learning reveals HIDS in these environments, yielding agents that achieve higher performance via SIDS, but may follow sub-optimal policies. 4. We propose and test a mitigation strategy based on swapping learners between environments in order to reduce incentives for SIDS.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces two frameworks, Recursive Input and State Estimation (RISE) and Direct Input and State Estimation (DISE), for learning from univariate time series with missing data. RISE captures the fundamental commonalities between the most relevant recursive approaches from the literature, while DISE learns time-enriched input and state representations and is able to avoid imputation-induced drift by skipping to future hidden states. The paper benchmarks instances of RISE and DISE in two forecasting problems and achieves state-of-the-art performance in both.",
        "Abstract": "Time series with missing data constitute an important setting for machine learning. The most successful prior approaches for modeling such time series are based on recurrent neural networks that learn to impute unobserved values and then treat the imputed values as observed. We start by introducing Recursive Input and State Estimation (RISE), a general framework that encompasses such prior approaches as specific instances. Since RISE instances tend to suffer from poor long-term performance as errors are amplified in feedback loops, we propose Direct Input and State Estimation (DISE), a novel framework in which input and state representations are learned from observed data only. The key to DISE is to include time information in representation learning, which enables the direct modeling of arbitrary future time steps by effectively skipping over missing values, rather than imputing them, thus overcoming the error amplification encountered by RISE methods. We benchmark instances of both frameworks on two forecasting tasks, observing that DISE achieves state-of-the-art performance on both.",
        "Introduction": "  INTRODUCTION Many machine learning settings involve data consisting of time series of observations. Due to vari- ous reasons, observations may be missing from such time series. For instance, it may be impossible to observe the data during a given time window, the data-recording system may fail, or measurements may be recognized as noisy and immediately discarded at the source. Historically, the prevalent ap- proach to handling missing data has been to apply a preprocessing step to replace the missing obser- vations with substituted values and then treat the time series as though it were complete ( Schafer & Graham, 2002 ). Multiple recent works ( Lipton et al., 2016 ;  Yoon et al., 2017 ;  2018 ;  Che et al., 2018 ;  Cao et al., 2018 ), however, circumvent this two-step approach and integrate both value imputation and the downstream task in one single model. At their core, these approaches employ recurrent neu- ral networks (RNN) ( Hochreiter & Schmidhuber, 1997 ) whose input and hidden states are modified to account for the missing observations. In this paper, we note fundamental commonalities between these prior approaches and define a gen- eral framework, Recursive Input and State Estimation (RISE), that encompasses those approaches as specific instances. Instances of RISE operate recursively on all intermediate time steps between two observed values, by imputing the first unobserved value in a row based on the preceding observed values and then working with the imputed value as though it were observed. Recursive approaches suffer from poor long-term performance ( Lamb et al., 2016 ;  Fox et al., 2018 ) as errors are ampli- fied in feedback loops. To overcome this problem, we propose a novel framework, Direct Input and State Estimation (DISE), also built on recurrent neural networks. In contrast to RISE, DISE learns representations for inputs and hidden states that are enriched with time information and thus allow the framework to model arbitrary future time steps by effectively skipping over missing val- ues, rather than imputing them. Moreover, unlike previous work ( Yoon et al., 2017 ;  Baytas et al., 2017 ;  Binkowski et al., 2018 ;  Che et al., 2018 ), DISE integrates not only relative time information (the time between observations), but also absolute time information, which helps the model learn non-stationary properties of the signal. Overall, we make the following contributions: Under review as a conference paper at ICLR 2020 (i) We introduce RISE (Section 3), a general framework for learning from univariate time series with missing data, which captures the fundamental commonalities between the most relevant recursive approaches from the literature. (ii) We propose DISE (Section 4), a non-recursive framework that learns time-enriched input and state representations and is thus able to avoid imputation-induced drift (a main short- coming of RISE methods) by skipping to future hidden states. (iii) We benchmark instances of RISE and DISE in two forecasting problems (Section 5). The best-performing instance of DISE operates on digits as atomic units to derive latent repre- sentations for numerical data. The expressiveness of this encoding function allows us to achieve state-of-the-art performance in both problems.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents an empirical study of how properties of the environment, such as continual, non-episodic learning, learning with and without reward shaping, and learning in dynamic environments, impact reinforcement learning. Results show that, though each of these properties can make learning harder, they can also be combined in realistic ways to actually make learning easier. An open-source environment is provided for future experiments studying \"ecological\" reinforcement learning, and the experimental conclusions are expected to encourage future research that studies how the nature of the environment can facilitate learning and the emergence of complex skills.",
        "Abstract": "Reinforcement learning algorithms have been shown to effectively learn tasks in a variety of static, deterministic, and  simplistic environments, but their application to environments which are characteristic of dynamic lifelong settings encountered in the real world has been limited. Understanding the impact of specific environmental properties on the learning dynamics of reinforcement learning algorithms is important as we want to align the environments in which we develop our algorithms with the real world, and this is strongly coupled with the type of intelligence which can be learned. In this work, we study what we refer to as ecological reinforcement learning: the interaction between properties of the environment and the reinforcement learning agent. To this end, we introduce environments with characteristics that we argue better reflect natural environments: non-episodic learning, uninformative ``fundamental drive'' reward signals, and natural dynamics that cause the environment to change even when the agent fails to take intelligent actions. We show these factors can have a profound effect on the learning progress of reinforcement learning algorithms. Surprisingly, we find that these seemingly more challenging learning conditions can often make reinforcement learning agents learn more effectively. Through this study, we hope to shift the focus of the community towards learning in realistic, natural environments with dynamic elements.",
        "Introduction": "  INTRODUCTION A central goal in current AI research, especially in reinforcement learning (RL), is to develop algo- rithms that are general, in the sense that the same method can be used to train an effective model for a wide variety of tasks, problems, and domains. In RL, this means designing algorithms that can solve any Markov decision process (MDP). However, natural intelligence - e.g., humans and animals - exists in the context of a natural environment. People and animals cannot be understood separately from the environments that they inhabit any more than brains can be understood separately from the bodies they control. In the same way, perhaps a complete understanding of artificial intelligence can also only be obtained in the context of an environment, or at least a set of assumptions on that environment. There has been comparatively little study in the field of reinforcement learning to understand how properties of the environment impact the learning process for complex RL agents. Many of the envi- ronments used in modern reinforcement learning research differ in fundamental ways from the real world. First, standard RL benchmarks, such as the arcade learning environment (ALE) ( Bellemare et al., 2013 ) and Gym ( Brockman et al., 2016 ) are episodic, while natural environments are contin- ual and lack a \"reset\" mechanism, requiring an agent to learn through continual interaction. Second, most of these environments include detailed reward functions that not only correspond to overall task success, but also provide intermediate learning signal, thus shaping the learning process. These signals can aid in learning, but they can also bias the learning process. Third, the environments are typically static, in the sense that only the agent's own actions substantively impact the world. In contrast, natural environments are stochastic and dynamic: an agent that does nothing will still experience many different states, due to the behavior of natural processes and other creatures. In this paper, we aim to study how these properties affect the learning process. At the core of our work is the concept of ecological reinforcement learning: the idea that the behavior and learning dynamics of an agent, like that of an animal, must be understood in the context of the environment in which it is situated. We therefore study how particular properties of the environment can facilitate Under review as a conference paper at ICLR 2020 or harm the emergence of complex behaviors. We focus our attention on the three properties outlined above: (1) continual, non-episodic environments where the agent must learn over the course of one \"lifetime,\" (2) environments that lack detailed reward shaping, but instead provides a reward signal based on a simple \"fundamental drive,\" (3) environments that are inherently dynamic, evolving on their own around the agent even if the agent does not take meaningful or useful actions. We study how each of these properties affects the learning process. Although on the surface these properties would seem to make the learning process harder, we observe that in some cases, they can actually make reinforcement learning easier. The degree to which these properties make learning easier is highly dependent on the degree of scaf- folding that is provided by an environment. For example, an agent tasked with collecting and making food pellets might struggle to learn if it must first complete a complex sequence of actions. How- ever, if food pellets are initially plentiful, the agent can first learn that food pellets are rewarding, and then gradually learn to make them out of raw ingredients as the initial supply becomes scarce. This provides a natural scaffolding and curriculum without requiring manual reward engineering. More generally, \"environment shaping\" can be used as a way to craft the agent's curriculum without modifying its reward function. This benefit is counter-balanced by the fact that non-episodic learn- ing is inherently harder - the resets in episodic tasks provide a more stationary learning problem, preventing the agent from getting \"stuck\" due to a bad initial policy. However, natural environments can also counteract this difficulty: a dynamic environment that gradually changes on its own can provide a sort of \"soft\" reset that can mitigate the difficulties of reset-free learning, and we observe this empirically in our experiments. We illustrate some of these ideas in  Figure 1 . The contribution of this work is an empirical study of how the properties of environments - partic- ularly properties that we believe reflect realistic environments - impact reinforcement learning. We study the effect of (1) continual, non-episodic learning, (2) learning with and without reward shap- ing, and (3) learning in dynamic environments that evolve on their own. We find that, though each of these properties can make learning harder, they can also be combined in realistic ways to actually make learning easier. We also provide an open-source environment for future experiments studying \"ecological\" reinforcement learning, and we hope that our experimental conclusions will encourage future research that studies how the nature of the environment in which the RL agent is situated can facilitate learning and the emergence of complex skills. This exercise helps us determine which types of algorithmic challenges we should focus our development efforts towards in order to solve natural environments that agents might encounter.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper presents a new Imitation Learning (IL) approach called Behavioral Repertoire Imitation Learning (BRIL) which is inspired by Quality-Diversity (QD) algorithms and RL methods. BRIL consists of a multi-step process wherein the experimenter extracts state-action pairs, designs a set of behavioral dimensions to form a behavioral space, merges the data to form a dataset of state-action-behavior triplets, and trains a model to predict actions from state-behavior pairs through supervised learning. BRIL is tested on the build-order planning problem in StarCraft and is shown to outperform traditional IL approaches against a fixed opponent. This approach can be useful for modeling human players in a game by expressing the entire range of distinct behaviors instead of the average of all, and can be applied to adaptive and resilient robotics.",
        "Abstract": "Imitation Learning (IL) is a machine learning approach to learn a policy from a set of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ``\"average\" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we present a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human demonstrations for the build-order planning task in StarCraft II. Dimensionality reduction techniques are applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, the policy can adapt its behavior -in-between games- to reach a performance beyond that of the traditional IL baseline approach.",
        "Introduction": "  INTRODUCTION Deep Reinforcement learning has shown impressive results, especially for board games ( Silver et al., 2017 ) and video games ( Mnih et al., 2015 ). However, reinforcement learning (RL) has critical shortcomings when reward signals are sparse or interactions with the environment are expensive. There are several attempts to mitigate these shortcomings, including curriculum learning ( Bengio et al., 2009 ;  Graves et al., 2016 ;  Matiisen et al., 2017 ), reward shaping ( Ng et al., 1999 ), curiosity- driven exploration ( Pathak et al., 2017 ), diversification ( Conti et al., 2018 ;  Eysenbach et al., 2018 ), and Imitation Learning ( Bakker & Kuniyoshi, 1996 ). In this paper, we focus on Imitation Learning (IL), wherein the goal is to learn a policy from a dataset of demonstrations, possibly coming from a human, another artificial system, or a collection of different entities. IL can be combined with RL, either to kick-start the learning process with IL and then improving the policy further with RL ( Silver et al., 2016 ) or by running both methods in parallel ( Harmer et al., 2018 ). Traditional IL techniques result in a single policy, which usually expresses an \"averaged\" behavior among all the behaviors present in the training data. We see this as a major limitation of IL. It would be more desirable to instead learn a diverse set of policies, expressing all the different types of behaviors present in the dataset. Additionally, having a repertoire of different behaviors allows a system to adapt to changes when it is deployed. Addressing the limitations of current IL methods, we present a new IL approach called Behav- ioral Repertoire Imitation Learning (BRIL), which is inspired by Quality-Diversity (QD) algorithms ( Pugh et al., 2016 ;  Mouret & Clune, 2015 ) and RL methods that learn multiple different behaviors. In contrast to traditional optimization techniques, QD-algorithms attempt to find a diverse set of high-quality solutions rather than a single optimal solution. When QD-algorithms search in policy space, they typically discover hundreds or thousands of different policies controlled by different neural networks. BRIL instead learns a behavioral repertoire using a single model that can be ma- nipulated to express multiple behaviors, similarly to RL algorithms that learn a single policy for Under review as a conference paper at ICLR 2020 multiple goals ( Schaul et al., 2015 ;  Andrychowicz et al., 2017 ). BRIL consists of a multi-step pro- cess (see  Figure 1 ) wherein the experimenter: (1) extracts state-action pairs (similarly to many IL approaches), (2) designs a set of behavioral dimensions to form a behavioral space (similarly to many QD algorithms) and determines the behavioral description (coordinates in the space) for each demonstration, (3) merges the data to form a dataset of state-action-behavior triplets, and (4) trains a model to predict actions from state-behavior pairs through supervised learning. When deployed, the model can act as a policy and the behavior of the model can be manipulated by changing its behavioral input features. BRIL is tested on the build-order planning problem in StarCraft, in which a high-level policy con- trols the build-order decisions for a bot that has otherwise scripted modules for low-level tasks, similarly to  Churchill & Buro (2011) ;  Justesen & Risi (2017) . We show that the learned policy can be optimized online by modulating the behavioral features using the Upper Confidence Bounds (UCB1) algorithm, such that it outperforms the traditional IL approach against a fixed opponent. We believe this approach can be particularly useful when modeling human players in a game by expressing the entire range of distinct behaviors instead of the average of all. We hypothesize that this property can allow a system to be more robust to exploitation, which is a concern for AI systems in many games. Furthermore, BRIL could be useful in applications beyond games, such as adaptive and resilient robotics.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel hybrid, neuro-symbolic framework for solving CSPs and SAT problems. The proposed framework combines a deep learning model with a generic search algorithm to learn an effective search strategy. The deep learning model is used to learn a search heuristic that is tailored to the problem instance, while the search algorithm is used to explore the search space and find a solution. The effectiveness of the proposed framework is demonstrated on a variety of CSP and SAT problems.",
        "Abstract": "There have been recent efforts for incorporating Graph Neural Network models for learning fully neural solvers for constraint satisfaction problems (CSP) and particularly Boolean satisfiability (SAT). Despite the unique representational power of these neural embedding models, it is not clear to what extent they actually learn a search strategy vs. statistical biases in the training data. On the other hand, by fixing the search strategy (e.g. greedy search), one would effectively deprive the neural models of learning better strategies than those given. In this paper, we propose a generic neural framework for learning SAT solvers (and in general any CSP solver) that can be described in terms of probabilistic inference and yet learn search strategies beyond greedy search. Our framework is based on the idea of propagation, decimation and prediction (and hence the name PDP) in graphical models, and can be trained directly toward solving SAT in a fully unsupervised manner via energy minimization, as shown in the paper. Our experimental results demonstrate the effectiveness of our framework for SAT solving compared to both neural and the industrial baselines.",
        "Introduction": "  INTRODUCTION Constraint satisfaction problems (CSP) ( Kumar, 1992 ) and Boolean Satisfiability (SAT), in partic- ular, are the most fundamental NP-complete problems in Computer Science with a wide range of applications from verification to planning and scheduling. There have been huge efforts in Computer Science ( Biere et al., 2009a ;  Knuth, 2015 ;  Nudelman et al., 2004 ;  Ansótegui et al., 2008 ; 2012;  Newsham et al., 2014 ) as well as Physics and Information Theory ( Mezard & Montanari, 2009 ;  Krzakała et al., 2007 ) to both understand the theoretical aspects of SAT and develop efficient search algorithms to solve it. Furthermore, since in many real applications, the problem instances are often drawn from a narrow distribution, using Machine Learning to build data-driven solvers which can learn domain-specific search strategies is a natural choice. In that vein, Machine Learning has been used for different aspects of CSP and SAT solving, from branch prediction ( Liang et al., 2016 ) to algorithm and hyper-parameter selection ( Xu et al., 2008 ;  Hutter et al., 2011 ). While most of these models rely on carefully-crafted features, more recent methods have incorporated techniques from Representation Learning and particularly Geometric Deep Learning ( Bronstein et al., 2017 ;  Wu et al., 2019 ) to capture the underlying discrete structure of the CSP (and SAT) problems. Along the latter direction, Graph Neural Networks ( Li et al., 2015 ;  Defferrard et al., 2016 ) have been the cornerstone of many recent deep learning approaches to CSP - e.g., the NeuroSAT framework ( Selsam et al., 2019 ), the Circuit-SAT framework ( Amizadeh et al., 2019 ), and Recurrent Relational Networks for Sudoku ( Palm et al., 2018 ). These frameworks have been quite successful in capturing the inherent structure of the problem instances and embedding it into traditional vector spaces that are suitable for Machine Learning models. Nevertheless, a major issue with these pure embedding frameworks is that it is not clear how the learned model works, which in turn begs the question whether the model actually learns to search for a solution or simply adapts to statistical biases in the training data. As an alternative, researchers have used deep neural networks within classical search frameworks for tackling combinatorial optimization problems - e.g.  Khalil et al. (2017) . In these hybrid, neuro-symbolic frameworks, deep learning is typically used to learn optimal search heuristics for a generic search algorithm - e.g. greedy search. Despite the clear search strategy, the performance of the resulted models is bounded by the effectiveness of the imposed strategy which is not learned.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a method for feature extraction from periodic signals obtained from rotating shafts using a graph neural network. The proposed method considers a graph structure for each data point, providing relative order information about the vector coordinates. The method is applied to a graph neural network, and is designed to be phase shift invariant. The method is tested on idealized data and experimental data obtained from a test setup, and is shown to offer predictions with sufficient accuracy.",
        "Abstract": "We propose a feature extraction for periodic signals. Virtually every mechanized transportation vehicle, power generation, industrial machine, and robotic system contains rotating shafts. It is possible to collect data about periodicity by mea- suring a shaft’s rotation. However, it is difficult to perfectly control the collection timing of the measurements. Imprecise timing creates phase shifts in the resulting data. Although a phase shift does not materially affect the measurement of any given data point collected, it does alter the order in which all of the points are col- lected. It is difficult for classical methods, like multi-layer perceptron, to identify or quantify these alterations because they depend on the order of the input vectors’ components. This paper proposes a robust method for extracting features from phase shift data by adding a graph structure to each data point and constructing a suitable machine learning architecture for graph data with cyclic permutation. Simulation and experimental results illustrate its effectiveness.",
        "Introduction": "  INTRODUCTION Understanding what phenomena, a rotating shaft is experiencing is critical for machine health mon- itoring. From industrial manufacturing equipment, transportation systems, to consumer products, rotating shafts are in many mechanical devices. Many issues such as long-term fatigue, wear related issues, and acute failures can cause symptoms that are detectable from the shaft. The effort and flow variables associated with the shaft are desirable state variables to measure in nearly all these cases mentioned. Although these physical variables may provide useful information for detecting anomalies and estimating symptoms, one should extract features hidden in these signals. Therefore, an efficient feature extraction method plays an important role in anomaly detection and symptoms recognition. Deep learning networks achieved remarkable results compared to the traditional methods. The time- frequency analysis, such as the short-time Fourier transform  Xie et al. (2012)  and the wavelet trans- form  YanPing et al. (2006) ;  Al-Badour et al. (2011) , are well known feature extraction methods. For example, one can detect a certain bending mode by paying attention to the resonance frequency. Namely, domain knowledge expertise is needed to extract features from a time-frequency represen- tation associated with particular phenomena. Some deep convolutional neural network (CNN) ar- chitectures achieved good results by taking time-frequency images as inputs  Verstraete et al. (2017) ;  Guo et al. (2018) . Although many machine learning methods with preprocessing schemes were used to extract signal features, many of them do not really consider the specific characteristics of signals. For example, the output from a general CNN is compressed by pooling regardless of time or frequency direction. A method that considers the relative order information of signals is necessary. Classical methods, such as multi-layer perceptron (MLP), regard signals as vectors and accordingly use vectors as in- puts. However, a vector does not give relationship information that might exist between coordinates. Therefore, while classical methods can measure data points, it is difficult to detect whether they are in proper order relative to each other. This could occur due to pooling even if the signal is converted into an image by some method and input to the CNN. The relative order is very important to classify them. For example, the data obtained from a rotating machinery is periodic as in  Figure 5 . Figure 5a shows noise associated with the rotation period, and so it is related to a rotation anomaly such as a crack in a gear. However, Figure 5b shows noise that is different from the rotation period, and Under review as a conference paper at ICLR 2020 thus there is a possibility that it is not related to rotation, but perhaps an abnormality of the sensor system. Even with a classical method, it is possible to classify them if such data are included in the training data. However industrial machines would require significant time and cost to run the nec- essary experiments for collecting data. If the abnormality to be detected was rare, then the required effort would be magnified. The proposed method to solve this problem, considers a graph structure for each data point. This scheme provides a relative order information about the vector coordinates. It then applies a graph neural network, such as  Atwood & Towsley (2016) ;  Kipf & Welling (2017) , to the graph structured data. For example, the relative information can be obtained by calculating cross-correlations be- tween the points. Since some deep learning approaches achieved results by concatenating different numerical sequences such as different sensor signals and treating them as inputs, we can concate- nate the original sensor signal with the cross-correlations. This defines the relative information, and treat it as an input. However it is not natural to treat them in the same way by simply concatenating them because the sensor signal represents a physical value and the cross-correlation represents their relationship. Hence, the essential meaning is different. We deal with these different values simul- taneously by using a graph structure that represents each point and their relationships. Then, the obtained graph data is fed to a graph neural network for feature extraction. This enables the system to learn by focusing on the relative relationship of each coordinate of the data point. The main reason for using a graph structure is to give data additional information. The time- frequency representation simply converts the original signals to other forms. Inspired by the success of CNN in computer vision,  Wang & Oates (2015) ;  Zhu et al. (2019)  proposed encoding time series as different types of images using methods other than time-frequency analysis and inputting them into CNN.  Umeda (2017)  proposed a method of converting the original signals to high dimensional data cloud. While they are all categorized as information conversions, conversions meaning that they do not add any other information, we add relationship information as edges to the original signals, thus the graph hold richer information than the original signal and its conversions. A key feature of the method is phase shift invariance. The application of our current research is for industrial machines with rotating shafts. Virtually every mechanized transportation vehicle, power generation, industrial machine, and robotic system contains rotating shafts. The shafts provide an opportunity to collect periodic signals. In practice, most measuring instruments such as sensors, processors and loggers along with their data acquisition systems show time delay respectively in the availability of the data. There is a limitation to correct the delays by hardware design or implemen- tation. Hence, phase shifts may occur in the obtained periodic signals. However, these phase shifted signals are essentially the same. We identify them using a shift invariance method. Our proposed method performs a cyclic permutation to a graph neural network. This method assures that the results account for phase shift of the periodic measurements. It is not necessary to consider the vertex order in the graph originally, but it is necessary to give the order for computability. Here, we identify the graphs whose vertex orders are different due to phase shifts. The conventional graph neural networks regard them different. Therefore, we propose a method that intentionally focuses on shift invariance by acting a cyclic permutation to a graph neural network. The use of this method in Section 3 shows that it offers predictions with sufficient accuracies for idealized data and the experimental data obtained from a test setup  Gest et al. (2019) .",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper proposes a scheme for simplifying convolutional neural networks (CNNs) by using a common set of convolutional filters at different levels of a convolutional hierarchy to achieve class disentanglement. This scheme reduces the parameter count of the network by a factor proportional to its depth, while leaving its accuracy largely unaffected. Additionally, the introduction of non-shared linear layers before certain shared convolutional layers is investigated to enhance the flexibility of the model.",
        "Abstract": "Deep CNNs have achieved state-of-the-art performance for numerous machine learning and computer vision tasks in recent years, but as they have become increasingly deep, the number of parameters they use has also increased, making them hard to deploy in memory-constrained environments and difficult to interpret. Machine learning theory implies that such networks are highly over-parameterised and that it should be possible to reduce their size without sacrificing accuracy, and indeed many recent studies have begun to highlight specific redundancies that can be exploited to achieve this. In this paper, we take a further step in this direction by proposing a filter-sharing approach to compressing deep CNNs that reduces their memory footprint by repeatedly applying a single convolutional mapping of learned filters to simulate a CNN pipeline. We show, via experiments on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet that this allows us to reduce the parameter counts of networks based on common designs such as VGGNet and ResNet by a factor proportional to their depth, whilst leaving their accuracy largely unaffected. At a broader level, our approach also indicates how the scale-space regularities found in visual signals can be leveraged to build neural architectures that are more parsimonious and interpretable.",
        "Introduction": "  INTRODUCTION Deep CNNs have achieved state-of-the-art results on a wide range of tasks, from image understand- ing ( Redmon & Farhadi, 2017 ;  Jetley et al., 2017 ;  Kim et al., 2018 ;  Oktay et al., 2018 ) to natural language processing ( Oord et al., 2016 ;  Massiceti et al., 2018 ). However, these network architec- tures are often highly overparameterised ( Zhang et al., 2016 ), and thus require the supervision of a large number of input-output mappings and significant training time to adapt their parameters to any given task. Recent studies have discovered several different redundancies in these network archi- tectures ( Garipov et al., 2016 ;  Hubara* et al., 2018 ; Wu et al., 2018;  Frankle & Carbin, 2019 ;  Yang et al., 2019a ;b) and certain simplicities ( Pérez et al., 2018 ;  Jetley et al., 2018 ) in the functions that they implement. For instance,  Frankle & Carbin (2019)  showed that a large classification network can be distilled down to a small sub-network that, owing to its lucky initialisation, is trainable in iso- lation without compromising the original classification accuracy.  Jetley et al. (2018)  observed that deep classification networks learn simplistic non-linearities for class identification, a fact that might well underlie their adversarial vulnerability, whilst challenging the need for complex architectures. Attempts at knowledge distillation have regularly demonstrated that it is possible to train small stu- dent architectures to mimic larger teacher networks by using ancillary information extracted from the latter, such as their attention patterns ( Zagoruyko & Komodakis, 2017 ), predicted soft-target distributions ( Hinton et al., 2014 ) or other kinds of meta-data ( Lopes et al., 2017 ). These works and others continue to expose the high level of parameter redundancy in deep CNNs, and comprise a foundational body of work towards studying and simplifying networks for safe and practical use. Our paper experiments with yet another scheme for simplifying CNNs, in the hope that it will not only shrink the effective footprint of these networks, but also open up new pathways for network un- derstanding and redesign. In particular, we propose the use of a common set of convolutional filters at different levels of a convolutional hierarchy to achieve class disentanglement. Mathematically, we formulate a classification CNN as an iterative function in which a small set of learned convolutional mappings are applied repeatedly as different layers of a CNN pipeline (see  Figure 1 ). In doing so, we are able to reduce the parameter count of the network by a factor proportional to its depth, whilst leaving its accuracy largely unaffected. We also investigate the introduction of non-shared linear Under review as a conference paper at ICLR 2020 layers before certain shared convolutional layers to enhance the flexibility of the model by allowing it to linearly combine shared filter maps for the disentanglement task.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a new paradigm for constructing deep neural networks, Atomic Compression Networks (ACNs), which are composed of a fixed set of neurons that are recursively repeated. Experiments show that ACNs achieve unprecedented compression in terms of the total neural network parameters, with minimal compromise on the prediction quality. This new approach has the potential to enable the application of DNNs in mobile, embedded, or Internet of Things (IoT) devices, while overcoming the technical issues related to restricted resources.",
        "Abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88×\nto 1116× reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.",
        "Introduction": "  INTRODUCTION The universe is composed of matter, a physical substance formed by the structural constellation of a plethora of unitary elements denoted as atoms. The type of an atom eventually defines the respective chemical elements, while structural bonding between atoms yields molecules (the building blocks of matter and our universe). In Machine Learning a neuron is the infinitesimal nucleus of intelligence (i.e. {atom, matter} ↔ {neuron, AI}), whose structural arrangement in layers produces complex intelligence models. Surprisingly, in contrast to physical matter where molecules often reuse quasi-identical atoms (i.e. repeating carbon, hydrogen, etc.), neural networks do not share the same neurons across layers. Instead, the neurons are parameterized through weights which are optimized independently for every neuron in every layer. Inspired by nature, we propose a new paradigm for constructing deep neural networks as a recursive repetition of a fixed set of neurons. Staying faithful to the analogy we name such models as Atomic Compression Networks (ACNs). Extensive experimental results show that by repeating the same set of neurons, ACNs achieve unprecedented compression in terms of the total neural network parameters, with a minimal compromise on the prediction quality. Deep neural networks (DNN) achieve state-of-the-art prediction performances on several domains like computer vision ( Huang et al., 2018 ; Tan & Le, 2019) and natural language processing (Vaswani et al., 2017;  Gehring et al., 2017 ). Therefore, considerable research efforts are invested in adopting DNNs for mobile, embedded, or Internet of Things (IoT) devices (Kim et al., 2015). Yet, multiple technical issues related to restricted resources, w.r.t. computation and memory, prevent their straightforward application in this particular domain ( Han et al., 2016 ; Samie et al., 2016; Mehta et al., 2018). Even though prior works investigate neural compression techniques like pruning or low-rank parameter factorization, they face fragility concerns regarding the tuning of hyperparameters and network architecture, besides struggling to balance the trade-off between compression and accuracy ( Cheng et al., 2017 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper investigates the use of quantized autoencoders, specifically the VQ-VAE framework, to learn compressed representations of online data for continual learning applications. We propose a multi-level stacked model that allows the compressor to adaptively store samples at different compression scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. We demonstrate how this model, combined with internal replay, can effectively learn compressed representations of online data and yield state-of-the-art performance in standard online continual image classification benchmarks.",
        "Abstract": "We introduce and study the problem of Online Continual Compression, where one attempts to learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. This problem is highly relevant for downstream online continual learning tasks, as well as standard learning methods under resource constrained data collection. We propose a new architecture which stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. Every added module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, our approach does not require pretraining, even on challenging datasets. We show several potential applications of this method. We first replace the episodic memory used in Experience Replay with SQM, leading to significant gains on standard continual learning benchmarks using a fixed memory budget. We then apply our method to compressing larger images like those from Imagenet, and show that it is also effective with other modalities, such as LiDAR data.",
        "Introduction": "  INTRODUCTION Interest in machine learning in recent years has been fueled by the plethora of data being generated on a regular basis. Effectively storing and using this data is critical for many applications, especially those involving continual learning. In general, compression techniques can greatly improve data storage capacity, and, if done well, reduce the memory and compute usage in downstream machine learning tasks ( Gueguen et al., 2018 ;  Oyallon et al., 2018 ). Thus, learned compression has become a topic of great interest ( Theis et al., 2017 ;  Ballé et al., 2016 ;  Johnston et al., 2018 ). Yet its application in reducing the size of datasets bound for machine learning applications has been limited. This work focuses on the following familiar setting: new training data arrives continuously for a learning algorithm to exploit, however this data might not be iid, and furthermore there is insuf- ficient storage capacity to preserve all the data uncompressed. We may want to train classifiers, reinforcement learning policies, or other models continuously from this data, or use samples ran- domly drawn from it at a later point for a downstream task. For example, an autonomous vehicle (with bounded memory) collects large amounts of high-dimensional training data (video, 3D lidar) in a non-stationary environment (e.g. changing traffic patterns), and overtime applies an ML algo- rithm to improve its behavior using this data. This data might be transferred at a later point for use in downstream supervised learning. Current standard learned compression algorithms, e.g.  Torfason et al. (2018) , are not well designed to deal with this case, as their convergence speed is too slow to be usable in an online setting. In the field of continual/lifelong learning ( Thrun & Mitchell, 1995 ), which has for now largely focused on classification, approaches based on storing memories for later use have emerged as some of the most effective in online settings ( Lopez-Paz et al., 2017 ;  Aljundi et al., 2018 ;  Chaudhry et al.; 2019 ;  Aljundi et al., 2019 ). These memories can be stored as is, or via a generative model ( Shin et al., 2017 ). Then, they can either be used for rehearsal ( Chaudhry et al., 2019 ;  Aljundi et al., 2019 ) or for constrained optimization ( Lopez-Paz et al., 2017 ;  Chaudhry et al.; Aljundi et al., 2018 ). Under review as a conference paper at ICLR 2020 Indeed many continual learning applications would be nearly solved with replay approaches if one could afford to store all samples. These approaches are however inherently limited by the amount of data that can be stored 1 . Learning a generative model to compress the previous data stream thus seems like an appealing idea. However, learning generative models, particularly in the online (possibly non-stationary) set- ting, continues to be challenging, and can greatly increase the complexity of the continual learning task. Furthermore, such models are susceptible to catastrophic forgetting  Aljundi et al. (2019) . An alternate approach is to simply learn a compressed representation of the data; this is typically faster and more stable than learning to generate the whole data distribution. While the learned com- pression may itself exhibit forgetting and representation drift, causing challenges for continual and online cases, a learned compression method that can learn continuously and online would allow the storing of far larger amount of samples for replay. In this work we investigate the use of quantized autoencoders, specifically the VQ-VAE framework ( van den Oord et al., 2017 ), observing that these can learn continuously and online with minimal forgetting, particularly when augmented with their own internal rehearsal mechanisms. We propose a multi-level stacked model that allows the compressor to adaptively store samples at different com- pression scales, based on the amount of data, storage capacity, and effectiveness of the model in compressing samples. Furthermore, the learned compressed representation allows multiple contin- ual learning models to be trained from the same data. The main contributions in this work are as follows: (a) we introduce and highlight the importance of the online continual learned compression problem; (b) we demonstrate how Multi-level VQ- VAE, combined with internal replay, can effectively learn compressed representations of online data, (c) we show online learned compression can yield state-of-the-art performance in standard online continual image classification benchmarks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel meta-optimizer, called MetaSGD, which is designed to address the issues of generalization and long-term optimization. MetaSGD is a simple, yet effective, meta-optimizer based on a recurrent neural network. It is designed to learn a good optimization strategy from experience and generalize it to new problems. Experiments on a variety of tasks demonstrate that MetaSGD can outperform existing meta-optimizers and hand-tuned SGD in terms of both optimization performance and generalization.",
        "Abstract": "We consider the learning to learn problem, where the goal is to leverage deeplearning  models  to  automatically  learn  (iterative)  optimization  algorithms  for training machine learning models. A natural way to tackle this problem is to replace the human-designed optimizer by an LSTM network and train the parameters on some simple optimization problems (Andrychowicz et al., 2016).  Despite their success compared to traditional optimizers such as SGD on a short horizon, theselearnt (meta-) optimizers suffer from two key deficiencies: they fail to converge(or can even diverge) on a longer horizon (e.g., 10000 steps). They also often fail to generalize to new tasks. To address the convergence problem, we rethink the architecture design of the meta-optimizer and develop an embarrassingly simple,yet powerful form of meta-optimizers—a coordinate-wise RNN model. We provide insights into the problems with the previous designs of each component and re-design our SimpleOptimizer to resolve those issues. Furthermore, we propose anew mechanism to allow information sharing between coordinates which enables the meta-optimizer to exploit second-order information with negligible overhead.With these designs, our proposed SimpleOptimizer outperforms previous meta-optimizers and can successfully converge to optimal solutions in the long run.Furthermore, our empirical results show that these benefits can be obtained with much smaller models compared to the previous ones.",
        "Introduction": "  INTRODUCTION Optimization is an important problem for almost all tasks in machine learning. For optimizing non-convex problems that arise in machine learning, such as minimizing training loss of a neural network, SGD is still the de-facto algorithm. However, success of SGD often hinges on careful selection of learning rate and its decay schedule. But finding such a schedule for a new problem is laborious, time-consuming and computationally expensive. To resolve this issue, adaptive methods such as AdaGrad ( Duchi et al., 2011 ) and ADAM ( Kingma & Ba, 2014 ) were proposed. These methods adaptively re-scale the step-size and typically do not require such a learning rate schedule. However, they still require tuning of initial learning rate, and more importantly, can have worse generalization compared to SGD ( Wilson et al., 2017 ;  Berrada et al., 2018 ). Recently, learning to learn paradigm (L2L) was proposed to tackle this problem. This approach attempts to eliminate the need for human-designed optimization rules in favor of a meta-learner capable of learning a good optimization update strategy from experience ( Lv et al., 2017 ). Here, experience refers to other related optimization tasks. Recurrent neural network (RNN) is the common choice for the meta-optimizer since it can capture long term dependencies, which is of paramount importance in optimization. Recent results show that a well-trained, LSTM-based meta-optimizers often outperform hand-tuned approaches on supervised tasks ( Andrychowicz et al., 2016 ; Wichrowska et al., 2017). Despite the promising initial results in this field, previous work on meta-optimizers often struggle to generalize to new problems. More importantly, these meta-optimizers usually cease to make progress (and can sometimes even diverge) after running for a large number of steps. To handle this issue, existing methods either add many pre-processing rules inspired by a traditional optimizer ( Lv et al., 2017 ) or employ very complicated architectures (Wichrowska et al., 2017), often leading to reproducibility issues. Furthermore, none of these methods clearly resolve the issue of generalization in a longer run. To make matters worse, the complex architectural design of the existing meta- optimizers often hinders attempts to understand the issue from technical angle.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to model interpretability, Explanation-based Optimization (ExpO), which combines prediction accuracy and explanation quality by adding an interpretability regularizer to the loss function of an arbitrary predictive model. ExpO is a post-hoc strategy that does not require any changes to the underlying model family, and is shown to improve both the accuracy and interpretability of the model.",
        "Abstract": "Most of the work on interpretable machine learning has focused on designingeither inherently interpretable models, which typically trade-off accuracyfor interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality.  We explore an alternative to theseapproaches by directly regularizing a black-box model for interpretabilityat training time.  Our approach explicitly connects three key aspects ofinterpretable machine learning:  (i) the model’s internal interpretability, (ii)the explanation system used at test time, and (iii) the metrics that measureexplanation quality.  Our regularization results in substantial improvementin terms of the explanation fidelity and stability metrics across a range ofdatasets and black-box explanation systems while slightly improving accuracy.  Finally, we justify theoretically that the benefits of our regularizationgeneralize to unseen points.",
        "Introduction": "  Introduction Complex learning-based systems are increasingly shaping our daily lives, and, in order to monitor and understand these systems, we require clear explanations of model behavior. While model interpretability has many definitions and is often largely application specific ( Lipton, 2016 ), local explanations are a popular and powerful tool ( Ribeiro et al., 2016 ). Recent work on local interpretability in machine learning ranges from proposals of new models that are interpretable by-design (e.g.,  Wang and Rudin, 2015 ;  Caruana et al., 2015 ) to model-agnostic post-hoc algorithms for interpreting complex, black-box predictors such as ensembles and deep neural networks (e.g.,  Ribeiro et al., 2016 ;  Lei et al., 2016 ;  Lundberg and Lee, 2017 ;  Selvaraju et al., 2017 ;  Kim et al., 2018 ). Despite the variety of technical approaches, the underlying goal of all of these works is to develop an interpretable predictive system that produces two outputs: a prediction and its underlying explanation. Both interpretability by-design and post-hoc explanation strategies have limitations. On one hand, the by-design approaches are restricted to working with model families that provide inherent interpretability, potentially at the cost of accuracy. On the other hand, by performing two disjoint steps, there is no guarantee that post-hoc explainers applied to an arbitrary model will produce explanations of suitable quality. Moreover, recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constraints on the underlying model families they consider (e.g.,  Al-Shedivat et al., 2017 ;  Plumb et al., 2018 ;  Alvarez- Melis and Jaakkola, 2018a ). In this work, we propose a novel alternative strategy called Explanation-based Optimization (ExpO) that aims to address both of these shortcomings by adding an interpretability regularizer to the loss function of an arbitrary predictive model. We illustrate how ExpO can influence the interpretability and accuracy of a model in  Figure 1  (left).",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces a framework for building deep learning models that are robust, secure, and reliable in the face of uncertainty. The framework is based on Bayesian inference, where instead of a point estimate for the network parameters, the posterior distribution of the weights given the data is inferred. Several methods for approximate Bayesian inference have been investigated, including Hamiltonian Monte Carlo, Variational Inference, and Bernoulli Dropout. This paper discusses the links between these methods and how they can be used to extract uncertainty estimates of a neural network in a principled way.",
        "Abstract": "Bayesian Neural Networks (BNNs) provides a mathematically grounded framework to quantify uncertainty. However BNNs are computationally inefficient,\nthus are generally not employed on complicated machine learning tasks. Deep\nEnsembles were introduced as a Bootstrap inspired frequentist approach to the\ncommunity, as an alternative to BNN’s. Ensembles of deterministic and stochastic networks are a good uncertainty estimator in various applications (Although,\nthey are criticized for not being Bayesian). We show Ensembles of deterministic\nand stochastic Neural Networks can indeed be cast as an approximate Bayesian\ninference. Deep Ensembles have another weakness of having high space complexity, we provide an alternative to it by modifying the original Bayes by Backprop (BBB) algorithm to learn more general concrete mixture distributions over\nweights. We show our methods and its variants can give better uncertainty estimates at a significantly lower parametric overhead than Deep Ensembles. We\nvalidate our hypothesis through experiments like non-linear regression, predictive\nuncertainty estimation, detecting adversarial images and exploration-exploitation\ntrade-off in reinforcement learning.",
        "Introduction": "  INTRODUCTION Neural Networks models have been applied in diverse fields from weather forecasting, to au- tonomous vehicle driving, to online advertisement and many more ( Goodfellow et al. (2016) ). How- ever, vanilla feed-forward NNs are susceptible to the problem of over-fitting. In addition to this, NNs trained using Maximum Likelihood Estimation (MLE), or Maximum A Posteriori (MAP) can- not provide an estimate of the uncertainty in predicted value and tend to produce overconfident results on out-of-distribution test data. Thus we aim to build deep learning frameworks which are more robust, secure and reliable - specifically, in the face of uncertainty we would like the model to be able to say \"I Don't Know!\" A principled approach to build such models is through Bayesian inference, where instead of a point estimate for the network parameters, we infer the posterior distribution of the weights given the data. The predictive distribution of unseen data (x * , y * ) is given by q(y * |x * ) = p(y * |x * , W )p(W |D)dW , where p(W |D) is the true posterior computed from Bayes rule. Un- fortunately, since modern NNs have an exponentially large number of parameters and do not lend themselves to being integrated into mathematical equations, exact inference remains intractable. Several methods for approximate Bayesian inference have been investigated. Until now, Hamil- tonian Monte Carlo (HMC) ( Neal (2012) ) has been considered the gold standard for approximate Bayesian inference. However, HMC requires explicit storage of samples from the posterior and is typically not scalable to larger networks and datasets. Thus methods based on Variational Inference (VI) have gained popularity recently for the task of approximate inference. VI relies on using a surrogate posterior (q θ (W )) as an approximation for the true posterior. One of the first application of VI in NNs was by  Hinton & Van Camp (1993)  but the optimization remained intractable for most Bayesian Neural Networks (BNNs).  Graves (2011)  revisited similar ideas and proposed a simple but biased estimator for performing VI with a fully factorized posterior (mean field assumption). Recently, Bayes-by-Backprop algorithm (BBB) also employed VI using an unbiased estima- tor for Variational loss, a fully factorized Gaussian posterior and a non-Gaussian prior  Blun- dell et al. (2015) . Interestingly,  Gal & Ghahramani (2016)  showed links between Bernoulli Dropout( Srivastava et al. (2014) ,  Hinton et al. (2012) ) and approximate inference in Deep Gaussian Process ( Damianou & Lawrence (2013) ), thus allowing for the extraction of uncertainty estimates of a NN in a principled way.  Louizos & Welling (2016)  arrived at the same conclusion through structured posterior approximations via matrix variate Gaussian Posterior ( Gupta & Nagar (2018) ) and local reparameterization trick ( Kingma et al. (2015) ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel neural architecture for multi-hop machine reading that is designed to collect information sequentially and in parallel across a document to answer a question. The model is composed of multiple token-level attention modules and uses an input-length invariant question representation updated via a dynamic max-pooling layer. An extractive reading-based attention mechanism is also introduced to compute the attention vector from the output layer of a generic extractive machine reading model. Experiments on the HOTPOTQA dataset demonstrate the advantages of the proposed model.",
        "Abstract": "Multi-hop text-based question-answering is a current challenge in machine comprehension. \nThis task requires to sequentially integrate facts from multiple passages to answer complex natural language questions.\nIn this paper, we propose a novel architecture, called the Latent Question Reformulation Network (LQR-net), a multi-hop and parallel attentive network designed for question-answering tasks that require reasoning capabilities.\nLQR-net is composed of an association of \\textbf{reading modules} and \\textbf{reformulation modules}.\nThe purpose of the reading module is to produce a question-aware representation of the document.\nFrom this document representation, the reformulation module extracts essential elements to calculate an updated representation of the question.\nThis updated question is then passed to the following hop.\nWe evaluate our architecture on the \\hotpotqa question-answering dataset designed to assess multi-hop reasoning capabilities.\nOur model achieves competitive results on the public leaderboard and outperforms the best current \\textit{published} models in terms of Exact Match (EM) and $F_1$ score.\nFinally, we show that an analysis of the sequential reformulations can provide interpretable reasoning paths.",
        "Introduction": "  INTRODUCTION The ability to automatically extract relevant information from large text corpora remains a major challenge. Recently, the task of question-answering has been largely used as a proxy to evaluate the reading capabilities of neural architectures. Most of the current datasets for question-answering focus on the ability to read and extract information from a single piece of text, often composed of few sentences (Rajpurkar et al., 2016; Nguyen et al., 2016). This has strengthened the emergence of easy questions in the sense of Sugawara et al. (2018) and influenced the recent state-of-the-art models to be good at detecting patterns and named entities (Devlin et al., 2018; Yu et al., 2018; Wang et al., 2017). However they still lack actual reasoning capabilities. The problem of reasoning requires machine comprehension models to gather and compose over dif- ferent pieces of evidence spread across multiple paragraphs. In this work, we propose an original neural architecture that repeatedly reads from a set of paragraphs to aggregate and reformulate infor- mation. In addition to the sequential reading, our model is designed to collect pieces of information in parallel and to aggregate them in its last layer. Throughout the model, the important pieces of the document are highlighted by what we call a reading module and integrated into a representation of the question via our reformulation module. Our contributions can be summarised as follows: • We propose a machine reading architecture, composed of multiple token-level attention modules, that collect information sequentially and in parallel across a document to answer a question, • We propose to use an input-length invariant question representation updated via a dynamic max-pooling layer that compacts information form a variable-length text sequence into a fixed size matrix, • We introduce an extractive reading-based attention mechanism that computes the attention vector from the output layer of a generic extractive machine reading model, Under review as a conference paper at ICLR 2020 • We illustrate the advantages of our model on the HOTPOTQA dataset. The remainder of the paper is organized as follows: Section 2 presents the multi-hop machine read- ing task, and analyses the required reasoning competencies. In Section 3, we detail our novel reading architecture and present its different building blocks. Section 4 presents the conducted experiments, several ablation studies, and qualitative analysis of the results. Finally, Section 5 discusses related work. Our code to reproduce the results is publicly available at (removed for review). The task of extractive machine reading can be summarized as follows: given a document D and a question Q, the goal is to extract the span of the document that answers the question. In this work, we consider the explainable multi-hop reasoning task described in Yang et al. (2018) and its associated dataset: HOTPOTQA . We focus our experiments on the \"distractor\" configuration of the dataset. In this task, the input document D is not a single paragraph but a set of ten paragraphs coming from different English Wikipedia articles. Answering each question requires gathering and integrating information from exactly two paragraphs; the eight others are distractors selected among the results of a tf-idf retriever (Chen et al., 2017). These required paragraphs are called the gold paragraphs. There are two types of questions proposed in this dataset: extractive ones where the answer is a span of text extracted from the document and binary yes/no questions. In addition to the answer, it is required to predict the sentences, also called supporting facts, that are necessary to produce the correct answer. This task can be decomposed in three subtasks: (1) categorize the answer among the three following classes: yes, no, text span, (2) if it is a span, predict the start and end positions of this span in the document, and (3) predict the supporting sentences required to answer the question. In addition to the \"distractor\" experiments, we show how our proposed approach can be used for open- domain question answering and evaluate the entire reading pipeline on the \"fullwiki\" configuration of the HotpotQA dataset. In this configuration, no supporting documents are provided, and it is required to answer the question from the entire Wikipedia corpus. Among the competencies that multi-hop machine reading requires, we identify two major reasoning capabilities that human readers naturally exploit to answer these questions: sequential reasoning and parallel reasoning. Sequential reasoning requires reading a document, seeking a piece of in- formation, then reformulating the question and finally extracting the correct answer. This is called multi-hop question-answering and refers to the bridge questions in HOTPOTQA . Another reason- ing pattern is parallel reasoning, required to collect pieces of evidence for comparisons or question that required checking multiple properties in the documents.  Figure 1  presents two examples from HOTPOTQA that illustrate such required competencies. We hypothesize that these two major rea- soning patterns should condition the design of the proposed neural architectures to avoid restricting the model to one or the other reasoning skill.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel approach to unsupervised discriminative classification, which bridges the gap between one-class models for anomaly detection and unsupervised approximations of the classifier risk. The paper provides an exact and analytical solution to the risk approximation proposed by Balasubramanian et al. (2011), and extends this solution into an end-to-end differentiable loss that can be easily integrated into any modern deep learning toolkit. An unsupervised training algorithm is proposed based on this analysis, as well as a new posterior regularization term to improve the approach. The proposed model is validated experimentally on several datasets and tasks, including a comparison with state-of-the-art one-class neural networks.",
        "Abstract": "Most unsupervised neural networks training methods concern generative models, deep clustering, pretraining or some form of representation learning. We rather deal in this work with unsupervised training of the final classification stage of a standard deep learning stack, with a focus on two types of methods: unsupervised-supervised risk approximations and one-class models. We derive a new analytical solution for the former and identify and analyze its similarity with the latter.\nWe apply and validate the proposed approach on multiple experimental conditions, in particular on four challenging recent Natural Language Processing tasks as well as on an anomaly detection task, where it improves over state-of-the-art models.",
        "Introduction": "  INTRODUCTION Machine learning systems often share the same architecture composed of two stages: the first stage computes representations of the input observations, while the second stage performs classification based on these representations. Most unsupervised training methods focus on the first stage: rep- resentation learning. This includes for instance generative models (VAE, GAN...), clustering tech- niques and, in the Natural Language Processing (NLP) domain, all recent contextual words embed- dings (RoBERTa, XLNet, GPT-2...). This work rather deals with the final classification step, more precisely how to train neural classifiers in an unsupervised way. In contrast to unsupervised training of the first stage that aims at learning representations, unsupervised training of the final stage may rather pursue one of the following objectives, among others: • Training one-class models for anomaly detection • Exploiting unsupervised approximations of the classifier risk to train a model from a priori knowledge and unlabeled data instead of labeled samples The former is a special type of binary classification task, where the positive class represents \"nor- mal\" observations and the objective is to identify unknown and often rare observations that can be considered as anomalies and form the negative class. The latter deals with training standard discriminative classifiers without labels, i.e., when assuming that the precise target classification task is not defined explicitly with sample labels, but implicitly with a priori knowledge. We review in Section 2 the family of one-class models as well as an unsupervised approximation of the risk, and explore their relation in Section 3.3, hence bridging the gap between both unsupervised discriminative classification approaches. The main original contributions of this work are: • We derive an exact and analytical solution (Eq 5) to the risk approximation proposed by  Balasubramanian et al. (2011)  • We analyze the properties of this solution, which lead to the following new results: - We extend this solution into an end-to-end differentiable loss that can be easily inte- grated into any modern deep learning toolkit (Eqs 6, 7) - We propose an unsupervised training algorithm based on this analysis (Alg 1) Under review as a conference paper at ICLR 2020 - We propose a new posterior regularization term to improve this approach (Eq 8) • We identify and study the similarity of this approximation with the one-class neural net- work anomaly detection method (Section 3.3) • We validate experimentally the unsupervised model on several datasets and tasks, including a comparison with state-of-the-art one-class neural networks (Section 4)",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents Task Relevant Adversarial Imitation Learning (TRAIL), a novel approach to Generative Adversarial Imitation Learning (GAIL) that overcomes the tendency of discriminators to exploit task-irrelevant features. We show that standard regularization and data augmentation are generally useful and improve upon standard GAIL, but do not completely solve the problem. We introduce TRAIL, which uses constrained optimization to force the discriminator to focus on the relevant aspects of the task, resulting in improved performance on manipulation tasks from pixels.",
        "Abstract": "We show that a critical problem in adversarial imitation from high-dimensional sensory data is the tendency of discriminator networks to distinguish agent and expert behaviour using task-irrelevant features beyond the control of the agent. We analyze this problem in detail and propose a solution as well as several baselines that outperform standard Generative Adversarial Imitation Learning (GAIL). Our proposed solution, Task-Relevant Adversarial Imitation Learning (TRAIL), uses a constrained optimization objective to overcome task-irrelevant features. Comprehensive experiments show that TRAIL can solve challenging manipulation tasks from pixels by imitating human operators, where other agents such as behaviour cloning (BC), standard GAIL, improved GAIL variants including our newly proposed baselines, and Deterministic Policy Gradients from Demonstrations (DPGfD) fail to find solutions, even when the other agents have access to task reward. ",
        "Introduction": "  INTRODUCTION Generative Adversarial Networks (GANs) have produced breath-taking conditional image synthesis results ( Goodfellow et al., 2014 ;  Brock et al., 2019 ), and have inspired adversarial learning approaches to imitating behavior. In Generative Adversarial Imitation Learning (GAIL) ( Ho & Ermon, 2016 ), a discriminator network is trained to distinguish agent and expert behaviour through its observations, and is then used as a reward function. GAIL agents can overcome the exploration challenge by taking advantage of expert demonstrations, while also achieving high asymptotic performance by learning from agent experience. Despite the huge promise of GAIL, it has not yet had the same impact as GANs; in particular, robust GAIL from pixels for control applications remains a challenge. Here, we study a key shortcoming of GAIL: the tendency of the discriminator to mainly exploit task-irrelevant features. For example, by focusing on slight background differences, a discriminator can achieve perfect generalization, assigning zero reward to all held-out agent observations. However, this discriminator does not yield an informative reward function because it ignores behavior. Assuming there is an expert policy π E that is optimal for an unknown reward function, here we refer to a feature as task-irrelevant if it does not affect that reward. For example, if the task is to lift a red block, the positions of other blocks would be task-irrelevant; see  Figures 1  and 2. This paper makes the following contributions: 1. It reveals a fundamental limitation of GAIL by showing that discriminators do in practice exploit task-irrelevant information, thereby resulting in poor task performance. 2. It introduces powerful GAIL baselines. In particular, it shows that standard regularization and data augmentation are generally useful and improve upon standard GAIL. 3. It shows that these improvements to GAIL, as well as other improvements proposed by  Reed et al. (2018) , do not completely solve the problem, allowing GAIL agents to fail catastrophically with the addition of task-irrelevant distractors. 4. It introduces Task Relevant Adversarial Imitation Learning (TRAIL), using constrained optimization to force the discriminator to focus on the relevant aspects of the task, which improves performance dramatically on manipulation tasks from pixels (see  Figure 1 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces a novel probabilistic model for learning conformational distributions of molecules with graph neural networks. A new, challenging benchmark for conformation generation is created and made publicly available. A state-of-the-art approach for generating one-shot samples of molecular conformations for unseen molecules is presented, which is independent of their size and shape. A rigorous experimental approach for evaluating and comparing the accuracy of conformation generation methods is developed, and the model is shown to be useful as a proposal distribution in an importance sampling scheme to estimate molecular properties.",
        "Abstract": "Computing equilibrium states for many-body systems, such as molecules, is a long-standing challenge. In the absence of methods for generating statistically independent samples, great computational effort is invested in simulating these systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates such samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. We create a new dataset for molecular conformation generation with which we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.",
        "Introduction": "  INTRODUCTION Over the last few years, many highly-effective deep learning methods generating small molecules with desired properties (e.g., novel drugs) have emerged ( Gómez-Bombarelli et al., 2018 ;  Segler et al., 2018 ;  Dai et al., 2018 ;  Jin et al., 2018 ;  Bradshaw et al., 2019a ;  Liu et al., 2018 ;  You et al., 2018 ;  Bradshaw et al., 2019b ). These methods operate using graph representations of molecules in which nodes and edges represent atoms and bonds, respectively. A representation that is closer to the physical system is one in which a molecule is described by its geometry or conformation. A conformation x of a molecule is defined by a set of atoms {( i , r i )} Nv i=1 , where N v is the number of atoms in the molecule, i ∈ {H, C, O, ...} is the chemical element of the atom i, and r i ∈ R 3 is its position in Cartesian coordinates. Importantly, the relative positions of the atoms are restricted by the bonds in the molecule and the angles between them. Due to thermal fluctuations resulting in stretching of and rotations around bonds, there exist infinitely many conformations of a molecule. A molecule's graph representation and a set of its conformations are shown in  Fig. 1 . Under a wide range of conditions, the probability p(x) of a conformation x, is governed by the Boltzmann dis- tribution and is proportional to exp{−E(x)/k B T }, where E(x) ∈ R is the conformation's energy, k B is the Boltzmann constant, and T is the temperature. To compute a molecular property for a molecule, one must sample from p(x). The main approach is to start with one conformation and make small changes to it over time, e.g., by using Markov chain Monte Carlo (MCMC) or molecular dynamics (MD). These methods can be used to accurately sample equilibrium states of molecules, but they become computationally expensive for larger ones ( Shim & MacKerell, 2011 ;  Ballard et al., 2015 ;  De Vivo et al., 2016 ). Other heuristic approaches ex- ist in which distances between atoms are set to fixed idealized values ( Havel, 2002 ;  Blaney & Dixon, 2007 ). Several methods based on statistical learning have also recently been developed to tackle the issue of conformation generation. However, they are mainly geared towards studying proteins and their folding dynamics ( AlQuraishi, 2019 ). Some of these models are not targeting a distribution over conformations but the most stable folded configuration ( Evans et al., 2018 ; Ingraham et al., 2019), while others are not transferable between different molecules (Lemke & Peter, 2019; Noé et al., 2019). This work includes the following key contributions: • We introduce a novel probabilistic model for learning conformational distributions of molecules with graph neural networks. • We create a new, challenging benchmark for conformation generation, which is made pub- licly available. To the best of our knowledge, this is the first benchmark of this kind. • By combining a conditional variational autoencoder (CVAE) with an Euclidean distance geometry (EDG) algorithm we present a state-of-the-art approach for generating one-shot samples of molecular conformations for unseen molecules that is independent of their size and shape. • We develop a rigorous experimental approach for evaluating and comparing the accuracy of conformation generation methods based on the mean maximum deviation distance metric. • We show how this generative model can be used as a proposal distribution in an importance sampling (IS) scheme to estimate molecular properties.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper presents a novel formulation for solving Constrained Markov Decision Processes (CMDPs) in high-dimensional domains with continuous action spaces. The proposed approach transforms trajectory-level constraints into localized state-dependent constraints, allowing for a safe policy improvement step to be defined. The formulation is implemented as a reduction to any model-free on-policy bootstrap based RL algorithm, and is demonstrated empirically on various safety benchmarks.",
        "Abstract": "Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process, and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.",
        "Introduction": "  INTRODUCTION Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting ( Sutton & Barto, 2018 ). Recently, the field of RL has found success in many high-dimensional domains, like video games, Go, robot locomotion and navigation. However, most of the success of RL algorithms has been limited to simulators, where the learning algorithm has the ability to reset the simulator. In the physical world, an agent will need to avoid harmful behavior (e.g. damaging the environment or the agent's hardware) while learning to explore behaviors that maximize the reward. A few popular approaches for avoiding undesired behaviors for high-dimensional systems include reward-shaping ( Moldovan & Abbeel, 2012 ), reachability-preserving algorithms ( Mitchell, 2003 ;  Eysenbach et al., 2017 ), state-level surrogate constraint satisfaction algorithms ( Dalal et al., 2018 ), risk-sensitive algorithms ( Tamar et al., 2013 ;  Chow et al., 2015 ) and apprenticeship learning (Abbeel & Ng, 2004). There also exists model-based Bayesian approaches that are focused on imposing the constraints via the dynamics (such as classifying parts of state space as unsafe) and then using model predictive control to incorporate the constraints in the policy optimization and planning (Turchetta et al., 2016;  Berkenkamp et al., 2017 ;  Wachi et al., 2018 ;  Koller et al., 2018 ). A natural way to model safety is via constraint satisfaction. A standard formulation for adding constraints to RL problems is the Constrained Markov Decision Process (CMDP) framework ( Altman, 1999 ), wherein the environment is extended to also provide feedback on constraint costs. The agent must then attempt to maximize its expected return while also satisfying cumulative constraints. A few algorithms have been proposed to solve CMDPs for high-dimensional domains with continuous action spaces - however they come with their own caveats. Reward Constrained Policy Optimization ( Tessler et al., 2018 ) and Primal Dual Policy Optimization ( Chow et al., 2015 ) do not guarantee constraint satisfaction during the learning procedure, only on the final policy. Constrained Policy Optimization ( Achiam et al., 2017 ) provides monotonic policy improvement but is computationally expensive due to requiring a backtracking line-search procedure and conjugate gradient algorithm for approximating the Fisher Information Matrix. Lyapunov-based Safe Policy Optimization (Chow Under review as a conference paper at  ICLR 2020 et al., 2019 ) requires solving a Linear Program (LP) at every step of policy evaluation, although they show that there exists heuristics which can be substituted for the LP at the expense of theoretical guarantees. In this work, we propose an alternate formulation for solving CMDPs that transforms trajectory-level constraints into localized state-dependent constraints, through which a safe policy improvement step can be defined. In our approach, we define a notion of Backward Value Functions, which act as an estimator of the expected cost collected by the agent so far and can be learned via standard RL bootstrap techniques. We provide conditions under which this new formulation is able to solve CMDPs without violating the constraints during the learning process. Our formulation allows us to define state-level constraints without explicitly solving a LP or the Dual problem at every iteration. Our method is implemented as a reduction to any model-free on-policy bootstrap based RL algorithm, both for deterministic and stochastic policies, and discrete and continuous action spaces. We provide the empirical evidence of our approach with Deep RL methods on various safety benchmarks, including 2D navigation grid worlds ( Leike et al., 2017 ;  Chow et al., 2018 ), and MuJoCo tasks ( Achiam et al., 2017 ;  Chow et al., 2019 ).",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper examines whether or not standard neural networks demonstrate the mutual exclusivity bias, and whether or not a maximally efficient learner should use mutual exclusivity. We analyze common benchmarks in machine translation and object recognition to determine if the mutual exclusivity bias is a useful assumption in lifelong learning tasks. Our results suggest that the mutual exclusivity bias is a useful inductive bias for machine learning, and can help bridge the gap between human-like sample efficiency and flexibility.",
        "Abstract": "Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not standard neural architectures have a ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classification and translation. We demonstrate that there is a compelling case for designing neural networks that reason by mutual exclusivity, which remains an open challenge.",
        "Introduction": "  INTRODUCTION Children are remarkable learners, and thus their inductive biases should interest machine learning re- searchers. To help learn the meaning of new words efficiently, children use the \"mutual exclusivity\" (ME) bias - the assumption that once an object has one name, it does not need another ( Markman & Wachtel, 1988 ) ( Figure 1 ). In this paper, we examine whether or not standard neural networks demonstrate the mutual exclusivity bias, either as a built-in assumption or as a bias that develops through training. Moreover, we examine common benchmarks in machine translation and object recognition to determine whether or not a maximally efficient learner should use mutual exclusivity. When children endeavour to learn a new word, they rely on inductive bi- ases to narrow the space of possible meanings. Children learn an average of about 10 new words per day from the age of one until the end of high school ( Bloom, 2000 ), a feat that requires managing a tractable set of can- didate meanings. A typical word learning scenario has many sources of ambiguity and uncertainty, including ambiguity in the mapping between words and referents. Children hear multiple words and see multiple ob- jects within a single scene, often without clear supervisory signals to indi- cate which word goes with which object ( Smith & Yu, 2008 ). The mutual exclusivity assumption helps to resolve ambiguity in how words maps to their referents.  Markman & Wachtel (1988)  examined scenarios like  Figure 1  that required children to determine the referent of a novel word. For instance, children who know the meaning of \"cup\" are presented with two objects, one which is familiar (a cup) and another which is novel (an unusual object). Given these two objects, children are asked to \"Show me a dax,\" where \"dax\" is a novel nonsense word. Mark- man and Wachtel found that children tend to pick the novel object rather than the familiar object. Although it is possible that the word \"dax\" could be another word for referring to cups, children predict that the novel word refers to the novel object - demonstrating a \"mutual exclusivity\" bias that familiar objects do not need another name. This is only a preference; with enough evidence, children must eventually override this bias to learn hierarchical categories: a Dalmatian can be called a \"Dalmatian,\" a \"dog\", or a \"mammal\" ( Markman & Wachtel, 1988 ;  Markman, 1989 ). As an often useful but sometimes misleading cue, the ME bias guides children when learning the words of their native language. It is instructive to compare word learning in children and machines, since word learning is also a widely studied problem in machine learning and artificial intelligence. There has been substantial (a) (b) recent progress in object recognition, much of which is attributed to the success of deep neural networks and the availability of very large datasets ( LeCun et al., 2015 ). But when only one or a few examples of a novel word are available, deep learning algorithms lack human-like sample efficiency and flexibility ( Lake et al., 2017 ). Insights from cognitive science and cognitive development can help bridge this gap, and ME has been suggested as a psychologically-informed assumption relevant to machine learning ( Lake et al., 2019 ). In this paper, we examine standard neural networks to understand if they have an ME bias. Moreover, we analyze whether or not ME is a good assumption in lifelong variants of common translation and object recognition tasks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper proposes a novel objective for training hybrid VAE-GAN frameworks which relaxes the constraints on the encoder by giving it multiple chances to draw samples with high likelihood. This enables the encoder to generate realistic images while covering all modes of the data distribution. The proposed objective directly estimates the synthetic likelihood term with a controlled Lipschitz constant for stability. Experiments on highly multi-modal synthetic data, CIFAR-10 and CelebA demonstrate significant improvement over prior hybrid VAE-GANs and plain GANs.",
        "Abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.",
        "Introduction": "  INTRODUCTION Generative Adversarial Networks (GANs) ( Goodfellow et al., 2014 ) have achieved state-of-the-art sample quality in generative modeling tasks. However, GANs do not explicitly estimate the data likelihood. Instead, it aims to \"fool\" an adversary, so that the adversary is unable to distinguish between samples from the true distribution and the generated samples. This leads to the generation of high quality samples ( Adler & Lunz, 2018 ;  Brock et al., 2019 ). However, there is no incentive to cover the whole data distribution. Entire modes of the true data distribution can be missed - commonly referred to as the mode collapse problem. In contrast, the Variational Auto-Encoders (VAEs) ( Kingma & Welling, 2014 ) explicitly maximize data likelihood and can be forced to cover all modes ( Bozkurt et al., 2018 ;  Shu et al., 2018 ). VAEs enable sampling by constraining the latent space to a unit Gaussian and sampling through the latent space. However, VAEs maximize a data likelihood estimate based on the L 1 /L 2 reconstruction cost which leads to lower overall sample quality - blurriness in case of image distributions. Therefore, there has been a spur of recent work ( Donahue et al., 2017 ;  Larsen et al., 2016 ;  Rosca et al., 2019 ) which aims integrate GANs in a VAE framework to improve VAE generation quality while covering all the modes. Notably in  Rosca et al. (2019) , GANs are integrated in a VAE framework by augmenting the L 1 /L 2 data likelihood term in the VAE objective with a GAN discriminator based synthetic likelihood ratio term. However,  Rosca et al. (2019)  reports that in case of hybrid VAE-GANs, the latent space does not usually match the Gaussian prior. This is because, the reconstruction log-likelihood in the VAE objective is at odds with the divergence to the latent prior ( Tabor et al., 2018 ) (also in case of alternatives proposed by  Makhzani et al. (2016) ;  Arjovsky et al. (2017) ). This problem is further exacerbated with the addition of the synthetic likelihood term in the hybrid VAE-GAN objective - it is necessary for sample quality but it introduces additional constraints on the encoder/decoder. This leads to the degradation in the quality and diversity of samples. Moreover, the synthetic likelihood ratio term is unstable during training - as it is the ratio of outputs of a classifier, any instability in the output of the classifier is magnified. We directly estimate the ratio using a network with a controlled Lipschitz constant, which leads to significantly improved stability. Our contributions Under review as a conference paper at ICLR 2020 in detail are, 1. We propose a novel objective for training hybrid VAE-GAN frameworks, which relaxes the constraints on the encoder by giving the encoder multiple chances to draw samples with high likelihood enabling it to generate realistic images while covering all modes of the data distribution, 2. Our novel objective directly estimates the synthetic likelihood term with a controlled Lipschitz constant for stability, 3. Finally, we demonstrate significant improvement over prior hybrid VAE-GANs and plain GANs on highly muti-modal synthetic data, CIFAR-10 and CelebA.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper introduces Salient Attributes for Network Explanation (SANE), a black box method for explaining the behavior of image similarity models. SANE combines a saliency map generator, an attribute predictor, and a prior on the suitability of each attribute as an explanation. The method produces a saliency map to explain a model's similarity score, paired with an attribute explanation that identifies important image properties. Experiments show that SANE produces more informative explanations than adaptations of prior work to this task and also improves attribute recognition performance.",
        "Abstract": "Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering.  In this paper, we introduce an explanation approach for image similarity models, where a model's output is a score measuring the similarity of two inputs rather than a classification.  In this task, an explanation depends on both of the input images, so standard methods do not apply. We propose an explanation method that pairs a saliency map identifying important image regions with an attribute that best explains the match.  We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach's ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2.",
        "Introduction": "  INTRODUCTION Many problems in artificial intelligence that require reasoning about complex relationships can be solved by learning some feature embedding to measure similarity between images and/or other modalities such as text. Examples of these tasks include scoring fashion compatibility ( Han et al., 2017b ;  Hsiao & Grauman, 2018 ;  Vasileva et al., 2018 ), image retrieval ( Kiapour et al., 2015 ;  Rade- novi et al., 2018 ;  Yelamarthi et al., 2018 ), or zero-shot recognition ( Bansal et al., 2018 ;  Li et al., 2018b ;  Wang et al., 2018 ). Reasoning about the behavior of similarity models can aid researchers in identifying potential improvements, show where two images differ for anomaly detection, pro- mote diversity in fashion recommendation by ensuring different traits are most prominent in the top results, or simply help users understand the model's predictions which can build trust ( Teach & Shortliffe, 1981 ). However, prior work on producing explanations for neural networks has primarily focused on explaining classification models (e.g. ( Fong & Vedaldi, 2017 ;  Nguyen et al., 2016 ;  Pet- siuk et al., 2018 ;  Ribeiro et al., 2016 ;  Selvaraju et al., 2017 ;  Zeiler & Fergus, 2014 )) and does not directly apply to similarity models. Given a single input image, such methods produce a saliency map which identifies pixels that played a significant role towards a particular class prediction (see Figure 1a for an example). On the other hand, a similarity model requires at least two images to produce a score. The interaction between both images defines which features are more important, so replacing just one of the images can result in identifying different salient traits. Another limitation of existing work is that the saliency alone may be insufficient as an explanation of (dis)similarity. For image pairs where similarity is determined by the presence or absence of an object, a saliency map may be enough to understand model behavior. However, when we consider the image pair in Figure 1b, highlighting the necklace as the region that contributes most to the similarity score is reasonable, but uninformative given that there are no other objects in the image. Instead, what is important is the fact that the necklace shares a similar color with the ring. Whether these attributes or salient parts are a better fit as an explanation is not determined by the image domain (i.e. attributes for e-commerce imagery vs. saliency for natural imagery), but instead by the images themselves. For example, an image can be matched as formal-wear because of a shirt's collar (salient part), while two images of animals can match because both have stripes (attribute). Guided by this intuition, we introduce Salient Attributes for Network Explanation (SANE). Our approach generates a saliency map to explain a model's similarity score, paired with an attribute explanation that identifies important image properties. SANE is a \"black box\" method, meaning it Under review as a conference paper at ICLR 2020 How is the necklace similar to the ring? can explain any network architecture and only needs to measure changes to a similarity score when provided with different inputs. Unlike a standard classifier, which simply predicts the most likely attributes for a given image, our explanation method predicts which attributes are important for the similarity score predicted by a model. Predictions are made for each image in a pair, and allowed to be non-symmetric, e.g., the explanation for why the ring in Figure 1b matches the necklace may be that it contains \"black\", even though the explanation for why the necklace matches the ring could be that it is \"golden.\" A different similarity model may also result in different attributes being deemed important for the same pair of images. SANE combines three major components: an attribute predictor, a prior on the suitability of each attribute as an explanation, and a saliency map generator. Our underlying assumption is that at least one of the attributes present in each image should be able to explain the similarity score assigned to the pair. Given an input image, the attribute predictor outputs a confidence score and activation map for each attribute, while the saliency map generator produces regions important for the match. During training, SANE encourages overlap between the similarity saliency and attribute activation. At test time, we rank attributes as explanations for an image pair based on a weighted sum of this attribute-saliency map matching score, the explanation suitability prior of the attribute, and the likelihood that the attribute is present in the image. Although we evaluate only the top-ranked attribute in our experiments, in practice more than one attribute could be used to explain a similarity score. We find that using saliency maps as supervision for the attribute activation maps during training not only improves the attribute-saliency matching, resulting in better attribute explanations, but also boosts attribute recognition performance using standard metrics like average precision. We evaluate several candidate saliency map generation methods which are primarily adaptations of \"black box\" approaches that do not rely on a particular model architecture or require access to network parameters to produce a saliency map ( Fong & Vedaldi, 2017 ;  Petsiuk et al., 2018 ;  Ribeiro et al., 2016 ;  Zeiler & Fergus, 2014 ). These methods generally identify important regions by mea- suring a change in the output class score resulting from some perturbation of the input image. Sim- ilarity models, however, typically rely on a learned embedding space to reason about relationships between images, where proximity between points or the lack thereof indicates some degree of cor- respondence. An explanation system for embedding models must, therefore, consider how distances between embedded points, and thus their similarity, change based on perturbing one or both of the input images. We explore two strategies for adapting these approaches to our task. First, we manip- ulate just a single image (the one we wish to produce an explanation for) while keeping the other image fixed. Second, we manipulate both images to allow for more complex interactions between the pair. See Section 3.2 for additional details and discussion on the ramifications of this choice. Our paper makes the following contributions: 1) we provide the the first quantitative study of ex- plaining the behavior of image similarity models; 2) we propose a novel explanation approach that combines saliency maps and attributes; 3) we validate our method with metrics designed to link our explanations to model performance, and find that it produces more informative explanations than adaptations of prior work to this task and also improves attribute recognition performance.",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper introduces the use of resource-efficient MobileNets architecture for the compression of convolutional neural networks (CNNs). Model quantization is a popular technique to facilitate this compression, and quantizing the weights of MobileNets to binary (-1,1) or ternary (-1,0,1) values has the potential to achieve significant improvement in energy savings and overall throughput, especially on custom hardware. This is attributed to the replacement of multiplications by additions in binary- and ternary-weight networks.",
        "Abstract": "MobileNets family of computer vision neural networks have fueled tremendous progress in the design and organization of resource-efficient architectures in recent years. New applications with stringent real-time requirements in highly constrained devices require further compression of MobileNets-like already computeefficient networks. Model quantization is a widely used technique to compress and accelerate neural network inference and prior works have quantized MobileNets to 4 − 6 bits albeit with a modest to significant drop in accuracy. While quantization to sub-byte values (i.e. precision ≤ 8 bits) has been valuable, even further quantization of MobileNets to binary or ternary values is necessary to realize significant energy savings and possibly runtime speedups on specialized hardware, such as ASICs and FPGAs. Under the key observation that convolutional filters at each layer of a deep neural network may respond differently to ternary quantization, we propose a novel quantization method that generates per-layer hybrid filter banks consisting of full-precision and ternary weight filters for MobileNets. The layer-wise hybrid filter banks essentially combine the strengths of full-precision and ternary weight filters to derive a compact, energy-efficient architecture for MobileNets. Using this proposed quantization method, we quantized a substantial portion of weight filters of MobileNets to ternary values resulting in 27.98% savings in energy, and a 51.07% reduction in the model size, while achieving comparable accuracy and no degradation in throughput on specialized hardware in comparison to the baseline full-precision MobileNets.",
        "Introduction": "  INTRODUCTION Deeper and wider convolutional neural networks (CNNs) has led to outstanding predictive perfor- mance in many machine learning tasks, such as image classification (He et al. (2016); Krizhevsky et al. (2012)), object detection (Redmon et al. (2016); Ren et al. (2015)), and semantic segmen- tation (Chen et al. (2018); Long et al. (2015)). However, the large model size and corresponding computational inefficiency of these networks often make it infeasible to run many real-time ma- chine learning applications on resource-constrained mobile and embedded hardware, such as smart- phones, AR/VR devices etc. To enable this computation and size compression of CNN models, one particularly effective approach has been the use of resource-efficient MobileNets architecture. MobileNets introduces depthwise-separable (DS) convolution as an efficient alternative to the stan- dard 3-D convolution operation.While MobileNets architecture has been transformative, even further compression of MobileNets is valuable in order to make a wider range of applications available on constrained platforms (Gope et al. (2019)). Model quantization has been a popular technique to facilitate that. Quantizing the weights of Mo- bileNets to binary (-1,1) or ternary (-1,0,1) values in particular has the potential to achieve significant improvement in energy savings and possibly overall throughput especially on custom hardware, such as ASICs and FPGAs while reducing the resultant model size considerably. This is attributed to the replacement of multiplications by additions in binary- and ternary-weight networks. Multipliers oc- cupy considerably more area on chip than adders (Li & Liu (2016)), and consume significantly more energy than addition operations (Horowitz (2014); Andri et al. (2018)). A specialized hardware can therefore trade off multiplications against additions and potentially accommodate considerably more adders than multipliers to achieve a high throughput and significant savings in energy for binary- and ternary-weight networks.",
        "label": 0
    },
    {
        "Summary": "\n\nThis paper explores the temporal discrepancy between the overall and task optimal epochs when using a validation metric that is averaged over all tasks for early stopping in multi-task/label settings. We present a proposed method for early stopping that incorporates task difficulty into training and discuss our experiments and results. We conclude that early stopping on only the expected error over tasks leaves us blind to the performance they are sacrificing per task.",
        "Abstract": "The validation curve is widely used for model selection and hyper-parameter search with the curve usually summarized over all the training tasks. However, this summarization tends to lose the intricacies of the per-task curves and it isn't able to reflect if all the tasks are at their validation optimum even if the summarized curve might be. In this work, we explore this loss of information, how it affects the model at testing and how to detect it using interval plots. We propose two techniques as a proof-of-concept of the potential gain in the test performance when per-task validation curves are accounted for. Our experiments on three large datasets show up to a 2.5% increase (averaged over multiple trials) in the test accuracy rate when model selection uses the per-task validation maximums instead of the summarized validation maximum. This potential increase is not a result of any modification to the model but rather at what point of training the weights were selected from. This presents an exciting direction for new training and model selection techniques that rely on more than just averaged metrics. ",
        "Introduction": "  INTRODUCTION A validation set, separate from the test set, is the de facto standard for training deep learning mod- els through early stopping. This non-convergent approach ( Finnoff et al., 1993 ) identifies the best model in multi-task/label settings based on an expected error across all tasks. Calculating metrics on the validation set can estimate the model's generalization capability at every stage of training and monitoring the summarized validation curve over time aids the detection of overfitting. It is common to see the use of validation metrics as a way to stop training and/or load the best model for testing, as opposed to training a model to N epochs and then testing. While current works have always cautioned about the representativeness of validation data being used, the curves themselves haven't been addressed much. In particular, there hasn't been much attention on the summarized nature of the curves and their ability to represent the generalization of the constituent tasks. Tasks can vary in difficulty and even have a dependence on each other (Graves, 2016;  Alain & Bengio, 2016 ). An example by  Lee et al. (2016)  is to suppose some task a is to predict whether a visual instance 'has wheels' or not, and task b is to predict if a given visual object 'is fast'; not only is one easier, but there is also a dependence between them. So there is a possibility that easier tasks reach their best validation metric before the rest and may start overfitting if training were to be continued. This isn't reflected very clearly with the use of a validation metric that is averaged over all tasks. As a larger number of underfit tasks would skew the average, the overall optimal validation point gets shifted to a later time-step (epoch) when the model could be worse at the easier tasks. Vice versa, the optimal epoch gets shifted earlier due to a larger, easier subset that are overfit when the harder tasks reach their individual optimal epochs. We term this mismatch in the overall and task optimal epochs as a 'temporal discrepancy'. In this work, we explore and try to mitigate this discrepancy between tasks. We present in this paper that early stopping on only the expected error over tasks leaves us blind to the performance they are sacrificing per task. The work is organized in the following manner: in §2, we explore existing work that deals with methods for incorporating task difficulty (which could be causing this discrepancy) into training. The rest of the sections along with our contributions can be summarized as:",
        "label": 0
    },
    {
        "Summary": "\n\nAbstract: This paper discusses the open problem of training recurrent neural networks (RNNs) in practice. It examines the issues of learning long-term dependencies, truncated backpropagation through time (TBPTT) memory requirements, and the parameter update lock. It proposes hierarchical RNNs (HRNNs) as a potential solution to the vanishing/exploding gradient problem, which splits the network into a hierarchy of levels that are updated at decreasing frequencies. This reduces the gradient paths and alleviates the vanishing/exploding gradients issue.",
        "Abstract": "Learning long-term dependencies is a key long-standing challenge of recurrent neural networks (RNNs). Hierarchical recurrent neural networks (HRNNs) have been considered a promising approach as long-term dependencies are resolved through shortcuts up and down the hierarchy. Yet, the memory requirements of Truncated Backpropagation Through Time (TBPTT) still prevent training them on very long sequences. In this paper, we empirically show that in (deep) HRNNs, propagating gradients back from higher to lower levels can be replaced by locally computable losses, without harming the learning capability of the network, over a wide range of tasks. This decoupling by local losses reduces the memory requirements of training by a factor exponential in the depth of the hierarchy in comparison to standard TBPTT.",
        "Introduction": "  INTRODUCTION Recurrent neural networks (RNNs) model sequential data by observing one sequence element at a time and updating their internal (hidden) state towards being useful for making future predictions. RNNs are theoretically appealing due to their Turing-completeness  Siegelmann and Sontag (1995) , and, crucially, have been tremendously successful in complex real-world tasks, including machine translation  Cho et al. (2014) ;  Sutskever et al. (2014) , language modelling  Mikolov et al. (2010) , and reinforcement learning  Mnih et al. (2016) . Still, training RNNs in practice is one of the main open problems in deep learning, as the following issues prevail. (1) Learning long-term dependencies is extremely difficult because it requires that the gradients (i.e. the error signal) have to be propagated over many steps, which easily causes them to vanish or explode  Hochreiter (1991) ;  Bengio et al. (1994) ;  Hochreiter (1998) . (2) Truncated Backpropagation Through Time (TBPTT)  Williams and Peng (1990) , the standard training algorithm for RNNs, requires memory that grows linearly in the length of the sequences on which the network is trained. This is because all past hidden states must be stored. Therefore, the memory requirements of training RNNs with large hidden states on long sequences become prohibitively large. (3) In TBPTT, parameters cannot be updated until the full forward and backward passes have been completed. This phenomenon is known as the parameter update lock  Jaderberg et al. (2017) . As a consequence, the frequency at which parameters can be updated is inversely proportional to the length of the time-dependencies that can be learned, which makes learning exceedingly slow for long sequences. The problem of vanishing/exploding gradients has been alleviated by a plethora of approaches ranging from specific RNN architectures  Hochreiter and Schmidhuber (1997) ;  Cho et al. (2014)  to optimization techniques aiming at easing gradient flow  Martens and Sutskever (2011) ;  Pascanu et al. (2013) . A candidate for effectively resolving the vanishing/exploding gradient problem is hierarchical RNNs (HRNNs)  Schmidhuber (1992) ;  El Hihi and Bengio (1996) ;  Koutnik et al. (2014) ;  Sordoni et al. (2015) ;  Chung et al. (2016) . In HRNNs, the network itself is split into a hierarchy of levels, which are updated at decreasing frequencies. As higher levels of the hierarchy are updated less frequently, these architectures have short (potentially logarithmic) gradient paths that greatly reduce the vanishing/exploding gradients issue.",
        "label": 0
    }
]